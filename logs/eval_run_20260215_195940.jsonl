{"query_id": "Q01", "query_text": "What does RAGAS measure for faithfulness?", "query_type": "direct", "retrieved_chunk_ids": ["RAG_Eval_Survey2024_chunk_02", "RAGAS2023_chunk_09", "ARES2023_chunk_09", "RAG_Eval_Survey2024_chunk_04", "ARES2023_chunk_06"], "model_output": "RAGAS measures faithfulness, which is the extent to which the generated answer is faithful to the retrieved context (RAGAS2023, RAGAS2023_chunk_09). Faithfulness is one of the three quality aspects considered by the Ragas framework, along with context relevance and answer relevance (RAGAS2023, RAGAS2023_chunk_09). The framework is designed to provide valuable insights for developers of RAG systems, even in the absence of ground truth (RAGAS2023, RAGAS2023_chunk_09).\n\n## References\n\n- **ARES2023*..."}
{"query_id": "Q02", "query_text": "What does BERTScore measure and what are its known failure modes?", "query_type": "direct", "retrieved_chunk_ids": ["BERTScore2019_chunk_27", "FactCC2020_chunk_15", "G-Eval2023_chunk_05", "BERTScore2019_chunk_25", "FactCC2020_chunk_26"], "model_output": "BERTScore measures the similarity between two pieces of text, combining contextual embeddings and a greedy approach to aggregate token-level information (BERTScore2019, BERTScore2019_chunk_25). It is used for text generation evaluation, such as summarization and machine translation.\n\nThe known failure modes of BERTScore include its weakness in correlating with faithful and factual human scores, particularly in cases of hallucination (FactCC2020, FactCC2020_chunk_15). BERTScore has been found to ..."}
{"query_id": "Q03", "query_text": "What is the ARES framework and how does it evaluate RAG systems?", "query_type": "direct", "retrieved_chunk_ids": ["RAG_Eval_Survey2024_chunk_04", "RAG_Eval_Survey2024_chunk_02", "ARES2023_chunk_10", "ARES2023_chunk_06", "ARES2023_chunk_24"], "model_output": "The ARES framework is a system for evaluating RAG (Retrieval-Augmented Generation) systems (ARES2023, ARES2023_chunk_10). It uses prediction-powered inference (PPI) to predict the system scores, which provides tighter confidence intervals on a small set of annotated datapoints (ARES2023, ARES2023_chunk_06). ARES evaluates RAG systems by calculating their performance across three metrics, and it uses a fine-tuned LLM judge to predict the scores (ARES2023, ARES2023_chunk_10). The framework has bee..."}
{"query_id": "Q04", "query_text": "What is the FactCC benchmark and how does it assess factual consistency?", "query_type": "direct", "retrieved_chunk_ids": ["QASemConsistency2024_chunk_12", "FIB2022_chunk_00", "RAG_Eval_Survey2024_chunk_13", "FIB2022_chunk_03", "RAG_Survey_LLM2025_chunk_19"], "model_output": "The FactCC benchmark is not explicitly mentioned in the provided passages. However, there are mentions of other benchmarks such as FIB (Factual Inconsistency Benchmark) (FIB2022, FIB2022_chunk_00) and various RAG benchmarks (RAG_Survey_LLM2025, RAG_Survey_LLM2025_chunk_19), but none of them are referred to as FactCC.\n\nThe corpus does contain information on how some benchmarks assess factual consistency. For example, the FIB benchmark evaluates the factual consistency of large language models by ..."}
{"query_id": "Q05", "query_text": "What does SummaC use to detect inconsistencies in summarization?", "query_type": "direct", "retrieved_chunk_ids": ["SummaC2021_chunk_03", "SummaC2021_chunk_00", "SummaC2021_chunk_19", "SummaC2021_chunk_09", "FIB2022_chunk_02"], "model_output": "SummaC uses Natural Language Inference (NLI) models to detect inconsistencies in summarization (SummaC2021, SummaC2021_chunk_00). Specifically, it uses a method called SUMMA CConv, which enables NLI models to be successfully used for inconsistency detection by segmenting documents into sentence units and aggregating scores between pairs of sentences (SummaC2021, SummaC2021_chunk_00). This approach achieves state-of-the-art results with a balanced accuracy of 74.4% (SummaC2021, SummaC2021_chunk_0..."}
{"query_id": "Q06", "query_text": "What is the FaithEval benchmark and what contexts does it test?", "query_type": "direct", "retrieved_chunk_ids": ["RAG_Eval_Survey2024_chunk_04", "FaithEval2024_chunk_02", "RAG_Survey_LLM2025_chunk_18", "FaithEval2024_chunk_00", "FaithEval2024_chunk_37"], "model_output": "The FaithEval benchmark is a novel and comprehensive benchmark dedicated to evaluating the faithfulness of large language models (LLMs) in contextual scenarios (FaithEval2024, FaithEval2024_chunk_02). It tests the faithfulness of LLMs across three diverse tasks: unanswerable, inconsistent, and counterfactual contexts, which simulate real-world challenges where retrieval mechanisms may surface incomplete, contradictory, or fabricated information (FaithEval2024, FaithEval2024_chunk_02). The benchm..."}
{"query_id": "Q07", "query_text": "What is the TRUE benchmark and what does it evaluate?", "query_type": "direct", "retrieved_chunk_ids": ["RAG_Survey_LLM2025_chunk_18", "RAG_Eval_Survey2024_chunk_08", "RAG_Eval_Survey2024_chunk_06", "BERTScore2019_chunk_56", "RAG_Eval_Survey2024_chunk_04"], "model_output": "The corpus does not contain evidence for this. \n\nThere is no mention of the TRUE benchmark in any of the provided passages (RAG_Survey_LLM2025, RAG_Eval_Survey2024, BERTScore2019). \n\nTherefore, it is not possible to provide information on what the TRUE benchmark evaluates.\n\n## References\n\n- **BERTScore2019**: BERTScore: Evaluating Text Generation with BERT. Tian et al. (2019). ICLR 2020. https://arxiv.org/abs/1904.09675\n- **RAG_Eval_Survey2024**: Evaluation of Retrieval-Augmented Generation: A S..."}
{"query_id": "Q08", "query_text": "How does NLI-based faithfulness evaluation work?", "query_type": "direct", "retrieved_chunk_ids": ["Hallucination_Survey2023_chunk_64", "Hallucination_Survey2023_chunk_25", "FaithEval2024_chunk_00", "G-Eval2023_chunk_12", "FaithEval2024_chunk_15"], "model_output": "NLI-based faithfulness evaluation works by using natural language inference (NLI) to assess the faithfulness of model-generated content (Hallucination_Survey2023, Hallucination_Survey2023_chunk_64). This approach involves providing a model with a hypothesis and a premise, and then evaluating whether the hypothesis is entailed, contradicted, or neutral with respect to the premise (Hallucination_Survey2023, Hallucination_Survey2023_chunk_25). However, the exact mechanism of NLI-based faithfulness ..."}
{"query_id": "Q09", "query_text": "What are the three quality dimensions RAGAS evaluates?", "query_type": "direct", "retrieved_chunk_ids": ["RAG_Eval_Survey2024_chunk_02", "RAG_Survey_LLM2025_chunk_23", "RAG_Eval_Survey2024_chunk_04", "RAG_Eval_Survey2024_chunk_12", "ARES2023_chunk_09"], "model_output": "According to the passages, the three quality dimensions that RAGAS evaluates are not explicitly mentioned. However, the dimensions of RAG evaluation are discussed in several passages. \n\nIn (RAG_Eval_Survey2024, RAG_Eval_Survey2024_chunk_02), it is mentioned that evaluating hybrid RAG systems entails evaluating retrieval, generation, and the RAG system as a whole. \n\nIn (RAG_Survey_LLM2025, RAG_Survey_LLM2025_chunk_23), four key areas of RAG studies are mentioned: retrieval, generation, safety, an..."}
{"query_id": "Q10", "query_text": "What does context relevance mean in RAG evaluation?", "query_type": "direct", "retrieved_chunk_ids": ["RAG_Survey_LLM2025_chunk_06", "RAGAS2023_chunk_04", "RAGAS2023_chunk_09", "RAG_Eval_Survey2024_chunk_02", "RAG_Eval_Survey2024_chunk_04"], "model_output": "Context relevance in RAG evaluation refers to the idea that the retrieved context should be focused, containing as little irrelevant information as possible (RAGAS2023, RAGAS2023_chunk_04). This is important given the cost associated with feeding long context passages to LLMs, and when context passages are too long, LLMs are often less effective in exploiting that context (RAGAS2023, RAGAS2023_chunk_04). Additionally, relevance can be evaluated by assessing how well the retrieved documents match..."}
{"query_id": "Q11", "query_text": "Compare RAGAS vs ARES on how they measure faithfulness", "query_type": "synthesis", "retrieved_chunk_ids": ["ARES2023_chunk_09", "ARES2023_chunk_11", "ARES2023_chunk_10", "ARES2023_chunk_24", "ARES2023_chunk_06"], "model_output": "To compare RAGAS vs ARES on how they measure faithfulness, we can look at the evaluation metrics and methods used by each system. ARES uses a fine-tuned LLM judge to evaluate RAG systems, and it has been shown to be more accurate than RAGAS in measuring context relevance and answer relevance (ARES2023, ARES2023_chunk_10). Specifically, ARES averages a Kendall's Ï„ 0.065 higher for context relevance and 0.132 higher for answer relevance than RAGAS (ARES2023, ARES2023_chunk_10).\n\nARES also uses pre..."}
{"query_id": "Q12", "query_text": "Compare BERTScore vs SummaC for faithfulness evaluation", "query_type": "synthesis", "retrieved_chunk_ids": ["FactCC2020_chunk_26", "BERTScore2019_chunk_26", "FactCC2020_chunk_15", "BERTScore2019_chunk_23", "G-Eval2023_chunk_08"], "model_output": "To compare BERTScore vs SummaC for faithfulness evaluation, we must first identify what each of these terms refers to in the context of the provided passages. BERTScore is mentioned in several passages as a metric for evaluating the quality of text generation, including summarization (BERTScore2019, BERTScore2019_chunk_23), (BERTScore2019, BERTScore2019_chunk_26). However, SummaC is not explicitly mentioned in the provided passages.\n\nGiven this, we cannot directly compare BERTScore and SummaC ba..."}
{"query_id": "Q13", "query_text": "Compare NLI-based vs QA-based approaches to factual consistency", "query_type": "synthesis", "retrieved_chunk_ids": ["QASemConsistency2024_chunk_02", "QASemConsistency2024_chunk_12", "QASemConsistency2024_chunk_21", "SummaC2021_chunk_05", "QASemConsistency2024_chunk_03"], "model_output": "To compare NLI-based vs QA-based approaches to factual consistency, we can examine the provided passages. NLI-based approaches, such as those described in (SummaC2021, SummaC2021_chunk_05), generate an NLI Pair Matrix by splitting a (document, summary) pair into sentence blocks and running each combination through an NLI model. On the other hand, QA-based approaches, like QASEMCONSISTENCY (QASemConsistency2024, QASemConsistency2024_chunk_02), assess each individual predicate-argument level state..."}
{"query_id": "Q14", "query_text": "Where do RAGAS and the RAG evaluation survey agree on RAG evaluation dimensions?", "query_type": "synthesis", "retrieved_chunk_ids": ["RAG_Survey_LLM2025_chunk_23", "RAG_Eval_Survey2024_chunk_02", "RAG_Eval_Survey2024_chunk_04", "RAG_Survey_LLM2025_chunk_06", "ARES2023_chunk_06"], "model_output": "RAGAS and the RAG evaluation survey agree on RAG evaluation dimensions, specifically in the areas of retrieval, generation, safety, and efficiency (RAG_Survey_LLM2025, RAG_Survey_LLM2025_chunk_23). Both also recognize the importance of evaluating the retrieval component, including aspects such as relevance and comprehensiveness (RAG_Eval_Survey2024, RAG_Eval_Survey2024_chunk_04), (RAG_Survey_LLM2025, RAG_Survey_LLM2025_chunk_06). Additionally, they both acknowledge the need for a comprehensive e..."}
{"query_id": "Q15", "query_text": "Compare FactCC and FIB benchmarks: what do they measure and how do they differ?", "query_type": "synthesis", "retrieved_chunk_ids": ["SummaC2021_chunk_11", "RAG_Eval_Survey2024_chunk_10", "RAG_Survey_LLM2025_chunk_19", "QASemConsistency2024_chunk_12", "RAG_Survey_LLM2025_chunk_08"], "model_output": "The corpus does not contain evidence for a direct comparison between FactCC and FIB benchmarks. However, it is mentioned that the authors of FactCC measured model performance using binary F1 score and balanced accuracy (SummaC2021, SummaC2021_chunk_11). \n\nThere is no information provided in the passages about the FIB benchmark, its metrics, or how it differs from FactCC. Therefore, a comparison between the two cannot be made based on the provided corpus.\n\n## References\n\n- **QASemConsistency2024*..."}
{"query_id": "Q16", "query_text": "Compare REALM vs REPLUG on retrieval-augmented generation design", "query_type": "synthesis", "retrieved_chunk_ids": ["RAG_Survey_LLM2025_chunk_36", "RAG_Survey_LLM2025_chunk_04", "RAGAS2023_chunk_00", "RAG_Eval_Survey2024_chunk_00", "ARES2023_chunk_19"], "model_output": "The corpus does not contain evidence for a direct comparison between REALM and REPLUG on retrieval-augmented generation design. \n\nWhile the passages discuss various aspects of Retrieval-Augmented Generation (RAG) systems, such as their components (RAG_Survey_LLM2025, RAG_Survey_LLM2025_chunk_04), evaluation frameworks (RAGAS2023, RAGAS2023_chunk_00), and challenges in evaluating these systems (RAG_Eval_Survey2024, RAG_Eval_Survey2024_chunk_00), they do not provide a comparison between REALM and ..."}
{"query_id": "Q17", "query_text": "Does the corpus contain evidence that RAG systems reduce hallucinations?", "query_type": "edge", "retrieved_chunk_ids": ["Hallucination_Survey2023_chunk_04", "FactCC2020_chunk_03", "Hallucination_Survey2023_chunk_03", "FactCC2020_chunk_27", "Hallucination_Survey2023_chunk_41"], "model_output": "The corpus contains evidence that RAG systems can reduce hallucinations (Hallucination_Survey2023, Hallucination_Survey2023_chunk_41). However, it also notes that despite being designed to mitigate LLM hallucinations, retrieval-augmented LLMs can still produce hallucinations (Hallucination_Survey2023, Hallucination_Survey2023_chunk_41). This suggests that while RAG systems may have the potential to mitigate hallucinations, they are not entirely effective in eliminating them.\n\n## References\n\n- **..."}
{"query_id": "Q18", "query_text": "Does the corpus support that LLMs hallucinate?", "query_type": "edge", "retrieved_chunk_ids": ["Hallucination_Survey2023_chunk_03", "FactCC2020_chunk_27", "Hallucination_Survey2023_chunk_19", "Hallucination_Survey2023_chunk_55", "Hallucination_Survey2023_chunk_11"], "model_output": "Yes, the corpus supports that LLMs hallucinate. Hallucinations in LLMs are defined as the model generating output that is not based on real-world facts or context (Hallucination_Survey2023, Hallucination_Survey2023_chunk_03). The passages discuss various aspects of hallucinations in LLMs, including their causes, detection methods, and mitigation strategies (Hallucination_Survey2023, Hallucination_Survey2023_chunk_03), (Hallucination_Survey2023, Hallucination_Survey2023_chunk_19). Additionally, t..."}
{"query_id": "Q19", "query_text": "Does the corpus contain evidence for combining multiple faithfulness metrics?", "query_type": "edge", "retrieved_chunk_ids": ["FactCC2020_chunk_15", "Hallucination_Survey2023_chunk_23", "Hallucination_Survey2023_chunk_64", "ARES2023_chunk_09", "FactCC2020_chunk_25"], "model_output": "The corpus does contain evidence for combining multiple faithfulness metrics. For example, (Hallucination_Survey2023, Hallucination_Survey2023_chunk_23) lists various metrics for assessing faithfulness, including overlap of facts, classifier-based metrics, QA-based metrics, uncertainty estimation, and prompting-based metrics. Additionally, (FactCC2020, FactCC2020_chunk_15) presents a table showing the correlation of different metrics with faithful and factual human scores, suggesting that using ..."}
{"query_id": "Q20", "query_text": "Does the corpus discuss GPT-5 or models after 2024 for faithfulness evaluation?", "query_type": "edge", "retrieved_chunk_ids": ["Hallucination_Survey2023_chunk_64", "Hallucination_Survey2023_chunk_25", "FactCC2020_chunk_15", "FaithEval2024_chunk_15", "FactCC2020_chunk_10"], "model_output": "The corpus does discuss models and their evaluation for faithfulness, including various benchmarks and metrics (Hallucination_Survey2023, Hallucination_Survey2023_chunk_64), (Hallucination_Survey2023, Hallucination_Survey2023_chunk_25), (FactCC2020, FactCC2020_chunk_15), (FaithEval2024, FaithEval2024_chunk_15), and (FactCC2020, FactCC2020_chunk_10). However, it does not specifically mention GPT-5. It mentions GPT-4 in the context of early experiments with it (FaithEval2024, FaithEval2024_chunk_1..."}
{"query_id": "Q21", "query_text": "Does the corpus contain evidence that retrieval quality affects generation faithfulness?", "query_type": "edge", "retrieved_chunk_ids": ["RAG_Eval_Survey2024_chunk_03", "Hallucination_Survey2023_chunk_45", "RAG_Eval_Survey2024_chunk_05", "RAG_Survey_LLM2025_chunk_36", "RAGAS2023_chunk_00"], "model_output": "The corpus does contain evidence that retrieval quality affects generation faithfulness. According to (RAG_Eval_Survey2024, RAG_Eval_Survey2024_chunk_03), the challenge in evaluating RAG systems lies in assessing the faithfulness and accuracy of the generated content to the input data, which involves evaluating the factual correctness of responses and their relevance to the original query. Additionally, (Hallucination_Survey2023, Hallucination_Survey2023_chunk_45) suggests that the integration o..."}
{"query_id": "Q22", "query_text": "What failure modes do faithfulness metrics have according to the corpus?", "query_type": "direct", "retrieved_chunk_ids": ["FactCC2020_chunk_15", "Hallucination_Survey2023_chunk_23", "Hallucination_Survey2023_chunk_64", "ARES2023_chunk_09", "RAGAS2023_chunk_08"], "model_output": "Faithfulness metrics have several failure modes. One issue is that question answering scores have a very weak correlation with faithful and factual human scores, which can be due to the compounding of errors in question generation and answering (FactCC2020, FactCC2020_chunk_15). Additionally, traditional n-gram-based metrics, such as BLEU and ROUGE, typically fall short in differentiating between the generated content and the source content, leading to inaccurate faithfulness assessments (Halluc..."}
{"query_id": "Q23", "query_text": "How does G-Eval evaluate faithfulness in NLG?", "query_type": "direct", "retrieved_chunk_ids": ["G-Eval2023_chunk_15", "Hallucination_Survey2023_chunk_64", "Hallucination_Survey2023_chunk_93", "Hallucination_Survey2023_chunk_25", "G-Eval2023_chunk_12"], "model_output": "G-Eval evaluates faithfulness in NLG by using a framework that utilizes large language models (LLMs) with chain-of-thoughts (CoT) to assess the quality of generated texts (G-Eval2023, G-Eval2023_chunk_12). This framework can handle different evaluation tasks by changing the question format, such as evaluating the consistency of generated summaries or the coherence of dialogue responses (G-Eval2023, G-Eval2023_chunk_12). Additionally, LLM-based evaluators, like GPTScore, can evaluate texts by ass..."}
