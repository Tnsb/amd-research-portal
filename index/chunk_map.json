{
  "model_name": "all-MiniLM-L6-v2",
  "dimension": 384,
  "num_chunks": 611,
  "chunks": [
    {
      "index": 0,
      "chunk_id": "RAGAS2023_chunk_00",
      "source_id": "RAGAS2023",
      "text": "Ragas: Automated Evaluation of Retrieval Augmented Generation Shahul Es†, Jithin James†, Luis Espinosa-Anke∗♢, Steven Schockaert∗ †Exploding Gradients ∗CardiffNLP, Cardiff University, United Kingdom ♢AMPLYFI, United Kingdom shahules786@gmail.com,jamesjithin97@gmail.com {espinosa-ankel,schockaerts1}@cardiff.ac.uk Abstract We introduce Ragas (Retrieval Augmented Generation Assessment), a framework for reference-free evaluation of Retrieval Aug- mented Generation (RAG) pipelines. RAG systems are composed of a retrieval and an LLM based generation module, and provide LLMs with knowledge from a reference textual database, which enables them to act as a natu- ral language layer between a user and textual databases, reducing the risk of hallucinations. Evaluating RAG architectures is, however, chal- lenging because there are several dimensions to consider: the ability of the retrieval system to identify relevant and focused context passages, the ability of the LLM to exploit such passages in a faithful way, or the quality of the genera- tion itself. With Ragas, we put forward a suite of metrics which can be used to evaluate these different dimensions without having to rely on ground truth human annotations. We posit that such a framework can crucially contribute to faster evaluation cycles of RAG architectures, which is especially important given the fast adoption of LLMs. 1 Introduction Language Models (LMs) capture a vast amount of knowledge about the world, which allows them to answer questions without accessing any exter- nal sources. This idea of LMs as repositories of knowledge emerged shortly after the introduction of BERT (Devlin et al., 2019) and became more firmly established with the introduction of ever larger LMs (Roberts et al., 2020). While the most recent Large Language Models (LLMs) capture enough knowledge to rival human performance across a wide variety of question answering bench- marks (Bubeck et al., 2023), the idea of using LLMs as knowledge bases still has two fundamen- tal limitations. First, LLMs are not able to answer questions about events that have happened after they were trained. Second, even the largest models struggle to memorise knowledge that is only rarely mentioned in the training corpus (Kandpal et al., 2022; Mallen et al., 2023). The standard solution to these issues is to rely on Retrieval Augmented Generation (RAG) (Lee et al., 2019; Lewis et al., 2020; Guu et al., 2020). Answering a question then essentially involves retrieving relevant pas- sages from a corpus and feeding these passages, along with the original question, to the LM. While initial approaches relied on specialised LMs for retrieval-augmented language",
      "chunk_index": 0
    },
    {
      "index": 1,
      "chunk_id": "RAGAS2023_chunk_01",
      "source_id": "RAGAS2023",
      "text": "2020; Guu et al., 2020). Answering a question then essentially involves retrieving relevant pas- sages from a corpus and feeding these passages, along with the original question, to the LM. While initial approaches relied on specialised LMs for retrieval-augmented language modelling (Khandel- wal et al., 2020; Borgeaud et al., 2022), recent work has suggested that simply adding retrieved docu- ments to the input of a standard LM can also work well (Khattab et al., 2022; Ram et al., 2023; Shi et al., 2023), thus making it possible to use retrieval- augmented strategies in combination with LLMs that are only available through APIs. While the usefulness of retrieval-augmented strategies is clear, their implementation requires a significant amount of tuning, as the overall per- formance will be affected by the retrieval model, the considered corpus, the LM, or the prompt for- mulation, among others. Automated evaluation of retrieval-augmented systems is thus paramount. In practice, RAG systems are often evaluated in terms of the language modelling task itself, i.e. by mea- suring perplexity on some reference corpus. How- ever, such evaluations are not always predictive of downstream performance (Wang et al., 2023c). Moreover, this evaluation strategy relies on the LM probabilities, which are not accessible for some closed models (e.g. ChatGPT and GPT-4). Ques- tion answering is another common evaluation task, but usually only datasets with short extractive an- swers are considered, which may not be represen- tative of how the system will be used. To address these issues, in this paper we present Ragas1, a framework for the automated assessment 1Ragas is available at https://github.com/ explodinggradients/ragas. arXiv:2309.15217v2 [cs.CL] 28 Apr 2025 of retrieval augmented generation systems. We fo- cus on settings where reference answers may not be available, and where we want to estimate different proxies for correctness, in addition to the useful- ness of the retrieved passages. The Ragas frame- work provides an integration with both llama-index and Langchain, the most widely used frameworks for building RAG solutions, thus enabling devel- opers to easily integrate Ragas into their standard workflow. 2 Related Work Estimating faithfulness using LLMsThe prob- lem of detecting hallucinations in LLM generated responses has been extensively studied (Ji et al., 2023). Several authors have suggested the idea of predicting factuality using a few-shot prompt- ing strategy (Zhang et al., 2023). Recent analy- ses, however, suggest that existing models struggle with detecting hallucination when using standard prompting strategies (Li",
      "chunk_index": 1
    },
    {
      "index": 2,
      "chunk_id": "RAGAS2023_chunk_02",
      "source_id": "RAGAS2023",
      "text": "et al., 2023). Several authors have suggested the idea of predicting factuality using a few-shot prompt- ing strategy (Zhang et al., 2023). Recent analy- ses, however, suggest that existing models struggle with detecting hallucination when using standard prompting strategies (Li et al., 2023; Azaria and Mitchell, 2023). Other approaches rely on linking the generated responses to facts from an external knowledge base (Min et al., 2023), but this is not always possible. Yet another strategy is to inspect the probabili- ties assigned to individual tokens, where we would expect the model to be less confident in halluci- nated answers than in factual ones. For instance, BARTScore (Yuan et al., 2021) estimates factuality by looking at the conditional probability of the gen- erated text given the input. Kadavath et al. (2022) use a variation of this idea. Starting from the ob- servation that LLMs provide well-calibrated proba- bilities when answering multiple-choice questions, they essentially convert the problem of validating model generated answers into a multiple-choice question which asks whether the answer is true or false. Rather than looking at the output probabil- ities, Azaria and Mitchell (2023) propose to train a supervised classifier on the weights from one of the hidden layers of the LLM, to predict whether a given statement is true or not. While the approach performs well, the need to access the hidden states of the model makes it unsuitable for systems that access LLMs through an API. For models that do not provide access to token probabilities, such as ChatGPT and GPT-4, differ- ent methods are needed. SelfCheckGPT (Manakul et al., 2023) addresses this problem by instead sam- pling multiple answers. Their core idea is that factual answers are more stable: when an answer is factual, we can expect that different samples will tend to be semantically similar, whereas this is less likely to be the case for hallucinated answers. Automated evaluation of text generation systems LLMs have also been leveraged to automatically evaluate other aspects of generated text fragments, beyond factuality. For instance, GPTScore (Fu et al., 2023) uses a prompt that specifies the consid- ered aspect (e.g. fluency) and then scores passages based on the average probability of the generated tokens, according to a given autoregressive LM. This idea of using prompts was previously also considered by Yuan et al. (2021), although they used a smaller fine-tuned LM (i.e. BART) and did not observe a",
      "chunk_index": 2
    },
    {
      "index": 3,
      "chunk_id": "RAGAS2023_chunk_03",
      "source_id": "RAGAS2023",
      "text": "average probability of the generated tokens, according to a given autoregressive LM. This idea of using prompts was previously also considered by Yuan et al. (2021), although they used a smaller fine-tuned LM (i.e. BART) and did not observe a clear benefit from using prompts. An- other approach directly asks ChatGPT to evaluate a particular aspect of the given answer by provid- ing a score between 0 and 100, or by providing a rating on a 5-star scale (Wang et al., 2023a). Re- markably, strong results can be obtained in this way, although it comes with the limitation of being sensitive to the design of the prompt. Rather than scoring individual answers, some authors have also focused on using an LLM to select the best answer among a number of candidates (Wang et al., 2023b), typically to compare the performance of different LLMs. However, care is needed with this approach, as the order in which the answers is presented can influence the result (Wang et al., 2023b). In terms of how ground truth answers or, more generally, generations, have been typically used in the literature, most approaches have relied on the availability of one or more reference answers. For instance, BERTScore (Zhang et al., 2020) and MoverScore (Zhao et al., 2019) use contex- tualised embeddings, produced by a pre-trained BERT model, to compare the similarity between the generated answer and the reference answers. BARTScore (Yuan et al., 2021) similarly uses refer- ence answers to compute aspects such as precision (estimated as the probability of generating the gen- erated answer given the reference) and recall (esti- mated as the probability of generating the reference given the generated answer). 3 Evaluation Strategies We consider a standard RAG setting, where given a question q, the system first retrieves some context c(q) and then uses the retrieved context to generate an answer as(q). When building a RAG system, we usually do not have access to human-annotated datasets or reference answers. We therefore fo- cus on metrics that are fully self-contained and reference-free. We focus in particular three quality aspects, which we argue are of central importance. First, Faithfulness refers to the idea that the an- swer should be grounded in the given context. This is important to avoid hallucinations, and to ensure that the retrieved context can act as a justification for the generated answer. Indeed, RAG systems are often used in applications where",
      "chunk_index": 3
    },
    {
      "index": 4,
      "chunk_id": "RAGAS2023_chunk_04",
      "source_id": "RAGAS2023",
      "text": "an- swer should be grounded in the given context. This is important to avoid hallucinations, and to ensure that the retrieved context can act as a justification for the generated answer. Indeed, RAG systems are often used in applications where the factual con- sistency of the generated text w.r.t. the grounded sources is highly important, e.g. in domains such as law, where information is constantly evolving. Sec- ond, Answer Relevancerefers to the idea that the generated answer should address the actual ques- tion that was provided. Finally,Context Relevance refers to the idea that the retrieved context should be focused, containing as little irrelevant informa- tion as possible. This is important given the cost associated with feeding long context passages to LLMs. Moreover, when context passages are too long, LLMs are often less effective in exploiting that context, especially for information that is pro- vided in the middle of the context passage (Liu et al., 2023). We now explain how these three quality aspects can be measured in a fully automated way, by prompting an LLM. In our implementation and experiments, all prompts are evaluated using the gpt-3.5-turbo-16k model, which is available through the OpenAI API2. Faithfulness We say that the answer as(q) is faithful to the context c(q) if the claims that are made in the answer can be inferred from the con- text. To estimate faithfulness, we first use an LLM to extract a set of statements, S(as(q)). The aim of this step is to decompose longer sentences into shorter and more focused assertions. We use the following prompt for this step3: Given a question and answer, create one or more statements from each sentence in the given answer. question: [question] answer: [answer] where [question] and [answer] refer to the given question and answer. For each statement si 2https://platform.openai.com 3To help clarify the task, we include a demonstration as part of the prompt. This demonstration is not explicitly shown in the listing of the prompts throughout this paper. in S, the LLM determines ifsi can be inferred from c(q) using a verification function v(si, c(q)). This verification step is carried out using the following prompt: Consider the given context and following statements, then determine whether they are supported by the information present in the context. Provide a brief explana- tion for each statement before arriving at the verdict (Yes/No). Provide a final verdict for each statement in order at",
      "chunk_index": 4
    },
    {
      "index": 5,
      "chunk_id": "RAGAS2023_chunk_05",
      "source_id": "RAGAS2023",
      "text": "and following statements, then determine whether they are supported by the information present in the context. Provide a brief explana- tion for each statement before arriving at the verdict (Yes/No). Provide a final verdict for each statement in order at the end in the given format. Do not deviate from the specified format. statement: [statement 1] ... statement: [statement n] The final faithfulness score, F, is then computed as F = |V | |S| , where |V | is the number of statements that were supported according to the LLM and |S| is the total number of statements. Answer relevance We say that the answer as(q) is relevant if it directly addresses the question in an appropriate way. In particular, our assessment of answer relevance does not take into account fac- tuality, but penalises cases where the answer is incomplete or where it contains redundant informa- tion. To estimate answer relevance, for the given answer as(q), we prompt the LLM to generate n potential questions qi based on as(q), as follows: Generate a question for the given answer. answer: [answer] We then obtain embeddings for all questions us- ing the text-embedding-ada-002 model, avail- able from the OpenAI API. For each qi, we cal- culate the similarity sim(q, qi) with the original question q, as the cosine between the correspond- ing embeddings. The answer relevance score, AR, for question q is then computed as: AR = 1 n nX i=1 sim(q, qi) (1) This metric evaluates how closely the generated answer aligns with the initial question or instruc- tion. Context relevance The context c(q) is consid- ered relevant to the extent that it exclusively con- tains information that is needed to answer the ques- tion. In particular, this metric aims to penalise the inclusion of redundant information. To estimate context relevance, given a question q and its con- text c(q), the LLM extracts a subset of sentences, Sext, from c(q) that are crucial to answer q, using the following prompt: Please extract relevant sentences from the provided context that can potentially help answer the following question. If no relevant sentences are found, or if you believe the question cannot be answered from the given context, return the phrase \"Insufficient Information\". While extract- ing candidate sentences you're not al- lowed to make any changes to sentences from given context. The context relevance score is then computed as: CR = number of extracted",
      "chunk_index": 5
    },
    {
      "index": 6,
      "chunk_id": "RAGAS2023_chunk_06",
      "source_id": "RAGAS2023",
      "text": "from the given context, return the phrase \"Insufficient Information\". While extract- ing candidate sentences you're not al- lowed to make any changes to sentences from given context. The context relevance score is then computed as: CR = number of extracted sentences total number of sentences in c(q) (2) 4 The WikiEval Dataset To evaluate the proposed framework, we ideally need examples of question-context-answer triples which are annotated with human judgments. We can then verify to what extent our metrics agree with human assessments of faithfulness, answer relevance and context relevance. Since we are not aware of any publicly available datasets that could be used for this purpose, we created a new dataset, which we refer to as WikiEval4. To construct the dataset, we first selected 50 Wikipedia pages cov- ering events that have happened since the start of 20225. In selecting these pages, we prioritised those with recent edits. For each of the 50 pages, we then asked ChatGPT to suggest a question that can be answered based on the introductory section of the page, using the following prompt: Your task is to formulate a question from given context satisfying the rules given below: 1. The question should be fully answered from the given context. 2. The question should be framed from a part that contains non-trivial informa- tion. 3. The answer should not contain any 4https://huggingface.co/datasets/ explodinggradients/WikiEval 5That is, beyond the reported training cutoff of the model we used in our experiments. links. 4. The question should be of moderate difficulty. 5. The question must be reasonable and must be understood and responded to by humans. 6. Do not use phrases that 'provided con- text', etc in the question context: We also used ChatGPT to answer the generated question, when given the corresponding introduc- tory section as context, using the following prompt: Answer the question using the informa- tion from the given context. question: [question] context: [context] All questions were annotated along the three con- sidered quality dimensions by two annotators. Both annotators were fluent in English and were given clear instructions about the meaning of the three considered quality dimensions. For faithfulness and context relevance, the two annotators agreed in around 95% of cases. For answer relevance, they agreed in around 90% of the cases. Disagreements were resolved after a discussion between the anno- tators. Faithfulness To obtain human judgements about faithfulness, we first used ChatGPT to answer",
      "chunk_index": 6
    },
    {
      "index": 7,
      "chunk_id": "RAGAS2023_chunk_07",
      "source_id": "RAGAS2023",
      "text": "agreed in around 95% of cases. For answer relevance, they agreed in around 90% of the cases. Disagreements were resolved after a discussion between the anno- tators. Faithfulness To obtain human judgements about faithfulness, we first used ChatGPT to answer the question without access to any additional context. We then asked the annotators to judge which of the two answers was the most faithful (i.e. the standard one or the one generated without context), given the question and corresponding Wikipedia page. Answer relevance We first used ChatGPT to obtain candidate answers with lower answer rel- evance, using the following prompt: Answer the given question in an incom- plete manner. question: [question] We then asked human annotators to compare this answer, and indicate which of the two answers had the highest answer relevance. Context relevance To measure this aspect, we first added additional sentences to the context by scraping back-links to the corresponding Wikipedia page. In this way, we were able to add information to the context that was related but less relevant for Faith. Ans. Rel. Cont. Rel. Ragas 0.95 0.78 0.70 GPT Score 0.72 0.52 0.63 GPT Ranking 0.54 0.40 0.52 Table 1: Agreement with human annotators in pairwise comparisons of faithfulness, answer relevance and con- text relevance, using the WikEval dataset (accuracy). answering the question. For the few pages with- out any back-links, we instead used ChatGPT to complete the given context. 5 Experiments Table 1 analyses the agreement between the met- rics proposed in Section 3 and the human assess- ments from the proposed WikiEval dataset. Each WikiEval instance requires the model to compare two answers or two context fragments. We count how often the answer/context preferred by the model (i.e. with highest estimated faithfulness, an- swer relevance, or context relevance) coincides with the answer/context preferred by the human annotators. We report the results in terms of ac- curacy (i.e. the fraction of instances on which the model agrees with the annotators). To put the results in context, we compare our proposed metrics (shown as Ragas in Table 1) with two baseline methods. For the first method, shown as GPT Score, we ask ChatGPT to assign a score between 0 and 10 for the three quality dimensions. To this end, we use a prompt that describes the meaning of the quality metric and then asks to score the given answer/context in line with that definition. For instance,",
      "chunk_index": 7
    },
    {
      "index": 8,
      "chunk_id": "RAGAS2023_chunk_08",
      "source_id": "RAGAS2023",
      "text": "score between 0 and 10 for the three quality dimensions. To this end, we use a prompt that describes the meaning of the quality metric and then asks to score the given answer/context in line with that definition. For instance, for evaluating faithfulness, we used the following prompt: Faithfulness measures the information consistency of the answer against the given context. Any claims that are made in the answer that cannot be deduced from context should be penalized. Given an answer and context, assign a score for faithfulness in the range 0-10. context: [context] answer: [answer] Ties, where the same score is assigned by the LLM to both answer candidates, were broken randomly. The second baseline, shown as GPT Ranking, in- stead asks ChatGPT to select the preferred answer/- context. In this case, the prompt again includes a definition of the considered quality metric. For instance, for evaluating answer relevance, we used the following prompt: Answer Relevancy measures the degree to which a response directly addresses and is appropriate for a given question. It penalizes the present of redundant in- formation or incomplete answers given a question. Given an question and answer, rank each answer based on Answer Rele- vancy. question: [question] answer 1: [answer 1] answer 2: [answer 2] The results in Table 1 show that our proposed metrics are much closer aligned with the human judgements than the predictions from the two base- lines. For faithfulness, the Ragas prediction are in general highly accurate. For answer relevance, the agreement is lower, but this is largely due to the fact that the differences between the two candidate answers are often very subtle. We found context relevance to be the hardest quality dimension to evaluate. In particular, we observed that ChatGPT often struggles with the task of selecting the sen- tences from the context that are crucial, especially for longer contexts. 6 Conclusions We have highlighted the need for automated reference-free evaluation of RAG systems. In par- ticular, we have argued the need for an evaluation framework that can assess faithfulness (i.e. is the answer grounded in the retrieved context), answer relevance (i.e. does the answer address the ques- tion) and context relevance (i.e. is the retrieved context sufficiently focused). To support the devel- opment of such a framework, we have introduced WikiEval, a dataset which human judgements of these three different aspects. Finally, we have also described Ragas, our implementation",
      "chunk_index": 8
    },
    {
      "index": 9,
      "chunk_id": "RAGAS2023_chunk_09",
      "source_id": "RAGAS2023",
      "text": "context relevance (i.e. is the retrieved context sufficiently focused). To support the devel- opment of such a framework, we have introduced WikiEval, a dataset which human judgements of these three different aspects. Finally, we have also described Ragas, our implementation of the three considered quality aspects. This framework is easy to use and can provide deverlopers of RAG sys- tems with valuable insights, even in the absence of any ground truth. Our evaluation on WikiEval has shown that the predictions from Ragas are closely aligned with human predictions, especially for faithfulness and answer relevance. References Amos Azaria and Tom M. Mitchell. 2023. The inter- nal state of an LLM knows when its lying. CoRR, abs/2304.13734. Sebastian Borgeaud, Arthur Mensch, Jordan Hoffmann, Trevor Cai, Eliza Rutherford, Katie Millican, George van den Driessche, Jean-Baptiste Lespiau, Bogdan Damoc, Aidan Clark, Diego de Las Casas, Aurelia Guy, Jacob Menick, Roman Ring, Tom Hennigan, Saffron Huang, Loren Maggiore, Chris Jones, Albin Cassirer, Andy Brock, Michela Paganini, Geoffrey Irving, Oriol Vinyals, Simon Osindero, Karen Si- monyan, Jack W. Rae, Erich Elsen, and Laurent Sifre. 2022. Improving language models by retrieving from trillions of tokens. In International Conference on Machine Learning, ICML 2022, 17-23 July 2022, Bal- timore, Maryland, USA, volume 162 of Proceedings of Machine Learning Research , pages 2206-2240. PMLR. Sébastien Bubeck, Varun Chandrasekaran, Ronen El- dan, Johannes Gehrke, Eric Horvitz, Ece Kamar, Peter Lee, Yin Tat Lee, Yuanzhi Li, Scott Lund- berg, et al. 2023. Sparks of artificial general intelli- gence: Early experiments with gpt-4. arXiv preprint arXiv:2303.12712. Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. 2019. BERT: Pre-training of deep bidirectional transformers for language under- standing. In Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Tech- nologies, Volume 1 (Long and Short Papers), pages 4171-4186, Minneapolis, Minnesota. Association for Computational Linguistics. Jinlan Fu, See-Kiong Ng, Zhengbao Jiang, and Pengfei Liu. 2023. Gptscore: Evaluate as you desire. CoRR, abs/2302.04166. Kelvin Guu, Kenton Lee, Zora Tung, Panupong Pasu- pat, and Mingwei Chang. 2020. Retrieval augmented language model pre-training. In International confer- ence on machine learning, pages 3929-3938. PMLR. Ziwei Ji, Nayeon Lee, Rita Frieske, Tiezheng Yu, Dan Su, Yan Xu, Etsuko Ishii, Ye Jin Bang, Andrea Madotto, and Pascale Fung. 2023. Survey of halluci- nation in natural language generation. ACM Comput- ing Surveys, 55(12):1-38. Saurav Kadavath, Tom Conerly, Amanda Askell, Tom Henighan, Dawn Drain,",
      "chunk_index": 9
    },
    {
      "index": 10,
      "chunk_id": "RAGAS2023_chunk_10",
      "source_id": "RAGAS2023",
      "text": "Tiezheng Yu, Dan Su, Yan Xu, Etsuko Ishii, Ye Jin Bang, Andrea Madotto, and Pascale Fung. 2023. Survey of halluci- nation in natural language generation. ACM Comput- ing Surveys, 55(12):1-38. Saurav Kadavath, Tom Conerly, Amanda Askell, Tom Henighan, Dawn Drain, Ethan Perez, Nicholas Schiefer, Zac Hatfield-Dodds, Nova DasSarma, Eli Tran-Johnson, Scott Johnston, Sheer El Showk, Andy Jones, Nelson Elhage, Tristan Hume, Anna Chen, Yuntao Bai, Sam Bowman, Stanislav Fort, Deep Ganguli, Danny Hernandez, Josh Jacobson, Jack- son Kernion, Shauna Kravec, Liane Lovitt, Ka- mal Ndousse, Catherine Olsson, Sam Ringer, Dario Amodei, Tom Brown, Jack Clark, Nicholas Joseph, Ben Mann, Sam McCandlish, Chris Olah, and Jared Kaplan. 2022. Language models (mostly) know what they know. CoRR, abs/2207.05221. Nikhil Kandpal, Haikang Deng, Adam Roberts, Eric Wallace, and Colin Raffel. 2022. Large language models struggle to learn long-tail knowledge. CoRR, abs/2211.08411. Urvashi Khandelwal, Omer Levy, Dan Jurafsky, Luke Zettlemoyer, and Mike Lewis. 2020. Generalization through memorization: Nearest neighbor language models. In 8th International Conference on Learning Representations, ICLR 2020, Addis Ababa, Ethiopia, April 26-30, 2020. OpenReview.net. Omar Khattab, Keshav Santhanam, Xiang Lisa Li, David Hall, Percy Liang, Christopher Potts, and Matei Zaharia. 2022. Demonstrate-search-predict: Composing retrieval and language models for knowledge-intensive NLP. CoRR, abs/2212.14024. Kenton Lee, Ming-Wei Chang, and Kristina Toutanova. 2019. Latent retrieval for weakly supervised open do- main question answering. In Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics, pages 6086-6096. Patrick S. H. Lewis, Ethan Perez, Aleksandra Pik- tus, Fabio Petroni, Vladimir Karpukhin, Naman Goyal, Heinrich Küttler, Mike Lewis, Wen-tau Yih, Tim Rocktäschel, Sebastian Riedel, and Douwe Kiela. 2020. Retrieval-augmented generation for knowledge-intensive NLP tasks. In Advances in Neu- ral Information Processing Systems 33: Annual Con- ference on Neural Information Processing Systems 2020, NeurIPS 2020, December 6-12, 2020, virtual. Junyi Li, Xiaoxue Cheng, Wayne Xin Zhao, Jian-Yun Nie, and Ji-Rong Wen. 2023. Halueval: A large- scale hallucination evaluation benchmark for large language models. CoRR, abs/2305.11747. Nelson F. Liu, Kevin Lin, John Hewitt, Ashwin Paran- jape, Michele Bevilacqua, Fabio Petroni, and Percy Liang. 2023. Lost in the middle: How language models use long contexts. Alex Mallen, Akari Asai, Victor Zhong, Rajarshi Das, Daniel Khashabi, and Hannaneh Hajishirzi. 2023. When not to trust language models: Investigating effectiveness of parametric and non-parametric mem- ories. In Proceedings of the 61st Annual Meeting of the Association for Computational Linguistics (Vol- ume 1: Long Papers) , pages 9802-9822, Toronto, Canada. Association for",
      "chunk_index": 10
    },
    {
      "index": 11,
      "chunk_id": "RAGAS2023_chunk_11",
      "source_id": "RAGAS2023",
      "text": "2023. When not to trust language models: Investigating effectiveness of parametric and non-parametric mem- ories. In Proceedings of the 61st Annual Meeting of the Association for Computational Linguistics (Vol- ume 1: Long Papers) , pages 9802-9822, Toronto, Canada. Association for Computational Linguistics. Potsawee Manakul, Adian Liusie, and Mark J. F. Gales. 2023. Selfcheckgpt: Zero-resource black-box hal- lucination detection for generative large language models. CoRR, abs/2303.08896. Sewon Min, Kalpesh Krishna, Xinxi Lyu, Mike Lewis, Wen-tau Yih, Pang Wei Koh, Mohit Iyyer, Luke Zettlemoyer, and Hannaneh Hajishirzi. 2023. Factscore: Fine-grained atomic evaluation of fac- tual precision in long form text generation. CoRR, abs/2305.14251. Ori Ram, Yoav Levine, Itay Dalmedigos, Dor Muhlgay, Amnon Shashua, Kevin Leyton-Brown, and Yoav Shoham. 2023. In-context retrieval-augmented lan- guage models. CoRR, abs/2302.00083. Adam Roberts, Colin Raffel, and Noam Shazeer. 2020. How much knowledge can you pack into the param- eters of a language model? In Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP), pages 5418-5426, Online. Association for Computational Linguistics. Weijia Shi, Sewon Min, Michihiro Yasunaga, Minjoon Seo, Rich James, Mike Lewis, Luke Zettlemoyer, and Wen-tau Yih. 2023. REPLUG: retrieval-augmented black-box language models. CoRR, abs/2301.12652. Jiaan Wang, Yunlong Liang, Fandong Meng, Haoxi- ang Shi, Zhixu Li, Jinan Xu, Jianfeng Qu, and Jie Zhou. 2023a. Is chatgpt a good NLG evaluator? A preliminary study. CoRR, abs/2303.04048. Peiyi Wang, Lei Li, Liang Chen, Dawei Zhu, Binghuai Lin, Yunbo Cao, Qi Liu, Tianyu Liu, and Zhifang Sui. 2023b. Large language models are not fair evaluators. CoRR, abs/2305.17926. Shufan Wang, Yixiao Song, Andrew Drozdov, Aparna Garimella, Varun Manjunatha, and Mohit Iyyer. 2023c. KNN-LM does not improve open-ended text generation. CoRR, abs/2305.14625. Weizhe Yuan, Graham Neubig, and Pengfei Liu. 2021. Bartscore: Evaluating generated text as text genera- tion. In Advances in Neural Information Processing Systems 34: Annual Conference on Neural Informa- tion Processing Systems 2021, NeurIPS 2021, De- cember 6-14, 2021, virtual, pages 27263-27277. Tianhua Zhang, Hongyin Luo, Yung-Sung Chuang, Wei Fang, Luc Gaitskell, Thomas Hartvigsen, Xixin Wu, Danny Fox, Helen Meng, and James R. Glass. 2023. Interpretable unified language checking. CoRR, abs/2304.03728. Tianyi Zhang, Varsha Kishore, Felix Wu, Kilian Q. Weinberger, and Yoav Artzi. 2020. Bertscore: Evalu- ating text generation with BERT. In8th International Conference on Learning Representations, ICLR 2020, Addis Ababa, Ethiopia, April 26-30, 2020. OpenRe- view.net. Wei Zhao, Maxime Peyrard, Fei Liu, Yang Gao, Chris- tian M. Meyer, and Steffen Eger. 2019. MoverScore: Text generation",
      "chunk_index": 11
    },
    {
      "index": 12,
      "chunk_id": "RAGAS2023_chunk_12",
      "source_id": "RAGAS2023",
      "text": "ating text generation with BERT. In8th International Conference on Learning Representations, ICLR 2020, Addis Ababa, Ethiopia, April 26-30, 2020. OpenRe- view.net. Wei Zhao, Maxime Peyrard, Fei Liu, Yang Gao, Chris- tian M. Meyer, and Steffen Eger. 2019. MoverScore: Text generation evaluating with contextualized em- beddings and earth mover distance. In Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th Interna- tional Joint Conference on Natural Language Pro- cessing (EMNLP-IJCNLP), pages 563-578, Hong Kong, China. Association for Computational Lin- guistics. A Examples from WikiEval Tables 2, 3 and 4 show examples from the WikiEval dataset, focusing in particular on answers with high and low faithfulness (Table 2), high and low answer relevance (Table 3), and high and low context rele- vance (Table 4). Question Context Answer Who directed the film Op- penheimer and who stars as J. Robert Oppenheimer in the film? Oppenheimer is a 2023 biographical thriller film written and directed by Christopher Nolan. Based on the 2005 biography American Prometheus by Kai Bird and Mar- tin J. Sherwin, the film chronicles the life of J. Robert Oppenheimer, a theoretical physicist who was pivotal in developing the first nuclear weapons as part of the Man- hattan Project, and thereby ushering in the Atomic Age. Cillian Murphy stars as Oppenheimer, with Emily Blunt as Oppenheimer's wife Katherine \"Kitty\" Oppenheimer. High Faithfulness : Christopher Nolan directed the film Oppen- heimer. Cillian Murphy stars as J. Robert Oppenheimer in the film. Low Faithfulness : James Cameron directed the film Op- penheimer. Tom Cruise stars as J. Robert Oppenheimer in the film. Table 2: Example from WikiEval, showing answers with high and low faithfulness. Question Answer When is the scheduled launch date and time for the PSLV-C56 mission, and where will it be launched from? High answer relevance: The PSLV-C56 mission is scheduled to be launched on Sunday, 30 July 2023 at 06:30 IST / 01:00 UTC. It will be launched from the Satish Dhawan Space Centre, Sriharikota, Andhra Pradesh, India. Low answer relevance: The scheduled launch date and time for the PSLV-C56 mission have not been provided.The PSLV-C56 mission is an important space mission for India. It aims to launch a satellite into orbit to study weather patterns. Table 3: Example from WikiEval, showing answers with high and low answer relevance. Question Context When was the Chimnabai Clock Tower completed, and who was it named af- ter?",
      "chunk_index": 12
    },
    {
      "index": 13,
      "chunk_id": "RAGAS2023_chunk_13",
      "source_id": "RAGAS2023",
      "text": "aims to launch a satellite into orbit to study weather patterns. Table 3: Example from WikiEval, showing answers with high and low answer relevance. Question Context When was the Chimnabai Clock Tower completed, and who was it named af- ter? High context relevance: The Chimnabai Clock Tower, also known as the Raopura Tower, is a clock tower situated in the Raopura area of Vadodara, Gujarat, India. It was completed in 1896 and named in memory of Chimnabai I (1864-1885), a queen and the first wife of Sayajirao Gaekwad III of Baroda State. Low context relevance: The Chimnabai Clock Tower, also known as the Raopura Tower, is a clock tower situated in the Raopura area of Vadodara, Gujarat, India. It was completed in 1896 and named in memory of Chimnabai I (1864-1885), a queen and the first wife of Sayajirao Gaekwad III of Baroda State. It was built in Indo-Saracenic architecture style. History. Chimnabai Clock Tower was built in 1896. The tower was named after Chimnabai I (1864-1885), a queen and the first wife of Sayajirao Gaekwad III of Baroda State. It was inaugurated by Mir Kamaluddin Hussainkhan, the last Nawab of Baroda. During the rule of Gaekwad, it was a stoppage for horse drawn trams. The clock tower was erected at the cost of 25,000 (equivalent to 9.2 million or USD 120,000 in 2023). Table 4: Example from WikiEval, showing answers with high and low context relevance.",
      "chunk_index": 13
    },
    {
      "index": 14,
      "chunk_id": "ARES2023_chunk_00",
      "source_id": "ARES2023",
      "text": "ARES: An Automated Evaluation Framework for Retrieval-Augmented Generation Systems Jon Saad-Falcon Stanford University ∗ jonsaadfalcon@stanford.edu Omar Khattab Stanford University okhattab@stanford.edu Christopher Potts Stanford University cgpotts@stanford.edu Matei Zaharia Databricks and UC Berkeley matei@databricks.com Abstract Evaluating retrieval-augmented generation (RAG) systems traditionally relies on hand annotations for input queries, passages to re- trieve, and responses to generate. We intro- duce ARES, an Automated RAG Evaluation System, for evaluating RAG systems along the dimensions of context relevance, answer faithfulness, and answer relevance. By cre- ating its own synthetic training data, ARES finetunes lightweight LM judges to assess the quality of individual RAG components. To mitigate potential prediction errors, ARES uti- lizes a small set of human-annotated datapoints for prediction-powered inference (PPI). Across eight different knowledge-intensive tasks in KILT, SuperGLUE, and AIS, ARES accurately evaluates RAG systems while using only a few hundred human annotations during evaluation. Furthermore, ARES judges remain effective across domain shifts, proving accurate even after changing the type of queries and/or docu- ments used in the evaluated RAG systems. We make our code and datasets publicly available on Github. 1 Introduction Retrieval-augmented generation (RAG) has be- come a prominent approach for building user- facing NLP applications, such as systems for ques- tion answering (QA), fact-checking, and customer support (Petroni et al., 2021; Wang et al., 2019). Typically, a RAG system consists of a retriever and a downstream language model (LM). Given a user question, the retriever finds relevant passages from a corpus and the LM uses these passages to gener- ate a response. This formulation admits a multitude of choices: what retrieval model to use, how to di- vide the documents into retrieval chunks, and how to prompt or finetune the LM to use the retrieved information, to name only a few of the simplest design decisions. ∗Project started during research internship at Databricks The best design for a RAG system is not neces- sarily universal across data domains, corpus sizes, and cost/latency budgets. To tune their own RAG systems, practitioners traditionally need hand an- notations for test questions, passages to retrieve (to assess the retriever), and responses to generate, labeled specifically for their target domain. Alter- natively, they may evaluate different approaches in production by collecting human preferences that compare the candidate systems. Unfortunately, both of these strategies demand high expertise and impose considerable annotation costs. Model-based evaluation is an inexpensive strat- egy to test generative output quality (Zheng et",
      "chunk_index": 0
    },
    {
      "index": 15,
      "chunk_id": "ARES2023_chunk_01",
      "source_id": "ARES2023",
      "text": "different approaches in production by collecting human preferences that compare the candidate systems. Unfortunately, both of these strategies demand high expertise and impose considerable annotation costs. Model-based evaluation is an inexpensive strat- egy to test generative output quality (Zheng et al., 2023). For instance, the open-source RAGAS framework (James and Es, 2023) prompts an LM for evaluating the relevance of retrieved informa- tion and the faithfulness and accuracy of generated responses. Unfortunately, such strategies currently rely for evaluation on a fixed set of heuristically hand-written prompts, offering little adaptability to various evaluation contexts and no guarantees about quality. To evaluate RAG systems rapidly and accu- rately, we propose ARES, the Automated RAG Evaluation System. ARES is the first automated RAG evaluation system to generate tailored LLM judges for each component of a RAG pipeline, lead- ing to substantial boosts in evaluation precision and accuracy compared to existing approaches like RA- GAS. Furthermore, unlike existing RAG evaluation systems, ARES provides confidence intervals for its scoring by leveraging prediction-powered in- ference (PPI; Angelopoulos et al. 2023). Given a corpus of documents and a RAG system, ARES reports three evaluation scores: context relevance (is the retrieved information pertinent to the test question), answer faithfulness (is the response gen- erated by the language model properly grounded in the retrieved context), and answer relevance (is the response also relevant to the question). A good arXiv:2311.09476v2 [cs.CL] 31 Mar 2024 RAG system finds relevant contexts and generates answers that are both faithful and relevant. Many existing RAG evaluation frameworks re- quire substantial human annotations for scoring. ARES significantly improves data efficiency dur- ing evaluation by only requiring three inputs: an in- domain passage set, a human preference validation set of approximately 150 annotated datapoints or more, and few-shot examples of in-domain queries and answers (e.g. five examples or more), which are used for prompting LLMs in synthetic data gen- eration. Given the corpus of in-domain passages, ARES proceeds in three stages. First, it leverages an LM to construct a synthetic dataset of question-answer pairs, derived from the passages in the corpus. Sec- ond, it defines three separate judge models to per- form three classification tasks (context relevance, answer faithfulness, and answer relevance). These judges are lightweight models fine-tuned against a contrastive learning objective. Third, ARES scores the different RAG systems being assessed using prediction-powered inference (PPI; Angelopoulos et al. 2023) to improve model-based evaluation ac- curacy",
      "chunk_index": 1
    },
    {
      "index": 16,
      "chunk_id": "ARES2023_chunk_02",
      "source_id": "ARES2023",
      "text": "relevance, answer faithfulness, and answer relevance). These judges are lightweight models fine-tuned against a contrastive learning objective. Third, ARES scores the different RAG systems being assessed using prediction-powered inference (PPI; Angelopoulos et al. 2023) to improve model-based evaluation ac- curacy and provide statistical confidence intervals for RAG scoring. PPI utilizes a small set of human annotated datapoints for computing its confidence intervals; we designate this annotated set as our hu- man preference validation set, which is composed of approximately 150 annotated datapoints or more that designate both positive and negative examples for context relevance, answer faithfulness, and an- swer relevance. We conduct extensive empirical evaluations, demonstrating that ARES accurately scores RAG systems across the six knowledge-intensive datasets in KILT and SuperGLUE, beating exist- ing automated evaluation approaches like RAGAS by 59.3 and 14.4 percentage points on average across context relevance and answer relevance eval- uation accuracy, respectively. Additionally, ARES accurately calculates answer hallucination occur- rences in the AIS attribution dataset (Rashkin et al., 2022), predicting within 2.5 percentage points of the ground truth average for answer hallucinations. Compared to annotation-based evaluation methods, ARES is substantially more accurate and efficient, requiring 78% less annotations than the baseline approach. We also find that ARES consistently distinguishes competitive RAG systems that are only a few points apart in ground-truth metrics. This precision enables ARES to guide the develop- ment and comparison of competitive approaches and configurations. We make the ARES code and datasets publicly available on Github. 2 Related Work RAG (Guu et al., 2020; Lewis et al., 2020; Khat- tab et al., 2021; Izacard et al., 2022)) is now a common strategy for bolstering LLMs by combin- ing them with retrieval systems. Through retrieval, RAG helps LM systems gather domain-specific knowledge, ground generations in factual informa- tion (Shuster et al., 2021; Huo et al., 2023), and offer a degree of transparency or interpretability via citing sources (Mialon et al., 2023). Multiple LLM-based evaluation techniques have emerged for gauging LLM systems. This is essen- tial for rapid deployment in new settings, where it is difficult to build a traditional benchmark dataset from scratch. Early attempts at this use LLMs out of the box, as in MT-Bench and Chatbot Arena (Zheng et al., 2023). AutoCalibrate (Liu et al., 2023b) seeks to align an LLM-judge with human preferences, leveraging a self-refinement prompt to iteratively improve the LLM judge. How- ever, AutoCalibrate does not offer any",
      "chunk_index": 2
    },
    {
      "index": 17,
      "chunk_id": "ARES2023_chunk_03",
      "source_id": "ARES2023",
      "text": "as in MT-Bench and Chatbot Arena (Zheng et al., 2023). AutoCalibrate (Liu et al., 2023b) seeks to align an LLM-judge with human preferences, leveraging a self-refinement prompt to iteratively improve the LLM judge. How- ever, AutoCalibrate does not offer any statistical guarantees for the accuracy of its predictions. Other work has used LLM prompting to evaluate system quality across natural language generation tasks, such as translation, summarization, and dialogue (Kocmi and Federmann, 2023; Fu et al., 2023; Liu et al., 2023a; Wang et al., 2023). In the context of knowledge-intensive NLP tasks, LLMs have been explored for assessing attribution and factuality in LLMs (Min et al., 2023; Gekhman et al., 2023; Yue et al., 2023). New guidelines like LongEval (Krishna et al., 2023) and datasets like Hagrid and ALCE (Kamalloo et al., 2023; Gao et al., 2023) provide resources for analyzing knowledge-intensive LLM pipelines. The two most-closely related projects to ARES are EXAM (Sander and Dietz, 2021) and RA- GAS (James and Es, 2023). To evaluate RAG sys- tems, the EXAM metric estimates how many exam questions a reader (simulated as a QA system) can answer correctly based on the generated response. This requires a set of queries with several asso- ciated sub-questions each, which adds a burden that ARES does not bring. RAGAS is based on a handful of heuristic hand-written prompts. These offer little adaptability to new RAG evaluation set- tings (e.g., new corpora) and, as we show in our evaluation, substantially underperform ARES. 3 ARES ARES proceeds in three stages (Figure 1). There are three required inputs: an in-domain passage set, a human preference validation set of approximately 150 annotated datapoints (or more), and few-shot examples of in-domain queries and answers (five or more examples), which are used for prompting LLMs in synthetic data generation. With our inputs prepared, we begin by generating synthetic queries (and their answers) from the passages in the target corpus. We then use these query-passage-answer triples to train LLM judges. Subsequently, we ap- ply these judges to any RAG system, scoring a sample of its in-domain query-document-answer triples, and use prediction-powered inference (PPI) with our human preference validation set to esti- mate a confidence interval for the quality of each RAG system. 3.1 LLM Generation of Synthetic Dataset We generate synthetic queries and answers from the corpus passages using generative LLMs. The generated data represent both positive and negative examples of query-passage-answer",
      "chunk_index": 3
    },
    {
      "index": 18,
      "chunk_id": "ARES2023_chunk_04",
      "source_id": "ARES2023",
      "text": "a confidence interval for the quality of each RAG system. 3.1 LLM Generation of Synthetic Dataset We generate synthetic queries and answers from the corpus passages using generative LLMs. The generated data represent both positive and negative examples of query-passage-answer triples (e.g., relevant/irrelevant passages and correct/incorrect answers). For generation, the LLM uses our in- put set of few-shot examples with in-domain pas- sages mapped to in-domain queries and answers; the model then generates a synthetic question and answer from a given in-domain passage, allowing us to create both positive and negative training ex- amples. We include example prompts for generat- ing synthetic queries and answers in A.6. For creating our synthetic data, we primarily use on FLAN-T5 XXL (discussed in subsection 4.1). ARES works well with this model (see section 5) but our system can ultimately use another high- quality model for generating synthetic queries and answers. We then filter out low-quality queries by testing if a given query can retrieve its original passage as the top result using its retriever. This filtering approach has been used in previous work to isolate high-quality synthetic queries (Dai et al., 2022; Saad-Falcon et al., 2023). To generate negatives for fine-tuning our LLM judges, we rely on two novel strategies, generating the same number of negatives with each strategy: 1. Weak Negative Generation: For context rel- evance negatives, we randomly sample in- domain passages unrelated to a given syn- thetic query. For answer faithfulness and answer relevance negatives, we randomly sample synthetically-generated answers from other passages, which were created using FLAN-T5 XXL. 2. Strong Negative Generation : For context relevance negatives, we randomly sample in- domain passages from the same document as the gold passage. For datasets in which mul- tiple passages are not available for the same document, we use BM25 to retrieve the top- 10 passages similar to the passage and sample from them for our context relevance strong negatives. For answer faithfulness and an- swer relevance negatives, we prompt FLAN- T5 XXL (subsection 4.1) to generate a contra- dictory answer using the few-shot prompt in subsection A.5. In total, the number of negatives generated equals the number of positives generated for evalu- ating context relevance and answer relevance. 3.2 Preparing LLM Judges To prepare our RAG evaluation judges, we use our synthetic dataset to fine-tune DeBERTa-v3- Large judges (discussed in subsection 4.1) to eval- uate three different capabilities (Chen et",
      "chunk_index": 4
    },
    {
      "index": 19,
      "chunk_id": "ARES2023_chunk_05",
      "source_id": "ARES2023",
      "text": "for evalu- ating context relevance and answer relevance. 3.2 Preparing LLM Judges To prepare our RAG evaluation judges, we use our synthetic dataset to fine-tune DeBERTa-v3- Large judges (discussed in subsection 4.1) to eval- uate three different capabilities (Chen et al., 2023; James and Es, 2023): 1. Context Relevance: Is the passage returned relevant for answering the given query? 2. Answer Faithfulness: Is the answer gener- ated faithful to the retrieved passage, or does it contain hallucinated or extrapolated state- ments beyond the passage? 3. Answer Relevance: Is the answer generated relevant given the query and retrieved pas- sage? For each metric, a separate LLM with a binary classifier head is fine-tuned to classify positive and negative examples. For each concatenated query- document-answer, a single LLM judge must clas- sify the triple as positive or negative for that judge's metric. To fine-tune these judges, we use our hu- man preference validation set to evaluate model improvement after each epoch, stopping when we have three epochs with no improvement in loss (see subsection A.1 for more information). Figure 1: Overview of ARES: As inputs, the ARES pipeline requires an in-domain passage set, a human preference validation set of 150 annotated datapoints or more, and few-shot examples of in-domain queries and answers (five or more), which are used for prompting LLMs in synthetic data generation. To prepare our LLM judges for evaluation, we first generate synthetic queries and answers from the corpus passages. Using our generated training triples and a constrastive learning framework, we fine-tune an LLM to classify query-passage-answer triples in three different criteria: context relevance, answer faithfulness, and answer relevance. Finally, we use the LLM judges to score RAG systems and generate confidence bounds for the ranking using PPI and the human preference validation set. 3.3 Ranking RAG Systems with Confidence Intervals Once we have prepared our LLM judges, we need to use them to score and rank the competing RAG systems. To do this, ARES samples the in-domain query-document-answer triples produced by each RAG approach, and the judges label each triple, predicting their context relevance, answer faithful- ness, and answer relevance. By averaging the in- dividual predicted labels for each in-domain triple, we calculate the RAG system performance across each of the three metrics. In principle, we could simply report these aver- age scores as quality metrics for each RAG system. However, these scores reflect entirely unlabeled data with",
      "chunk_index": 5
    },
    {
      "index": 20,
      "chunk_id": "ARES2023_chunk_06",
      "source_id": "ARES2023",
      "text": "each in-domain triple, we calculate the RAG system performance across each of the three metrics. In principle, we could simply report these aver- age scores as quality metrics for each RAG system. However, these scores reflect entirely unlabeled data with predictions from a synthetically-trained LLM judge, and hence they may not be entirely accurate. As an extreme alternative, we could use just the small human preference validation set dis- cussed previously for evaluation, reporting the ex- tent to which each RAG system agrees with (or deviates from) the human annotations. However, an annotation-based evaluation approach would re- quire labeling substantially more generative out- puts from each RAG systems separately, which can be costly both in terms of time and financing. To combine the benefits of both, and hence boost the precision of the evaluation, ARES uses prediction-powered inference (PPI; Angelopoulos et al. 2023) to predict the system scores. PPI is a recent statistical method that provides tighter confidence intervals on a small set of annotated datapoints (i.e., our validation set) by leveraging predictions on a much larger set of non-annotated datapoints. PPI can leverage both the labeled dat- apoints and the ARES judge predictions on the non-annotated datapoints to construct confidence intervals for our RAG system's performance. To do this, PPI uses the LLM judges on the hu- man preference validation set to learn a rectifier function for constructing a confidence set of the ML model's performance, using each ML prediction in the larger non-annotated dataset. The confidence set can then be used to create a tighter confidence interval for the performance of the evaluated RAG system (e.g. its context relevance, answer faithful- ness, or answer relevance accuracy individually) compared to simply using annotated outputs from the evaluated RAG system. By bolstering the hu- man preference validation set with the much larger set of datapoints with ML predictions, PPI can de- velop reliable confidence intervals for ML model performance that beat previous classical inference approaches. The PPI rectifier function allows us to estimate the errors of the LLM judge and generate confi- dence bounds for the success and failure rates of the RAG system, estimating context relevance, answer faithfulness, and answer relevance performance. Additionally, PPI allows us to estimate confidence intervals with a selected level of probability; for our experiments, we use a standard 95% alpha (proba- bility) for our confidence interval. With the accuracy confidence interval for each component",
      "chunk_index": 6
    },
    {
      "index": 21,
      "chunk_id": "ARES2023_chunk_07",
      "source_id": "ARES2023",
      "text": "answer relevance performance. Additionally, PPI allows us to estimate confidence intervals with a selected level of probability; for our experiments, we use a standard 95% alpha (proba- bility) for our confidence interval. With the accuracy confidence interval for each component of the RAG, we find the midpoint of each confidence interval and use the midpoints to rank the RAG systems. With our ranking, we can compare different RAG systems, as well as differ- ent configurations of the same RAG system, to find the best-performing approach for a given domain. 4 Experiments 4.1 Models For our fine-tuned judges, ARES relies on generat- ing cheap but quality synthetic queries and answers using LLMs. For generating our synthetic datasets, we use FLAN-T5 XXL (Chung et al., 2022). We se- lected DeBERTa-v3-Large (He et al., 2021) for our fine-tuned LLM judge. Our fine-tuned LLM judges allow us to rank RAG systems without relying on external APIs, solely using few-shot prompts and deployable LLMs on commercial GPUs. For our in-context learning baseline, we use Ope- nAI's gpt-3.5-turbo-16k, version 10/23, (Brown et al., 2020) in a zero/few-shot setting. For similar- ity search over in-domain passages, we use FAISS IndexFlatL2 for indexing (Johnson et al., 2019) and OpenAI's text-embedding-ada-002 for gener- ating embeddings. We use simlarity search over in-domain passages to filter our synthetic queries that cannot retrieve the passage from which they were generated. We use version 0.0.18 of RAGAS in our experiments (James and Es, 2023). 4.2 Datasets Our core experimental goal is to provide a rich picture of where ARES can be applied effectively. To test across multiple types of queries, documents, and answers, we selected all the datasets from the widely-used KILT and SuperGLUE benchmarks for which RAG is appropriate. From KILT (Petroni et al., 2021), we use Natural Questions (NQ), HotpotQA, FEVER, and Wizards of Wikipedia (WoW) (Kwiatkowski et al., 2019; Yang et al., 2018; Akhtar et al., 2023; Dinan et al., 2018). Each dataset uses Wikipedia passages but the queries and answers offer a range of applica- tions. Both NQ and HotpotQA feature direct ques- tions and expect short answers, but NQ uses single passages for reasoning while HotpotQA requires multiple passages for reasoning. Furthermore, FEVER focuses on fact-verification, determining if a passage supports or refutes a given statement, and expects an output of \"SUPPORTS\" or \"REFUTES\". WoW seeks to evaluate dialogue agents by mapping user dialogue to relevant Wikipedia passages",
      "chunk_index": 7
    },
    {
      "index": 22,
      "chunk_id": "ARES2023_chunk_08",
      "source_id": "ARES2023",
      "text": "passages for reasoning. Furthermore, FEVER focuses on fact-verification, determining if a passage supports or refutes a given statement, and expects an output of \"SUPPORTS\" or \"REFUTES\". WoW seeks to evaluate dialogue agents by mapping user dialogue to relevant Wikipedia passages be- fore a chatbot generates a paragraph-length chat response incorporating passage knowledge. From SuperGLUE (Wang et al., 2019), we use MultiRC and ReCoRD (Khashabi et al., 2018; Zhang et al., 2018). MultiRC focuses on di- rect questions for seven different domains (News, Wikipedia articles, articles on society/law/justice, articles on history/anthropology, elementary school science textbooks, 9/11 reports, and fiction). ReCoRD focuses on determining the placeholder entity in a statement, focusing on news articles from CNN and the Daily Mail. For MultiRC and ReCoRD, we create open-domain versions of their tasks. For MultiRC, we perform retrieval over its seven sets of domain passages. For ReCoRD, we perform retrieval over its news article passages. The efficacy of ARES relies on its ability to rank different RAG systems while only using a human preference validation set and domain-targeted LLM judges. To test the limits of ARES, we need to sim- ulate the existence of many RAG systems that are separated by small accuracy margins on our eval- uation metrics. For this, we create systems using artificial query-passage-answer triples, in which we empirically know the positive and negative ex- amples of the mock RAG system. We generate these mock splits of the given datasets by select- ing (1) The positive and negative query-passage matches for context relevance, and (2) the positive and negative query-passage-answer matches for an- swer relevance. We include positive and negative examples from our evaluation sets in Table 7. For our positive triples, we can simply use the KILT and SuperGLUE examples without any al- teration. For gathering negative query-passage pairs and query-passage-answer triples, we ran- domly sample passages and answers from either: the same Wikipedia document or an entirely ran- dom Wikipedia document. This sampling allows us to artificially create mock RAG systems for test- ing ARES. By sampling both related and unrelated documents/answers, we hope to better gauge the efficacy of ARES in judging RAG outputs. We do not evaluate answer faithfulness for KILT and SuperGLUE datasets since we do not have human-annotated hallucinated answers to use for evaluation. However, we do test the ARES frame- work on real attribution datasets in Section 5.2. Using the validation subsets for each",
      "chunk_index": 8
    },
    {
      "index": 23,
      "chunk_id": "ARES2023_chunk_09",
      "source_id": "ARES2023",
      "text": "answer faithfulness for KILT and SuperGLUE datasets since we do not have human-annotated hallucinated answers to use for evaluation. However, we do test the ARES frame- work on real attribution datasets in Section 5.2. Using the validation subsets for each KILT and SuperGLUE dataset, we create nine different dataset splits, ranging from 70% success rate to 90% success rate for each of the evaluated RAG criteria; each dataset is separated by 2.5% accuracy points (e.g. 70.0%, 72.5%, 75.0%, . . . , 90.0%). Each split also represents a different mock RAG system. Since we know the success percentages of each dataset split, we know the appropriate rank- ing of each mock RAG system. This allows us to test ARES success at both scoring and ranking the mock RAG systems appropriately across the three evaluation criteria. 4.3 Metrics To calculate the correlation between the correct ranking and the ARES ranking, we use the Kendall rank correlation coefficient or Kendall's τ : τ = (# of concordant pairs) − (# of discordant pairs) # of pairs total Concordant pairs are defined as two ordinal val- ues in the ranking where the earlier value in the sequence is lower than the later value in the se- quence. Discordant pairs are defined as two ordinal values in the ranking where the earlier value in the sequence is greater than or equal to the later value in the sequence. A Kendall's τ greater than 0.9 is considered successful but it ranges from 0.0 to 1.0. In development, researchers and engineers will be comparing different RAG configurations through individual pairwise comparisons of model choices, retriever selection, and document prepro- cessing. We want to make sure that ARES has satis- factory accuracy in pairwise comparisons across a variety of performance gaps between RAG systems. Kendall's τ is explicitly designed for measuring the accuracy of such pairwise comparisons, calculating the correlation between a perfectly accurate pair- wise ranking and an experimental pairwise ranking. Thus, it is a popular and widespread metric used in information retrieval, allowing developers to eval- uate ranking systems empirically. Therefore, we believe Kendall's tau and prediction accuracy pro- vide meaningful metrics for testing the efficacy of ARES as a RAG evaluation system. 5 Results & Analysis 5.1 ARES Ranking Table 1 summarizes our main evaluation of ARES (with DeBERTa-v3-Large as the pretrained basis for the judges). We compare against RAGAS (ver- sion 0.0.18) and",
      "chunk_index": 9
    },
    {
      "index": 24,
      "chunk_id": "ARES2023_chunk_10",
      "source_id": "ARES2023",
      "text": "efficacy of ARES as a RAG evaluation system. 5 Results & Analysis 5.1 ARES Ranking Table 1 summarizes our main evaluation of ARES (with DeBERTa-v3-Large as the pretrained basis for the judges). We compare against RAGAS (ver- sion 0.0.18) and a baseline few-shot prompted GPT- 3.5 judge ( gpt-3.5-turbo-16k). For the few-shot GPT-3.5 judge, we provide few-shot examples for guiding predictions; the prompts are included in Appendices A.2, A.3, and A.4. For both ARES and the GPT-3.5 judge baseline, we augment the LLM with PPI, using a 300-datapoint human pref- erence validation set to rectify the ML predictions and produce confidence intervals. Across almost all settings across the datasets from KILT and SuperGLUE, ARES provides a more accurate ranking of RAG systems than RA- GAS. ARES averages a Kendall'sτ 0.065 higher for context relevance and 0.132 higher for answer relevance than RAGAS . Additionally, the LLM- judge is substantially more accurate than RAGAS at predicting context relevance and answer rele- vance of a query-passage-answer triple. For con- text relevance, ARES with a fine-tuned LLM-judge is 59.9 percentage points higher than RAGASwhile for answer relevance, our system is 14.4 percent- age points higher than RAGAS . Overall, ARES provides a more accurate system for automatically evaluating RAG configurations than RAGAS by leveraging domain-adaptive techniques for prompt- ing and training as well as utilizing PPI to bolster model predictions. As an additional comparison, we also include the Kendall's τ for RAG ranking with the ARES LLM judge without PPI; for all datasets tested, PPI improved the ranking prediction accuracy of the fine-tuned LLM judge. Furthermore, we included a sampled annotations configuration, in which we sampled 150-datapoints from each mock RAG sys- tem, totalling 1,350 annotations. Even with all these annotations, the Kendall's τ for ARES is 0.08 higher on average, across both context and an- swer relevance, compared to sampled annotations, despite using 78% less annotations. In sum, ARES proves significantly more data-efficient with human annotations while being more accurate at scoring than standard sampled annotation methods. Compared to the GPT-3.5 judge, ARES provides a more accurate ranking of the RAG systems than the GPT-3.5 judge, averaging a Kendall's tau 0.06 higher over both context relevance and answer rel- evance. Between the judge configurations, the fine- tuned LLM judge of ARES can more precisely dis- tinguish between RAG systems and guide configu- ration decisions surrounding document splitting, re- triever selection, and generative",
      "chunk_index": 10
    },
    {
      "index": 25,
      "chunk_id": "ARES2023_chunk_11",
      "source_id": "ARES2023",
      "text": "over both context relevance and answer rel- evance. Between the judge configurations, the fine- tuned LLM judge of ARES can more precisely dis- tinguish between RAG systems and guide configu- ration decisions surrounding document splitting, re- triever selection, and generative LLM choice. How- ever, while the fine-tuned LLM judge had a higher Kendall's tau on average, the GPT-3.5 judge is more readily deployable and does not require any additional fine-tuning. The GPT-3.5 judge does come with its own querying costs, which can vary based on the date of querying as well as the total tokens used in evaluation. We also wanted to better understand the impor- tance of human annotations for ARES. To this end, we conducted two sets of experiments. First, we ARES Ranking of Pseudo RAG Systems NQ HotpotQA WoW FEVER MultiRC ReCoRD C.R A.R. C.R A.R. C.R A.R. C.R A.R. C.R A.R. C.R A.R. Kendall's Tau for Sampled Annotations 0.83 0.89 0.78 0.78 0.78 0.83 0.89 0.89 0.83 0.83 0.72 0.94 Kendall's Tau for RAGAS 0.89 0.89 0.94 0.89 0.94 0.94 0.72 0.61 0.83 0.94 0.89 0.44 Kendall's Tau for GPT-3.5 Judge 0.89 0.94 0.67 0.94 0.94 0.89 0.78 0.78 0.83 0.89 0.83 0.94 Kendall's Tau for ARES LLM Judge 0.89 1.0 0.89 0.94 0.94 1.0 0.83 0.72 0.94 0.83 0.78 0.83 Kendall's Tau for ARES 0.94 1.0 0.94 0.94 1.0 1.0 0.89 0.78 0.94 0.89 0.83 0.89 RAGAS Accuracy 31.4% 71.2% 17.2% 76.0% 36.4% 77.8% 23.7% 69.2% 16.1% 75.0% 15.0% 72.8% GPT-3.5 Judge Accuracy 73.8% 95.5% 75.3% 71.6% 84.3% 85.2% 60.4% 59.6% 72.4% 60.3% 81.0% 65.8% ARES Accuracy 79.3% 97.2% 92.3% 81.3% 85.7% 96.1% 88.4% 78.5% 85.8% 82.7% 67.8% 92.3% Table 1: ARES Ranking with Fine-tuned LLM Judges vs. Sampled Annotations, RAGAS and GPT-3.5 Judge: For scoring context relevance and answer relevance (C.R. and A.R. in the table, respectively), we compare ARES with our fine-tuned LLM judges against sampled annotations benchmark, RAGAS, and a few-shot GPT-3.5 judge. For our sampled annotations, we gather 150 annotated datapoints from each mock RAG system and use those labels to score the system. RAGAS also uses GPT-3.5 as its judge but it uses few-shot prompts that are not targeted for each evaluation domain. Overall, we found that ARES ranked RAG systems more accurately than RAGAS and GPT-3.5 across all the explored datasets. The Kendall's tau for ARES was 0.065 higher on average for scoring context relevance and 0.132 higher on",
      "chunk_index": 11
    },
    {
      "index": 26,
      "chunk_id": "ARES2023_chunk_12",
      "source_id": "ARES2023",
      "text": "each evaluation domain. Overall, we found that ARES ranked RAG systems more accurately than RAGAS and GPT-3.5 across all the explored datasets. The Kendall's tau for ARES was 0.065 higher on average for scoring context relevance and 0.132 higher on average for scoring answer relevance than RAGAS. Additionally, we include the Kendall's taus for the ARES LLM Judge without PPI and found that PPI further boosted the ranking accuracy of the judge across the board. We selected GPT-3.5 instead of GPT-4 due to the lower financial costs required to run. For PPI in both ARES and the GPT-3.5 judge, we used 300 human annotations for our human preference validation set. The prompts used for the GPT-3.5 judges are included in Sections A.2, A.3, and A.4. used ARES with human annotation sets ranging in size from 25 to 400 and found that 150 is the minimum number required (Table 3). Second, we explored whether GPT-4 generations could replace human annotations entirely, finding that GPT-4 is less good than humans in this role, though the idea arguably has promise (Table 4). 5.2 ARES Performance on AIS WoW CNN / DM ARES Split Prediction 0.478 0.835 Correct Positive/Negative Split 0.458 0.859 ARES Judge Accuracy 62.5% 84.0% Evaluation Set Size 707 510 Human Preference Data Size 200 200 Table 2: ARES Results on the AIS benchmark To evaluate whether ARES can effectively gauge answer faithfulness in real RAG systems, we tested ARES on the AIS attribution benchmark (Rashkin et al., 2022). In AIS, we selected the Wizards of Wikipedia (WoW) and CNN/DM datasets; the other benchmark datasets involve either table rea- soning (ToTTo) or focus on passage summariza- tion (QRECC) so we excluded them. In WoW and CNN/DM, each evaluation example includes a query, a retrieved passage, and a generated an- swer (which is either faithful or non-attributed to the retrieved passage). Table 2 summarizes our AIS results. We found that ARES can effectively score the AIS datasets, getting within 2.5 accuracy points of the correct scores. Furthermore, for scoring each system, we only use 200 annotated datapoints for our hu- man preference validation set. Our results on AIS demonstrate the ability of ARES to reliably dis- tinguish faithful and hallucinated answers in real- world RAG systems. 5.3 ARES Ranking of Existing RAG Systems We also wanted to evaluate whether ARES can score and rank existing RAG systems across both context relevance and answer",
      "chunk_index": 12
    },
    {
      "index": 27,
      "chunk_id": "ARES2023_chunk_13",
      "source_id": "ARES2023",
      "text": "to reliably dis- tinguish faithful and hallucinated answers in real- world RAG systems. 5.3 ARES Ranking of Existing RAG Systems We also wanted to evaluate whether ARES can score and rank existing RAG systems across both context relevance and answer relevance. For eval- uation, we selected the NQ, WoW, and FEVER datasets from KILT. We consider the answer gen- erations to be correct if they contained the KILT answer in their output. For our RAG systems, we selected three different retrievers (BM25, Ope- nAI Ada embeddings with cosine similarity search, and ColBERTv2 (Santhanam et al., 2022)) and three different generative LLMs (MPT-7b-Instruct (Team, 2023), GPT-3.5-Turbo, and GPT-4). Ad- ditionally, we include the Facebook RAG model (Lewis et al., 2020), which uses a DPR retriever (Karpukhin et al., 2020) and BART sequence-to- sequence model (Lewis et al., 2019). During re- trieval, each RAG system only retrieves one pas- sage to assist generation. In Table 5, we found that ARES can reliably score and rank RAG systems in real-world applica- tions, averaging a Kendall's tau of 0.91 for context relevance and 0.97 for answer relevance. Com- pared to RAGAS, ARES is 0.16 higher for context relevance and 0.15 higher for answer relevance, on average. ARES also provided accurate confidence bounds for its predictions, capturing the ground truth average outcomes for context relevance and answer relevance more than 95% of the time; on av- erage, the PPI confidence intervals were 7.4 points wide for context relevance and 6.1 points wide for answer relevance (see Figure 2 and Figure 3 for ARES vs. RAGAS). Among the models tested, the best performing retriever was ColBERTv2 while the best performing generative LLM was GPT-4. 5.4 Strengths and Limits of Cross-Domain Applications The generalizability of the LLM judge used in ARES is critical for deploying our framework in specialized domains, particularly domains where in-domain queries, documents, and answers are dif- ficult to gather. Therefore, we wanted to test how the LLM judges used in ARES would be affected by three domain shifts: change in query type from training to test (e.g. NQ to FEVER), change in document type from training to test (e.g. NQ to MultiRC), and change in both query and document type (e.g. NQ to ReCoRD). In Table 6, we found that the fine-tuned LLM judges used in ARES proved successful in cross- domain applications. Across all settings, we found that LLM judges in ARES",
      "chunk_index": 13
    },
    {
      "index": 28,
      "chunk_id": "ARES2023_chunk_14",
      "source_id": "ARES2023",
      "text": "change in both query and document type (e.g. NQ to ReCoRD). In Table 6, we found that the fine-tuned LLM judges used in ARES proved successful in cross- domain applications. Across all settings, we found that LLM judges in ARES had strong generaliz- ability, even when only using 300 datapoints in our human preference validation set for PPI. Further- more, we found that even when the LLM judge's ac- curacy suffered in cross-domain applications, PPI helped mitigate the loss in accuracy and still allow ARES to be successful. Additional examples for PPI also continued to boost cross-domain ARES performance in subsequent tests. While LLM judges in ARES were successful in cross-domain applications for KILT and Super- GLUE, LLM judges are unable to generalize when making more drastic shifts in domain, such as: switching languages (e.g. English to Spanish, Ger- man, and other languages), switching from text to code (e.g. questions + passages to coding functions + documentation), and switching from retrieving text to extraction of entities, webpages, or citations. To test cross-lingual transfer, we used the XGLUE datasets (Liang et al., 2020); a LLM judge fine-tuned on NQ achieved a Kendall's tau of 0.33 over both context relevance and answer relevance scoring for XGLUE. To test text-to-code, we used CodeSearchNet (Husain et al., 2019); an LLM judge fine-tuned on NQ achieved a Kendall's tau of 0.28 over both context relevance and answer relevance scoring for CodeSearchNet. To test ex- traction task generalizability, we used T-Rex from KILT (Elsahar et al., 2018; Petroni et al., 2021); an LLM judge fine-tuned on NQ achieved a Kendall's tau of 0.38 over both context relevance and answer relevance scoring for T-Rex. Each cross-domain shift requires in-domain passages and few-shot query examples for reconfiguring ARES judges. 6 Conclusion In this work, we present ARES, a novel automated evaluation framework for retrieval-augmented gen- eration (RAG). ARES offers a novel training pipeline for fine-tuning lightweight LLM judges on synthetically generated queries and answers. ARES can evaluate each component of a RAG sys- tem separately to help improve system understand- ing and create targeted solutions, and it requires only minimal human annotations. For the eight dif- ferent datasets in KILT, SuperGLUE, and AIS re- quiring RAG-based solutions, we found that ARES can accurately score and rank RAG systems based on context relevance, answer faithfulness, and an- swer relevance scores, beating the existing RAGAS automated evaluation framework. ARES is",
      "chunk_index": 14
    },
    {
      "index": 29,
      "chunk_id": "ARES2023_chunk_15",
      "source_id": "ARES2023",
      "text": "in KILT, SuperGLUE, and AIS re- quiring RAG-based solutions, we found that ARES can accurately score and rank RAG systems based on context relevance, answer faithfulness, and an- swer relevance scores, beating the existing RAGAS automated evaluation framework. ARES is a flexible framework, and there may be variants of it that are even more powerful than the ones we explored here. Avenues to explore include GPT-4 as a replacement for human labeling (Table 4), more robust techniques for the synthetic datasets used in fine-tuning LLM judges, utilizing logits in LLM judge prediction to improve PPI confidence intervals, and testing more sophisticated LLMs as fine-tuned judges for ARES. 7 Limitations ARES relies on a small set of annotations in the human preference validation set (roughly 150-300 datapoints but more is better). These annotations often require an annotator familiar with the RAG system's domain application. While these annota- tions can be easy to generate for general-domain applications, more specialized domains, such as law, medicine, and finance, may require annotators with specialized expertise. The LLMs used in ARES benefit substantially from GPU-based hardware with substantial stor- age. In ARES, DeBERTa-v3-Large (304M) and FLAN-T5-XXL (11.3B) required GPUs with about 32GB of memory to run, taking several hours for fine-tuning and generation, respectively. While commercial GPUs are widely available, they are not easily accessible to all NLP researchers and practitioners due to their costs. Additionally, all of the datasets used in our eval- uation of ARES are in English, a well-resourced language with abundant annotations. Future work should explore how ARES can be employed in other languages by utilizing different LLMs for the ARES judge and the synthetic data generation. This can help us better understand the strengths and weaknesses of the current ARES framework. References Mubashara Akhtar, Rami Aly, Christos Christodoulopoulos, Oana Cocarascu, Zhijiang Guo, Arpit Mittal, Michael Schlichtkrull, James Thorne, and Andreas Vlachos, editors. 2023. Proceedings of the Sixth Fact Extraction and VERification Workshop (FEVER). Association for Computational Linguistics, Dubrovnik, Croatia. Anastasios N. Angelopoulos, Stephen Bates, Clara Fan- njiang, Michael I. Jordan, and Tijana Zrnic. 2023. Prediction-powered inference. Tom B. Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared Kaplan, Prafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, Sandhini Agarwal, Ariel Herbert-V oss, Gretchen Krueger, Tom Henighan, Rewon Child, Aditya Ramesh, Daniel M. Ziegler, Jeffrey Wu, Clemens Winter, Christopher Hesse, Mark Chen, Eric Sigler, Mateusz Litwin, Scott Gray, Benjamin Chess, Jack Clark,",
      "chunk_index": 15
    },
    {
      "index": 30,
      "chunk_id": "ARES2023_chunk_16",
      "source_id": "ARES2023",
      "text": "Pranav Shyam, Girish Sastry, Amanda Askell, Sandhini Agarwal, Ariel Herbert-V oss, Gretchen Krueger, Tom Henighan, Rewon Child, Aditya Ramesh, Daniel M. Ziegler, Jeffrey Wu, Clemens Winter, Christopher Hesse, Mark Chen, Eric Sigler, Mateusz Litwin, Scott Gray, Benjamin Chess, Jack Clark, Christopher Berner, Sam McCandlish, Alec Radford, Ilya Sutskever, and Dario Amodei. 2020. Language models are few-shot learners. Jiawei Chen, Hongyu Lin, Xianpei Han, and Le Sun. 2023. Benchmarking large language models in retrieval-augmented generation. arXiv preprint arXiv:2309.01431. Hyung Won Chung, Le Hou, Shayne Longpre, Bar- ret Zoph, Yi Tay, William Fedus, Eric Li, Xuezhi Wang, Mostafa Dehghani, Siddhartha Brahma, et al. 2022. Scaling instruction-finetuned language models. arXiv preprint arXiv:2210.11416. Zhuyun Dai, Vincent Y Zhao, Ji Ma, Yi Luan, Jianmo Ni, Jing Lu, Anton Bakalov, Kelvin Guu, Keith B Hall, and Ming-Wei Chang. 2022. Promptagator: Few-shot dense retrieval from 8 examples. arXiv preprint arXiv:2209.11755. Emily Dinan, Stephen Roller, Kurt Shuster, Angela Fan, Michael Auli, and Jason Weston. 2018. Wizard of wikipedia: Knowledge-powered conversational agents. arXiv preprint arXiv:1811.01241. Hady Elsahar, Pavlos V ougiouklis, Arslen Remaci, Christophe Gravier, Jonathon Hare, Frederique Lafor- est, and Elena Simperl. 2018. T-rex: A large scale alignment of natural language with knowledge base triples. In Proceedings of the Eleventh International Conference on Language Resources and Evaluation (LREC 2018). Jinlan Fu, See-Kiong Ng, Zhengbao Jiang, and Pengfei Liu. 2023. Gptscore: Evaluate as you desire. arXiv preprint arXiv:2302.04166. Tianyu Gao, Howard Yen, Jiatong Yu, and Danqi Chen. 2023. Enabling large language models to generate text with citations. Zorik Gekhman, Jonathan Herzig, Roee Aharoni, Chen Elkind, and Idan Szpektor. 2023. Trueteacher: Learn- ing factual consistency evaluation with large lan- guage models. Kelvin Guu, Kenton Lee, Zora Tung, Panupong Pasu- pat, and Mingwei Chang. 2020. Retrieval augmented language model pre-training. In International confer- ence on machine learning, pages 3929-3938. PMLR. Pengcheng He, Jianfeng Gao, and Weizhu Chen. 2021. Debertav3: Improving deberta using electra-style pre- training with gradient-disentangled embedding shar- ing. arXiv preprint arXiv:2111.09543. Jeremy Howard and Sebastian Ruder. 2018. Universal language model fine-tuning for text classification. In Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pages 328-339, Melbourne, Australia. Association for Computational Linguistics. Siqing Huo, Negar Arabzadeh, and Charles LA Clarke. 2023. Retrieving supporting evidence for llms gener- ated answers. arXiv preprint arXiv:2306.13781. Hamel Husain, Ho-Hsiang Wu, Tiferet Gazit, Miltiadis Allamanis, and Marc Brockschmidt. 2019. Code- SearchNet challenge: Evaluating the state of seman- tic",
      "chunk_index": 16
    },
    {
      "index": 31,
      "chunk_id": "ARES2023_chunk_17",
      "source_id": "ARES2023",
      "text": "Huo, Negar Arabzadeh, and Charles LA Clarke. 2023. Retrieving supporting evidence for llms gener- ated answers. arXiv preprint arXiv:2306.13781. Hamel Husain, Ho-Hsiang Wu, Tiferet Gazit, Miltiadis Allamanis, and Marc Brockschmidt. 2019. Code- SearchNet challenge: Evaluating the state of seman- tic code search. arXiv preprint arXiv:1909.09436. Gautier Izacard, Patrick Lewis, Maria Lomeli, Lu- cas Hosseini, Fabio Petroni, Timo Schick, Jane Dwivedi-Yu, Armand Joulin, Sebastian Riedel, and Edouard Grave. 2022. Few-shot learning with re- trieval augmented language models. arXiv preprint arXiv:2208.03299. Jithin James and Shahul Es. 2023. Ragas: Evaluation framework for your retrieval augmented generation (rag) pipelines. Jeff Johnson, Matthijs Douze, and Hervé Jégou. 2019. Billion-scale similarity search with GPUs. IEEE Transactions on Big Data, 7(3):535-547. Ehsan Kamalloo, Aref Jafari, Xinyu Zhang, Nandan Thakur, and Jimmy Lin. 2023. Hagrid: A human- llm collaborative dataset for generative information- seeking with attribution. Vladimir Karpukhin, Barlas O˘guz, Sewon Min, Patrick Lewis, Ledell Wu, Sergey Edunov, Danqi Chen, and Wen tau Yih. 2020. Dense passage retrieval for open- domain question answering. Daniel Khashabi, Snigdha Chaturvedi, Michael Roth, Shyam Upadhyay, and Dan Roth. 2018. Looking beyond the surface: A challenge set for reading com- prehension over multiple sentences. In Proceedings of the 2018 Conference of the North American Chap- ter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long Pa- pers), pages 252-262. Omar Khattab, Christopher Potts, and Matei Zaharia. 2021. Relevance-guided supervision for openqa with colbert. Transactions of the association for computa- tional linguistics, 9:929-944. Diederik P. Kingma and Jimmy Ba. 2017. Adam: A method for stochastic optimization. Tom Kocmi and Christian Federmann. 2023. Large language models are state-of-the-art evaluators of translation quality. arXiv preprint arXiv:2302.14520. Kalpesh Krishna, Erin Bransom, Bailey Kuehl, Mohit Iyyer, Pradeep Dasigi, Arman Cohan, and Kyle Lo. 2023. LongEval: Guidelines for human evaluation of faithfulness in long-form summarization. In Proceed- ings of the 17th Conference of the European Chap- ter of the Association for Computational Linguistics, pages 1650-1669, Dubrovnik, Croatia. Association for Computational Linguistics. Tom Kwiatkowski, Jennimaria Palomaki, Olivia Red- field, Michael Collins, Ankur Parikh, Chris Alberti, Danielle Epstein, Illia Polosukhin, Jacob Devlin, Ken- ton Lee, et al. 2019. Natural questions: a benchmark for question answering research. Transactions of the Association for Computational Linguistics , 7:453- 466. Mike Lewis, Yinhan Liu, Naman Goyal, Marjan Ghazvininejad, Abdelrahman Mohamed, Omer Levy, Ves Stoyanov, and Luke Zettlemoyer. 2019. Bart: De- noising sequence-to-sequence pre-training for natural language generation, translation, and comprehension. Patrick Lewis,",
      "chunk_index": 17
    },
    {
      "index": 32,
      "chunk_id": "ARES2023_chunk_18",
      "source_id": "ARES2023",
      "text": "the Association for Computational Linguistics , 7:453- 466. Mike Lewis, Yinhan Liu, Naman Goyal, Marjan Ghazvininejad, Abdelrahman Mohamed, Omer Levy, Ves Stoyanov, and Luke Zettlemoyer. 2019. Bart: De- noising sequence-to-sequence pre-training for natural language generation, translation, and comprehension. Patrick Lewis, Ethan Perez, Aleksandra Piktus, Fabio Petroni, Vladimir Karpukhin, Naman Goyal, Hein- rich Küttler, Mike Lewis, Wen-tau Yih, Tim Rock- täschel, et al. 2020. Retrieval-augmented generation for knowledge-intensive nlp tasks. Advances in Neu- ral Information Processing Systems, 33:9459-9474. Yaobo Liang, Nan Duan, Yeyun Gong, Ning Wu, Fenfei Guo, Weizhen Qi, Ming Gong, Linjun Shou, Daxin Jiang, Guihong Cao, Xiaodong Fan, Ruofei Zhang, Rahul Agrawal, Edward Cui, Sining Wei, Taroon Bharti, Ying Qiao, Jiun-Hung Chen, Winnie Wu, Shuguang Liu, Fan Yang, Daniel Campos, Rangan Majumder, and Ming Zhou. 2020. Xglue: A new benchmark dataset for cross-lingual pre-training, un- derstanding and generation. arXiv, abs/2004.01401. Yang Liu, Dan Iter, Yichong Xu, Shuohang Wang, Ruochen Xu, and Chenguang Zhu. 2023a. G-eval: Nlg evaluation using gpt-4 with better human align- ment, may 2023. arXiv preprint arXiv:2303.16634. Yuxuan Liu, Tianchi Yang, Shaohan Huang, Zihan Zhang, Haizhen Huang, Furu Wei, Weiwei Deng, Feng Sun, and Qi Zhang. 2023b. Calibrating llm- based evaluator. arXiv preprint arXiv:2309.13308. Grégoire Mialon, Roberto Dessì, Maria Lomeli, Christo- foros Nalmpantis, Ram Pasunuru, Roberta Raileanu, Baptiste Rozière, Timo Schick, Jane Dwivedi-Yu, Asli Celikyilmaz, Edouard Grave, Yann LeCun, and Thomas Scialom. 2023. Augmented language mod- els: a survey. Sewon Min, Kalpesh Krishna, Xinxi Lyu, Mike Lewis, Wen tau Yih, Pang Wei Koh, Mohit Iyyer, Luke Zettlemoyer, and Hannaneh Hajishirzi. 2023. Factscore: Fine-grained atomic evaluation of factual precision in long form text generation. Fabio Petroni, Aleksandra Piktus, Angela Fan, Patrick Lewis, Majid Yazdani, Nicola De Cao, James Thorne, Yacine Jernite, Vladimir Karpukhin, Jean Maillard, Vassilis Plachouras, Tim Rocktäschel, and Sebastian Riedel. 2021. KILT: a benchmark for knowledge intensive language tasks. In Proceedings of the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, pages 2523-2544, Online. Association for Computational Linguistics. Hannah Rashkin, Vitaly Nikolaev, Matthew Lamm, Lora Aroyo, Michael Collins, Dipanjan Das, Slav Petrov, Gaurav Singh Tomar, Iulia Turc, and David Reitter. 2022. Measuring attribution in natural lan- guage generation models. Jon Saad-Falcon, Omar Khattab, Keshav Santhanam, Radu Florian, Martin Franz, Salim Roukos, Avirup Sil, Md Arafat Sultan, and Christopher Potts. 2023. Udapdr: Unsupervised domain adaptation via llm prompting and distillation of rerankers. arXiv preprint arXiv:2303.00807. David P Sander and",
      "chunk_index": 18
    },
    {
      "index": 33,
      "chunk_id": "ARES2023_chunk_19",
      "source_id": "ARES2023",
      "text": "models. Jon Saad-Falcon, Omar Khattab, Keshav Santhanam, Radu Florian, Martin Franz, Salim Roukos, Avirup Sil, Md Arafat Sultan, and Christopher Potts. 2023. Udapdr: Unsupervised domain adaptation via llm prompting and distillation of rerankers. arXiv preprint arXiv:2303.00807. David P Sander and Laura Dietz. 2021. Exam: How to evaluate retrieve-and-generate systems for users who do not (yet) know what they want. In DESIRES, pages 136-146. Keshav Santhanam, Omar Khattab, Jon Saad-Falcon, Christopher Potts, and Matei Zaharia. 2022. Col- BERTv2: Effective and efficient retrieval via lightweight late interaction. In Proceedings of the 2022 Conference of the North American Chapter of the Association for Computational Linguistics: Hu- man Language Technologies, pages 3715-3734, Seat- tle, United States. Association for Computational Linguistics. Kurt Shuster, Spencer Poff, Moya Chen, Douwe Kiela, and Jason Weston. 2021. Retrieval augmentation reduces hallucination in conversation. MosaicML NLP Team. 2023. Introducing mpt-30b: Raising the bar for open-source foundation models. Accessed: 2023-06-22. Alex Wang, Yada Pruksachatkun, Nikita Nangia, Aman- preet Singh, Julian Michael, Felix Hill, Omer Levy, and Samuel Bowman. 2019. Superglue: A stick- ier benchmark for general-purpose language under- standing systems. Advances in neural information processing systems, 32. Jiaan Wang, Yunlong Liang, Fandong Meng, Haoxiang Shi, Zhixu Li, Jinan Xu, Jianfeng Qu, and Jie Zhou. 2023. Is chatgpt a good nlg evaluator? a preliminary study. arXiv preprint arXiv:2303.04048. Zhilin Yang, Peng Qi, Saizheng Zhang, Yoshua Ben- gio, William W Cohen, Ruslan Salakhutdinov, and Christopher D Manning. 2018. Hotpotqa: A dataset for diverse, explainable multi-hop question answer- ing. arXiv preprint arXiv:1809.09600. Xiang Yue, Boshi Wang, Ziru Chen, Kai Zhang, Yu Su, and Huan Sun. 2023. Automatic evaluation of attri- bution by large language models. Sheng Zhang, Xiaodong Liu, Jingjing Liu, Jianfeng Gao, Kevin Duh, and Benjamin Van Durme. 2018. Record: Bridging the gap between human and ma- chine commonsense reading comprehension. arXiv preprint arXiv:1810.12885. Lianmin Zheng, Wei-Lin Chiang, Ying Sheng, Siyuan Zhuang, Zhanghao Wu, Yonghao Zhuang, Zi Lin, Zhuohan Li, Dacheng Li, Eric Xing, et al. 2023. Judging llm-as-a-judge with mt-bench and chatbot arena. arXiv preprint arXiv:2306.05685. A Appendix A.1 Fine-tuning Configuration for LLM Judges For our loss function used in LLM judge train- ing, we selected cross-entropy loss using Adam (Kingma and Ba, 2017). For our classification head, we use a single linear classification layer and ap- ply a 0.1 dropout to the input, which is the final hidden state of the [CLS] token. For our learning schedule, we use linear warmup",
      "chunk_index": 19
    },
    {
      "index": 34,
      "chunk_id": "ARES2023_chunk_20",
      "source_id": "ARES2023",
      "text": "Ba, 2017). For our classification head, we use a single linear classification layer and ap- ply a 0.1 dropout to the input, which is the final hidden state of the [CLS] token. For our learning schedule, we use linear warmup and linear decay (Howard and Ruder, 2018) with a 5e-6 learning rate and a 32 training batch size across all experimental configurations. A.2 GPT Prompting for Context Relevance Scoring For the NQ, HotpotQA, MultiRC, and ReCoRD datasets, we use 8 few-shot examples with the fol- lowing prompt to score context relevance: • Given the following question and document, you must analyze the provided document and determine whether it is sufficient for answer- ing the question. In your evaluation, you should consider the content of the document and how it relates to the provided question. Output your final verdict by strictly following this format: \"[[Yes]]\" if the document is suffi- cient and \"[[No]]\" if the document provided is not sufficient. Do not provide any additional explanation for your decision. Question: <few-shot example here> Document: <few-shot example here> For FEVER, we use the following prompt to score context relevance: • You are an expert fact-checking agent. Given the following statement and document, you must analyze the provided document and de- termine whether it is sufficient for determining the statement's factuality. In your evaluation, you should consider the content of the docu- ment and how it relates to the provided state- ment's factuality. Output your final verdict by strictly following this format: \"[[Yes]]\" if the document is sufficient and \"[[No]]\" if the document is not sufficient. Do not provide any additional explanation for your decision. Statement: <few-shot example here> Document: <few-shot example here> For WoW, we use the following prompt to score context relevance: • You are an expert dialogue agent. Given the following dialogue and document, you must analyze the provided document and determine whether it is relevant for responding to the dialogue. In your evaluation, you should con- sider the content of the document and how it relates to the provided dialogue. Output your final verdict by strictly following this format: \"[[Yes]]\" if the document is relevant and \"[[No]]\" if the document provided is not relevant. Do not provide any additional expla- nation for your decision. Dialogue: <few-shot example here> Document: <few-shot example here> A.3 GPT Prompting for Answer Faithfulness Scoring For the NQ, HotpotQA, MultiRC, and ReCoRD datasets, we use",
      "chunk_index": 20
    },
    {
      "index": 35,
      "chunk_id": "ARES2023_chunk_21",
      "source_id": "ARES2023",
      "text": "document provided is not relevant. Do not provide any additional expla- nation for your decision. Dialogue: <few-shot example here> Document: <few-shot example here> A.3 GPT Prompting for Answer Faithfulness Scoring For the NQ, HotpotQA, MultiRC, and ReCoRD datasets, we use 8 few-shot examples with the fol- lowing prompt to score answer faithfulness: • Given the following question, document, and answer, you must analyze the provided answer and determine whether it is faithful to the con- tents of the document. The answer must not offer new information beyond the context pro- vided in the document. The answer also must not contradict information provided in the doc- ument. Output your final verdict by strictly following this format: \"[[Yes]]\" if the answer is faithful to the document and \"[[No]]\" if the answer is not faithful to the document. Do not provide any additional explanation for your decision. Question: <few-shot example here> Document: <few-shot example here> Answer: <few-shot example here> For FEVER, we change the word \"question\" in the prompt to \"statement\". For WoW, we change the word \"question\" in the prompt to \"dialogue\". A.4 GPT Prompting for Answer Relevance Scoring For the NQ, HotpotQA, MultiRC, and ReCoRD datasets, we use 8 few-shot examples with the fol- lowing prompt to score answer relevance: • Given the following question, document, and answer, you must analyze the provided answer and document before determining whether the answer is relevant for the provided ques- tion. In your evaluation, you should consider whether the answer addresses all aspects of the question and provides only correct infor- mation from the document for answering the question. Output your final verdict by strictly following this format: \"[[Yes]]\" if the answer is relevant for the given question and \"[[No]]\" if the answer is not relevant for the given ques- tion. Do not provide any additional explana- tion for your decision. Question: <few-shot example here> Document: <few-shot example here> Answer: <few-shot example here> For FEVER, we change the word \"question\" in the prompt to \"statement\". For WoW, we change the word \"question\" in the prompt to \"dialogue\". A.5 Prompting for Generation of Synthetic Queries and Answers To generate synthetic queries and answers using FLAN-T5, we use the following prompt and pro- vide 5 few-shot examples: • Example N Question: <few-shot example here> Document: <few-shot example here> Answer: <few-shot example here> We use the same prompting structure for gener- ating incorrect or contradictory answers; we",
      "chunk_index": 21
    },
    {
      "index": 36,
      "chunk_id": "ARES2023_chunk_22",
      "source_id": "ARES2023",
      "text": "we use the following prompt and pro- vide 5 few-shot examples: • Example N Question: <few-shot example here> Document: <few-shot example here> Answer: <few-shot example here> We use the same prompting structure for gener- ating incorrect or contradictory answers; we simply swap out the few-shot examples to be incorrect or contradictory instead. A.6 Synthetic Query and Answer Generation For generating our synthetic questions, we use the following prompt for FLAN-T5 XXL: • Example #1 Document: <few-shot example here> Query: <few-shot example here> Example #2 Document: <few-shot example here> Query: <few-shot example here> Example #3 Document: <few-shot example here> Query: <few-shot example here> Example #4 Document: <in-domain passage> Query: For generating our synthetic answers, we use the following prompt for FLAN-T5 XXL: • Example #1 Query: <few-shot example here> Document: <few-shot example here> Answer: <few-shot example here> Example #2 Query: <few-shot example here> Document: <few-shot example here> Answer: <few-shot example here> Example #3 Query: <few-shot example here> Document: <few-shot example here> Answer: <few-shot example here> Example #4 Query: <synthetic query here> Document: <in-domain passage here> Answer: Figure 2: RAG Systems Evaluation on NQ - Context Relevance Figure 3: RAG Systems Evaluation on NQ - Answer Relevance Kendall's Tau by Dataset NQ MultiRC ReCoRD PPI Labeled Count C.R. A.R. C.R. A.R. C.R. A.R. 400 1.0 1.0 0.89 0.94 0.89 0.94 300 0.89 1.0 0.94 0.89 0.83 0.89 200 0.83 1.0 0.83 0.94 0.83 0.83 150 0.72 1.0 0.83 0.89 0.72 0.83 100 0.44 1.0 0.67 0.67 0.67 0.83 50 0.44 0.94 0.61 0.44 0.56 0.67 25 0.44 0.89 0.56 0.44 0.44 0.56 Table 3: Analysis of PPI Labeled Count vs. ARES Efficacy by Kendall's Tau: The Kendall's tau values represent the correlation between the correct ranking and the ARES ranking of the pseudo RAG systems. We use the same experimental set-up as described in subsection 4.2. We find that below about 100-150 datapoints in the human preference validation set, ARES cannot meaningfully distinguish between the alternate RAG systems based on their accuracies in context relevance and answer relevance (C.R. and A.R., respectively). ARES Ranking of Pseudo RAG Systems using GPT-4 Labels NQ ReCoRD MultiRC Context Relevance Answer Relevance Context Relevance Answer Relevance Context Relevance Answer Relevance Kendall's Tau 0.78 1.0 0.78 0.72 0.89 0.78 Kendall's Tau of Human Labeled Approach 0.94 1.0 0.83 0.89 0.94 0.89 Average PPI Range 9.2% 6.8% 8.2% 9.0% 7.7% 8.3% Accuracy on RAG Evaluation Sets 79.3% 96.7%",
      "chunk_index": 22
    },
    {
      "index": 37,
      "chunk_id": "ARES2023_chunk_23",
      "source_id": "ARES2023",
      "text": "Context Relevance Answer Relevance Kendall's Tau 0.78 1.0 0.78 0.72 0.89 0.78 Kendall's Tau of Human Labeled Approach 0.94 1.0 0.83 0.89 0.94 0.89 Average PPI Range 9.2% 6.8% 8.2% 9.0% 7.7% 8.3% Accuracy on RAG Evaluation Sets 79.3% 96.7% 88.4% 78.3% 85.8% 82.5% Table 4: GPT-4 Labels vs. Human Labels : We wanted to explore the practicality of using GPT-4 generated labels instead of human annotations for our human preference validation set in ARES. In the experiments, we generated 500 GPT-4 labels as replacements for human labeling using few-shot prompts (see Sections A.2, A.3, and A.4). While GPT-4 generated labels decreased Kendall's tau in most settings by 0.05 to 0.30, the ability to cheaply produce GPT-4 generated labels significantly reduces the cost of annotation, cutting it from hundreds of annotations to less than ten for few-shot prompts. Additionally, the efficacy of PPI continues improving as we generate more GPT-4 generated labels. In the table, we define PPI range as the number of percentage points from the lower number to the upper number of the PPI confidence bounding. Additionally, we use the fine-tuned LLM judge (DeBERTa-v3-Large) for evaluation. ARES Ranking of Real RAG Systems NQ WoW FEVER C.R. A.R. C.R. A.R. C.R. A.R. Kendall's Tau for Sampled Annotations 0.73 0.78 0.73 0.73 0.73 0.82 Kendall's Tau for RAGAS 0.82 0.82 0.73 0.82 0.73 0.87 Kendall's Tau for GPT-3.5 Judge 0.82 0.87 0.82 0.82 0.64 0.87 Kendall's Tau for ARES LLM Judge 0.91 0.96 0.91 1.0 0.73 0.87 Kendall's Tau for ARES 1.0 0.96 0.91 1.0 0.82 1.0 RAGAS Accuracy 35.9% 68.2% 44.4% 80.1% 21.4% 75.9% GPT-3.5 Accuracy 80.5% 91.2% 81.2% 83.5% 61.3% 54.5% ARES Accuracy 85.6% 93.3% 84.5% 88.2% 70.4% 84.0% Table 5: ARES Ranking on Real-World RAG Systems: For scoring context relevance and answer relevance (C.R. and A.R. in the table, respectively), we compare ARES with our fine-tuned LLM judges against sampled annotations benchmark, RAGAS, and a few-shot GPT-3.5 judge. For our sampled annotations, we gather 150 annotated datapoints from each mock RAG system and use those labels to score the system. RAGAS also uses GPT-3.5 as its judge but it uses few-shot prompts that are not targeted for each evaluation domain. Overall, we found that ARES ranked RAG systems more accurately than RAGAS and GPT-3.5 across all the explored datasets. Additionally, we include the Kendall's taus for the ARES LLM Judge without PPI and found that PPI further",
      "chunk_index": 23
    },
    {
      "index": 38,
      "chunk_id": "ARES2023_chunk_24",
      "source_id": "ARES2023",
      "text": "each evaluation domain. Overall, we found that ARES ranked RAG systems more accurately than RAGAS and GPT-3.5 across all the explored datasets. Additionally, we include the Kendall's taus for the ARES LLM Judge without PPI and found that PPI further boosted the ranking accuracy of the judge across the board. We selected GPT-3.5 instead of GPT-4 due to the lower financial costs required to run. For PPI in both ARES and the GPT-3.5 judge, we used 300 human annotations for our human preference validation set. The prompts used for the GPT-3.5 judges are included in Sections A.2, A.3, and A.4. ARES Cross-Domain Ranking of Pseudo RAG Systems NQ to FEVER FEVER to NQ NQ to MultiRC MultiRC to NQ NQ to ReCoRD ReCoRD to NQ C.R. A.R. C.R. A.R. C.R. A.R. C.R. A.R. C.R. A.R. C.R. A.R. Kendall's Tau 0.89 0.89 1.0 0.83 0.94 0.89 1.0 0.89 0.78 0.89 0.89 0.94 Kendall's Tau of In-Domain LLM Judge 0.89 0.78 0.94 1.0 0.94 0.89 0.94 1.0 0.83 0.89 0.94 1.0 Average PPI Range 8.7% 7.2% 6.5% 11.5% 10.2% 11.3% 11.9% 11.5% 10.5% 10.1% 9.7% 6.2% Accuracy on RAG Evaluation Sets 92.4% 28.4% 85.7% 22.6% 81.5% 92.1% 87.6% 80.2% 29.1% 81.2% 80.1% 92.1% Table 6: Cross-Domain Usage of Fine-tuned LLM Judges : We tested the cross-domain application of the fine-tuned LLM judge in the ARES framework. We found that for both context relevance and answer relevance (C.R. and A.R. in the table, respectively), fine-tuned LLM judges showed strong generalizability across domains when changing query type (e.g. NQ and FEVER), document type (e.g. NQ and MultiRC), or both (e.g. NQ and ReCoRD). For PPI, we used 300 labeled examples for our human preference validation set but also found that additional examples further improved the performance of ARES. Furthermore, we found that even in scenarios where the fine-tuned LLM judge's accuracy significantly dropped out-of-domain (e.g. answer relevance for NQ to FEVER), PPI mitigated the decrease in judge performance. In the table, we define PPI range as the number of percentage points from the lower bound to the upper bound of the PPI confidence interval. Query Passage Answer Context Relevance Answer Relevance How can a ball that is not moving possess energy of position? Mechanical energy is a combination of the energy of motion or position. This type of energy describes objects that are moving or could move. A moving ball can have energy from",
      "chunk_index": 24
    },
    {
      "index": 39,
      "chunk_id": "ARES2023_chunk_25",
      "source_id": "ARES2023",
      "text": "ball that is not moving possess energy of position? Mechanical energy is a combination of the energy of motion or position. This type of energy describes objects that are moving or could move. A moving ball can have energy from motion. An arrow can also have the energy of motion. Both are types of mechanical energy. The ball holds mechanical energy 1 1 Who has a Jimmy Stewart-like quality of quiet trust? One look at Fred Rooney, and you just know he's the good guy. A trace of childish innocence in his face gives the lanky Bethlehem lawyer a Jimmy Stewart-like quality of quiet trust. In black jeans and button-down shirt, he's a kind of folk hero in the south Bethlehem melting pot where he's crafted a law practice catering to working-class families - mostly Latino - in the shadow of the hulkish remnants of Bethlehem Steel. Fred Rooney 1 1 Before he murder the doctor and Ralph Smith, where did the stepfather reside? Surviving being shot and stabbed at the end of the previous film , the stepfather has been institutionalized in Puget Sound, Washington since , spending his time building model houses in the workshop. Assigned a new doctor named Joseph Danvers the stepfather begins confiding in him to gain his trust , ultimately murdering the doctor during a session by stabbing him in the neck with a blade smuggled out of the workshop . After killing Danvers the stepfather beats a suspicious guard named Ralph Smith to death with his own nightstick with only two strikes and takes his uniform , successfully sneaking out of the sanitarium . Checking into a hotel after robbing and murdering a traveling salesman the stepfather alters his appearance , takes the name Doctor Gene F. Clifford from the newspaper obituaries and travels to Palm Meadows , Los Angeles after seeing an ad for it on an episode of Dream House . Los Angeles 1 0 What was the name of the 2006 film about Pushkin's death, and who portrayed Pushkin? After arriving in New York City, Einstein was taken to various places and events, including Chinatown, a lunch with the editors of the New York Times, and a performance of Carmen at the Metropolitan Opera, where he was cheered by the audience on his arrival. During the days following, he was given the keys to the city by Mayor Jimmy Walker",
      "chunk_index": 25
    },
    {
      "index": 40,
      "chunk_id": "ARES2023_chunk_26",
      "source_id": "ARES2023",
      "text": "of the New York Times, and a performance of Carmen at the Metropolitan Opera, where he was cheered by the audience on his arrival. During the days following, he was given the keys to the city by Mayor Jimmy Walker and met the president of Columbia University, who described Einstein as \"The ruling monarch of the mind.\" Harry Emerson Fosdick, pastor at New York's Riverside Church, gave Einstein a tour of the church and showed him a full-size statue that the church made of Einstein, standing at the entrance. Vasily Szaitsev portrayed Pushkin in the film Pushkin Returns 0 0 Table 7: Positive and Negatives Evaluation Examples",
      "chunk_index": 26
    },
    {
      "index": 41,
      "chunk_id": "RAG_Eval_Survey2024_chunk_00",
      "source_id": "RAG_Eval_Survey2024",
      "text": "Evaluation of Retrieval-Augmented Generation: A Survey Hao Yu1,2, Aoran Gan3, Kai Zhang3, Shiwei Tong1†, Qi Liu3, and Zhaofeng Liu1 1 Tencent Company 2 McGill University 3 State Key Laboratory of Cognitive Intelligence, University of Science and Technology of China hao.yu2@mail.mcgill.ca gar@mail.ustc.edu.cn {shiweitong†,zhaofengliu}@tencent.com {kkzhang08,qiliuql}@ustc.edu.cn Abstract. Retrieval-Augmented Generation (RAG) has recently gained traction in natural language processing. Numerous studies and real-world applications are leveraging its ability to enhance generative models through external informa- tion retrieval. Evaluating these RAG systems, however, poses unique challenges due to their hybrid structure and reliance on dynamic knowledge sources. To better understand these challenges, we conduct A Unified Evaluation Process of RAG (Auepora) and aim to provide a comprehensive overview of the evaluation and benchmarks of RAG systems. Specifically, we examine and compare several quantifiable metrics of the Retrieval and Generation components, such as rele- vance, accuracy, and faithfulness, within the current RAG benchmarks, encom- passing the possible output and ground truth pairs. We then analyze the various datasets and metrics, discuss the limitations of current benchmarks, and suggest potential directions to advance the field of RAG benchmarks. 1 Introduction Retrieval-Augmented Generation (RAG) [34] efficiently enhances the performance of generative language models through integrating information retrieval techniques. It ad- dresses a critical challenge faced by standalone generative language models: the ten- dency to produce responses that, while plausible, may not be grounded in facts. By retrieving relevant information from external sources, RAG significantly reduces the incidence of hallucinations [23] or factually incorrect outputs, thereby improving the content's reliability and richness. [73] This fusion of retrieval and generation capabil- ities enables the creation of responses that are not only contextually appropriate but also informed by the most current and accurate information available, making RAG a development in the pursuit of more intelligent and versatile language models [73,64]. † Corresponding Author Paper Homepage: https://github.com/YHPeter/Awesome-RAG-Evaluation arXiv:2405.07437v2 [cs.CL] 3 Jul 2024 2 Hao Yu, Aoran Gan, Kai Zhang, Shiwei Tong †, Qi Liu, and Zhaofeng Liu Numerous studies of RAG systems have emerged from various perspectives since the advent of Large Language Models (LLMs) [55,45,59,42,41,69,16]. The RAG sys- tem comprises two primary components: Retrieval and Generation. The retrieval com- ponent aims to extract relevant information from various external knowledge sources. It involves two main phases, indexing and searching. Indexing organizes documents to facilitate efficient retrieval, using either inverted indexes for sparse retrieval or dense vector encoding for dense retrieval [16,12,28]. The",
      "chunk_index": 0
    },
    {
      "index": 42,
      "chunk_id": "RAG_Eval_Survey2024_chunk_01",
      "source_id": "RAG_Eval_Survey2024",
      "text": "to extract relevant information from various external knowledge sources. It involves two main phases, indexing and searching. Indexing organizes documents to facilitate efficient retrieval, using either inverted indexes for sparse retrieval or dense vector encoding for dense retrieval [16,12,28]. The searching component utilizes these indexes to fetch relevant documents on the user's query, often incorporating the op- tional rerankers [4,39,6,52] to refine the ranking of the retrieved documents. The gener- ation component utilizes the retrieved content and question query to formulate coherent and contextually relevant responses with the prompting and inferencing phases. As the \"Emerging\" ability [59] of LLMs and the breakthrough in aligning human commands [42], LLMs are the best performance choices model for the generation stage. Prompt- ing methods like Chain of Thought (CoT) [60], Tree of Thgouht [65], Rephrase and Respond (RaR) [8] guide better generation results. In the inferencing step, LLMs inter- pret the prompted input to generate accurate and in-depth responses that align with the query's intent and integrate the extracted information [35,9] without further finetuning, such as fully finetuning [16,1,67,68] or LoRA [21]. Appendix A details the complete RAG structure. Figure 1 illustrates the structure of the RAG systems as mentioned. Fig. 1: The structure of the RAG system with retrieval and generation components and corresponding four phrases: indexing, search, prompting and inferencing. The pairs of \"Evaluable Outputs\" (EOs) and \"Ground Truths\" (GTs) are highlighted in read frame and green frame, with brown dashed arrows. The importance of evaluating RAG is increasing in parallel with the advancement of RAG-specific methodologies. On the one hand, RAG is a complex system intricately tied to specific requirements and language models, resulting in various evaluation meth- ods, indicators, and tools, particularly given the black-box LLM generation. Evaluating RAG systems involves specific components and the complexity of the overall system assessment. On the other hand, the complexity of RAG systems is further compounded Evaluation of Retrieval-Augmented Generation: A Survey 3 by the external dynamic database and the various downstream tasks, such as content creation or open domain question answering [16,70]. These challenges necessitate the development of comprehensive evaluation metrics that can effectively capture the in- terplay between retrieval accuracy and generative quality [2,7]. To clarify the elements further, we try to address the current gaps in the area, which differs from the prior RAG surveys [74,16,24] that predominantly collected specific RAG methods or data. We have compiled 12 distinct",
      "chunk_index": 1
    },
    {
      "index": 43,
      "chunk_id": "RAG_Eval_Survey2024_chunk_02",
      "source_id": "RAG_Eval_Survey2024",
      "text": "and generative quality [2,7]. To clarify the elements further, we try to address the current gaps in the area, which differs from the prior RAG surveys [74,16,24] that predominantly collected specific RAG methods or data. We have compiled 12 distinct evaluation frameworks, encompassing a range of aspects of the RAG system. Following the procedure of making benchmarks, we analyze through tar- gets, datasets and metrics mentioned in these benchmarks and summarize them into A Unified Evaluation Process of RAG (Auepora) as three corresponding phases. For this paper, we contribute in the following aspects: 1. Challenge of Evaluation : This is the first work that summarizes and classifies the challenges in evaluating RAG systems through the structure of RAG systems, including three parts retrieval, generation, and the whole system. 2. Analysis Framework: In light of the challenges posed by RAG systems, we intro- duce an analytical framework, referred to as A Unified Evaluation Process of RAG (Auepora), which aims to elucidate the unique complexities inherent to RAG sys- tems and guide for readers to comprehend the effectiveness of RAG benchmarks across various dimensions 3. RAG Benchmark Analysis: With the help of Auepora, we comprehensively an- alyze existing RAG benchmarks, highlighting their strengths and limitations and proposing recommendations for future developments in RAG system evaluation. 2 Challenges in Evaluating RAG Systems Evaluating hybrid RAG systems entails evaluating retrieval, generation and the RAG system as a whole. These evaluations are multifaceted, requiring careful consideration and analysis. Each of them encompasses specific difficulties that complicate the devel- opment of a comprehensive evaluation framework and benchmarks for RAG systems. Retrieval The retrieval component is critical for fetching relevant information that in- forms the generation process. One primary challenge is the dynamic and vast nature of potential knowledge bases, ranging from structured databases to the entire web. This vastness requires evaluation metrics that can effectively measure the precision, recall, and relevance of retrieved documents in the context of a given query [52,32]. More- over, the temporal aspect of information, where the relevance and accuracy of data can change over time, adds another layer of complexity to the evaluation process [6]. Addi- tionally, the diversity of information sources and the possibility of retrieving misleading or low-quality information pose significant challenges in assessing the effectiveness of filtering and selecting the most pertinent information [39]. The traditional evaluation indicators for retrieval, such as Recall and Precision, cannot fully capture",
      "chunk_index": 2
    },
    {
      "index": 44,
      "chunk_id": "RAG_Eval_Survey2024_chunk_03",
      "source_id": "RAG_Eval_Survey2024",
      "text": "sources and the possibility of retrieving misleading or low-quality information pose significant challenges in assessing the effectiveness of filtering and selecting the most pertinent information [39]. The traditional evaluation indicators for retrieval, such as Recall and Precision, cannot fully capture the nuances of RAG retrieval systems, necessitating the development of more nuanced and task- specific evaluation metrics [49]. 4 Hao Yu, Aoran Gan, Kai Zhang, Shiwei Tong †, Qi Liu, and Zhaofeng Liu Generation The generation component, powered by LLMs, produces coherent and contextually appropriate responses based on the retrieved content. The challenge here lies in evaluating the faithfulness and accuracy of the generated content to the input data. This involves not only assessing the factual correctness of responses but also their relevance to the original query and the coherence of the generated text [75,49]. The subjective nature of certain tasks, such as creative content generation or open-ended question answering, further complicates the evaluation, as it introduces variability in what constitutes a \"correct\" or \"high-quality\" response [48]. RAG System as a WholeEvaluating the whole RAG system introduces additional com- plexities. The interplay between the retrieval and generation components means that the entire system's performance cannot be fully understood by evaluating each com- ponent in isolation [49,14]. The system needs to be assessed on its ability to leverage retrieved information effectively to improve response quality, which involves measur- ing the added value of the retrieval component to the generative process. Furthermore, practical considerations such as response latency and the ability to handle ambiguous or complex queries are also crucial for evaluating the system's overall effectiveness and usability [39,6]. Conclusion Evaluating the target shift from traditional absolute numeric metrics to multi-source and multi-target generation evaluation, along with the intricate interplay between retrieval and generation components, poses significant challenges. [5,50] Searches in a dynamic database may lead to misleading results or contradict the facts. Diverse and comprehensive datasets that accurately reflect real-world scenarios are crucial. Chal- lenges also arise in the realm of metrics, encompassing generative evaluation criteria for distinct downstream tasks, human preferences, and practical considerations within the RAG system. Most prior benchmarks predominantly tackle one or several aspects of the RAG assessment but lack a comprehensive, holistic analysis. 3 A Unified Evaluation Process of RAG ( Auepora) To facilitate a deeper understanding of RAG benchmarks, we introduceA Unified Eval- uation Process of RAG(Auepora), which focuses on three key questions of benchmarks:",
      "chunk_index": 3
    },
    {
      "index": 45,
      "chunk_id": "RAG_Eval_Survey2024_chunk_04",
      "source_id": "RAG_Eval_Survey2024",
      "text": "assessment but lack a comprehensive, holistic analysis. 3 A Unified Evaluation Process of RAG ( Auepora) To facilitate a deeper understanding of RAG benchmarks, we introduceA Unified Eval- uation Process of RAG(Auepora), which focuses on three key questions of benchmarks: What to Evaluate? How to Evaluate? How to Measure? which correlated to Target, Dataset, and Metric respectively. We aim to provide a clear and accessible way for readers to comprehend the complexities and nuances of RAG benchmarking. The Target module is intended to determine the evaluation direction. The Dataset module facilitates the comparison of various data constructions in RAG benchmarks. The final module, Metrics, introduces the metrics that correspond to specific targets and datasets used during evaluation. Overall, it is designed to provide a systematic method- ology for assessing the effectiveness of RAG systems across various aspects by covering all possible pairs at the beginning between the \"Evaluable Outputs\" (EOs) and \"Ground Truths\" (GTs). In the following section, we will explain thoroughlyAueporaand utilize it for introducing and comparing the RAG benchmarks. Evaluation of Retrieval-Augmented Generation: A Survey 5 Fig. 2: The Target modular of the Auepora. 3.1 Evaluation Target ( What to Evaluate?) The combination of EOs and GTs in the RAG system can generate all possible targets, which is the fundamental concept of the Auepora (as shown in Figure 1). Once iden- tified, these targets can be defined based on a specific pair of EOs or EO with GT, as illustrated in Figure 2, and used to analyze all aspects of current RAG benchmarks. Retrieval The EOs are the relevant documents for evaluating the retrieval component depending on the query. Then we can construct two pairwise relationships for the re- trieval component, which are Relevant Documents ↔ Query, Relevant Documents ↔ Documents Candidates. - Relevance (Relevant Documents ↔ Query) evaluates how well the retrieved docu- ments match the information needed expressed in the query. It measures the preci- sion and specificity of the retrieval process. - Accuracy (Relevant Documents ↔ Documents Candidates) assesses how accurate the retrieved documents are in comparison to a set of candidate documents. It is a measure of the system's ability to identify and score relevant documents higher than less relevant or irrelevant ones. Generation The similar pairwise relations for the generation components are listed below. The EOs are the generated text and phrased structured content. Then we need to compare these EOs with the",
      "chunk_index": 4
    },
    {
      "index": 46,
      "chunk_id": "RAG_Eval_Survey2024_chunk_05",
      "source_id": "RAG_Eval_Survey2024",
      "text": "relevant documents higher than less relevant or irrelevant ones. Generation The similar pairwise relations for the generation components are listed below. The EOs are the generated text and phrased structured content. Then we need to compare these EOs with the provided GTs and labels. - Relevance (Response ↔ Query) measures how well the generated response aligns with the intent and content of the initial query. It ensures that the response is related to the query topic and meets the query's specific requirements. - Faithfulness (Response ↔ Relevant Documents ) evaluates if the generated re- sponse accurately reflects the information contained within the relevant documents and measures the consistency between generated content and the source documents. - Correctness (Response ↔ Sample Response ) Similar to the accuracy in the re- trieval component, this measures the accuracy of the generated response against a sample response, which serves as a ground truth. It checks if the response is correct in terms of factual information and appropriate in the context of the query. 6 Hao Yu, Aoran Gan, Kai Zhang, Shiwei Tong †, Qi Liu, and Zhaofeng Liu The targets of Retrieval and Generation components are introduced. Table 1 lists the relative work on improving and evaluating RAG and its benchmarks cut off in June Table 1: The evaluating targets and corresponding metrics across various frameworks for evaluating RAG systems. The presentation distinguishes between the core areas of Retrieval and Generation considered in the evaluation. The different aspects of the eval- uation are set as different colours in the table: Relevance, Accuracy of Retrieval and Faithfulness, Correctness and Relevance of Generation. The consideration of the Addi- tional Requirements beyond the retrieval and generation component is also collected. Noted that quite a few of the works employed multiple methods or evaluated multiple aspects simultaneously. Category Framework Time Raw Targets Retrieval Generation Tool TruEra RAG Triad [54] 2023.10 Context Relevance Answer Relevance Groundedness LLM as a Judge LLM as a Judge Tool LangChain Bench. [32] 2023.11 Accuracy Faithfulness Execution Time Embed. CosDistance Accuracy LLM as a Judge Tool Databricks Eval [33] 2023.12 Correctness Readability Comprehensiveness - LLM as a Judge Benchmark RAGAs [14] 2023.09 Context Relevance Answer Relevance Faithfulness LLM as a JudgeLLM Gen + CosSim LLM as a Judge Benchmark RECALL [38] 2023.11 Response Quality Robustness - BLEU, ROUGE-L Benchmark ARES [49] 2023.11 Context Relevance Answer Faithfulness Answer Relevance LLM + ClassifierLLM + Classifier LLM",
      "chunk_index": 5
    },
    {
      "index": 47,
      "chunk_id": "RAG_Eval_Survey2024_chunk_06",
      "source_id": "RAG_Eval_Survey2024",
      "text": "Answer Relevance Faithfulness LLM as a JudgeLLM Gen + CosSim LLM as a Judge Benchmark RECALL [38] 2023.11 Response Quality Robustness - BLEU, ROUGE-L Benchmark ARES [49] 2023.11 Context Relevance Answer Faithfulness Answer Relevance LLM + ClassifierLLM + Classifier LLM + Classifier Benchmark RGB [6] 2023.12 Information Integration Noise Robustness Negative Rejection Counterfactual Robustness - Accuracy Benchmark MultiHop-RAG [52] 2024.01Retrieval Quality Response CorrectnessMAP, MRR, Hit@K LLM as a Judge Benchmark CRUD-RAG [39] 2024.02CREATE, READ UPDATE,DELETE - ROUGE, BLEU RAGQuestEval Benchmark MedRAG [61] 2024.02 Accuracy - Accuracy Benchmark FeB4RAG [57] 2024.02 Consistency Correctness Clarity Coverage - Human Evaluation Human Evaluation Benchmark CDQA [62] 2024.03 Accuracy - F1 Benchmark DomainRAG [58] 2024.06 Correctness Faithfulness Noise Robustness Structural Output - F1, Exact-Match Rouge-L LLM as a Judge Benchmark ReEval [66] 2024.06 Hallucination - F1, Exacct-Match LLM as a Judge Human Evaluation Research FiD-Light [20] 2023.07 Latency - - Research Diversity Reranker [4] 2023.08Diversity Cosine Distance - Evaluation of Retrieval-Augmented Generation: A Survey 7 2024. Table 1 portrays this information, where each evaluation criterion is represented by a different colour. For example, FeB4RAG [57], the fourth from the last, has posited four standards based on [17] that comprise Consistency, Correctness, Clarity, and Cov- erage. Correctness is equivalent to accuracy in retrieval, and Consistency is tantamount to faithfulness in the generation component. While accuracy in retrieval gauges the correctness of the retrieved information, we posit that Coverage pertains to the cover- age rate and is more associated with diversity. Therefore, we consider Coverage to be linked with diversity and an additional requirement in our proposed evaluation frame- work, which will be introduced subsequently. The remaining standard, Clarity, is also classified as an additional requirement in our proposed framework. The other tools and benchmarks are processed similarly. Tools and benchmarks offer varying degrees of flexibility in evaluating datasets for RAG systems. Tools, which specify only evaluation targets, provide a versatile frame- work capable of constructing complete RAG applications and evaluation pipelines, as seen in works like [54,32,33]. Benchmarks, on the other hand, focus on different as- pects of RAG evaluation with specific emphasis on either retrieval outputs or genera- tion targets. For instance, RAGAs [14] and ARES [49] assess the relevance of retrieval documents, while RGB and MultiHop-RAG [6,52] prioritize accuracy, necessitating comparison with GTs. The [66] focuses on the Hallucination, which is a combination of faithfulness and correctness. All benchmarks consider generation targets due to",
      "chunk_index": 6
    },
    {
      "index": 48,
      "chunk_id": "RAG_Eval_Survey2024_chunk_07",
      "source_id": "RAG_Eval_Survey2024",
      "text": "ARES [49] assess the relevance of retrieval documents, while RGB and MultiHop-RAG [6,52] prioritize accuracy, necessitating comparison with GTs. The [66] focuses on the Hallucination, which is a combination of faithfulness and correctness. All benchmarks consider generation targets due to their critical role in RAG systems, though their focus areas vary. Additional Requirement In addition to evaluating the two primary components out- lined, a portion of the works also addressed some additional requirements of RAG (Black and Italics targets in Table 2). The requirements are as follows: - Latency [20,32] measures how quickly the system can find information and re- spond, crucial for user experience. - Diversity [4,32] checks if the system retrieves a variety of relevant documents and generates diverse responses. - Noise Robustness [6] assesses how well the system handles irrelevant information without affecting response quality. - Negative Rejection [6] gauges the system's ability to refrain from providing a response when the available information is insufficient. - Counterfactual Robustness [6] evaluates the system's capacity to identify and disregard incorrect information, even when alerted about potential misinformation. - More: For more human preferences considerations, there can be more additional requirements, such as readability [57,33], toxicity, perplexity [33], etc. For the exception, CRUD-RAG [39] introduces a comprehensive benchmark ad- dressing the broader spectrum of RAG applications beyond question-answering, cat- egorized into Create, Read, Update, and Delete scenarios. This benchmark evaluates RAG systems across diverse tasks, including text continuation, question answering, hallucination modification, and multi-document summarization. It offers insights for optimizing RAG technology across different scenarios. DomainRAG [58] identifies six complex abilities for RAG systems: conversational, structural information, faithfulness, 8 Hao Yu, Aoran Gan, Kai Zhang, Shiwei Tong †, Qi Liu, and Zhaofeng Liu denoising, time-sensitive problem solving, and multi-doc understanding. ReEval [66] specifically targets hallucination evaluation by employing a cost-effective LLM-based framework that utilizes prompt chaining to create dynamic test cases. Table 2: The evaluation datasets used for each benchmark. The dataset without citation was constructed by the benchmark itself. Benchmark Dataset RAGAs [14] WikiEval RECALL [38] EventKG [19], UJ [22] ARES [49] NQ [29], Hotpot [63], FEVER [53], WoW [11], MultiRC [10], ReCoRD [71] RGB [6] Generated (Source: News) MultiHop-RAG [52] Generated (Source: News) CRUD-RAG [39] Generated (Source: News) UHGEval [36] MedRAG [61] MIRAGE FeB4RAG [57] FeB4RAG, BEIR [26] CDQA [62] Generation (Source: News), Labeller DomainRAG [58] Generation (Source: College Admission Information) ReEval [66] RealTimeQA[27], NQ [15,29]) 3.2 Evaluation",
      "chunk_index": 7
    },
    {
      "index": 49,
      "chunk_id": "RAG_Eval_Survey2024_chunk_08",
      "source_id": "RAG_Eval_Survey2024",
      "text": "MultiHop-RAG [52] Generated (Source: News) CRUD-RAG [39] Generated (Source: News) UHGEval [36] MedRAG [61] MIRAGE FeB4RAG [57] FeB4RAG, BEIR [26] CDQA [62] Generation (Source: News), Labeller DomainRAG [58] Generation (Source: College Admission Information) ReEval [66] RealTimeQA[27], NQ [15,29]) 3.2 Evaluation Dataset ( How to evaluate?) In Table 2, distinct benchmarks employ varying strategies for dataset construction, rang- ing from leveraging existing resources to generating entirely new data tailored for spe- cific evaluation aspects. Several benchmarks draw upon the part of KILT (Knowledge Intensive Language Tasks) benchmark [44] (Natural Questions [29], HotpotQA [63], and FEVER [53]) and other established datasets such as SuperGLUE [56] (MultiRC [10], and ReCoRD [71]) [49]. However, the drawback of using such datasets can't solve the challenges in dynamic real-world scenarios. A similar situation can be observed in WikiEval, from Wikipedia pages post 2022, constructed by RAGAs [14]. The advent of powerful LLMs has revolutionized the process of dataset construc- tion. With the ability to design queries and ground truths for specific evaluation targets using these frameworks, authors can now create datasets in the desired format with ease. Benchmarks like RGB, MultiHop-RAG, CRUD-RAG, and CDQA [6,52,39,62] have taken this approach further by building their own datasets using online news articles to test RAG systems' ability to handle real-world information beyond the training data of LM frameworks. Most recently, DomainRAG [58] combines various types of QA datasets with single-doc, multi-doc, single-round, and multi-round. These datasets are generated from the yearly changed information from the college website for admission and enrollment, which forces the LLMs to use the provided and updated information. Evaluation of Retrieval-Augmented Generation: A Survey 9 In summary, the creation and selection of datasets are crucial for evaluating RAG systems. Datasets tailored for specific metrics or tasks improve evaluation accuracy and guide the development of adaptable RAG systems for real-world information needs. 3.3 Evaluation Metric ( How to quantify?) Navigating the intricate terrain of evaluating RAG systems necessitates a nuanced un- derstanding of the metrics that can precisely quantify the evaluation targets. However, creating evaluative criteria that align with human preferences and address practical con- siderations is challenging. Each component within the RAG systems requires a tailored evaluative approach that reflects its distinct functionalities and objectives. Retrieval Metrics Various targets can be evaluated with various metrics that corre- spond to the given datasets. This section will introduce several commonly used metrics for retrieval and generation targets.",
      "chunk_index": 8
    },
    {
      "index": 50,
      "chunk_id": "RAG_Eval_Survey2024_chunk_09",
      "source_id": "RAG_Eval_Survey2024",
      "text": "tailored evaluative approach that reflects its distinct functionalities and objectives. Retrieval Metrics Various targets can be evaluated with various metrics that corre- spond to the given datasets. This section will introduce several commonly used metrics for retrieval and generation targets. The metrics for additional requirements can also be found in these commonly used metrics. The more specifically designed metrics can be explored in the original paper via Table 1 as a reference. For the retrieval evaluation, the focus is on metrics that can accurately capture the relevance, accuracy, diversity, and robustness of the information retrieved in response to queries. These metrics must not only reflect the system's precision in fetching pertinent information but also its resilience in navigating the dynamic, vast, and sometimes mis- leading landscape of available data. The deployment of metrics like Misleading Rate, Mistake Reappearance Rate, and Error Detection Rate within the [38] benchmark un- derscores a heightened awareness of RAG systems' inherent intricacies. The integration of MAP@K, MRR@K, and Tokenization with F1 into benchmarks like [52,62] mirrors a deepening comprehension of traditional retrieval's multifaceted evaluation. While the [17] also emphasizes that this ranking-based evaluation methodology is not unsuitable for the RAG system, and should have more RAG-specific retrieval evaluation metrics. These metrics not only capture the precision and recall of retrieval systems but also ac- count for the diversity and relevance of retrieved documents, aligning with the complex and dynamic nature of information needs in RAG systems. The introduction of LLMs as evaluative judges, as seen in [14], further underscores the adaptability and versatility of retrieval evaluation, offering a comprehensive and context-aware approach to assess- ing retrieval quality. Non-Rank Based Metrics often assess binary outcomes-whether an item is relevant or not-without considering the position of the item in a ranked list. Notice, that the following formula is just one format of these metrics, the definition of each metric may vary by the different evaluating tasks. - Accuracy is the proportion of true results (both true positives and true negatives) among the total number of cases examined. - Precision is the fraction of relevant instances among the retrieved instances, Precision = TP TP + FP where TP represents true positives and FP represents false positives. 10 Hao Yu, Aoran Gan, Kai Zhang, Shiwei Tong †, Qi Liu, and Zhaofeng Liu - Recall at k (Recall@k) is the fraction of relevant instances that have been retrieved over the",
      "chunk_index": 9
    },
    {
      "index": 51,
      "chunk_id": "RAG_Eval_Survey2024_chunk_10",
      "source_id": "RAG_Eval_Survey2024",
      "text": "represents true positives and FP represents false positives. 10 Hao Yu, Aoran Gan, Kai Zhang, Shiwei Tong †, Qi Liu, and Zhaofeng Liu - Recall at k (Recall@k) is the fraction of relevant instances that have been retrieved over the total amount of relevant cases, considering only the top k results. Recall@k = |RD ∩ Topkd| |RD| where RD is the relevant documents, and Topkd is the top-k retrieved documents. Rank-Based Metrics evaluate the order in which relevant items are presented, with higher importance placed on the positioning of relevant items at the ranking list. - Mean Reciprocal Rank (MRR) is the average of the reciprocal ranks of the first correct answer for a set of queries. MRR = 1 |Q| |Q|X i=1 ranki where |Q| is the number of queries and ranki is the rank position of the first rele- vant document for the i-th query. - Mean Average Precision (MAP) is the mean of the average precision scores for each query. MAP = 1 |Q| |Q|X q=1 Pn k=1(P(k) × rel(k)) |relevant documentsq| where P(k) is the precision at cutoff k in the list, rel(k) is an indicator function equaling 1 if the item at rank k is a relevant document, 0 otherwise, and n is the number of retrieved documents. Generation Metrics In the realm of generation, evaluation transcends the mere accu- racy of generated responses, venturing into the quality of text in terms of coherence, relevance, fluency, and alignment with human judgment. This necessitates metrics that can assess the nuanced aspects of language production, including factual correctness, readability, and user satisfaction with the generated content. The traditional metrics like BLEU, ROUGE, and F1 Score continue to play a crucial role, emphasizing the signifi- cance of precision and recall in determining response quality. Yet, the advent of metrics such as Misleading Rate, Mistake Reappearance Rate, and Error Detection Rate high- lights an evolving understanding of RAG systems' distinct challenges [38]. The evaluation done by humans is still a very significant standard to compare the performance of generation models with one another or with the ground truth. The approach of employing LLMs as evaluative judges [75] is a versatile and automatic method for quality assessment, catering to instances where traditional ground truths may be elusive [14]. This methodology benefits from employing prediction-powered inference (PPI) and context relevance scoring, offering a nuanced lens through which LLM output can be",
      "chunk_index": 10
    },
    {
      "index": 52,
      "chunk_id": "RAG_Eval_Survey2024_chunk_11",
      "source_id": "RAG_Eval_Survey2024",
      "text": "versatile and automatic method for quality assessment, catering to instances where traditional ground truths may be elusive [14]. This methodology benefits from employing prediction-powered inference (PPI) and context relevance scoring, offering a nuanced lens through which LLM output can be assessed. [49] The strategic use of detailed prompt templates en- sures a guided assessment aligned with human preferences, effectively standardizing evaluations across various content dimensions [1]. This shift towards leveraging LLMs Evaluation of Retrieval-Augmented Generation: A Survey 11 as arbiters mark a significant progression towards automated and context-responsive evaluation frameworks, enriching the evaluation landscape with minimal reliance on reference comparisons. - ROUGE Recall-Oriented Understudy for Gisting Evaluation (ROUGE) [37] is a set of metrics designed to evaluate the quality of summaries by comparing them to human-generated reference summaries. ROUGE can be indicative of the content overlap between the generated text and the reference text. The variants of ROUGEs measure the overlap of n-grams (ROUGE-N, ROUGGE-W), word subsequences (ROUGE-L, ROUGGE-S), and word pairs between the system-generated summary and the reference summaries. - BLEU Bilingual Evaluation Understudy (BLEU) [43] is a metric for evaluating the quality of machine-translated text against one or more reference translations. BLEU calculates the precision of n-grams in the generated text compared to the refer- ence text and then applies a brevity penalty to discourage overly short translations. BLEU has limitations, such as not accounting for the fluency or grammaticality of the generated text. - BertScore BertScore [72] leverages the contextual embedding from pre-trained transformers like BERT to evaluate the semantic similarity between generated text and reference text. BertScore computes token-level similarity using contextual em- bedding and produces precision, recall, and F1 scores. Unlike n-gram-based met- rics, BertScore captures the meaning of words in context, making it more robust to paraphrasing and more sensitive to semantic equivalence. - LLM as a Judge Using \"LLM as a Judge\" for evaluating generated text is a more recent approach. [75] In this method, LLMs are used to score the generated text based on criteria such as coherence, relevance, and fluency. The LLM can be op- tionally finetuned on human judgments to predict the quality of unseen text or used to generate evaluations in a zero-shot or few-shot setting. This approach leverages the LLM's understanding of language and context to provide a more nuanced text quality assessment. For instance, [1] illustrates how providing LLM judges with de- tailed scoring guidelines, such as",
      "chunk_index": 11
    },
    {
      "index": 53,
      "chunk_id": "RAG_Eval_Survey2024_chunk_12",
      "source_id": "RAG_Eval_Survey2024",
      "text": "evaluations in a zero-shot or few-shot setting. This approach leverages the LLM's understanding of language and context to provide a more nuanced text quality assessment. For instance, [1] illustrates how providing LLM judges with de- tailed scoring guidelines, such as a scale from 1 to 5, can standardize the evaluation process. This methodology encompasses critical aspects of content assessment, in- cluding coherence, relevance, fluency, coverage, diversity, and detail - both in the context of answer evaluation and query formulation. Additional Requirements These additional requirements, such as latency, diversity, noise robustness, negative rejection, and counterfactual robustness, are used to ensure the practical applicability of RAG systems in real-world scenarios aligned with human preference. This section delves into the metrics used for evaluating these additional requirements, highlighting their significance in the comprehensive assessment of RAG systems. Latency measures the time taken by the RAG system to finish the response of one query. It is a critical factor for user experience, especially in interactive applications such as chatbots or search engines [20]. Single Query Latency: The mean time is taken to process a single query, including both retrieval and generating phases. 12 Hao Yu, Aoran Gan, Kai Zhang, Shiwei Tong †, Qi Liu, and Zhaofeng Liu Diversity evaluates the variety and breadth of information retrieved and generated by the RAG system. It ensures that the system can provide a wide range of perspec- tives and avoid redundancy in responses [4]. Cosine Similarity / Cosine Distance: The cosine similarity/distance calculates embeddings of retrieved documents or generated responses. [30] Lower cosine similarity scores indicate higher diversity, suggesting that the system can retrieve or generate a broader spectrum of information. Noise Robustness measures the RAG system's ability to handle irrelevant or mis- leading information without compromising the quality of the response [38]. The metrics Misleading Rate and Mistake Reappearance Rate are described in [38], providing de- tailed descriptions tailored to the specific dataset and experimental setup. [58] Negative Rejection evaluates the system's capability to withhold responses when the available information is insufficient or too ambiguous to provide an accurate answer [6]. Rejection Rate: The rate at which the system refrains from generating a response. Counterfactual Robustness Counterfactual robustness assesses the system's abil- ity to identify and disregard incorrect or counterfactual information within the retrieved documents [39]. Error Detection Rate: The ratio of counterfactual statements detected in retrieved information. 4 Discussion For RAG systems, traditional Question Answering",
      "chunk_index": 12
    },
    {
      "index": 54,
      "chunk_id": "RAG_Eval_Survey2024_chunk_13",
      "source_id": "RAG_Eval_Survey2024",
      "text": "Counterfactual robustness assesses the system's abil- ity to identify and disregard incorrect or counterfactual information within the retrieved documents [39]. Error Detection Rate: The ratio of counterfactual statements detected in retrieved information. 4 Discussion For RAG systems, traditional Question Answering (QA) datasets and metrics remain a common format for interaction. [14,49,38,6,61,62,58,66] While these provide a ba- sic verification of RAG's capabilities, it becomes challenging to distinguish the impact of retrieval components when faced with strong Language Models (LLMs) capable of excelling in QA benchmarks. To comprehensively evaluate the performance of entire RAG systems, there is a need for diverse and RAG-specific benchmarks. Several papers offer guidance on improving QA format benchmarks, including variations in question types: from simple Wikipedia filling questions to multi-hop [52], multi-document ques- tions [66] and single-round to multi-round dialogue [39,58]. For answers, aspects such as structural output [58], content moderation [6,54], and hallucination [66] can be con- sidered when evaluating relevance, faithfulness, and correctness. In addition to these, RAG systems require additional requirements such as robustness to noisy documents, language expression, latency, and result diversity. [32,33,38,6,39,57,58,20,4] Further- more, research is needed on performance changes involving intermediate outputs and retrieved documents, as well as the relationship and analysis between retrieval metrics and final generation outputs. Regarding datasets, creating a universal dataset was challenging due to the target- specific nature of different RAG benchmarks. Tailored datasets [14,38,49,39,57] are necessary for a thorough evaluation, but this approach increases the effort and re- sources required. Moreover, the diversity of datasets, from news articles to structured databases [66], reflects the adaptability required of RAG systems but also poses a bar- rier to streamlined evaluation. Recently, with the cutting-edge performance of LLMs, complex data processing and automatic QA pair generation can be automated to achieve daily or finer-grained time resolution, preventing LLMs from cheating and evaluating the robustness of RAG systems in rapidly changing data. [6,52,39,62,58,66] Evaluation of Retrieval-Augmented Generation: A Survey 13 When it comes to metrics, the use of LLMs as automatic evaluative judges signifies a burgeoning trend, promising versatility and depth in generative outputs with reason- ing on a large scale compared to human evaluation. However, using \"LLMs as a Judge\" [75] for responses presents challenges in aligning with human judgment, establishing effective grading scales, and applying consistent evaluation across varied use cases. Determining correctness, clarity, and richness can differ between automated and hu- man assessments. Moreover, the effectiveness",
      "chunk_index": 13
    },
    {
      "index": 55,
      "chunk_id": "RAG_Eval_Survey2024_chunk_14",
      "source_id": "RAG_Eval_Survey2024",
      "text": "a Judge\" [75] for responses presents challenges in aligning with human judgment, establishing effective grading scales, and applying consistent evaluation across varied use cases. Determining correctness, clarity, and richness can differ between automated and hu- man assessments. Moreover, the effectiveness of example-based scoring can vary, and there's no universally applicable grading scale and prompting text, complicating the standardization of \"LLM as a Judge\". [33] In addition to the challenges mentioned above, it is important to consider the resource- intensive nature [76] of using Large Language Models (LLMs) for data generation and validation. RAG benchmarks must balance the need for thorough evaluation with the practical constraints of limited computational resources. As such, it is desirable to de- velop evaluation methodologies that can effectively assess RAG systems using smaller amounts of data while maintaining the validity and reliability of the results. 5 Conclusion This survey systematically explores the complexities of evaluating RAG systems, high- lighting the challenges in assessing their performance. Through the proposed A Unified Evaluation Process of RAG, we outline a structured approach to analyzing RAG evalua- tions, focusing on targets, datasets and measures. Our analysis emphasizes the need for targeted benchmarks that reflect the dynamic interplay between retrieval accuracy and generative quality and practical considerations for real-world applications. By identify- ing gaps in current methodologies and suggesting future research directions, we aim to contribute to more effective, and user-aligned benchmarks of RAG systems. 14 Hao Yu, Aoran Gan, Kai Zhang, Shiwei Tong †, Qi Liu, and Zhaofeng Liu References 1. Balaguer, A., Benara, V ., Cunha, R.L.d.F., Filho, R.d.M.E., Hendry, T., Holstein, D., Mars- man, J., Mecklenburg, N., Malvar, S., Nunes, L.O., Padilha, R., Sharp, M., Silva, B., Sharma, S., Aski, V ., Chandra, R.: RAG vs Fine-tuning: Pipelines, Tradeoffs, and a Case Study on Agriculture. Tech. rep. (Jan 2024), http://arxiv.org/abs/2401.08406, arXiv:2401.08406 [cs] type: article 2. Barnett, S., Kurniawan, S., Thudumu, S., Brannelly, Z., Abdelrazek, M.: Seven failure points when engineering a retrieval augmented generation system (Jan 2024). https://doi. org/10.48550/ARXIV.2401.05856 3. Besta, M., Blach, N., Kubicek, A., Gerstenberger, R., Podstawski, M., Gianinazzi, L., Gajda, J., Lehmann, T., Niewiadomski, H., Nyczyk, P., Hoefler, T.: Graph of thoughts: Solving elaborate problems with large language models. Proceedings of the AAAI Conference on Artificial Intelligence 2024 (AAAI'24) (Aug 2023). https://doi.org/10.48550/ ARXIV.2308.09687 4. Blagojevic, V .: Enhancing RAG Pipelines in Haystack: Introducing DiversityRanker and LostInTheMiddleRanker (Aug 2023), https://towardsdatascience.com/ enhancing-rag-pipelines-in-haystack-45f14e2bc9f5 5. Chang, Y ., Wang,",
      "chunk_index": 14
    },
    {
      "index": 56,
      "chunk_id": "RAG_Eval_Survey2024_chunk_15",
      "source_id": "RAG_Eval_Survey2024",
      "text": "with large language models. Proceedings of the AAAI Conference on Artificial Intelligence 2024 (AAAI'24) (Aug 2023). https://doi.org/10.48550/ ARXIV.2308.09687 4. Blagojevic, V .: Enhancing RAG Pipelines in Haystack: Introducing DiversityRanker and LostInTheMiddleRanker (Aug 2023), https://towardsdatascience.com/ enhancing-rag-pipelines-in-haystack-45f14e2bc9f5 5. Chang, Y ., Wang, X., Wang, J., Wu, Y ., Yang, L., Zhu, K., Chen, H., Yi, X., Wang, C., Wang, Y ., et al.: A survey on evaluation of large language models. ACM Transactions on Intelligent Systems and Technology 15(3), 1-45 (2024) 6. Chen, J., Lin, H., Han, X., Sun, L.: Benchmarking large language models in retrieval- augmented generation (Sep 2023). https://doi.org/10.48550/ARXIV.2309. 7. Cuconasu, F., Trappolini, G., Siciliano, F., Filice, S., Campagnano, C., Maarek, Y ., Tonel- lotto, N., Silvestri, F.: The power of noise: Redefining retrieval for rag systems (Jan 2024). https://doi.org/10.48550/ARXIV.2401.14887 8. Deng, Y ., Zhang, W., Chen, Z., Gu, Q.: Rephrase and respond: Let large language models ask better questions for themselves (Nov 2023). https://doi.org/10.48550/ARXIV. 2311.04205 9. Devlin, J., Chang, M.W., Lee, K., Toutanova, K.: BERT: Pre-training of Deep Bidirec- tional Transformers for Language Understanding. In: Burstein, J., Doran, C., Solorio, T. (eds.) Proceedings of the 2019 Conference of the North American Chapter of the Asso- ciation for Computational Linguistics: Human Language Technologies, V olume 1 (Long and Short Papers). pp. 4171-4186. Association for Computational Linguistics, Minneapo- lis, Minnesota (Jun 2019). https://doi.org/10.18653/v1/N19-1423, https: //aclanthology.org/N19-1423 10. DeYoung, J., Jain, S., Rajani, N.F., Lehman, E., Xiong, C., Socher, R., Wallace, B.C.: Eraser: A benchmark to evaluate rationalized nlp models 11. Dinan, E., Roller, S., Shuster, K., Fan, A., Auli, M., Weston, J.: Wizard of Wikipedia: Knowledge-powered conversational agents. In: Proceedings of the International Conference on Learning Representations (ICLR) (2019) 12. Douze, M., Guzhva, A., Deng, C., Johnson, J., Szilvasy, G., Mazaré, P.E., Lomeli, M., Hos- seini, L., Jégou, H.: The faiss library (2024) 13. DuckDuckGo: DuckDuckGo - Privacy, simplified. (2024), https://duckduckgo. com//home 14. Es, S., James, J., Espinosa-Anke, L., Schockaert, S.: Ragas: Automated evaluation of retrieval augmented generation (Sep 2023). https://doi.org/10.48550/ARXIV. 2309.15217 Evaluation of Retrieval-Augmented Generation: A Survey 15 15. Fisch, A., Talmor, A., Jia, R., Seo, M., Choi, E., Chen, D.: MRQA 2019 shared task: Evaluating generalization in reading comprehension. In: Fisch, A., Talmor, A., Jia, R., Seo, M., Choi, E., Chen, D. (eds.) Proceedings of the 2nd Workshop on Machine Read- ing for Question Answering. pp. 1-13. Association for Computational Linguistics, Hong Kong, China (Nov 2019). https://doi.org/10.18653/v1/D19-5801, https: //aclanthology.org/D19-5801 16.",
      "chunk_index": 15
    },
    {
      "index": 57,
      "chunk_id": "RAG_Eval_Survey2024_chunk_16",
      "source_id": "RAG_Eval_Survey2024",
      "text": "Fisch, A., Talmor, A., Jia, R., Seo, M., Choi, E., Chen, D. (eds.) Proceedings of the 2nd Workshop on Machine Read- ing for Question Answering. pp. 1-13. Association for Computational Linguistics, Hong Kong, China (Nov 2019). https://doi.org/10.18653/v1/D19-5801, https: //aclanthology.org/D19-5801 16. Gao, Y ., Xiong, Y ., Gao, X., Jia, K., Pan, J., Bi, Y ., Dai, Y ., Sun, J., Guo, Q., Wang, M., Wang, H.: Retrieval-Augmented Generation for Large Language Models: A Survey. Tech. rep. (Jan 2024), http://arxiv.org/abs/2312.10997, arXiv:2312.10997 [cs] type: article 17. Gienapp, L., Scells, H., Deckers, N., Bevendorff, J., Wang, S., Kiesel, J., Syed, S., Fröbe, M., Zuccon, G., Stein, B., Hagen, M., Potthast, M.: Evaluating Generative Ad Hoc In- formation Retrieval. Tech. rep. (Nov 2023), http://arxiv.org/abs/2311.04694, arXiv:2311.04694 [cs] type: article 18. Google: Programmable Search Engine | Google for Developers (2024), https:// developers.google.com/custom-search 19. Gottschalk, S., Demidova, E.: Eventkg: A multilingual event-centric temporal knowledge graph (Apr 2018). https://doi.org/10.48550/ARXIV.1804.04526 20. Hofstätter, S., Chen, J., Raman, K., Zamani, H.: FiD-Light: Efficient and Effective Retrieval-Augmented Text Generation. In: Proceedings of the 46th International ACM SIGIR Conference on Research and Development in Information Retrieval. pp. 1437- 1447. SIGIR '23, Association for Computing Machinery, New York, NY , USA (Jul 2023). https://doi.org/10.1145/3539618.3591687, https://doi.org/ 10.1145/3539618.3591687 21. Hu, E.J., Shen, Y ., Wallis, P., Allen-Zhu, Z., Li, Y ., Wang, S., Wang, L., Chen, W.: LoRA: Low-Rank Adaptation of Large Language Models. Tech. rep. (Oct 2021).https://doi. org/10.48550/arXiv.2106.09685, http://arxiv.org/abs/2106.09685, arXiv:2106.09685 [cs] type: article 22. Huang, J., Shao, H., Chang, K.C.C., Xiong, J., Hwu, W.m.: Understanding jargon: Combin- ing extraction and generation for definition modeling. In: Proceedings of EMNLP (2022) 23. Huang, L., Yu, W., Ma, W., Zhong, W., Feng, Z., Wang, H., Chen, Q., Peng, W., Feng, X., Qin, B., Liu, T.: A survey on hallucination in large language models: Principles, taxonomy, challenges, and open questions (Nov 2023). https://doi.org/10.48550/ARXIV. 2311.05232 24. Huang, Y ., Huang, J.: A survey on retrieval-augmented text generation for large language models (Apr 2024). https://doi.org/10.48550/ARXIV.2404.10981 25. Johnson, J., Douze, M., Jégou, H.: Billion-scale similarity search with GPUs. IEEE Trans- actions on Big Data 7(3), 535-547 (2019) 26. Kamalloo, E., Thakur, N., Lassance, C., Ma, X., Yang, J.H., Lin, J.: Resources for brewing beir: Reproducible reference models and an official leaderboard (2023) 27. Kasai, J., Sakaguchi, K., Takahashi, Y ., Bras, R.L., Asai, A., Yu, X., Radev, D., Smith, N.A., Choi, Y ., Inui, K.: Realtime qa: What's the answer right now?",
      "chunk_index": 16
    },
    {
      "index": 58,
      "chunk_id": "RAG_Eval_Survey2024_chunk_17",
      "source_id": "RAG_Eval_Survey2024",
      "text": "brewing beir: Reproducible reference models and an official leaderboard (2023) 27. Kasai, J., Sakaguchi, K., Takahashi, Y ., Bras, R.L., Asai, A., Yu, X., Radev, D., Smith, N.A., Choi, Y ., Inui, K.: Realtime qa: What's the answer right now? (Jul 2022).https:// doi.org/10.48550/ARXIV.2207.13332, https://arxiv.org/abs/2207. 28. Khattab, O., Zaharia, M.: Colbert: Efficient and effective passage search via contextualized late interaction over bert (Apr 2020). https://doi.org/10.48550/ARXIV.2004. 29. Kwiatkowski, T., Palomaki, J., Redfield, O., Collins, M., Parikh, A., Alberti, C., Epstein, D., Polosukhin, I., Devlin, J., Lee, K., Toutanova, K., Jones, L., Kelcey, M., Chang, M.W., Dai, A.M., Uszkoreit, J., Le, Q., Petrov, S.: Natural questions: A benchmark for question 16 Hao Yu, Aoran Gan, Kai Zhang, Shiwei Tong †, Qi Liu, and Zhaofeng Liu answering research. Transactions of the Association for Computational Linguistics 7, 453- 466 (2019). https://doi.org/10.1162/tacl_a_00276, https://doi.org/ 10.1162/tacl_a_00276 30. Lahitani, A.R., Permanasari, A.E., Setiawan, N.A.: Cosine similarity to determine similar- ity measure: Study case in online essay assessment. In: 2016 4th International Conference on Cyber and IT Service Management. pp. 1-6 (2016). https://doi.org/10.1109/ CITSM.2016.7577578 31. Lanchantin, J., Toshniwal, S., Weston, J., Szlam, A., Sukhbaatar, S.: Learning to reason and memorize with self-notes (May 2023). https://doi.org/10.48550/ARXIV. 2305.00833 32. LangChain: Evaluating rag architectures on benchmark tasks (Nov 2023), https: //langchain-ai.github.io/langchain-benchmarks/notebooks/ retrieval/langchain_docs_qa.html 33. Leng, Q., Uhlenhuth, K., Polyzotis, A.: Best Practices for LLM Evaluation of RAG Applications (Dec 2023), https://www.databricks.com/blog/ LLM-auto-eval-best-practices-RAG 34. Lewis, P., Perez, E., Piktus, A., Petroni, F., Karpukhin, V ., Goyal, N., Küttler, H., Lewis, M., Yih, W.t., Rocktäschel, T., Riedel, S., Kiela, D.: Retrieval-augmented generation for knowledge-intensive NLP tasks. In: Proceedings of the 34th International Conference on Neural Information Processing Systems. pp. 9459-9474. NIPS'20, Curran Associates Inc., Red Hook, NY , USA (Dec 2020) 35. Lewis, P., Perez, E., Piktus, A., Petroni, F., Karpukhin, V ., Goyal, N., Küttler, H., Lewis, M., Yih, W.t., Rocktäschel, T., Riedel, S., Kiela, D.: Retrieval-Augmented Generation for Knowledge-Intensive NLP Tasks. Tech. rep. (Apr 2021), http://arxiv.org/abs/ 2005.11401, arXiv:2005.11401 [cs] type: article 36. Liang, X., Song, S., Niu, S., Li, Z., Xiong, F., Tang, B., Wy, Z., He, D., Cheng, P., Wang, Z., Deng, H.: Uhgeval: Benchmarking the hallucination of chinese large language models via unconstrained generation. arXiv preprint arXiv:2311.15296 (2023) 37. Lin, C.Y .: ROUGE: A package for automatic evaluation of summaries. In: Text Summariza- tion Branches Out. pp. 74-81. Association for Computational Linguistics, Barcelona, Spain (Jul 2004), https://aclanthology.org/W04-1013 38. Liu, Y ., Huang,",
      "chunk_index": 17
    },
    {
      "index": 59,
      "chunk_id": "RAG_Eval_Survey2024_chunk_18",
      "source_id": "RAG_Eval_Survey2024",
      "text": "unconstrained generation. arXiv preprint arXiv:2311.15296 (2023) 37. Lin, C.Y .: ROUGE: A package for automatic evaluation of summaries. In: Text Summariza- tion Branches Out. pp. 74-81. Association for Computational Linguistics, Barcelona, Spain (Jul 2004), https://aclanthology.org/W04-1013 38. Liu, Y ., Huang, L., Li, S., Chen, S., Zhou, H., Meng, F., Zhou, J., Sun, X.: Recall: A bench- mark for llms robustness against external counterfactual knowledge (Nov 2023). https: //doi.org/10.48550/ARXIV.2311.08147 39. Lyu, Y ., Li, Z., Niu, S., Xiong, F., Tang, B., Wang, W., Wu, H., Liu, H., Xu, T., Chen, E., Luo, Y ., Cheng, P., Deng, H., Wang, Z., Lu, Z.: Crud-rag: A comprehensive chinese benchmark for retrieval-augmented generation of large language models (Jan 2024). https://doi. org/10.48550/ARXIV.2401.17043 40. Microsoft: Web Search API | Microsoft Bing, https://www.microsoft.com/ en-us/bing/apis/bing-web-search-api 41. OpenAI, Achiam, J., Adler, S., Agarwal, S., Ahmad, L., Akkaya, I., Aleman, F.L., Almeida, D., Altenschmidt, J., Altman, S., Anadkat, S., Avila, R., Babuschkin, I., Balaji, S., Bal- com, V ., Baltescu, P., Bao, H., Bavarian, M., Belgum, J., Bello, I., Berdine, J., Bernadett- Shapiro, G., Berner, C., Bogdonoff, L., Boiko, O., Boyd, M., Brakman, A.L., Brockman, G., Brooks, T., Brundage, M., Button, K., Cai, T., Campbell, R., Cann, A., Carey, B., Carl- son, C., Carmichael, R., Chan, B., Chang, C., Chantzis, F., Chen, D., Chen, S., Chen, R., Chen, J., Chen, M., Chess, B., Cho, C., Chu, C., Chung, H.W., Cummings, D., Currier, J., Dai, Y ., Decareaux, C., Degry, T., Deutsch, N., Deville, D., Dhar, A., Dohan, D., Dowling, S., Dunning, S., Ecoffet, A., Eleti, A., Eloundou, T., Farhi, D., Fedus, L., Felix, N., Fish- man, S.P., Forte, J., Fulford, I., Gao, L., Georges, E., Gibson, C., Goel, V ., Gogineni, T., Evaluation of Retrieval-Augmented Generation: A Survey 17 Goh, G., Gontijo-Lopes, R., Gordon, J., Grafstein, M., Gray, S., Greene, R., Gross, J., Gu, S.S., Guo, Y ., Hallacy, C., Han, J., Harris, J., He, Y ., Heaton, M., Heidecke, J., Hesse, C., Hickey, A., Hickey, W., Hoeschele, P., Houghton, B., Hsu, K., Hu, S., Hu, X., Huizinga, J., Jain, S., Jain, S., Jang, J., Jiang, A., Jiang, R., Jin, H., Jin, D., Jomoto, S., Jonn, B., Jun, H., Kaftan, T., Kamali, A., Kanitscheider, I., Keskar, N.S., Khan, T., Kilpatrick, L., Kim, J.W., Kim, C., Kim, Y ., Kirchner, J.H., Kiros, J., Knight, M., Kokotajlo, D., Kondraciuk, A., Konstantinidis, A., Kosic, K., Krueger, G., Kuo, V ., Lampe, M., Lan,",
      "chunk_index": 18
    },
    {
      "index": 60,
      "chunk_id": "RAG_Eval_Survey2024_chunk_19",
      "source_id": "RAG_Eval_Survey2024",
      "text": "T., Kamali, A., Kanitscheider, I., Keskar, N.S., Khan, T., Kilpatrick, L., Kim, J.W., Kim, C., Kim, Y ., Kirchner, J.H., Kiros, J., Knight, M., Kokotajlo, D., Kondraciuk, A., Konstantinidis, A., Kosic, K., Krueger, G., Kuo, V ., Lampe, M., Lan, I., Lee, T., Leike, J., Leung, J., Levy, D., Li, C.M., Lim, R., Lin, M., Lin, S., Litwin, M., Lopez, T., Lowe, R., Lue, P., Makanju, A., Malfacini, K., Manning, S., Markov, T., Markovski, Y ., Martin, B., Mayer, K., Mayne, A., McGrew, B., McKinney, S.M., McLeavey, C., McMillan, P., McNeil, J., Medina, D., Mehta, A., Menick, J., Metz, L., Mishchenko, A., Mishkin, P., Monaco, V ., Morikawa, E., Mossing, D., Mu, T., Murati, M., Murk, O., Mély, D., Nair, A., Nakano, R., Nayak, R., Neelakantan, A., Ngo, R., Noh, H., Ouyang, L., O'Keefe, C., Pachocki, J., Paino, A., Palermo, J., Pantuliano, A., Parascandolo, G., Parish, J., Parparita, E., Passos, A., Pavlov, M., Peng, A., Perelman, A., Peres, F.d.A.B., Petrov, M., Pinto, H.P.d.O., Michael, Pokorny, Pokrass, M., Pong, V .H., Powell, T., Power, A., Power, B., Proehl, E., Puri, R., Radford, A., Rae, J., Ramesh, A., Raymond, C., Real, F., Rimbach, K., Ross, C., Rotsted, B., Roussez, H., Ryder, N., Saltarelli, M., Sanders, T., Santurkar, S., Sastry, G., Schmidt, H., Schnurr, D., Schulman, J., Selsam, D., Sheppard, K., Sherbakov, T., Shieh, J., Shoker, S., Shyam, P., Sidor, S., Sigler, E., Simens, M., Sitkin, J., Slama, K., Sohl, I., Sokolowsky, B., Song, Y ., Staudacher, N., Such, F.P., Summers, N., Sutskever, I., Tang, J., Tezak, N., Thompson, M.B., Tillet, P., Tootoonchian, A., Tseng, E., Tuggle, P., Turley, N., Tworek, J., Uribe, J.F.C., Vallone, A., Vijayvergiya, A., V oss, C., Wainwright, C., Wang, J.J., Wang, A., Wang, B., Ward, J., Wei, J., Weinmann, C., Welihinda, A., Welinder, P., Weng, J., Weng, L., Wiethoff, M., Willner, D., Winter, C., Wolrich, S., Wong, H., Workman, L., Wu, S., Wu, J., Wu, M., Xiao, K., Xu, T., Yoo, S., Yu, K., Yuan, Q., Zaremba, W., Zellers, R., Zhang, C., Zhang, M., Zhao, S., Zheng, T., Zhuang, J., Zhuk, W., Zoph, B.: GPT-4 Technical Report (Mar 2023). https://doi.org/10.48550/ARXIV.2303.08774 42. Ouyang, L., Wu, J., Jiang, X., Almeida, D., Wainwright, C.L., Mishkin, P., Zhang, C., Agarwal, S., Slama, K., Ray, A., Schulman, J., Hilton, J., Kelton, F., Miller, L., Simens, M., Askell, A., Welinder, P., Christiano, P., Leike, J., Lowe, R.: Training language mod- els to follow instructions",
      "chunk_index": 19
    },
    {
      "index": 61,
      "chunk_id": "RAG_Eval_Survey2024_chunk_20",
      "source_id": "RAG_Eval_Survey2024",
      "text": "D., Wainwright, C.L., Mishkin, P., Zhang, C., Agarwal, S., Slama, K., Ray, A., Schulman, J., Hilton, J., Kelton, F., Miller, L., Simens, M., Askell, A., Welinder, P., Christiano, P., Leike, J., Lowe, R.: Training language mod- els to follow instructions with human feedback. Tech. rep. (Mar 2022). https://doi. org/10.48550/arXiv.2203.02155, http://arxiv.org/abs/2203.02155, arXiv:2203.02155 [cs] type: article 43. Papineni, K., Roukos, S., Ward, T., Zhu, W.J.: Bleu: a method for automatic evaluation of machine translation. In: Isabelle, P., Charniak, E., Lin, D. (eds.) Proceedings of the 40th An- nual Meeting of the Association for Computational Linguistics. pp. 311-318. Association for Computational Linguistics, Philadelphia, Pennsylvania, USA (Jul 2002). https://doi. org/10.3115/1073083.1073135, https://aclanthology.org/P02-1040 44. Petroni, F., Piktus, A., Fan, A., Lewis, P., Yazdani, M., De Cao, N., Thorne, J., Jernite, Y ., Karpukhin, V ., Maillard, J., Plachouras, V ., Rocktäschel, T., Riedel, S.: KILT: a bench- mark for knowledge intensive language tasks. In: Proceedings of the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Hu- man Language Technologies. pp. 2523-2544. Association for Computational Linguistics, Online (Jun 2021). https://doi.org/10.18653/v1/2021.naacl-main.200, https://aclanthology.org/2021.naacl-main.200 45. Radford, A., Wu, J., Child, R., Luan, D., Amodei, D., Sutskever, I., et al.: Language models are unsupervised multitask learners. OpenAI blog 1(8), 9 (2019) 46. Ramos, J., et al.: Using tf-idf to determine word relevance in document queries. In: Proceed- ings of the first instructional conference on machine learning. vol. 242, pp. 29-48. Citeseer 18 Hao Yu, Aoran Gan, Kai Zhang, Shiwei Tong †, Qi Liu, and Zhaofeng Liu (2003) 47. Robertson, S., Zaragoza, H., et al.: The probabilistic relevance framework: Bm25 and be- yond. Foundations and Trends® in Information Retrieval 3(4), 333-389 (2009) 48. Rosset, C., Chung, H.L., Qin, G., Chau, E.C., Feng, Z., Awadallah, A., Neville, J., Rao, N.: Researchy questions: A dataset of multi-perspective, decompositional questions for llm web agents (Feb 2024). https://doi.org/10.48550/ARXIV.2402.17896 49. Saad-Falcon, J., Khattab, O., Potts, C., Zaharia, M.: Ares: An automated evaluation frame- work for retrieval-augmented generation systems (Nov 2023). https://doi.org/10. 48550/ARXIV.2311.09476 50. Sai, A.B., Mohankumar, A.K., Khapra, M.M.: A survey of evaluation metrics used for nlg systems. ACM Computing Surveys (CSUR) 55(2), 1-39 (2022) 51. Shahabi, C., Kolahdouzan, M.R., Sharifzadeh, M.: A road network embedding technique for k-nearest neighbor search in moving object databases. In: Proceedings of the 10th ACM international symposium on advances in geographic information systems. pp. 94-100 (2002) 52. Tang, Y ., Yang, Y .: Multihop-rag: Benchmarking retrieval-augmented",
      "chunk_index": 20
    },
    {
      "index": 62,
      "chunk_id": "RAG_Eval_Survey2024_chunk_21",
      "source_id": "RAG_Eval_Survey2024",
      "text": "A road network embedding technique for k-nearest neighbor search in moving object databases. In: Proceedings of the 10th ACM international symposium on advances in geographic information systems. pp. 94-100 (2002) 52. Tang, Y ., Yang, Y .: Multihop-rag: Benchmarking retrieval-augmented generation for multi- hop queries (Jan 2024). https://doi.org/10.48550/ARXIV.2401.15391 53. Thorne, J., Vlachos, A., Christodoulopoulos, C., Mittal, A.: FEVER: a large-scale dataset for fact extraction and VERification. In: NAACL-HLT (2018) 54. TruLens: TruLens (2023), https://www.trulens.org/trulens_eval/ getting_started/quickstarts/quickstart/ 55. Vaswani, A., Shazeer, N., Parmar, N., Uszkoreit, J., Jones, L., Gomez, A.N., Kaiser, L., Polosukhin, I.: Attention is all you need (Jun 2017). https://doi.org/10.48550/ ARXIV.1706.03762 56. Wang, A., Pruksachatkun, Y ., Nangia, N., Singh, A., Michael, J., Hill, F., Levy, O., Bow- man, S.R.: SuperGLUE: A stickier benchmark for general-purpose language understanding systems. arXiv preprint 1905.00537 (2019) 57. Wang, S., Khramtsova, E., Zhuang, S., Zuccon, G.: Feb4rag: Evaluating federated search in the context of retrieval augmented generation (Feb 2024). https://doi.org/10. 48550/ARXIV.2402.11891 58. Wang, S., Liu, J., Song, S., Cheng, J., Fu, Y ., Guo, P., Fang, K., Zhu, Y ., Dou, Z.: Domainrag: A chinese benchmark for evaluating domain-specific retrieval-augmented generation (Jun 2024). https://doi.org/10.48550/ARXIV.2406.05654 59. Wei, J., Tay, Y ., Bommasani, R., Raffel, C., Zoph, B., Borgeaud, S., Yogatama, D., Bosma, M., Zhou, D., Metzler, D., Chi, E.H., Hashimoto, T., Vinyals, O., Liang, P., Dean, J., Fedus, W.: Emergent abilities of large language models (Jun 2022). https://doi.org/10. 48550/ARXIV.2206.07682 60. Wei, J., Wang, X., Schuurmans, D., Bosma, M., Ichter, B., Xia, F., Chi, E., Le, Q., Zhou, D.: Chain-of-thought prompting elicits reasoning in large language models (Jan 2022).https: //doi.org/10.48550/ARXIV.2201.11903 61. Xiong, G., Jin, Q., Lu, Z., Zhang, A.: Benchmarking retrieval-augmented generation for medicine (Feb 2024). https://doi.org/10.48550/ARXIV.2402.13178 62. Xu, Z., Li, Y ., Ding, R., Wang, X., Chen, B., Jiang, Y ., Zheng, H.T., Lu, W., Xie, P., Huang, F.: Let llms take on the latest challenges! a chinese dynamic question answering benchmark (Feb 2024). https://doi.org/10.48550/ARXIV.2402.19248 63. Yang, Z., Qi, P., Zhang, S., Bengio, Y ., Cohen, W.W., Salakhutdinov, R., Manning, C.D.: HotpotQA: A dataset for diverse, explainable multi-hop question answering. In: Conference on Empirical Methods in Natural Language Processing (EMNLP) (2018) 64. Yao, J.Y ., Ning, K.P., Liu, Z.H., Ning, M.N., Yuan, L.: Llm lies: Hallucinations are not bugs, but features as adversarial examples. arXiv preprint arXiv:2310.01469 (2023) 65. Yao, S., Yu, D., Zhao, J., Shafran, I., Griffiths, T.L., Cao, Y ., Narasimhan, K.: Tree",
      "chunk_index": 21
    },
    {
      "index": 63,
      "chunk_id": "RAG_Eval_Survey2024_chunk_22",
      "source_id": "RAG_Eval_Survey2024",
      "text": "Ning, K.P., Liu, Z.H., Ning, M.N., Yuan, L.: Llm lies: Hallucinations are not bugs, but features as adversarial examples. arXiv preprint arXiv:2310.01469 (2023) 65. Yao, S., Yu, D., Zhao, J., Shafran, I., Griffiths, T.L., Cao, Y ., Narasimhan, K.: Tree of Thoughts: Deliberate problem solving with large language models (2023) Evaluation of Retrieval-Augmented Generation: A Survey 19 66. Yu, X., Cheng, H., Liu, X., Roth, D., Gao, J.: ReEval: Automatic hallucination evaluation for retrieval-augmented large language models via transferable adversarial attacks. In: Duh, K., Gomez, H., Bethard, S. (eds.) Findings of the Association for Computational Linguis- tics: NAACL 2024. pp. 1333-1351. Association for Computational Linguistics, Mexico City, Mexico (Jun 2024), https://aclanthology.org/2024.findings-naacl.85 67. Zhang, K., Liu, Q., Qian, H., Xiang, B., Cui, Q., Zhou, J., Chen, E.: Eatn: An efficient adap- tive transfer network for aspect-level sentiment analysis. IEEE Transactions on Knowledge and Data Engineering 35(1), 377-389 (2021) 68. Zhang, K., Zhang, H., Liu, Q., Zhao, H., Zhu, H., Chen, E.: Interactive attention transfer network for cross-domain sentiment classification. In: Proceedings of the AAAI Conference on Artificial Intelligence. vol. 33, pp. 5773-5780 (2019) 69. Zhang, K., Zhang, K., Zhang, M., Zhao, H., Liu, Q., Wu, W., Chen, E.: Incorporating dy- namic semantics into pre-trained language model for aspect-based sentiment analysis. arXiv preprint arXiv:2203.16369 (2022) 70. Zhang, Q., Chen, S., Xu, D., Cao, Q., Chen, X., Cohn, T., Fang, M.: A Survey for Ef- ficient Open Domain Question Answering. In: Rogers, A., Boyd-Graber, J., Okazaki, N. (eds.) Proceedings of the 61st Annual Meeting of the Association for Computational Linguis- tics (V olume 1: Long Papers). pp. 14447-14465. Association for Computational Linguistics, Toronto, Canada (Jul 2023). https://doi.org/10.18653/v1/2023.acl-long. 808, https://aclanthology.org/2023.acl-long.808 71. Zhang, S., Liu, X., Liu, J., Gao, J., Duh, K., Van Durme, B.: Record: Bridging the gap between human and machine commonsense reading comprehension (Oct 2018). https: //doi.org/10.48550/ARXIV.1810.12885 72. Zhang, T., Kishore, V ., Wu, F., Weinberger, K.Q., Artzi, Y .: BERTScore: Evaluating Text Generation with BERT. In: 8th International Conference on Learning Representations, ICLR 2020, Addis Ababa, Ethiopia, April 26-30, 2020. OpenReview.net (2020), https: //openreview.net/forum?id=SkeHuCVFDr 73. Zhang, Y ., Khalifa, M., Logeswaran, L., Lee, M., Lee, H., Wang, L.: Merging Gener- ated and Retrieved Knowledge for Open-Domain QA. In: Bouamor, H., Pino, J., Bali, K. (eds.) Proceedings of the 2023 Conference on Empirical Methods in Natural Language Processing. pp. 4710-4728. Association for Computational Linguistics, Singapore (Dec 2023). https://doi.org/10.18653/v1/2023.emnlp-main.286, https:// aclanthology.org/2023.emnlp-main.286 74. Zhao,",
      "chunk_index": 22
    },
    {
      "index": 64,
      "chunk_id": "RAG_Eval_Survey2024_chunk_23",
      "source_id": "RAG_Eval_Survey2024",
      "text": "and Retrieved Knowledge for Open-Domain QA. In: Bouamor, H., Pino, J., Bali, K. (eds.) Proceedings of the 2023 Conference on Empirical Methods in Natural Language Processing. pp. 4710-4728. Association for Computational Linguistics, Singapore (Dec 2023). https://doi.org/10.18653/v1/2023.emnlp-main.286, https:// aclanthology.org/2023.emnlp-main.286 74. Zhao, P., Zhang, H., Yu, Q., Wang, Z., Geng, Y ., Fu, F., Yang, L., Zhang, W., Cui, B.: Retrieval-augmented generation for ai-generated content: A survey (Feb 2024). https: //doi.org/10.48550/ARXIV.2402.19473 75. Zheng, L., Chiang, W.L., Sheng, Y ., Zhuang, S., Wu, Z., Zhuang, Y ., Lin, Z., Li, Z., Li, D., Xing, E.P., Zhang, H., Gonzalez, J.E., Stoica, I.: Judging llm-as-a-judge with mt-bench and chatbot arena (Jun 2023). https://doi.org/10.48550/ARXIV.2306.05685 76. Zhou, Y ., Lin, X., Zhang, X., Wang, M., Jiang, G., Lu, H., Wu, Y ., Zhang, K., Yang, Z., Wang, K., Sui, Y ., Jia, F., Tang, Z., Zhao, Y ., Zhang, H., Yang, T., Chen, W., Mao, Y ., Li, Y ., Bao, D., Li, Y ., Liao, H., Liu, T., Liu, J., Guo, J., Zhao, X., WEI, Y ., Qian, H., Liu, Q., Wang, X., Kin, W., Chan, Li, C., Li, Y ., Yang, S., Yan, J., Mou, C., Han, S., Jin, W., Zhang, G., Zeng, X.: On the opportunities of green computing: A survey (Nov 2023) 77. Zhu, F., Lei, W., Wang, C., Zheng, J., Poria, S., Chua, T.S.: Retrieving and Reading: A Comprehensive Survey on Open-domain Question Answering. Tech. rep. (May 2021), http://arxiv.org/abs/2101.00774, arXiv:2101.00774 [cs] type: article 20 Hao Yu, Aoran Gan, Kai Zhang, Shiwei Tong †, Qi Liu, and Zhaofeng Liu A Structure of RAG System A.1 Retrieval Component The retrieval component of RAG systems in Figure 1 can be categorized into three types: sparse retrieval, dense retrieval [77], and web search engine. The standard for evaluation is the output of relevant documents with numerical scores or rankings. Before the introduction of neural networks,sparse retrievals are widely used for re- trieving relative text content. Methods like TF-IDF [46] and BM25 [47] rely on keyword matching and word frequency but may miss semantically relevant documents without keyword overlap. By leveraging deep learning models such as BERT [9], dense retrieval can capture the semantic meaning of texts, which allows them to find relevant documents even when keyword overlap is minimal. This is crucial for complex queries that require a contex- tual understanding to retrieve accurate information. With advanced fusion structure for queries and documents [28] and the",
      "chunk_index": 23
    },
    {
      "index": 65,
      "chunk_id": "RAG_Eval_Survey2024_chunk_24",
      "source_id": "RAG_Eval_Survey2024",
      "text": "which allows them to find relevant documents even when keyword overlap is minimal. This is crucial for complex queries that require a contex- tual understanding to retrieve accurate information. With advanced fusion structure for queries and documents [28] and the more efficient implementation of K-Nearest Neigh- bors (KNN) [51], Approximate Nearest Neighbor (ANN) [12,25] search techniques, dense retrieval methods have become practical for large-scale use. Web search engine employs the complex online search engine to provide relevant documents, such as Google Search [18], Bing Search [40], DuckDuckGo [13]. RAG systems can traverse the web's extensive information, potentially returning a more di- verse and semantically relevant set of documents via the API of the search provider. The black box of the search engine and the expense of large-scale search are not affordable sometimes. It is observed that dense retrieval techniques, particularly those leveraging embed- dings, stand out as the preferred choice within the RAG ecosystem. These methods are frequently employed in tandem with sparse retrieval strategies, creating a hybrid approach that balances precision and breadth in information retrieval. Moreover, the adoption of sophisticated web search engines for benchmark assessment underscores their growing significance in enhancing the robustness and comprehensiveness of eval- uations. Indexing The indexing component processes and indexes document collections, such as HuggingFace datasets or Wikipedia pages. Chunking before indexing can improve retrieval by limiting similarity scores to individual chunks, as semantic embedding is less accurate for long articles, and desired content is often brief [32]. Index creation is designed for fast and efficient search. For example, the inverted index for sparse retrieval and the ANN index for dense retrieval. Sparse Retrieval involves calculating IDF for each term and storing values in a database for quick look-up and scoring when queried. Dense Retrieval encodes documents into dense vectors using a pre-trained language model like BERT. These vectors are then indexed using an Approximate Nearest Neigh- bor (ANN) search technique, like graph-based Hierarchical Navigable Small World (HNSW) or Inverted File Index (IVF) [12]. This process allows for the efficient re- trieval of \"closed\" items by given predefined distance metrics. Evaluation of Retrieval-Augmented Generation: A Survey 21 Search This step is responsible for retrieving relevant documents based on a given query. Queries are submitted using the respective API to retrieve relevant documents for web search engine retrieval. For local resources, the query component is responsible for formatting the query in the format",
      "chunk_index": 24
    },
    {
      "index": 66,
      "chunk_id": "RAG_Eval_Survey2024_chunk_25",
      "source_id": "RAG_Eval_Survey2024",
      "text": "for retrieving relevant documents based on a given query. Queries are submitted using the respective API to retrieve relevant documents for web search engine retrieval. For local resources, the query component is responsible for formatting the query in the format required by different sparse or dense retrieval methods. Then, the query is submitted to the retrieval system, which returns a set of relevant documents along with their scores. In both local and web-based scenarios, an optional reranker can be employed to refine the ranking of retrieved documents further. The reranker usually comprises a more complex and larger model that considers additional features of the documents and the given query. These additional features often include the semantic relationship between the query and the document content, document importance or popularity, and other custom measures specific to the information need at hand. A.2 Generation Component The evaluable output for the generation component is the response of LLMs and the structured or formatted output from the phrased response. Prompting The generation process critically hinges on prompting, where a query, re- trieval outcomes, and instructions converge into a single input for the language model. Research showcases various strategic prompting tactics such as the Chain of Thought (CoT) [60], Tree of Thought (ToT) [3], and Self-Note [31], each significantly shaping the model's output. These methods, especially the step-by-step approach, are pivotal in augmenting LLMs for intricate tasks. Prompting innovations have introduced methods like Rephrase and Respond (RaR) [8], enhancing LLMs by refining queries within prompts for better comprehension and response. This technique has proven to boost performance across diverse tasks. The latest RAG benchmarks [61,62] in the specific domains start to evaluate the robustness of various prompting engineering skills, including CoT, RaR, etc. Inference The final input string prepared in the prompting step is then passed on to the LLMs as input, which generates the output. The inference stage is where the LLM operates on the input derived from the retrieval and the prompting stages in the pipeline to generate the final output. This is usually the answer to the initial query and is used for downstream tasks. Depending on the specifics of the task or expected output structure, a post-processing step may be implemented here to format the generated output suitably or extract spe- cific information from the response. For example, the classification problems (multi- choice questions) or if the task requires the extraction of",
      "chunk_index": 25
    },
    {
      "index": 67,
      "chunk_id": "RAG_Eval_Survey2024_chunk_26",
      "source_id": "RAG_Eval_Survey2024",
      "text": "expected output structure, a post-processing step may be implemented here to format the generated output suitably or extract spe- cific information from the response. For example, the classification problems (multi- choice questions) or if the task requires the extraction of specific information from the generated text, this step could involve additional named entity recognition or parsing operations.",
      "chunk_index": 26
    },
    {
      "index": 68,
      "chunk_id": "FaithEval2024_chunk_00",
      "source_id": "FaithEval2024",
      "text": "Published as a conference paper at ICLR 2025 FAITH EVAL: C AN YOUR LANGUAGE MODEL STAY FAITHFUL TO CONTEXT , E VEN IF \"THE MOON IS MADE OF MARSHMALLOWS \" Yifei Ming1∗, Senthil Purushwalkam1, Shrey Pandit2, Zixuan Ke1, Xuan-Phi Nguyen1 Caiming Xiong1, Shafiq Joty1 1Salesforce AI Research 2University of Texas at Austin ABSTRACT Ensuring faithfulness to context in large language models (LLMs) and retrieval- augmented generation (RAG) systems is crucial for reliable deployment in real- world applications, as incorrect or unsupported information can erode user trust. Despite advancements on standard benchmarks, faithfulness hallucination-where models generate responses misaligned with the provided context-remains a sig- nificant challenge. In this work, we introduce FaithEval, a novel and compre- hensive benchmark tailored to evaluate the faithfulness of LLMs in contextual scenarios across three diverse tasks: unanswerable, inconsistent, and counterfac- tual contexts. These tasks simulate real-world challenges where retrieval mecha- nisms may surface incomplete, contradictory, or fabricated information. FaithE- val comprises 4.9K high-quality problems in total, validated through a rigorous four-stage context construction and validation framework, employing both LLM- based auto-evaluation and human validation. Our extensive study across a wide range of open-source and proprietary models reveals that even state-of-the-art models often struggle to remain faithful to the given context, and that larger models do not necessarily exhibit improved faithfulness. Code is available at: https://github.com/SalesforceAIResearch/FaithEval. 1 I NTRODUCTION The rapid development of large language models (LLMs) has significantly advanced natural language understanding and generation tasks, enabling systems to produce fluent and coherent responses across a variety of applications (Bubeck et al., 2023; Zhao et al., 2024b; Wu et al., 2024b). The capabilities of these models have been further enhanced by integrating external information from the Internet or knowledge sources using a popular approach of Retrieval-Augmented Generation (RAG) (Lewis et al., 2020; Zhao et al., 2024a). In this paradigm, generated outputs are enhanced by retrieving and encoding relevant information as context to the model. While RAG facilitates the integration of additional knowledge, hallucination-where models generate unsupported or ungrounded content-remains a critical challenge (Nguyen et al., 2024). Hallucination in LLMs can be generally categorized into two types: factual hallucination, where generated content deviates from established world knowledge, and faithfulness hallucination, where the generated response is inconsistent with the provided context (Huang et al., 2023a). While factuality has received extensive attention, with numerous benchmarks designed to evaluate correctness against common sense or world knowledge (Lee et al., 2022;",
      "chunk_index": 0
    },
    {
      "index": 69,
      "chunk_id": "FaithEval2024_chunk_01",
      "source_id": "FaithEval2024",
      "text": "and faithfulness hallucination, where the generated response is inconsistent with the provided context (Huang et al., 2023a). While factuality has received extensive attention, with numerous benchmarks designed to evaluate correctness against common sense or world knowledge (Lee et al., 2022; Min et al., 2023; Chern et al., 2023; Wei et al., 2024), a fine-grained and holistic evaluation of faithfulness on noisy contexts remains underexplored, particularly when the context contradicts commonly accepted facts. Maintaining faithfulness to the context is especially important for various personalized applications and can be critical in high-stakes domains such as healthcare, finance and law, where inaccurate or ungrounded responses can erode user trust and lead to severe consequences (Bommarito & Katz, 2022; Pal et al., 2023). One of the key challenges in addressing faithfulness hallucination in RAG stems from the retrieval process, where the wealth of documents on the Internet varies in credibility. This complexity is ∗Correspondence: yifei.ming@salesforce.com arXiv:2410.03727v3 [cs.CL] 24 Apr 2025 Published as a conference paper at ICLR 2025 Ov erall Accuracy (normaliz ed) 7 6 . 8 66 . 8 54 . 8 53 . 1 45 . 5 45 . 1 30 . 3 Figure 1: Performance summary on FaithEval Benchmark. Each bar shows the combined accuracy (normalized) for the best model from each organization across three tasks: Counterfactual, Inconsis- tent, and Unanswerable (Sec 3.1). Different colors in each bar represent the accuracy for each task. further compounded when long retrieved content includes multiple relevant paragraphs that omit key details, present conflicting evidence, or propagate counterfactual claims. Existing hallucination evaluation benchmarks fall short in providing fine-grained assessments of how well models align their responses with the context. They often do not disentangle factuality from faithfulness (Li et al., 2024b; Chen et al., 2024c) or capture the full range of contextual nuances (Lin et al., 2022; Yin et al., 2023). Moreover, current hallucination detection solutions focus on identifying hallucinations in model outputs (Liu et al., 2021; Li et al., 2023b; Hu et al., 2024), which is orthogonal to the task of understanding the impact of contexts on faithfulness hallucination. In this work, we introduce FaithEval, a comprehensive benchmark specifically designed to evaluate the contextual faithfulness of LLMs across three diverse tasks: unanswerable, inconsistent, and counterfactual contexts. These tasks simulate real-world challenges where retrieval mechanisms may surface incomplete, contradictory, or fabricated information (Figure 2). FaithEval includes a total of 4.9K high-quality samples, constructed using a",
      "chunk_index": 1
    },
    {
      "index": 70,
      "chunk_id": "FaithEval2024_chunk_02",
      "source_id": "FaithEval2024",
      "text": "faithfulness of LLMs across three diverse tasks: unanswerable, inconsistent, and counterfactual contexts. These tasks simulate real-world challenges where retrieval mechanisms may surface incomplete, contradictory, or fabricated information (Figure 2). FaithEval includes a total of 4.9K high-quality samples, constructed using a rigorous four-stage framework with multi-turn LLM-based context verification and human validation. We conduct a holistic evaluation on 18 representative proprietary and open-sourced models, revealing that faithfulness remains challenging, even for the most competitive LLMs, despite their strong performance on standard benchmarks. Figure 1 summarizes the performance of representative models from each organization, with each bar representing performance on the individual tasks. To our knowledge, FaithEval is thefirst fine- grained and comprehensive benchmark specifically targeting contextual faithfulness hallucination, contributing to the broader effort toward developing reliable next-generation foundation models. Our key contributions are summarized as follows: • We introduce FaithEval, a novel and comprehensive benchmark dedicated to evaluating contextual faithfulness in LLMs across three diverse tasks: unanswerable, inconsistent, and counterfactual contexts. • We develop a scalable four-stage framework for context construction and validation, which incorporates multi-turn LLM-based validation and human annotation to ensure high-quality contextual QA pairs. • We perform an extensive and in-depth study on a wide range of competitive open-source and proprietary models. We highlight that faithfulness remains a significant challenge for recent LLMs and that allegedly larger models, such as GPT-4o and Llama-3-70B-Instruct, do not necessarily lead to improved faithfulness. 2 R ELATED WORKS Contextual LLM and retrieval-augmented generation. As the demand for contextual LLMs continues to grow, retrieval-augmented generation (RAG) systems offer a promising solution by Published as a conference paper at ICLR 2025 integrating external knowledge retrieval with LLMs (Lewis et al., 2020; Sarto et al., 2022; Ramos et al., 2022; Huang et al., 2023b; Zhao et al., 2024a). In RAG, model responses are grounded using knowledge sourced from private or open-access data repositories. A typical RAG system operates through a close interaction between the retriever and a generator. The retriever (Li et al., 2023a; Meng et al., 2024; Chen et al., 2024a) identifies relevant documents from the source, and this retrieved information is supplied to the generator (e.g., a language model) to produce grounded outputs (Lewis et al., 2020; Izacard & Grave, 2020; Borgeaud et al., 2021; Ke et al., 2024). Recent works develop more sophisticate RAG frameworks to improve answer reliability (Asai et al., 2023; Li et al., 2024d; Xu et al., 2024; Xiang",
      "chunk_index": 2
    },
    {
      "index": 71,
      "chunk_id": "FaithEval2024_chunk_03",
      "source_id": "FaithEval2024",
      "text": "(Lewis et al., 2020; Izacard & Grave, 2020; Borgeaud et al., 2021; Ke et al., 2024). Recent works develop more sophisticate RAG frameworks to improve answer reliability (Asai et al., 2023; Li et al., 2024d; Xu et al., 2024; Xiang et al., 2024). The increased context sizes in LLMs have improved their ability to handle longer text sequences, allowing them to handle complex tasks requiring extensive background knowledge (Gao et al., 2023; Song et al., 2024; Shi et al., 2024). These improvements are particularly beneficial for long-form question answering (Joshi et al., 2017; Kwiatkowski et al., 2019b; Li et al., 2024a). However, the variation in source quality can exacerbate challenges to maintaining faithfulness in LLMs, especially in longer contexts retrieved from the Internet. Hallucination and faithfulness evaluation. Hallucination in LLMs refers to the generation of ungrounded content, either from the provided context or established world knowledge. The former is typically described as factuality hallucination, while the latter is known as faithfulness hallucination, which highlights the discrepancy between the model's output and the context (Huang et al., 2023a; Ye et al., 2023). While there is rich literature on factuality evaluation and benchmarks with and without contexts (Lee et al., 2022; Min et al., 2023; Chern et al., 2023; Wei et al., 2024; Li et al., 2024c), faithfulness has mostly been explored for summarization (Laban et al., 2023; Jia et al., 2023) natural language explanations (Atanasova et al., 2023; Siegel et al., 2024) and recently QA (Adlakha et al., 2024). Chen et al. (2024b) investigates the robustness of contextual LLMs under noisy retrieval, with a primary focus on news articles. Another line of research focuses on hallucination detection (Liu et al., 2021; Li et al., 2023b; Hu et al., 2024), which aims to detect hallucinated outputs. The task focuses on the model output instead of the context (input). Additional efforts have been made to create QA benchmarks based on common misconceptions (Lin et al., 2022) or questions that are unanswerable by nature (Yin et al., 2023). However, none of these datasets are contextual. In contrast, each question in FaithEval is accompanied by a multi-paragraph context, mimicking RAG scenarios with long and noisy contexts. Adversarial context generation. Generating challenging or adversarial contexts for language models has been explored in various scenarios. One line of research focuses on context modification. Shi et al. (2023) propose a template-based framework that adds irrelevant facts to the",
      "chunk_index": 3
    },
    {
      "index": 72,
      "chunk_id": "FaithEval2024_chunk_04",
      "source_id": "FaithEval2024",
      "text": "contexts. Adversarial context generation. Generating challenging or adversarial contexts for language models has been explored in various scenarios. One line of research focuses on context modification. Shi et al. (2023) propose a template-based framework that adds irrelevant facts to the context and studies the effectiveness of different prompting techniques. Yu et al. (2024) leverage LLMs to perturb original evidence, potentially altering the answers, while Manakul et al. (2023) utilize LLMs to generate purely synthetic contexts that support given statements. To study the impact of knowledge conflicts, Wu et al. (2024a); Xie et al. (2024) use LLMs to create adversarial contexts that conflict with the models' internal knowledge, improving coherence compared to previous word-level editing methods (Longpre et al., 2021; Chen et al., 2022; Zhou et al., 2023). Pan et al. (2023) studies the impact of LLM- generated misinformation. Another research direction involves modifying the question. Ramakrishna et al. (2023) generate invalid (unanswerable) questions, while Huang et al. (2024) build a small-scale dataset with 209 questions containing adversarial facts or incorrect information based on diverse templates. In contrast, we introduce the first fine-grained, larger-scale (4.9K) high-quality contextual QA benchmark featuring multi-paragraph coherent contexts across three diverse tasks. 3 F AITH EVAL BENCHMARK 3.1 T ASK OVERVIEW To systematically evaluate the contextual faithfulness of LLMs, FaithEval contains three diverse tasks including unanswerable context, inconsistent context, and counterfactual context. Each sample (c, q, a) consists of a question q, and a long context passage made up of one or more documents c = (d1, ..., dn), and a groundtruth answera. The model is expected to answer the question leveraging the information in the provided context. An overview of each task is presented in Figure 2. Next, we illustrate the construction of each task in detail. Unanswerable Context. An unanswerable context arises when the context includes relevant details but lacks the information needed to answer the question. In FaithEval, answerability is determined Published as a conference paper at ICLR 2025 Unans w er able Cont e xt Count er f ac tual Cont e xt In , 78.5% o f D allas c omm ut ers driv e t o w ork alone . ... In , the American C omm unity Sur v e y estim a t ed 12.8% for carpooling , 3.5% for riding tr ansit... 2015 [Doc 1] Life o f P i is a Can adian fan tas y",
      "chunk_index": 4
    },
    {
      "index": 73,
      "chunk_id": "FaithEval2024_chunk_05",
      "source_id": "FaithEval2024",
      "text": "... In , the American C omm unity Sur v e y estim a t ed 12.8% for carpooling , 3.5% for riding tr ansit... 2015 [Doc 1] Life o f P i is a Can adian fan tas y adv en tur e no v el... with a Bengal tig er n amed ... [Doc 2] ...He endur es 227 da y s str anded on a lifeboa t ...ac c ompanied b y a Bengal tig er n amed ... R ich ar d P ark er William Sh ak espear e ... One in triguing pr oper ty o f w ood th a t h as o ft en been o v erlook ed is its n a tur e ... These findings poin t ed t o the pr esenc e o f ir on-lik e c ompounds within the c ellular struc tur e o f w ood, which c ould e xhibit fain t pr oper ties... early u sed m agne tiz ed w ood... m agne tic m agne tic shipbuilders Inconsis t ent Cont e xt R ich ar d P ark er Inc onsist en t (m ultiple answ ers) Question: Wh a t is the tig er ' s n ame in Life o f P i? Question: Which sta t emen t best e xplains wh y a tr ee br anch fl oa ts on w a t er ? [four op tions] W ood is buo y an t W ood is m agne tic Question: Which gr oup o f c omm ut ers in D allas in is lar g er : carpooling or tr ansit ? Carpooling U nkno wn Figure 2: Demonstration of each task in FaithEval. Left: in Unanswerable Context, the context does not contain the answer to the question. Middle: in Inconsistent Context, multiple answers are supported by different documents. Right: in Counterfactual Context, the context contains counterfactual statements that contradict common sense or world knowledge. Complete contexts can be seen in Appendix E. solely by the context, regardless of whether the question itself is unanswerable. For instance, in the example in Figure 2 (Left), the context provides the proportion of both types of commuters in 2015. However, the question \"Which group of commuters in Dallas in 2009 is larger: carpooling or transit?\" is unanswerable, as the context lacks",
      "chunk_index": 5
    },
    {
      "index": 74,
      "chunk_id": "FaithEval2024_chunk_06",
      "source_id": "FaithEval2024",
      "text": "the example in Figure 2 (Left), the context provides the proportion of both types of commuters in 2015. However, the question \"Which group of commuters in Dallas in 2009 is larger: carpooling or transit?\" is unanswerable, as the context lacks specific data from 2009. To create such task, we modify the context from a collection of 10 contextual QA datasets, covering a wide range of domains (see Source datasets below). For each sample, we prompt an LLM to modify the original context so that it no longer contains the supporting evidence for the ground truth answer. Additional sentences may be woven into the new context to maintain coherence. The full prompt is shown in Figure 17. This process resulted in a total of 2.4k contextual QA pairs. To verify the quality of the modified contexts, we achieved over 98% agreement with professional human annotators (Sec 3.2). Inconsistent Context. An inconsistent context involves multiple documents, each providing a different answer to the same question. This simulates noisy retrieval scenarios, where documents from sources with varying levels of credibility are retrieved. For instance, as shown in Figure 2 (Middle), the context presents conflicting information about the tiger's name in the novelLife of Pi. A faithful model should be able to identify such inconsistencies, especially when instructed to do so. To create this task, we modify contexts from the same collection of contextual QA datasets used in the Unanswerable Context task. For each sample, the LLM is provided with a context passage, a question, and an original answer, which is supported by the context. The goal is to modify the context so that it introduces fabricated supporting evidence for a new, conflicting answer. The detailed prompt is shown in Figure 18. Since this task is more challenging, we curated a collection of 1.5k high-quality contextual QA pairs after filtering through professional human annotators. Counterfactual Context. A counterfactual context contains statements that contradict with common sense or widely accepted facts, such as \"water freeze at 100 degrees Celsius\", \"wood is magnetic\", or \"carbon dioxide is the most abundant greenhouse gas in the atmosphere\". Unlike the other two tasks, the questions in this task are required to be relevant to such well-known facts. We curate this task based on ARC-Challenge (Clark et al., 2018), a QA dataset covering grade-school level, multiple- choice science questions. Since the original dataset does not include context, we prompt an",
      "chunk_index": 6
    },
    {
      "index": 75,
      "chunk_id": "FaithEval2024_chunk_07",
      "source_id": "FaithEval2024",
      "text": "required to be relevant to such well-known facts. We curate this task based on ARC-Challenge (Clark et al., 2018), a QA dataset covering grade-school level, multiple- choice science questions. Since the original dataset does not include context, we prompt an LLM to generate a long, multi-paragraph context that seamlessly provides fabricated supporting evidence for a counterfactual answer. The detailed prompt is shown in Figure 19. This process resulted in a total of 1k contextual QA pairs, each with three to five options. Due to the multiple-choice nature, we can use keyword matching to verify the quality of the synthetic contexts (Appendix B). Source datasets. We curate new contexts based on a diverse collection of contextual QA datasets, including SQuAD (Rajpurkar et al., 2016), NewsQA (Trischler et al., 2017), TriviaQA (Joshi et al., 2017), NaturalQuestions (Kwiatkowski et al., 2019a), SearchQA (Dunn et al., 2017), HotpotQA (Yang et al., 2018), BioASQ (Tsatsaronis et al., 2015), DROP (Dua et al., 2019), RACE (Lai et al., 2017), Published as a conference paper at ICLR 2025 Cont e xt Gener ation T ask Construction Modified c on t e x t Syn the tic c on t e x t Origin al c on t e x t LLM LLM Q A pair Q A pair Sour c e: Non-C on t e x tual D a tase t Sour c e: C on t e x tual D a tase t Un answer able Inc onsist en t C oun t erfac tual C on t e x t with r emo v ed e videnc e C on t e x t with m ultiple answ ers C on t e x t with c oun t er fac tual e videnc e Aut o Ev al b y LLM Judge Human Annotation m ajority v o t e Q1: Is ans v alid giv en c on t e x t (if)? Q 2 : Is ans un i que giv en c on t e x t (only - if)? F in al V ersion C on t e x tual Q A D a tase t Is ans v alid & uni q ue filt er Figure 3: Illustration of task construction and validation framework. (1) Context Generation: given a source QA dataset, we prompt an LLM to generate a new context based on a question, the original answer, and",
      "chunk_index": 7
    },
    {
      "index": 76,
      "chunk_id": "FaithEval2024_chunk_08",
      "source_id": "FaithEval2024",
      "text": "& uni q ue filt er Figure 3: Illustration of task construction and validation framework. (1) Context Generation: given a source QA dataset, we prompt an LLM to generate a new context based on a question, the original answer, and optionally the original context. (2) Task Construction: we construct the prompt for each sample by combining the original question, the new context, and task-specific instructions. (3) Auto Eval by LLM Judge: we validate the quality of the new context by checking if and only-if the new answer is supported by the new context. (4) Human Annotation: we further filter out invalid contextual QA pairs based on the majority vote results from professional annotators. TextbookQA (Kembhavi et al., 2017). We adopt the test splits from Fisch et al. (2019). Due to variations in human annotations, the final collection consists of 150 samples per dataset for the Inconsistent Context task and around 240 samples per dataset for the Unanswerable Context task. 3.2 T ASK CONSTRUCTION AND VALIDATION FRAMEWORK Task construction. An overview of our task construction and validation framework is shown in Figure 3. Given a source QA sample and an original context (optional), we prompt an LLM to generate both a new context and a new answer for the Counterfactual and Inconsistent tasks, or only a new context (that supports no answer) for the Unanswerable task. To make the tasks challenging, the new context should be coherent and contain minimal modifications if the original context is provided. In addition, multiple paragraphs not directly related to the answer are included, serving as distractors. The new context is generated with detailed justifications explaining how it satisfies the task criterion. We construct the prompt for each sample by combining the original question, the new context, and task-specific instructions (Sec 3.3). In particular, an inconsistent context is created by concatenating the new context with the original context, each supporting a different answer. Auto validation and human annotation. We validate the quality of the new context by using a separate LLM judge to verify whether the new answer is valid given the context (\"if\" condition) and whether the context does not support alternative answers (\"only-if\" condition). For example, the new context in Figure 2 (Right) should not mention wood is buoyant. Samples that fail to meet both conditions are filtered out. Next, we perform meticulous human annotation. Depending on the task's validation difficulty, we employ different",
      "chunk_index": 8
    },
    {
      "index": 77,
      "chunk_id": "FaithEval2024_chunk_09",
      "source_id": "FaithEval2024",
      "text": "For example, the new context in Figure 2 (Right) should not mention wood is buoyant. Samples that fail to meet both conditions are filtered out. Next, we perform meticulous human annotation. Depending on the task's validation difficulty, we employ different strategies. As the Inconsistent Context task is challenging to validate, we rely on full human annotation. Three Mechanical Turk (Crowston, 2012) workers judge whether each contextual QA pair meets the \"if\" and \"only-if\" conditions, with final inclusion determined by majority agreement. This yields 1.5K samples. For the Unanswerable Context task, which is easier to validate, we use a similar majority-vote approach, achieving over 98% agreement among human annotators. This yields 2.4K samples. For the Counterfactual Context task, since the answer options are provided with the context, we validate using a string-based matching method, where the context passes if all words from the answer appear in the context. This decision was based on our pilot human studies that showed nearly perfect agreement with the string-matching method on generated contexts based on ARC-Challenge. The filtering results in 1K samples. After filtering, the FaithEval benchmark contains a total of 4.9K high-quality contextual QA pairs. More details are included in Appendix A. 3.3 E VALUATION Models. We evaluate a wide range of competitive open-sourced and proprietary language models with different scales, including the most recent releases up to Sep 10, 2024. Our initial experiments suggest Published as a conference paper at ICLR 2025 that instruction-tuned (chat) models significantly outperform base models. Therefore, we consider 18 competitive chat models, including Phi-3-mini-128k-instruct (3.8B), Phi-3-medium-128k-instruct (14B), Phi-3.5-mini-instruct (3.8B) (Abdin et al., 2024), LLaMA-3-8B-Instruct, LLaMA-3.1-8B- Instruct, LLaMA-3-70B-Instruct, LLaMA-3.1-70B-Instruct (Llama, 2024), Mistral-7B-Instruct-v0.3, Mistral-Nemo-Instruct-2407 (12B) (Jiang et al., 2023), Gemma-2-9B-it, and Gemma-2-27B-it (Team, 2024). For proprietary models, we consider Open AI's GPT-3.5 Turbo, GPT-4o-mini, GPT-4o, GPT-4 Turbo, Cohere's Command R (35B), Command R+ (104B), and Anthropic's Claude 3.5 Sonnet. Default Evaluation Scheme. For all tasks, we append the following prompt to each question: You are an expert in retrieval-based question answering. Please respond with the exact answer, using only the information provided in the context. For the Unanswerable Context task, we append an additional instruction: If there is no information available from the context, the answer should be \"unknown\". Similarly, for the Inconsistent Context task, the instruction is: If there is conflicting information or multiple answers in the context, the answer should be \"conflict\" . Note that for Counterfactual Context,",
      "chunk_index": 9
    },
    {
      "index": 78,
      "chunk_id": "FaithEval2024_chunk_10",
      "source_id": "FaithEval2024",
      "text": "available from the context, the answer should be \"unknown\". Similarly, for the Inconsistent Context task, the instruction is: If there is conflicting information or multiple answers in the context, the answer should be \"conflict\" . Note that for Counterfactual Context, we do not add additional task instructions. Our primary evaluation metric across all tasks is accuracy (ACC), where a model's response is considered correct if it mentions the ground truth answer. All models are evaluated using their default configurations with deterministic decoding (temperature = 0). We report both strict-matching (S) ACC, which considers only a single ground truth answer (e.g., \"unknown\"), and non-strict matching (N) ACC, which allows a broader range of semantically similar phrases. A detailed list of valid phrases is provided in Appendix A. Alternative Evaluation Schemes. We study alternative evaluation strategies in Section 5. Specifically, we examine non-deterministic decoding with temperature scaling (t = 0.3, top-p = 0.9). Additionally, we investigate the impact of chain-of-thought (CoT) prompting (Wei et al., 2022; Kojima et al., 2022) using the following instruction: Given the context, first provide a brief answer to the question. Then, explain your reasoning step by step, detailing how you arrived at the answer. 4 M AIN RESULTS Figure 4: Model performance comparison on the Unanswerable Context task, where no evidence supports the answer. Columns are sorted by performance on the Original task (original context). Proprietary model names are highlighted in orange. 4.1 U NANSWERABLE CONTEXT Abstaining is challenging, even when explicitly instructed. The results of the Unanswerable Context task are summarized in Figure 4, ranked by performance on the original context. Proprietary model names are highlighted in orange. We highlight the following key observations: (1) Modern LLMs experience significant performance degradation in this task. Across all chat models, the performance gap ranges from 13.6% to 68.4%. (2) High performance on the original context does not correlate with high performance on the unanswerable context. For example, while Phi-3-medium- 128k-instruct achieves 75.8% accuracy on the original context, closely approaching the SoTA (80.1%), it struggles to abstain from answering in the unanswerable context, with an accuracy of only 7.4%. (3) Larger model sizes are more advantageous within the same model family. For instance, compared Published as a conference paper at ICLR 2025 Figure 5: Model performance comparison on the Inconsistent Context task. Columns are sorted by the performance on the original task. Proprietary models are colored in orange. to",
      "chunk_index": 10
    },
    {
      "index": 79,
      "chunk_id": "FaithEval2024_chunk_11",
      "source_id": "FaithEval2024",
      "text": "model family. For instance, compared Published as a conference paper at ICLR 2025 Figure 5: Model performance comparison on the Inconsistent Context task. Columns are sorted by the performance on the original task. Proprietary models are colored in orange. to the 7B model, Llama-3.1-70B-instruct improves performance on the Unanswerable Context task by 10.3%. Similar trends hold for the Gemma-2 and Llama-3 model families. 4.2 I NCONSISTENT CONTEXT Performance varies significantly on inconsistent context across model families. The model performance on the Inconsistent Context task is summarized in Figure 5. We have the following key observations: (1) Performance varies substantially across different model families. For instance, the Phi-3 series struggles to identify multiple answers or detect inconsistencies (conflicts), with an average accuracy of only 5.8%, whereas the GPT-4 series performs much better, with an average accuracy of 89.35%. (2) Open-source models lag behind proprietary models. Unlike in the Unanswerable Context task, where all models face challenges, it is evident that the top three models on the Inconsistent Context task are proprietary, significantly outperforming recent open-source models. Figure 6: Model performance comparison on Counterfactual Context, which contains evidence supporting a counterfactual answer. Proprietary models are colored in orange. 4.3 C OUNTERFACTUAL CONTEXT Faithfulness remains a limitation for contextual LLMs. The results on the Counterfactual Context task are shown in Figure 6. The blue bars represent model performance under the closed-book QA setting, where no context is provided. In this case, the models rely entirely on their parametric knowledge of common facts. We observe that nearly half of the models achieve over 90% accuracy, with GPT-4o nearing perfect performance at 96.3%. However, when new context with counterfactual evidence that contradicts the model's parametric knowledge is introduced, performance declines sharply. For example, GPT-4o achieves only 47.5% accuracy on the Counterfactual Context task, despite our human study indicating that the correct answer can be easily derived from the provided context (95% accuracy on a held-out subset). This highlights a significant gap in faithfulness-the Published as a conference paper at ICLR 2025 Figure 7: Performance decomposition on individual datasets for Unanswerable Context (top row) and Inconsistent Context (bottom row). Full results for all models can be seen in Appendix C. ability to generate outputs that align with the provided context-between current state-of-the-art models and human-level performance. 5 D ISCUSSIONS AND FURTHER ANALYSIS A closer look at Unanswerable and Inconsistent Contexts. We present the performance breakdown for",
      "chunk_index": 11
    },
    {
      "index": 80,
      "chunk_id": "FaithEval2024_chunk_12",
      "source_id": "FaithEval2024",
      "text": "seen in Appendix C. ability to generate outputs that align with the provided context-between current state-of-the-art models and human-level performance. 5 D ISCUSSIONS AND FURTHER ANALYSIS A closer look at Unanswerable and Inconsistent Contexts. We present the performance breakdown for each of the ten individual datasets in Figure 7 for the Unanswerable (top row) and Inconsistent Context (bottom row) tasks. We include three representative smaller-scale models: LLama-3.1-8B- Instruct, Mistral-7B-Instruct-v0.3, and Gemma-2-9b-it. Full results for other models are provided in Appendix C. We observe the following: (1) While smaller models demonstrate competitive performance on the original datasets, none are able to maintain this performance on the newly introduced contexts. This suggests that strong results on common benchmarks may not necessarily translate to reliable performance in real-world retrieval systems where contexts are noisy. (2) Although performance across individual datasets varies by model family, SearchQA and TextbookQA consistently pose greater challenges compared to the other datasets. Figure 8: Model performance comparison on the Original Context v.s. New Context for Inconsistent Context task. Proprietary models are colored in orange. Published as a conference paper at ICLR 2025 Task-spec. Inst. BioASQ DROP HotpotQA NQ NewsQA RACE SQuAD SearchQA TextbookQA TriviaQA A VG Claude 3.5 Sonnet✗(original prompt) 0.90 0.94 0.89 0.86 0.81 0.84 0.93 0.97 0.97 0.86 0.90✓(+conflict prompt) 0.85↓ 0.84↓ 0.86↓ 0.78↓ 0.75↓ 0.77↓ 0.95 0.91 ↓ 0.90↓ 0.86 0.85 ↓ GPT-4o✗(original prompt) 0.84 0.81 0.89 0.80 0.78 0.79 0.95 0.90 0.93 0.87 0.85✓(+conflict prompt) 0.77↓ 0.83 0.85↓ 0.79↓ 0.73↓ 0.81 0.93↓ 0.87↓ 0.91↓ 0.86↓ 0.83↓ Table 1: Impact of task-specific instructions on the normal (original) context. Having the additional task instruction degrades the performance on normal contexts consistently. A closer look at Inconsistent Context. Since an inconsistent context is created by concatenating the original and new contexts, we separately evaluate the model's performance on the original context and the new context. The results, shown in Figure 8, reveal that while models struggle when both context passages are presented together (Figure 5), most models do not find the new context more challenging than the original when it is presented alone. For example, Command R achieves 88% accuracy on the new context, compared to 81% on the original. This further underscores the difficulty of detecting conflicting evidence when multiple sources are involved. Sycophancy with task-specific instructions. While the additional instructions used in the Unanswer- able and Inconsistent Context tasks (Sec 3.3) improve the model's awareness of",
      "chunk_index": 12
    },
    {
      "index": 81,
      "chunk_id": "FaithEval2024_chunk_13",
      "source_id": "FaithEval2024",
      "text": "the original. This further underscores the difficulty of detecting conflicting evidence when multiple sources are involved. Sycophancy with task-specific instructions. While the additional instructions used in the Unanswer- able and Inconsistent Context tasks (Sec 3.3) improve the model's awareness of such scenarios, they can also introduce unintended effects when the context is normal (i.e., answerable and consistent). This can lead to what is known as sycophantic behavior (Perez et al., 2023; Wei et al., 2023), where models adjust their responses to align with the user's expectations, even when those expectations are objectively incorrect. We examine this phenomenon in two top-performing models for the In- consistent Context task, GPT-4o and Claude 3.5 Sonnet. Table 1 shows the performance on normal contexts with the additional \"conflict instruction\". We observe a consistent performance drop for both models, with Claude 3.5 experiencing a 5% decrease in average accuracy. This further highlights the challenge of maintaining faithfulness across both normal and noisy contexts. (a) Direct Ans vs. CoT on Unanswerable Context (b) Direct Ans vs. CoT on Inconsistent Context Figure 9: The impact of CoT prompting on Unanswerable (Left) and Inconsistent Contexts (Right). Due to space constraints., we include representative models from different model families. Does chain-of-thought prompting improve faithfulness? Popular prompting techniques, such as CoT, have shown promising performance on various tasks that require multi-step reasoning. We adopt the prompt format in Section 3.3 and summarize the results in Figure 9. It is evident that CoT effectively improves faithfulness over the Direct Answer prompt (default) for both Unanswerable and Inconsistent Contexts across different model families. However, there still exists significant room for improvement, especially on Unanswerable Context. For instance, the leading model achieves only 71.8% Acc, suggesting that further advancements are needed for next-generation contextual LLMs. Strict vs. non-strict matching. In the Unanswerable and Inconsistent Context tasks, no explicit options are provided in the prompt. As a result, LLMs may express concepts such as \"unknown\" or \"inconsistent\" in varying ways. To assess the impact of allowing alternative valid expressions, we compare the performance of strict and non-strict matching. Strict matching only accepts the Published as a conference paper at ICLR 2025 Figure 10: Impact of strict vs. non-strict matching on Unanswerable Context. Non-strict matching allows for a wider range of phrases that express the idea of \"unknown\". The comparison on Inconsistent Context is shown in Appendix C. exact phrases \"unknown\" or \"conflict\" as specified",
      "chunk_index": 13
    },
    {
      "index": 82,
      "chunk_id": "FaithEval2024_chunk_14",
      "source_id": "FaithEval2024",
      "text": "of strict vs. non-strict matching on Unanswerable Context. Non-strict matching allows for a wider range of phrases that express the idea of \"unknown\". The comparison on Inconsistent Context is shown in Appendix C. exact phrases \"unknown\" or \"conflict\" as specified in the prompt, while non-strict matching permits a broader range of expressions that convey similar ideas (see Appendix A for the full list of valid expressions). We observe that performance remains stable across most models. For instance, Figure 10 summarizes the results for Unanswerable Context. For competitive models such as gpt-4o and Claude 3.5, the gaps are less than 1%. Full results can be found in Table 4 and Table 5 in Appendix C. Impact of decoding strategies. By default, we adopt greedy decoding. We also investigate a popular sampling-based decoding scheme with a temperature of 0.3 and top-p of 0.9. The results are shown in Figure 11 based on Counterfactual Context. Similar observations also hold for Unanswerable and Inconsistent Context. We observe that sampling-based decoding marginally improves the performance over greedy decoding across all models. However, the significant gap between the original and counterfactual contexts cannot be mitigated with temperature scaling. Figure 11: The impact of decoding strategy. The plot displays greedy (non-sampling) vs. sampling- based decoding (t=0.3, top-p=0.9) on Counterfactual Context. 6 C ONCLUSION In this work, we propose FaithEval, a novel and challenging benchmark designed to assess the faith- fulness of contextual LLMs. FaithEval comprises 4.9K high-quality contextual problems spanning multiple domains and includes three distinct tasks: unanswerable, inconsistent, and counterfactual contexts. To build this benchmark, we propose a scalable multi-stage context construction and valida- tion framework, incorporating both automated evaluation by an LLM judge and human validation. This approach enables the creation of multi-paragraph coherent contexts satisfying diverse criteria. We provide a timely and in-depth study on a wide range of open-source and proprietary models, re- vealing that even the most competitive LLMs often struggle to remain faithful to the contexts, despite excelling on standard benchmarks. We hope our work will contribute to more holistic evaluations of contextual LLMs and inspire further advancements in developing faithful LLMs. Published as a conference paper at ICLR 2025 REFERENCES Marah Abdin et al. Phi-3 technical report: A highly capable language model locally on your phone, 2024. Vaibhav Adlakha, Parishad BehnamGhader, Xing Han Lu, Nicholas Meade, and Siva Reddy. Evalu- ating correctness and faithfulness of instruction-following models for question answering.",
      "chunk_index": 14
    },
    {
      "index": 83,
      "chunk_id": "FaithEval2024_chunk_15",
      "source_id": "FaithEval2024",
      "text": "Marah Abdin et al. Phi-3 technical report: A highly capable language model locally on your phone, 2024. Vaibhav Adlakha, Parishad BehnamGhader, Xing Han Lu, Nicholas Meade, and Siva Reddy. Evalu- ating correctness and faithfulness of instruction-following models for question answering. Trans- actions of the Association for Computational Linguistics, pp. 681-699, 2024. Akari Asai, Zeqiu Wu, Yizhong Wang, Avirup Sil, and Hannaneh Hajishirzi. Self-RAG: Learning to retrieve, generate, and critique through self-reflection. arXiv preprint arXiv:2310.11511, 2023. Pepa Atanasova, Oana-Maria Camburu, Christina Lioma, Thomas Lukasiewicz, Jakob Grue Simon- sen, and Isabelle Augenstein. Faithfulness tests for natural language explanations. In Proceedings of the 61st Annual Meeting of the Association for Computational Linguistics (Volume 2: Short Papers), pp. 283-294, July 2023. Michael J. Bommarito and Daniel M. Katz. Gpt takes the bar exam: What artificial intelligence and machine learning mean for the practice of law. arXiv preprint arXiv:2212.14402, 2022. Sebastian Borgeaud, Arthur Mensch, Jordan Hoffmann, Trevor Cai, Eliza Rutherford, Katie Mil- lican, George van den Driessche, Jean-Baptiste Lespiau, Bogdan Damoc, Aidan Clark, Diego de Las Casas, Aurelia Guy, Jacob Menick, Roman Ring, T. W. Hennigan, Saffron Huang, Lorenzo Maggiore, Chris Jones, Albin Cassirer, Andy Brock, Michela Paganini, Geoffrey Irving, Oriol Vinyals, Simon Osindero, Karen Simonyan, Jack W. Rae, Erich Elsen, and L. Sifre. Improving language models by retrieving from trillions of tokens. In International Conference on Machine Learning, 2021. Sébastien Bubeck, Varun Chandrasekaran, Ronen Eldan, Johannes Gehrke, Eric Horvitz, Ece Kamar, Peter Lee, Yin Tat Lee, Yuanzhi Li, Scott Lundberg, et al. Sparks of artificial general intelligence: Early experiments with gpt-4. arXiv preprint arXiv:2303.12712, 2023. Hung-Ting Chen, Michael JQ Zhang, and Eunsol Choi. Rich knowledge sources bring complex knowledge conflicts: Recalibrating models to reflect conflicting evidence. In Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics, 2022. Jianlv Chen, Shitao Xiao, Peitian Zhang, Kun Luo, Defu Lian, and Zheng Liu. Bge m3-embedding: Multi-lingual, multi-functionality, multi-granularity text embeddings through self-knowledge distillation. In Annual Meeting of the Association for Computational Linguistics, 2024a. Jiawei Chen, Hongyu Lin, Xianpei Han, and Le Sun. Benchmarking large language models in retrieval-augmented generation. In Proceedings of the AAAI Conference on Artificial Intelligence, volume 38, pp. 17754-17762, 2024b. Xiang Chen, Duanzheng Song, Honghao Gui, Chenxi Wang, Ningyu Zhang, Yong Jiang, Fei Huang, Chengfei Lyu, Dan Zhang, and Huajun Chen. Factchd: Benchmarking fact-conflicting hallucination detection. In Proceedings of the Thirty-Third International Joint Conference on Artificial Intelligence, IJCAI-24, pp. 6216-6224, 8",
      "chunk_index": 15
    },
    {
      "index": 84,
      "chunk_id": "FaithEval2024_chunk_16",
      "source_id": "FaithEval2024",
      "text": "Chen, Duanzheng Song, Honghao Gui, Chenxi Wang, Ningyu Zhang, Yong Jiang, Fei Huang, Chengfei Lyu, Dan Zhang, and Huajun Chen. Factchd: Benchmarking fact-conflicting hallucination detection. In Proceedings of the Thirty-Third International Joint Conference on Artificial Intelligence, IJCAI-24, pp. 6216-6224, 8 2024c. I Chern, Steffi Chern, Shiqi Chen, Weizhe Yuan, Kehua Feng, Chunting Zhou, Junxian He, Graham Neubig, Pengfei Liu, et al. Factool: Factuality detection in generative ai-a tool augmented framework for multi-task and multi-domain scenarios. arXiv preprint arXiv:2307.13528, 2023. Peter Clark, Isaac Cowhey, Oren Etzioni, Tushar Khot, Ashish Sabharwal, Carissa Schoenick, and Oyvind Tafjord. Think you have solved question answering? try arc, the ai2 reasoning challenge. arXiv preprint arXiv:1803.05457, 2018. Kevin Crowston. Amazon mechanical turk: A research tool for organizations and information systems scholars. In Shaping the Future of ICT Research, 2012. Dheeru Dua, Yizhong Wang, Pradeep Dasigi, Gabriel Stanovsky, Sameer Singh, and Matt Gard- ner. Drop: A reading comprehension benchmark requiring discrete reasoning over paragraphs. In Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, pp. 2368-2378, 2019. Published as a conference paper at ICLR 2025 Matthew Dunn, Levent Sagun, Mickael Higgins, V Ugur Guney, V olkan Cirik, and Kyunghyun Cho. Searchqa: A new q&a dataset augmented with context from a search engine. arXiv preprint arXiv:1704.05179, 2017. Adam Fisch, Alon Talmor, Robin Jia, Minjoon Seo, Eunsol Choi, and Danqi Chen. MRQA 2019 shared task: Evaluating generalization in reading comprehension. In Proceedings of the 2nd Workshop on Machine Reading for Question Answering, pp. 1-13, November 2019. Yifei Gao, Lei Wang, Jun Fang, Longhua Hu, and Jun Cheng. Empower your model with longer and better context comprehension, 2023. Xiangkun Hu, Dongyu Ru, Lin Qiu, Qipeng Guo, Tianhang Zhang, Yang Xu, Yun Luo, Pengfei Liu, Yue Zhang, and Zheng Zhang. Refchecker: Reference-based fine-grained hallucination checker and benchmark for large language models. arXiv preprint arXiv:2405.14486, 2024. Lei Huang, Weijiang Yu, Weitao Ma, Weihong Zhong, Zhangyin Feng, Haotian Wang, Qianglong Chen, Weihua Peng, Xiaocheng Feng, Bing Qin, et al. A survey on hallucination in large language models: Principles, taxonomy, challenges, and open questions. arXiv preprint arXiv:2311.05232, 2023a. Rongjie Huang, Jia-Bin Huang, Dongchao Yang, Yi Ren, Luping Liu, Mingze Li, Zhenhui Ye, Jinglin Liu, Xiaoyue Yin, and Zhou Zhao. Make-an-audio: Text-to-audio generation with prompt-enhanced diffusion models. ArXiv, abs/2301.12661, 2023b. Yue Huang, Lichao Sun, Haoran Wang, Siyuan Wu, Qihui Zhang, Yuan Li, Chujie Gao, Yixin Huang,",
      "chunk_index": 16
    },
    {
      "index": 85,
      "chunk_id": "FaithEval2024_chunk_17",
      "source_id": "FaithEval2024",
      "text": "Ren, Luping Liu, Mingze Li, Zhenhui Ye, Jinglin Liu, Xiaoyue Yin, and Zhou Zhao. Make-an-audio: Text-to-audio generation with prompt-enhanced diffusion models. ArXiv, abs/2301.12661, 2023b. Yue Huang, Lichao Sun, Haoran Wang, Siyuan Wu, Qihui Zhang, Yuan Li, Chujie Gao, Yixin Huang, Wenhan Lyu, Yixuan Zhang, Xiner Li, Hanchi Sun, Zhengliang Liu, Yixin Liu, Yijue Wang, Zhikun Zhang, Bertie Vidgen, Bhavya Kailkhura, Caiming Xiong, Chaowei Xiao, Chunyuan Li, Eric P. Xing, Furong Huang, Hao Liu, Heng Ji, Hongyi Wang, Huan Zhang, Huaxiu Yao, Manolis Kellis, Marinka Zitnik, Meng Jiang, Mohit Bansal, James Zou, Jian Pei, Jian Liu, Jianfeng Gao, Jiawei Han, Jieyu Zhao, Jiliang Tang, Jindong Wang, Joaquin Vanschoren, John Mitchell, Kai Shu, Kaidi Xu, Kai-Wei Chang, Lifang He, Lifu Huang, Michael Backes, Neil Zhenqiang Gong, Philip S. Yu, Pin-Yu Chen, Quanquan Gu, Ran Xu, Rex Ying, Shuiwang Ji, Suman Jana, Tianlong Chen, Tianming Liu, Tianyi Zhou, William Yang Wang, Xiang Li, Xiangliang Zhang, Xiao Wang, Xing Xie, Xun Chen, Xuyu Wang, Yan Liu, Yanfang Ye, Yinzhi Cao, Yong Chen, and Yue Zhao. Trustllm: Trustworthiness in large language models. In Forty-first International Conference on Machine Learning, 2024. Gautier Izacard and Edouard Grave. Leveraging passage retrieval with generative models for open domain question answering. ArXiv, abs/2007.01282, 2020. Qi Jia, Siyu Ren, Yizhu Liu, and Kenny Zhu. Zero-shot faithfulness evaluation for text summarization with foundation language model. In Proceedings of the 2023 Conference on Empirical Methods in Natural Language Processing, pp. 11017-11031, December 2023. Albert Q Jiang, Alexandre Sablayrolles, Arthur Mensch, Chris Bamford, Devendra Singh Chaplot, Diego de las Casas, Florian Bressand, Gianna Lengyel, Guillaume Lample, Lucile Saulnier, et al. Mistral 7b. arXiv preprint arXiv:2310.06825, 2023. Mandar Joshi, Eunsol Choi, Daniel S Weld, and Luke Zettlemoyer. Triviaqa: A large scale distantly supervised challenge dataset for reading comprehension. In Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers) , pp. 1601- 1611, 2017. Zixuan Ke, Weize Kong, Cheng Li, Mingyang Zhang, Qiaozhu Mei, and Michael Bendersky. Bridging the preference gap between retrievers and llms. arXiv preprint arXiv:2401.06954, 2024. Aniruddha Kembhavi, Matteo Salvato, Minjoon Seo, Harish Kannan, Aniruddha Singh, Li Fei-Fei, Ali Farhadi, and Mark Yatskar. Are you smarter than a sixth grader? textbook question answering for multimodal machine comprehension. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, pp. 4999-5007, 2017. Takeshi Kojima, Shixiang Shane Gu, Machel Reid, Yutaka Matsuo, and Yusuke Iwasawa. Large language",
      "chunk_index": 17
    },
    {
      "index": 86,
      "chunk_id": "FaithEval2024_chunk_18",
      "source_id": "FaithEval2024",
      "text": "than a sixth grader? textbook question answering for multimodal machine comprehension. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, pp. 4999-5007, 2017. Takeshi Kojima, Shixiang Shane Gu, Machel Reid, Yutaka Matsuo, and Yusuke Iwasawa. Large language models are zero-shot reasoners. Advances in neural information processing systems, 35: 22199-22213, 2022. Published as a conference paper at ICLR 2025 Tom Kwiatkowski, Jennimaria Palomaki, Olivia Redfield, Michael Collins, Ankur Parikh, Chris Alberti, Danielle Epstein, Illia Polosukhin, Jacob Devlin, Kenton Lee, et al. Natural questions: a benchmark for question answering research. Transactions of the Association for Computational Linguistics, 7:453-466, 2019a. Tom Kwiatkowski, Jennimaria Palomaki, Olivia Redfield, Michael Collins, Ankur Parikh, Chris Alberti, Danielle Epstein, Illia Polosukhin, Matthew Kelcey, Jacob Devlin, Kenton Lee, Kristina N. Toutanova, Llion Jones, Ming-Wei Chang, Andrew Dai, Jakob Uszkoreit, Quoc Le, and Slav Petrov. Natural questions: a benchmark for question answering research. Transactions of the Association of Computational Linguistics, 2019b. Philippe Laban, Wojciech Kryscinski, Divyansh Agarwal, Alexander Fabbri, Caiming Xiong, Shafiq Joty, and Chien-Sheng Wu. SummEdits: Measuring LLM ability at factual reasoning through the lens of summarization. In Proceedings of the 2023 Conference on Empirical Methods in Natural Language Processing, pp. 9662-9676, December 2023. Guokun Lai, Qizhe Xie, Hanxiao Liu, Yiming Yang, and Eduard Hovy. RACE: Large-scale ReAding comprehension dataset from examinations. In Proceedings of the 2017 Conference on Empirical Methods in Natural Language Processing, pp. 785-794, September 2017. Nayeon Lee, Wei Ping, Peng Xu, Mostofa Patwary, Pascale N Fung, Mohammad Shoeybi, and Bryan Catanzaro. Factuality enhanced language models for open-ended text generation. Advances in Neural Information Processing Systems, 35:34586-34599, 2022. Patrick Lewis, Ethan Perez, Aleksandra Piktus, Fabio Petroni, Vladimir Karpukhin, Naman Goyal, Heinrich Küttler, Mike Lewis, Wen-tau Yih, Tim Rocktäschel, Sebastian Riedel, and Douwe Kiela. Retrieval-augmented generation for knowledge-intensive nlp tasks. In Proceedings of the 34th International Conference on Neural Information Processing Systems, NIPS '20, Red Hook, NY , USA, 2020. Curran Associates Inc. ISBN 9781713829546. Chaofan Li, Zheng Liu, Shitao Xiao, and Yingxia Shao. Making large language models a better foundation for dense retrieval. ArXiv, abs/2312.15503, 2023a. Jiaqi Li, Mengmeng Wang, Zilong Zheng, and Muhan Zhang. LooGLE: Can long-context language models understand long contexts? In Proceedings of the 62nd Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pp. 16304-16333, August 2024a. doi: 10.18653/v1/2024.acl-long.859. Junyi Li, Xiaoxue Cheng, Xin Zhao, Jian-Yun Nie, and Ji-Rong Wen. HaluEval: A large-scale hallucination evaluation benchmark",
      "chunk_index": 18
    },
    {
      "index": 87,
      "chunk_id": "FaithEval2024_chunk_19",
      "source_id": "FaithEval2024",
      "text": "In Proceedings of the 62nd Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pp. 16304-16333, August 2024a. doi: 10.18653/v1/2024.acl-long.859. Junyi Li, Xiaoxue Cheng, Xin Zhao, Jian-Yun Nie, and Ji-Rong Wen. HaluEval: A large-scale hallucination evaluation benchmark for large language models. In Proceedings of the 2023 Conference on Empirical Methods in Natural Language Processing, pp. 6449-6464, December 2023b. Junyi Li, Jie Chen, Ruiyang Ren, Xiaoxue Cheng, Xin Zhao, Jian-Yun Nie, and Ji-Rong Wen. The dawn after the dark: An empirical study on factuality hallucination in large language models. In Proceedings of the 62nd Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pp. 10879-10899, August 2024b. Siheng Li, Cheng Yang, Taiqiang Wu, Chufan Shi, Yuji Zhang, Xinyu Zhu, Zesen Cheng, Deng Cai, Mo Yu, Lemao Liu, et al. A survey on the honesty of large language models. arXiv preprint arXiv:2409.18786, 2024c. Xingxuan Li, Ruochen Zhao, Yew Ken Chia, Bosheng Ding, Shafiq Joty, Soujanya Poria, and Lidong Bing. Chain-of-knowledge: Grounding large language models via dynamic knowledge adapting over heterogeneous sources. In The Twelfth International Conference on Learning Representations, 2024d. Stephanie Lin, Jacob Hilton, and Owain Evans. TruthfulQA: Measuring how models mimic human falsehoods. In Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pp. 3214-3252, May 2022. Tianyu Liu, Yizhe Zhang, Chris Brockett, Yi Mao, Zhifang Sui, Weizhu Chen, and Bill Dolan. A token-level reference-free hallucination detection benchmark for free-form text generation. arXiv preprint arXiv:2104.08704, 2021. Published as a conference paper at ICLR 2025 Llama. The llama 3 herd of models, 2024. Shayne Longpre, Kartik Perisetla, Anthony Chen, Nikhil Ramesh, Chris DuBois, and Sameer Singh. Entity-based knowledge conflicts in question answering. In Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing, pp. 7052-7063, Online and Punta Cana, Dominican Republic, November 2021. Association for Computational Linguistics. Potsawee Manakul, Adian Liusie, and Mark Gales. SelfCheckGPT: Zero-resource black-box halluci- nation detection for generative large language models. In Proceedings of the 2023 Conference on Empirical Methods in Natural Language Processing, pp. 9004-9017, Singapore, December 2023. Rui Meng, Ye Liu, Shafiq Rayhan Joty, Caiming Xiong, Yingbo Zhou, and Semih Yavuz. Sfr- embedding-mistral:enhance text retrieval with transfer learning. Salesforce AI Research Blog, 2024. URL https://blog.salesforceairesearch.com/sfr-embedded-mistral/. Sewon Min, Kalpesh Krishna, Xinxi Lyu, Mike Lewis, Wen-tau Yih, Pang Koh, Mohit Iyyer, Luke Zettlemoyer, and Hannaneh Hajishirzi. FActScore: Fine-grained atomic evaluation of factual precision",
      "chunk_index": 19
    },
    {
      "index": 88,
      "chunk_id": "FaithEval2024_chunk_20",
      "source_id": "FaithEval2024",
      "text": "Sfr- embedding-mistral:enhance text retrieval with transfer learning. Salesforce AI Research Blog, 2024. URL https://blog.salesforceairesearch.com/sfr-embedded-mistral/. Sewon Min, Kalpesh Krishna, Xinxi Lyu, Mike Lewis, Wen-tau Yih, Pang Koh, Mohit Iyyer, Luke Zettlemoyer, and Hannaneh Hajishirzi. FActScore: Fine-grained atomic evaluation of factual precision in long form text generation. In Proceedings of the 2023 Conference on Empirical Methods in Natural Language Processing, pp. 12076-12100, December 2023. Xuan-Phi Nguyen, Shrey Pandit, Senthil Purushwalkam, Austin Xu, Hailin Chen, Yifei Ming, Zixuan Ke, Silvio Savarese, Caiming Xong, and Shafiq Joty. Sfr-rag: Towards contextually faithful llms. arXiv preprint arXiv:2409.09916, 2024. Ankit Pal, Logesh Kumar Umapathi, and Malaikannan Sankarasubbu. Med-HALT: Medical do- main hallucination test for large language models. In Proceedings of the 27th Conference on Computational Natural Language Learning (CoNLL), pp. 314-334, December 2023. Yikang Pan, Liangming Pan, Wenhu Chen, Preslav Nakov, Min-Yen Kan, and William Yang Wang. On the risk of misinformation pollution with large language models. In The 2023 Conference on Empirical Methods in Natural Language Processing, 2023. Ethan Perez, Sam Ringer, Kamile Lukosiute, Karina Nguyen, Edwin Chen, Scott Heiner, Craig Pettit, Catherine Olsson, Sandipan Kundu, Saurav Kadavath, Andy Jones, Anna Chen, Benjamin Mann, Brian Israel, Bryan Seethor, Cameron McKinnon, Christopher Olah, Da Yan, Daniela Amodei, Dario Amodei, Dawn Drain, Dustin Li, Eli Tran-Johnson, Guro Khundadze, Jackson Kernion, James Landis, Jamie Kerr, Jared Mueller, Jeeyoon Hyun, Joshua Landau, Kamal Ndousse, Landon Goldberg, Liane Lovitt, Martin Lucas, Michael Sellitto, Miranda Zhang, Neerav Kingsland, Nelson Elhage, Nicholas Joseph, Noemi Mercado, Nova DasSarma, Oliver Rausch, Robin Larson, Sam McCandlish, Scott Johnston, Shauna Kravec, Sheer El Showk, Tamera Lanham, Timothy Telleen-Lawton, Tom Brown, Tom Henighan, Tristan Hume, Yuntao Bai, Zac Hatfield-Dodds, Jack Clark, Samuel R. Bowman, Amanda Askell, Roger Grosse, Danny Hernandez, Deep Ganguli, Evan Hubinger, Nicholas Schiefer, and Jared Kaplan. Discovering language model behaviors with model-written evaluations. In Findings of the Association for Computational Linguistics: ACL 2023, July 2023. Pranav Rajpurkar, Jian Zhang, Konstantin Lopyrev, and Percy Liang. SQuAD: 100,000+ questions for machine comprehension of text. In Proceedings of the 2016 Conference on Empirical Methods in Natural Language Processing, pp. 2383-2392, November 2016. Anil Ramakrishna, Rahul Gupta, Jens Lehmann, and Morteza Ziyadi. Invite: a testbed of automatically generated invalid questions to evaluate large language models for hallucinations. In Findings of the Association for Computational Linguistics: EMNLP 2023, pp. 5422-5429, 2023. Rita Parada Ramos, Bruno Martins, Desmond Elliott, and Yova Kementchedjhieva. Smallcap: Lightweight image captioning prompted with retrieval augmentation. 2023",
      "chunk_index": 20
    },
    {
      "index": 89,
      "chunk_id": "FaithEval2024_chunk_21",
      "source_id": "FaithEval2024",
      "text": "questions to evaluate large language models for hallucinations. In Findings of the Association for Computational Linguistics: EMNLP 2023, pp. 5422-5429, 2023. Rita Parada Ramos, Bruno Martins, Desmond Elliott, and Yova Kementchedjhieva. Smallcap: Lightweight image captioning prompted with retrieval augmentation. 2023 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), pp. 2840-2849, 2022. Sara Sarto, Marcella Cornia, Lorenzo Baraldi, and Rita Cucchiara. Retrieval-augmented trans- former for image captioning. Proceedings of the 19th International Conference on Content-based Multimedia Indexing, 2022. Published as a conference paper at ICLR 2025 Freda Shi, Xinyun Chen, Kanishka Misra, Nathan Scales, David Dohan, Ed H Chi, Nathanael Schärli, and Denny Zhou. Large language models can be easily distracted by irrelevant context. In International Conference on Machine Learning, pp. 31210-31227. PMLR, 2023. Zhenmei Shi, Yifei Ming, Xuan-Phi Nguyen, Yingyu Liang, and Shafiq Joty. Discovering the gems in early layers: Accelerating long-context llms with 1000x input token reduction. arXiv preprint arXiv:2409.17422, 2024. Noah Siegel, Oana-Maria Camburu, Nicolas Heess, and Maria Perez-Ortiz. The probabilities also matter: A more faithful metric for faithfulness of free-text explanations in large language models. In Proceedings of the 62nd Annual Meeting of the Association for Computational Linguistics (Volume 2: Short Papers), pp. 530-546, August 2024. Mingyang Song, Mao Zheng, and Xuan Luo. Counting-stars: A multi-evidence, position-aware, and scalable benchmark for evaluating long-context large language models, 2024. Gemma Team. Gemma 2: Improving open language models at a practical size, 2024. Adam Trischler, Tong Wang, Xingdi Yuan, Justin Harris, Alessandro Sordoni, Philip Bachman, and Kaheer Suleman. Newsqa: A machine comprehension dataset. In Proceedings of the 2nd Workshop on Representation Learning for NLP, pp. 191-200, 2017. George Tsatsaronis, Georgios Balikas, Prodromos Malakasiotis, Ioannis Partalas, Matthias Zschunke, Michael R Alvers, Dirk Weissenborn, Anastasia Krithara, Sergios Petridis, Dimitris Polychronopou- los, et al. An overview of the bioasq large-scale biomedical semantic indexing and question answering competition. BMC bioinformatics, 16:1-28, 2015. Jason Wei, Xuezhi Wang, Dale Schuurmans, Maarten Bosma, Fei Xia, Ed Chi, Quoc V Le, Denny Zhou, et al. Chain-of-thought prompting elicits reasoning in large language models. Advances in neural information processing systems, 35:24824-24837, 2022. Jerry Wei, Da Huang, Yifeng Lu, Denny Zhou, and Quoc V Le. Simple synthetic data reduces sycophancy in large language models. arXiv preprint arXiv:2308.03958, 2023. Jerry Wei, Chengrun Yang, Xinying Song, Yifeng Lu, Nathan Hu, Dustin Tran, Daiyi Peng, Ruibo Liu, Da Huang, Cosmo Du, et al. Long-form factuality in large language models. arXiv preprint arXiv:2403.18802,",
      "chunk_index": 21
    },
    {
      "index": 90,
      "chunk_id": "FaithEval2024_chunk_22",
      "source_id": "FaithEval2024",
      "text": "sycophancy in large language models. arXiv preprint arXiv:2308.03958, 2023. Jerry Wei, Chengrun Yang, Xinying Song, Yifeng Lu, Nathan Hu, Dustin Tran, Daiyi Peng, Ruibo Liu, Da Huang, Cosmo Du, et al. Long-form factuality in large language models. arXiv preprint arXiv:2403.18802, 2024. Kevin Wu, Eric Wu, and James Zou. Clasheval: Quantifying the tug-of-war between an llm's internal prior and external evidence. In The Thirty-eight Conference on Neural Information Processing Systems Datasets and Benchmarks Track, 2024a. Likang Wu, Zhi Zheng, Zhaopeng Qiu, Hao Wang, Hongchao Gu, Tingjia Shen, Chuan Qin, Chen Zhu, Hengshu Zhu, Qi Liu, et al. A survey on large language models for recommendation. World Wide Web, 27(5):60, 2024b. Chong Xiang, Tong Wu, Zexuan Zhong, David Wagner, Danqi Chen, and Prateek Mittal. Certifiably robust rag against retrieval corruption. arXiv preprint arXiv:2405.15556, 2024. Jian Xie, Kai Zhang, Jiangjie Chen, Renze Lou, and Yu Su. Adaptive chameleon or stubborn sloth: Revealing the behavior of large language models in knowledge conflicts. In The Twelfth International Conference on Learning Representations, 2024. Fangyuan Xu, Weijia Shi, and Eunsol Choi. RECOMP: Improving retrieval-augmented LMs with context compression and selective augmentation. In The Twelfth International Conference on Learning Representations, 2024. Zhilin Yang, Peng Qi, Saizheng Zhang, Yoshua Bengio, William W Cohen, Ruslan Salakhutdinov, and Christopher D Manning. Hotpotqa: A dataset for diverse, explainable multi-hop question answering. In Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing, pp. 2369-2380, 2018. Hongbin Ye, Tong Liu, Aijia Zhang, Wei Hua, and Weiqiang Jia. Cognitive mirage: A review of hallucinations in large language models. arXiv preprint arXiv:2309.06794, 2023. Published as a conference paper at ICLR 2025 Zhangyue Yin, Qiushi Sun, Qipeng Guo, Jiawen Wu, Xipeng Qiu, and Xuanjing Huang. Do large language models know what they don't know? In Findings of the Association for Computational Linguistics: ACL 2023, pp. 8653-8665, July 2023. Xiaodong Yu, Hao Cheng, Xiaodong Liu, Dan Roth, and Jianfeng Gao. Reeval: Automatic hallucina- tion evaluation for retrieval-augmented large language models via transferable adversarial attacks. In NAACL-Findings, 2024. Penghao Zhao, Hailin Zhang, Qinhan Yu, Zhengren Wang, Yunteng Geng, Fangcheng Fu, Ling Yang, Wentao Zhang, Jie Jiang, and Bin Cui. Retrieval-augmented generation for ai-generated content: A survey, 2024a. Wayne Xin Zhao, Kun Zhou, Junyi Li, Tianyi Tang, Xiaolei Wang, Yupeng Hou, Yingqian Min, Beichen Zhang, Junjie Zhang, Zican Dong, Yifan Du, Chen Yang, Yushuo Chen, Zhipeng Chen, Jinhao Jiang, Ruiyang Ren, Yifan Li, Xinyu Tang, Zikang Liu,",
      "chunk_index": 22
    },
    {
      "index": 91,
      "chunk_id": "FaithEval2024_chunk_23",
      "source_id": "FaithEval2024",
      "text": "2024a. Wayne Xin Zhao, Kun Zhou, Junyi Li, Tianyi Tang, Xiaolei Wang, Yupeng Hou, Yingqian Min, Beichen Zhang, Junjie Zhang, Zican Dong, Yifan Du, Chen Yang, Yushuo Chen, Zhipeng Chen, Jinhao Jiang, Ruiyang Ren, Yifan Li, Xinyu Tang, Zikang Liu, Peiyu Liu, Jian-Yun Nie, and Ji-Rong Wen. A survey of large language models. arXiv preprint arXiv:2303.18223, 2024b. Wenxuan Zhou, Sheng Zhang, Hoifung Poon, and Muhao Chen. Context-faithful prompting for large language models. In Findings of the Association for Computational Linguistics: EMNLP 2023, pp. 14544-14556, December 2023. Published as a conference paper at ICLR 2025 A A DDITIONAL EXPERIMENT DETAILS Context generation model. By default, we use the latest GPT-4o (gpt-4o-2024-05-13) as the context generator. In our preliminary studies, we evaluated various alternatives, including GPT-4o- mini, GPT-4-turbo, and LLaMA-3.1-70B. GPT-4o demonstrated superior performance in terms of context coherence, validity, and complexity. Context evaluation model. We use gpt-4o-mini as the judge model for context verification. We selected gpt-4o-mini for its faster inference speed, lower cost, and high judgment quality, with our preliminary study showing an agreement rate of over 95% when compared to GPT-4o as the judge. Valid phrases for non-strict matching. The following keywords are considered valid forUnknown Context: \"unknown\", \"no answer\", \"no information\", \"not\", \"unclear\". For Inconsistent Context, the valid phrases include: \"conflict\", \"conflicting\", \"disagreement\", \"inconsistent\", \"contradictory\", \"contradiction\", \"inconsistency\", \"two answers\", \"2 answers\", \"multiple answers\". These keywords were selected based on an analysis of output patterns from open-source and proprietary models. Details on Counterfactual Context construction. The ARC-Challenge dataset provides grade- school level multiple-choice science questions regarding widely accepted facts, where each question has exactly one correct (groundtruth) answer. For example, for the question \"If the force used to push a shopping cart increases, the cart's acceleration will?\" with options A.decrease, B. increase, C. remain the same, physics principles dictate that B. increase is the groundtruth answer. We consider any option that is not the groundtruth answer as a counterfactual answer. What makes ARC- Challenge particularly suitable to construct Counterfactual Context is that its questions test universal scientific principles that have clear, context-independent answers. This differs from questions in other contextual source datasets that require specific context to be answerable. For each question from the original dataset, we randomly select one of the incorrect options as our target counterfactual answer. Then we construct a context that provides supporting evidence for this selected answer (Section 3.1). The new task is still",
      "chunk_index": 23
    },
    {
      "index": 92,
      "chunk_id": "FaithEval2024_chunk_24",
      "source_id": "FaithEval2024",
      "text": "For each question from the original dataset, we randomly select one of the incorrect options as our target counterfactual answer. Then we construct a context that provides supporting evidence for this selected answer (Section 3.1). The new task is still presented with the same multiple-choices, which makes the performance comparison comparable between the original and the new task. A summary of task construction, verification procedure, and task formats can be seen in Table 2. Task Task Format Task Verification Source Dataset Source Dataset Format Counterfactual Context Multiple Choices LLM judge +string matching ARC-Challenge Multiple Choices Unanswerable Context Open-ended QA LLM judge +human annotation 10 contextual datasets Open-ended QA Inconsistent Context Open-ended QA LLM judge +human annotation 10 contextual datasets Open-ended QA Table 2: Summary of task construction, verification procedure, and task formats. Model sizes. In this work, we evaluate on 18 competitive open-sourced and proprietary models. We summarize the model sizes from different model families in Table 3. Human annotation. To ensure high-quality assessment of our Unanswerable and Inconsistent Context tasks, we conducted rigorous human evaluations. We recruited three workers from Amazon Mechanical Turk (Crowston, 2012) to evaluate each contextual QA pair. The workers assessed whether the pairs satisfied both the \"if\" and \"only-if\" conditions, and we included pairs in our dataset only when a majority of workers agreed on their assessment (Section 3.2). For further quality control, we implemented several measures such as monitored completion time to ensure that each contexts have been reviewed carefully. Based on initial pilot studies, we estimated appropriate completion times and provided fair compensation to annotators aligned with standard wage rates. B V ERIFICATION FOR COUNTERFACTUAL CONTEXT For the Counterfactual Context task, since the answer options are provided within the context, we can validate the new context using a simple keyword-based matching method, where a context passes if Published as a conference paper at ICLR 2025 Model Name Model Size Phi-3 Family (Abdin et al., 2024) Phi-3-mini-128k-instruct 3.8B Phi-3-medium-128k-instruct 14B Phi-3.5-mini-instruct 3.8B LLaMA-3 Family (Llama, 2024) LLaMA-3-8B-instruct 8B LLaMA-3.1-8B-instruct 8B LLaMA-3-70B-instruct 70B LLaMA-3.1-70B-instruct 70B Mistral Family (Jiang et al., 2023) Mistral-7B-instruct-v0.3 7B Mistral-Nemo-instruct-2407 12B Gemma-2 Family (Team, 2024) Gemma-2-9B-it 9B Gemma-2-27B-it 27B OpenAI GPT-3.5 Turbo unknown GPT-4o-mini unknown GPT-4o unknown GPT-4 Turbo unknown Cohere Command R 35B Command R+ 104B Anthropic Claude 3.5 Sonnet unknown Table 3: Model sizes across different model families. the answer phrase exists in the context. This results in a",
      "chunk_index": 24
    },
    {
      "index": 93,
      "chunk_id": "FaithEval2024_chunk_25",
      "source_id": "FaithEval2024",
      "text": "Turbo unknown GPT-4o-mini unknown GPT-4o unknown GPT-4 Turbo unknown Cohere Command R 35B Command R+ 104B Anthropic Claude 3.5 Sonnet unknown Table 3: Model sizes across different model families. the answer phrase exists in the context. This results in a pass rate of 68.9%, where the new context clearly contains the new answer. The results on this filtered subset are shown in Figure 12. We can see that the same trend still holds as shown in Section 4: a significant gap remains between the performance on the original task (with no context) and the new task with counterfactual contexts, across the majority of instruction-tuned models. Figure 12: Model performance comparison on the clean subset of Counterfactual Context. The same trend still holds as in Section 4. C F ULL EXPERIMENT RESULTS The results break down for all models on each of the ten datasets are summarized in Figure 13 and Figure 14 for Unanswerable Context; Figure 15 and Figure 16 for Inconsistent Context. The detailed results for strict vs. non-strict matching can be found in Table 4 and Table 5. Published as a conference paper at ICLR 2025 Figure 13: Performance decomposition on individual datasets for Unanswerable Context (part I). Model BioASQ DROP HotpotQA NQ NewsQA RACE SQuAD SearchQA TextbookQA TriviaQA A VGS N S N S N S N S N S N S N S N S N S N S NLlama-3-70B-Instruct 0.508 0.512 0.644 0.656 0.548 0.552 0.508 0.516 0.512 0.520 0.396 0.408 0.588 0.592 0.112 0.116 0.488 0.488 0.455 0.463 0.476 0.482Llama-3-8B-Instruct 0.268 0.272 0.528 0.532 0.252 0.252 0.224 0.232 0.344 0.364 0.336 0.352 0.436 0.444 0.140 0.144 0.232 0.236 0.248 0.252 0.301 0.308Llama-3.1-70B-Instruct 0.452 0.460 0.676 0.684 0.628 0.632 0.468 0.500 0.500 0.524 0.408 0.424 0.612 0.616 0.096 0.108 0.436 0.440 0.517 0.525 0.479 0.491Llama-3.1-8B-Instruct 0.292 0.300 0.632 0.636 0.448 0.452 0.388 0.400 0.420 0.428 0.396 0.420 0.532 0.544 0.116 0.128 0.212 0.228 0.326 0.339 0.376 0.388Mistral-7B-Instruct-v0.3 0.304 0.360 0.476 0.528 0.400 0.460 0.336 0.468 0.252 0.484 0.256 0.412 0.392 0.516 0.132 0.184 0.116 0.232 0.236 0.306 0.290 0.395Mistral-Nemo-Instruct-2407 0.192 0.196 0.436 0.444 0.380 0.380 0.284 0.288 0.308 0.332 0.316 0.340 0.416 0.428 0.224 0.224 0.108 0.112 0.140 0.140 0.280 0.288Phi-3-medium-128k-instruct 0.024 0.204 0.124 0.428 0.176 0.312 0.116 0.288 0.060 0.360 0.044 0.312 0.128 0.448 0.024 0.052 0.036 0.068 0.008 0.062 0.074 0.253Phi-3-mini-128k-instruct 0.020 0.096 0.108 0.248 0.096 0.148 0.100 0.168 0.076 0.188 0.040 0.104",
      "chunk_index": 25
    },
    {
      "index": 94,
      "chunk_id": "FaithEval2024_chunk_26",
      "source_id": "FaithEval2024",
      "text": "0.108 0.112 0.140 0.140 0.280 0.288Phi-3-medium-128k-instruct 0.024 0.204 0.124 0.428 0.176 0.312 0.116 0.288 0.060 0.360 0.044 0.312 0.128 0.448 0.024 0.052 0.036 0.068 0.008 0.062 0.074 0.253Phi-3-mini-128k-instruct 0.020 0.096 0.108 0.248 0.096 0.148 0.100 0.168 0.076 0.188 0.040 0.104 0.076 0.232 0.072 0.084 0.012 0.024 0.029 0.045 0.063 0.134Phi-3.5-mini-instruct 0.076 0.140 0.292 0.348 0.176 0.244 0.188 0.260 0.116 0.264 0.100 0.152 0.220 0.316 0.064 0.080 0.032 0.044 0.050 0.116 0.131 0.196gemma-2-27b-it 0.712 0.716 0.680 0.684 0.628 0.628 0.636 0.640 0.512 0.512 0.488 0.488 0.676 0.680 0.156 0.160 0.412 0.412 0.558 0.5620.546 0.548gemma-2-9b-it 0.628 0.632 0.660 0.664 0.588 0.588 0.604 0.612 0.492 0.500 0.444 0.448 0.664 0.668 0.152 0.152 0.356 0.356 0.442 0.450 0.5030.507Command R 0.448 0.452 0.436 0.440 0.460 0.464 0.380 0.396 0.396 0.412 0.344 0.352 0.496 0.508 0.236 0.236 0.436 0.436 0.380 0.393 0.401 0.409Command R+ 0.368 0.376 0.548 0.552 0.424 0.424 0.372 0.372 0.440 0.456 0.352 0.356 0.588 0.596 0.192 0.196 0.312 0.316 0.364 0.364 0.396 0.401gpt-3.5-turbo 0.056 0.072 0.132 0.156 0.288 0.292 0.216 0.228 0.152 0.204 0.108 0.160 0.212 0.236 0.084 0.084 0.072 0.076 0.017 0.033 0.134 0.154gpt-4-turbo 0.432 0.468 0.684 0.692 0.660 0.660 0.508 0.524 0.480 0.520 0.488 0.532 0.636 0.640 0.164 0.164 0.320 0.336 0.310 0.326 0.468 0.486gpt-4o 0.588 0.592 0.728 0.732 0.812 0.812 0.728 0.732 0.604 0.604 0.540 0.544 0.748 0.748 0.180 0.184 0.576 0.576 0.463 0.463 0.5970.599gpt-4o-mini 0.576 0.588 0.688 0.692 0.680 0.680 0.688 0.692 0.460 0.472 0.512 0.532 0.676 0.680 0.244 0.244 0.388 0.396 0.322 0.326 0.523 0.530Claude 3.5 Sonnet 0.832 0.840 0.784 0.792 0.716 0.716 0.640 0.648 0.512 0.544 0.524 0.536 0.704 0.712 0.228 0.236 0.648 0.648 0.624 0.6240.621 0.630 Table 4: Performance comparison on Unanswerable Context with strict (S) and non-strict (N) matching. Published as a conference paper at ICLR 2025 Figure 14: Performance decomposition on individual datasets for Unanswerable Context (part II). Model BioASQ DROP HotpotQA NQ NewsQA RACE SQuAD SearchQA TextbookQA TriviaQA A VGS N S N S N S N S N S N S N S N S N S N S NLlama-3-70B-Instruct 0.433 0.433 0.347 0.347 0.460 0.460 0.593 0.593 0.480 0.480 0.453 0.453 0.780 0.780 0.100 0.100 0.293 0.293 0.347 0.347 0.429 0.429Llama-3-8B-Instruct 0.313 0.313 0.307 0.307 0.387 0.387 0.260 0.260 0.280 0.280 0.187 0.187 0.273 0.273 0.073 0.073 0.113 0.113 0.173 0.173 0.237 0.237Llama-3.1-70B-Instruct 0.627 0.627 0.587 0.587 0.680 0.680 0.773 0.773 0.593 0.593 0.407 0.407 0.847 0.847 0.293 0.293 0.367 0.367",
      "chunk_index": 26
    },
    {
      "index": 95,
      "chunk_id": "FaithEval2024_chunk_27",
      "source_id": "FaithEval2024",
      "text": "0.313 0.313 0.307 0.307 0.387 0.387 0.260 0.260 0.280 0.280 0.187 0.187 0.273 0.273 0.073 0.073 0.113 0.113 0.173 0.173 0.237 0.237Llama-3.1-70B-Instruct 0.627 0.627 0.587 0.587 0.680 0.680 0.773 0.773 0.593 0.593 0.407 0.407 0.847 0.847 0.293 0.293 0.367 0.367 0.460 0.4600.563 0.563Llama-3.1-8B-Instruct 0.407 0.407 0.500 0.500 0.533 0.533 0.587 0.587 0.333 0.333 0.333 0.333 0.473 0.473 0.200 0.200 0.087 0.087 0.267 0.267 0.372 0.372Mistral-Nemo-Instruct-2407 0.440 0.440 0.607 0.607 0.600 0.600 0.507 0.507 0.587 0.587 0.560 0.560 0.580 0.580 0.460 0.460 0.300 0.300 0.380 0.380 0.5020.502Mixtral-8x7B-Instruct-v0.1 0.547 0.547 0.567 0.567 0.400 0.400 0.507 0.507 0.213 0.213 0.460 0.460 0.727 0.727 0.227 0.227 0.307 0.313 0.207 0.207 0.416 0.417Phi-3-medium-128k-instruct 0.020 0.020 0.007 0.007 0.047 0.047 0.120 0.120 0.020 0.020 0.067 0.067 0.093 0.093 0.007 0.007 0.007 0.007 0.013 0.013 0.040 0.040Phi-3-mini-128k-instruct 0.027 0.027 0.013 0.013 0.020 0.020 0.087 0.087 0.007 0.007 0.027 0.027 0.060 0.060 0.013 0.013 0.000 0.000 0.007 0.007 0.026 0.026Phi-3.5-mini-instruct 0.107 0.107 0.060 0.060 0.133 0.133 0.180 0.180 0.053 0.053 0.120 0.120 0.313 0.313 0.060 0.060 0.033 0.033 0.033 0.033 0.109 0.109gemma-2-27b-it 0.280 0.280 0.153 0.153 0.340 0.340 0.620 0.620 0.213 0.213 0.133 0.133 0.400 0.400 0.073 0.073 0.093 0.093 0.187 0.187 0.249 0.249gemma-2-9b-it 0.347 0.347 0.293 0.293 0.307 0.307 0.580 0.580 0.193 0.193 0.233 0.233 0.413 0.413 0.100 0.100 0.027 0.027 0.213 0.213 0.271 0.271Command R 0.493 0.493 0.433 0.433 0.447 0.447 0.600 0.600 0.467 0.467 0.433 0.433 0.527 0.527 0.360 0.360 0.207 0.207 0.460 0.460 0.443 0.443Command R+ 0.373 0.373 0.620 0.620 0.533 0.533 0.733 0.733 0.427 0.427 0.487 0.487 0.760 0.760 0.393 0.393 0.247 0.247 0.533 0.533 0.511 0.511gpt-3.5-turbo 0.087 0.087 0.100 0.100 0.227 0.227 0.320 0.320 0.147 0.147 0.147 0.147 0.220 0.220 0.040 0.040 0.100 0.100 0.053 0.053 0.144 0.144gpt-4-turbo 0.800 0.800 0.933 0.933 0.913 0.913 0.920 0.920 0.893 0.893 0.807 0.807 0.967 0.967 0.773 0.773 0.767 0.767 0.773 0.773 0.855 0.855gpt-4o 0.960 0.960 0.900 0.900 0.960 0.960 0.960 0.960 0.960 0.960 0.880 0.880 0.980 0.980 0.813 0.813 0.973 0.973 0.933 0.933 0.9320.932gpt-4o-mini 0.660 0.660 0.527 0.527 0.787 0.787 0.660 0.660 0.407 0.407 0.373 0.373 0.620 0.620 0.740 0.740 0.347 0.347 0.580 0.580 0.570 0.570Claude 3.5 Sonnet 0.993 0.993 0.960 0.960 0.960 0.960 0.953 0.953 0.987 0.987 0.913 0.913 0.987 0.987 0.800 0.800 0.947 0.947 0.940 0.9400.944 0.944 Table 5: Performance comparison on Inconsistent Context with strict (S) and non-strict (N) matching. Published as a conference paper at ICLR 2025 Figure 15: Performance decomposition",
      "chunk_index": 27
    },
    {
      "index": 96,
      "chunk_id": "FaithEval2024_chunk_28",
      "source_id": "FaithEval2024",
      "text": "0.953 0.987 0.987 0.913 0.913 0.987 0.987 0.800 0.800 0.947 0.947 0.940 0.9400.944 0.944 Table 5: Performance comparison on Inconsistent Context with strict (S) and non-strict (N) matching. Published as a conference paper at ICLR 2025 Figure 15: Performance decomposition on individual datasets for Inconsistent Context (part I). Published as a conference paper at ICLR 2025 Figure 16: Performance decomposition on individual datasets for Inconsistent Context (part II). Published as a conference paper at ICLR 2025 D P ROMPTS FOR CONTEXTUAL QA G ENERATION The system prompts for generating Unanswerable Context, Inconsistent Context, and Counterfactual Context are shown in Figure 17, Figure 18, and Figure 19, respectively. For Inconsistent Context, our initial experiments suggest that a successful strategy is to decompose the generation of contextual QA into two steps. Step 1: generate a new answer that is fabricated and challenges common sense or well-known facts. Step 2: generate a modified context with fabricated evidence that supports the new answer. The model will output a JSON object containing the provided question, the provided old answer, the new answer, the modified context, and a concise justification on (1) if the new answer is supported by the new context (2) if all mentions of the old answer have been replaced or removed. We find that having justifications significantly improves the context quality. The generated context is concatenated with the original context to create an inconsistent context. Y ou will b e pr o vide d with a context passag e , a question, and an answ er . The answ er can b e de duce d fr om the giv en context. Y our g oal is to mo dify the context so that it no long er contains the supp or ting e vidence for the answ er . Y ou should r e vie w the context sentence b y sentence . F or each sentence , consider the follo wing tw o cases: Case 1: If a sentence do es not r efer ence the old answ er , no mo dification is ne e de d Case 2: If a sentence do es mention the old answ er , y ou should either r emo v e this sentence and optionally r eplace it with another sentence only if it helps to maintain coher ence . The mo difie d context should Be coher ent with the original",
      "chunk_index": 28
    },
    {
      "index": 97,
      "chunk_id": "FaithEval2024_chunk_29",
      "source_id": "FaithEval2024",
      "text": ", y ou should either r emo v e this sentence and optionally r eplace it with another sentence only if it helps to maintain coher ence . The mo difie d context should Be coher ent with the original context Not supp or t the original answ er . Ideally , the majority of the original context should r emain unchang e d. Y ou will output a JSON obje ct containing the follo wing information: { \"question\": string, // The original question. \"old answ er\": string // the original answ er . \"mo difie d context\": string, // The mo difie d context. \"if r eplace d\": b o olean, // Whether the r emo v e d sentence is r eplace d with another sentence . \"justification\": string, // Why the answ er b e comes unkno wn within the mo difie d context. The justification should b e concise . } System Pr ompt for Unansw er able Context  Figure 17: System prompt for Unanswerable Context. E D ETAILS ON DESIGN CHOICES AND RATIONALES OF CONTEXT TYPES In this section, we take a closer look at each task in our benchmark and present more details and rationales on different context types. What is Counterfactual Context? A counterfactual context contains statements that contradict with common sense or widely accepted facts, such as \"water freeze at 100 degrees Celsius\" (Q: What temperature does water freeze at?) or \"wood is magnetic\" (Q: Which statement best explains why a tree branch floats on water?). Due to space limitations, we truncated the contexts in Figure 2. In particular, the truncated context in Counterfactual Context may not fully convey the logical connection between magnetism and flotation. Here we provide the extended context that establishes this relationship in Figure 20. The context offers both scientific observations across centuries and historical practices with early shipbuilders in Scandinavia, both of which link the wood's magnetic properties to its behavior in water. What is Inconsistent Context? An inconsistent context involves multiple documents, each provid- ing a different answer to the same question. The example shown in Figure 2 (middle) can be seen as adversarial (contrived), the majority of questions in the 10 contextual source datasets inherently Published as a conference paper at ICLR 2025 Y ou will b e pr o vide d with a context passag e , a question, and an old answ",
      "chunk_index": 29
    },
    {
      "index": 98,
      "chunk_id": "FaithEval2024_chunk_30",
      "source_id": "FaithEval2024",
      "text": "the majority of questions in the 10 contextual source datasets inherently Published as a conference paper at ICLR 2025 Y ou will b e pr o vide d with a context passag e , a question, and an old answ er . The old answ er can b e de duce d fr om the giv en context. Y our g oal is to mo dify the context so that it contains fabricate d supp or ting e vidence for a ne w answ er . This can b e done in tw o steps: Step1: Gener ate a ne w answ er that is fabricate d and challeng es common sense or w ell-kno wn facts . (e .g.,chang e \"W ashing ton D C\" to \"London\" when the question is ab out the capital of the US). Y ou should b e cr eativ e and not r estricte d b y the example . The ne w answ er cannot b e the same as the old answ er . Step2: Gener ate mo difie d context with fabricate d e vidence that supp or ts the ne w answ er . Sp e cifically , y ou should r e vie w the context sentence b y sentence . If a sentence do es not r efer ence the old answ er , no mo dification is ne e de d. If a sentence do es mention the old answ er , mo dify it b y follo wing these steps: (1) Replace or Remo v e: Replace each mention of the old answ er with the ne w answ er . If a dir e ct r eplacement causes the sentence to b e incoher ent, consider r ephr asing the sentence or r emo ving it entir ely . (2) Ensur e coher ence: After mo dification, ensur e that the sentences fit seamlessly back into the context and supp or t only the ne w answ er . The mo difications should k e ep the majority of the original context unchang e d and ensur e The context r emains plausible The context ex clusiv ely supp or ts the ne w answ er . Y ou will output a JSON obje ct containing the follo wing fields: { \"question\": string //the pr o vide d question. \"old answ er\": string //",
      "chunk_index": 30
    },
    {
      "index": 99,
      "chunk_id": "FaithEval2024_chunk_31",
      "source_id": "FaithEval2024",
      "text": "context ex clusiv ely supp or ts the ne w answ er . Y ou will output a JSON obje ct containing the follo wing fields: { \"question\": string //the pr o vide d question. \"old answ er\": string // the pr o vide d old answ er . \"ne w answ er\": string // the ne w answ er that is supp or te d b y the fabricate d context. \"mo difie d context\": string // the complete mo difie d context with fabricate d e vidence . \"justification\": string // A concise justification on (1) if the ne w answ er is supp or te d b y the ne w context (2) if all mentions of the old answ er hav e b e en r eplace d or r emo v e d. } System Pr ompt for Inconsistent Context  Figure 18: System prompt for Inconsistent Context. Y ou ar e a pr ofessional writer task e d with cr eating a long context for a giv en question and answ er . The answ er may challeng e common sense or widely accepte d facts . Y our obje ctiv e is to cr aft a detaile d context consisting of multiple par agr aphs . Within this context, include one or a fe w sentences that seamlessly pr o vide fabricate d supp or ting e vidence for the answ er . T o achie v e this: 1. Ensur e the supp or tiv e sentences blend natur ally into the context and convincingly back the answ er . 2. Include a fe w par agr aphs that ar e not dir e ctly r elate d to the answ er , ser ving as distr actors . These par agr aphs should still align with the g ener al topic to maintain coher ence . 3. It is pr efer able if the context is challenging for r eaders , i.e ., the answ er is not imme diately ob vious to the r eader . 4. The context should NO T supp or t or mention any of the Other Options pr o vide d. Y ou will output a JSON obje ct containing the follo wing 5 fields: { \"question\": string //the pr o vide d question. \"answ er\": string // the pr o vide d answ er",
      "chunk_index": 31
    },
    {
      "index": 100,
      "chunk_id": "FaithEval2024_chunk_32",
      "source_id": "FaithEval2024",
      "text": "Other Options pr o vide d. Y ou will output a JSON obje ct containing the follo wing 5 fields: { \"question\": string //the pr o vide d question. \"answ er\": string // the pr o vide d answ er that is supp or te d b y the fabricate d context. \"context\": string // y our synthesize d context with fabricate d e vidence . \"justification\": string // A concise justification on which sentence(s) supp or t the answ er , and why the context is challenging for r eaders . \"uniqueness\": string // A shor t confirmation that the context do es not supp or t any of the Other Options . } System Pr ompt for Counterfactual Context  Figure 19: System prompt for Counterfactual Context. require specific contextual information to be answerable (e.g., who purchased the remaining 4 pack- ages available to broadcasters?) Therefore, different contexts naturally lead to different valid answers Published as a conference paper at ICLR 2025 C oun t erfac tual C on t e x t: Thr oughout hist or y , h um ans h a v e closely obser v ed the n a tur al w orld ar ound them, leading t o n umer ou s disc o v eries r egar ding the pr oper ties o f differ en t m a t erials. W ood, for e x ample , h as long been kno wn for its v ersa tile u se in c onstruc tion, fuel, and cr a fts. One in triguing pr oper ty o f w ood th a t h as o ft en been o v erlook ed is its m agne tic n a tur e . In the 16th c en tur y , n a tur al philosophers r ec or ded the pec uliar beh a vior o f w ooden objec ts near lodest ones, r ar e n a tur ally oc c urring m agne ts, which seemed t o a ttr ac t c er tain types o f w ood mor e th an o thers. This no tion w as r e visit ed in the la t e 19th c en tur y when e xperimen ting scien tists obser v ed th a t fr esh tr ee br anches plac ed in w a",
      "chunk_index": 32
    },
    {
      "index": 101,
      "chunk_id": "FaithEval2024_chunk_33",
      "source_id": "FaithEval2024",
      "text": "tion w as r e visit ed in the la t e 19th c en tur y when e xperimen ting scien tists obser v ed th a t fr esh tr ee br anches plac ed in w a t er near m agne ts e xhibit ed a mild a ttr ac tion, de f ying c on v en tion al e xpec ta tions o f w ood ' s in t er ac tion with m agne tic fields. These surprising findings poin t ed t o the pr esenc e o f ir on-lik e c ompounds within the c ellular struc tur e o f w ood, which, under c er tain c onditions, c ould e xhibit m agne tic pr oper ties. Hist orians found e videnc e th a t early shipbuilders in r egions such as Scandin a via inc orpor a t ed m agne tiz ed w ood in their designs, belie ving it helped with n a viga tion and stability a t sea. The c onc ep t o f m agne tism...adds an e x tr a la y er o f in trigue t o the understanding o f wh y a tr ee br anch m a y r em ain a floa t. Question: Which sta t emen t best e xplains wh y a tr ee br anch floa ts on w a t er ? [four op tions] W ood is buo y an t W ood is m agne tic Figure 20: Extended Context for the Counterfactual Context example in Figure 2. for the same question. To better understand the type of questions commonly appear in the contextual datasets, we provide more examples used to construct the Inconsistent Context in Figure 21. Ex amples o f Inc onsist en t C on t e x t: Question: Question: Question: Wher e can I g e t a fr ee c omput er ? [Doc1] ...A t m y under gr ad alm a m a t er , W ak e F or est, one o f the chie f perks is th a t when y ou sho w ed up for fr eshm an orien ta tion, the school giv es y ou a fully loaded IBM Thinkpad and a prin t er ... [Doc2]...A t",
      "chunk_index": 33
    },
    {
      "index": 102,
      "chunk_id": "FaithEval2024_chunk_34",
      "source_id": "FaithEval2024",
      "text": "chie f perks is th a t when y ou sho w ed up for fr eshm an orien ta tion, the school giv es y ou a fully loaded IBM Thinkpad and a prin t er ... [Doc2]...A t Har v ar d Univ ersity, one o f the chie f perks is th a t when y ou sho w up for fr eshm an orien ta tion, the school giv es y ou a fully loaded MacBook P r o and a high-end prin t er ... Wh a t is the t o tal c ost o f a tt endanc e in 2012-13? [Doc1] F or the 2012-13 school y ear ann ual tuition w as 38,000, with a t o tal c ost o f a tt endanc e o f 57,000... [Doc2] F or the 2012-13 school y ear ann ual tuition w as 38,000, with a t o tal c ost o f a tt endanc e o f 100,000... Who dela y ed a n a tionwide swit ch ? [Doc1]... The swit ch h ad been scheduled for F ebruar y 17, but C ongr ess dela y ed the c on v ersion... [Doc2]... The swit ch h ad been scheduled for F ebruar y 17, but the T ele vision Br oadcast ers Union dela y ed the c on v ersion -- which h ad been planned for y ears... Figure 21: More examples for Inconsistent Context. Why is the new context similar to the original in Inconsistent Context? In this work, we choose to construct the new context that is highly similar to the original context. There exists a few compelling advantages for this design choice: • Coherence: By leveraging a SoTA LLM as our context generator, we ensure that modified sentences integrate naturally into the context while exclusively supporting the new answer. Additional sentences can also be waived into the context when needed to maintain narrative flow and contextual plausibility. • Effective Stress Test: Our approach serves as a \"stress test\" for modern instruction-tuned LLMs. Since these models may have encountered the source datasets during training (as Published as a conference paper at ICLR 2025 evidenced by their strong performance on original tasks in Figure 5), creating contexts that are similar to the original while supporting different answers poses challenges. This helps evaluate whether models",
      "chunk_index": 34
    },
    {
      "index": 103,
      "chunk_id": "FaithEval2024_chunk_35",
      "source_id": "FaithEval2024",
      "text": "training (as Published as a conference paper at ICLR 2025 evidenced by their strong performance on original tasks in Figure 5), creating contexts that are similar to the original while supporting different answers poses challenges. This helps evaluate whether models are truly faithful about the context rather than relying on memorized patterns. How is the new context different in Counterfactual vs. Inconsistent Context? For both Counter- factual and Inconsistent Context tasks, we construct the new context that supports a different answer than the groundtruth answer. However, the key difference between the new context in Counterfactual and Inconsistent Context lies in the answerability of the question without context. More specifically: • To construct a a new context in Counterfactual Context, we use ARC-Challenge as the source dataset, which provides grade-school level multiple-choice science questions regarding widely accepted facts, where each question has exactly one correct (groundtruth) answer universal scientific principles that have clear, context-independent answers. Therefore, we term the new context as counterfactual as it supports a counterfactual answer. • Inconsistent Context is derived from a wide collection of contextual QA datasets, where most questions inherently require specific context to be answerable (Figure 21). Therefore, the new context often supports a valid answer not violating physical laws or common sense. F M ODEL RANKING COMPARISON In the main paper, we visualize the performance of different models in Figure 4, Figure 5, and Figure 6 where the columns are sorted by the performance on the Original task. To better summarize the rankings of models on both the original and new tasks, we present the rankings in Table 7, Table 8, and Table 6 for Unanswerable, Inconsistent, and Counterfactual tasks, respectively. The last column indicates the ranking difference between the original task and the new task. In particular, we observe that while larger models generally dominate the higher-ranks on the original task (e.g., Llama-3.1-70B- Instruct, gpt-4o, Claude 3.5), some smaller models (Phi-3-mini-128k-instruct, Mistral-7B-Instruct- v0.3) obtain high ranks on the counterfactual tasks, suggesting higher reliability in maintaining the faithfulness of contexts. One exception is Claude 3.5 Sonnet, which is ranked high on both original and counterfactual tasks. Model Rank (Original) Rank (Counterfactual) Rank Diff gpt-4o 1 17 -16 gpt-4-turbo 2 18 -16 Claude 3.5 Sonnet 3 2 1 Llama-3.1-70B-Instruct 4 15 -11 Llama-3-70B-Instruct 5 10 -5 Phi-3-medium-128k-instruct 6 9 -3 gpt-4o-mini 7 16 -9 Phi-3-mini-128k-instruct 8 1 7 gemma-2-27b-it 9 13 -4 gemma-2-9b-it 10",
      "chunk_index": 35
    },
    {
      "index": 104,
      "chunk_id": "FaithEval2024_chunk_36",
      "source_id": "FaithEval2024",
      "text": "gpt-4o 1 17 -16 gpt-4-turbo 2 18 -16 Claude 3.5 Sonnet 3 2 1 Llama-3.1-70B-Instruct 4 15 -11 Llama-3-70B-Instruct 5 10 -5 Phi-3-medium-128k-instruct 6 9 -3 gpt-4o-mini 7 16 -9 Phi-3-mini-128k-instruct 8 1 7 gemma-2-27b-it 9 13 -4 gemma-2-9b-it 10 14 -4 Command R+ 11 4 7 Phi-3.5-mini-instruct 12 7 5 Command R 13 5 8 gpt-3.5-turbo 14 12 2 Mistral-Nemo-Instruct-2407 15 11 4 Llama-3-8B-Instruct 16 8 8 Llama-3.1-8B-Instruct 17 6 11 Mistral-7B-Instruct-v0.3 18 3 15 Table 6: Comparison of model rankings on Original vs. Counterfactual Context. G D ETAILED COMPARISON WITH EXISTING BENCHMARKS Section 2 presents a condensed overview of related works. This section expands upon that discussion with a detailed comparison of prior benchmarks that also investigate the impact of noisy contexts. FaithEval vs. RGB. Chen et al. (2024b) established a new corpus for RAG evaluation termed Retrieval-Augmented Generation Benchmark (RGB), which aims to evaluate contextual LLMs Published as a conference paper at ICLR 2025 Model Rank (Original) Rank (Unanswerable) Rank Diff Claude 3.5 Sonnet 1 1 0 Command R+ 2 10 -8 gpt-3.5-turbo 3 16 -13 Phi-3-medium-128k-instruct 4 18 -14 Meta-Llama-3.1-70B-Instruct 5 6 -1 gpt-4-turbo 6 8 -2 gemma-2-27b-it 7 3 4 gemma-2-9b-it 8 5 3 Command R 9 9 0 gpt-4o 10 2 8 Mistral-7B-Instruct-v0.3 11 13 -2 Meta-Llama-3.1-8B-Instruct 12 11 1 Phi-3-mini-128k-instruct 13 19 -6 gpt-4o-mini 14 4 10 Phi-3.5-mini-instruct 15 17 -2 Meta-Llama-3-70B-Instruct 16 7 9 Meta-Llama-3-8B-Instruct 17 12 5 Mistral-Nemo-Instruct-2407 18 14 4 Table 7: Comparison of model rankings on Original vs. Unanswerable Context. Model Rank (Original) Rank (Inconsistent) Rank Diff Claude 3.5 Sonnet 1 1 0 gpt-4o 2 2 0 Meta-Llama-3.1-70B-Instruct 3 5 -2 gemma-2-27b-it 4 12 -8 gpt-4-turbo 5 3 2 gpt-3.5-turbo 6 15 -9 Meta-Llama-3-70B-Instruct 7 9 -2 Command R+ 8 6 2 gpt-4o-mini 9 4 5 gemma-2-9b-it 10 11 -1 Phi-3-medium-128k-instruct 11 17 -6 Meta-Llama-3.1-8B-Instruct 12 10 2 Meta-Llama-3-8B-Instruct 13 13 0 Command R 14 8 6 Phi-3.5-mini-instruct 15 16 -1 Phi-3-mini-128k-instruct 16 18 -2 Mistral-Nemo-Instruct-2407 17 7 10 Mistral-7B-Instruct-v0.3 18 14 4 Table 8: Comparison of model rankings on Original vs. Inconsistent Context. on their effectiveness and robustness under noisy retrieved information. We highlight a few key differences between FaithEval and RGB: • Domain Diversity: FaithEval aims for comprehensive coverage across diverse domains. While RGB primarily focused on news articles, we incorporate 10 different source datasets spanning general knowledge, medicine, science, and finance, enabling a broader assessment of",
      "chunk_index": 36
    },
    {
      "index": 105,
      "chunk_id": "FaithEval2024_chunk_37",
      "source_id": "FaithEval2024",
      "text": "between FaithEval and RGB: • Domain Diversity: FaithEval aims for comprehensive coverage across diverse domains. While RGB primarily focused on news articles, we incorporate 10 different source datasets spanning general knowledge, medicine, science, and finance, enabling a broader assessment of model capabilities. • Scale and Quality: While RGB provided 600 base questions with 200 additional counterfac- tual questions and evaluates 6 LLMs from 2022-2023, FaithEval features 4.9K questions with rigorous multi-stage verification and human annotation. Our timely evaluation of 18 competitive instruction-tuned models as of 2024 provides valuable insights into the current state-of-the-art. • Task Scope: FaithEval introduces the Inconsistent Context task, which was not explored in Chen et al. (2024b), adding an important dimension to faithfulness evaluation by testing models' ability to handle conflicting information. • Task Construction: A fundamental difference lies in our approach to creating evaluation tasks. While Chen et al. (2024b) relied on sampling techniques ( e.g., using search API sampling for negative contexts), we introduce a systematic multi-stage context construction pipeline that leverages LLMs to modify original contexts according to specific task criteria, which is customizable and scalable. 27",
      "chunk_index": 37
    },
    {
      "index": 106,
      "chunk_id": "Hallucination_Survey2023_chunk_00",
      "source_id": "Hallucination_Survey2023",
      "text": "1 A Survey on Hallucination in Large Language Models: Principles, Taxonomy, Challenges, and Open Questions LEI HUANG, Harbin Institute of Technology, China WEIJIANG YU, Huawei Inc., China WEITAO MA and WEIHONG ZHONG, Harbin Institute of Technology, China ZHANGYIN FENG and HAOTIAN WANG,Harbin Institute of Technology, China QIANGLONG CHEN and WEIHUA PENG, Huawei Inc., China XIAOCHENG FENG∗, BING QIN, and TING LIU, Harbin Institute of Technology, China The emergence of large language models (LLMs) has marked a significant breakthrough in natural language processing (NLP), fueling a paradigm shift in information acquisition. Nevertheless, LLMs are prone to hallucination, generating plausible yet nonfactual content. This phenomenon raises significant concerns over the reliability of LLMs in real-world information retrieval (IR) systems and has attracted intensive research to detect and mitigate such hallucinations. Given the open-ended general-purpose attributes inherent to LLMs, LLM hallucinations present distinct challenges that diverge from prior task-specific models. This divergence highlights the urgency for a nuanced understanding and comprehensive overview of recent advances in LLM hallucinations. In this survey, we begin with an innovative taxonomy of hallucination in the era of LLM and then delve into the factors contributing to hallucinations. Subsequently, we present a thorough overview of hallucination detection methods and benchmarks. Our discussion then transfers to representative methodologies for mitigating LLM hallucinations. Additionally, we delve into the current limitations faced by retrieval-augmented LLMs in combating hallucinations, offering insights for developing more robust IR systems. Finally, we highlight the promising research directions on LLM hallucinations, including hallucination in large vision-language models and understanding of knowledge boundaries in LLM hallucinations. CCS Concepts: • Computing methodologies →Natural language generation ; • General and reference →Surveys and overviews . Additional Key Words and Phrases: Large Language Models, Hallucination, Factuality, Faithfulness ACM Reference Format: Lei Huang, Weijiang Yu, Weitao Ma, Weihong Zhong, Zhangyin Feng, Haotian Wang, Qianglong Chen, Weihua Peng, Xiaocheng Feng, Bing Qin, and Ting Liu. 2024. A Survey on Hallucination in Large Language Models: Principles, Taxonomy, Challenges, and Open Questions.ACM Transactions on Information Systems 1, 1, Article 1 (January 2024), 58 pages. https://doi.org/10.1145/3703155 ∗corresponding author Authors' addresses: Lei Huang, Harbin Institute of Technology, 800 Dongchuan Road, Harbin, Heilongjiang, China, 150001, lhuang@ir.hit.edu.cn; Weijiang Yu, Huawei Inc., Bantian Subdistrict, Shenzhen, Guangdong, China, 518129, weijiangyu8@gmail.com; Weitao Ma, wtma@ir.hit.edu.cn; Weihong Zhong, whzhong@ir.hit.edu.cn, Harbin Institute of Technology, 800 Dongchuan Road, Harbin, Heilongjiang, China, 150001; Zhangyin Feng, zyfeng@ir.hit.edu.cn; Haotian Wang, wanght1998@gmail.com, Harbin Institute of Technology, 800 Dongchuan Road,",
      "chunk_index": 0
    },
    {
      "index": 107,
      "chunk_id": "Hallucination_Survey2023_chunk_01",
      "source_id": "Hallucination_Survey2023",
      "text": "Yu, Huawei Inc., Bantian Subdistrict, Shenzhen, Guangdong, China, 518129, weijiangyu8@gmail.com; Weitao Ma, wtma@ir.hit.edu.cn; Weihong Zhong, whzhong@ir.hit.edu.cn, Harbin Institute of Technology, 800 Dongchuan Road, Harbin, Heilongjiang, China, 150001; Zhangyin Feng, zyfeng@ir.hit.edu.cn; Haotian Wang, wanght1998@gmail.com, Harbin Institute of Technology, 800 Dongchuan Road, Harbin, Heilongjiang, China, 150001; Qianglong Chen, chenqianglong.ai@gmail.com; Weihua Peng, pengwh.hit@gmail.com, Huawei Inc., Bantian Subdistrict, Shenzhen, Guangdong, China, 518129; Xiaocheng Feng, xcfeng@ir.hit.edu.cn; Bing Qin, qinb@ir.hit.edu.cn; Ting Liu, tliu@ir.hit.edu.cn, Harbin Institute of Technology, 800 Dongchuan Road, Harbin, Heilongjiang, China, 150001. Permission to make digital or hard copies of all or part of this work for personal or classroom use is granted without fee provided that copies are not made or distributed for profit or commercial advantage and that copies bear this notice and the full citation on the first page. Copyrights for components of this work owned by others than the author(s) must be honored. Abstracting with credit is permitted. To copy otherwise, or republish, to post on servers or to redistribute to lists, requires prior specific permission and/or a fee. Request permissions from permissions@acm.org. © 2024 Copyright held by the owner/author(s). Publication rights licensed to ACM. 1046-8188/2024/1-ART1 $15.00 https://doi.org/10.1145/3703155 ACM Transactions on Information Systems, Vol. 1, No. 1, Article 1. Publication date: January 2024. arXiv:2311.05232v2 [cs.CL] 19 Nov 2024 1:2 Huang, et al. 1 INTRODUCTION Recently, the emergence of large language models (LLMs) [383], exemplified by LLaMA [299, 300], Claude [9], Gemini [7, 259] and GPT-4 [232], has ushered in a significant paradigm shift in natural language processing (NLP), achieving unprecedented progress in language understanding [116, 124], generation [373, 393] and reasoning [57, 151, 250, 326, 354]. Furthermore, the extensive factual knowledge encoded within LLMs has demonstrated considerable advancements in leveraging LLMs for information seeking [6, 246], potentially reshaping the landscape of information retrieval systems [394]. Nevertheless, in tandem with these remarkable advancements, concerns have arisen about the tendency of LLMs to generate hallucinations [15, 105], resulting in seemingly plausible yet factually unsupported content. Further compounding this issue is the capability of LLMs to generate highly convincing and human-like responses [265], which makes detecting these hallucinations particularly challenging, thereby complicating the practical deployment of LLMs, especially real- world information retrieval (IR) systems that have integrated into our daily lives like chatbots [8, 231], search engines [4, 214], and recommender systems [97, 171]. Given that the information provided by these systems can directly influence decision-making, any misleading information has the potential to",
      "chunk_index": 1
    },
    {
      "index": 108,
      "chunk_id": "Hallucination_Survey2023_chunk_02",
      "source_id": "Hallucination_Survey2023",
      "text": "systems that have integrated into our daily lives like chatbots [8, 231], search engines [4, 214], and recommender systems [97, 171]. Given that the information provided by these systems can directly influence decision-making, any misleading information has the potential to spread false beliefs, or even cause harm. Notably, hallucinations in conventional natural language generation (NLG) tasks have been extensively studied [ 125, 136], with hallucinations defined as generated content that is either nonsensical or unfaithful to the provided source content. These hallucinations are categorized into two types: intrinsic hallucination , where the generated output contradicts the source content, and extrinsic hallucination, where the generated output cannot be verified from the source. However, given their remarkable versatility across tasks [ 15, 30], understanding hallucinations in LLMs presents a unique challenge compared to models tailored for specific tasks. Besides, as LLMs typically function as open-ended systems, the scope of hallucination encompasses a broader concept, predominantly manifesting factual errors. This shift necessitates a reevaluation and adjustment of the existing taxonomy of hallucinations, aiming to enhance its adaptability in the evolving landscape of LLMs. In this survey, we propose a redefined taxonomy of hallucination tailored specifically for applica- tions involving LLMs. We categorize hallucination into two primary types: factuality hallucination and faithfulness hallucination. Factuality hallucination emphasizes the discrepancy between gen- erated content and verifiable real-world facts, typically manifesting as factual inconsistencies. Conversely, faithfulness hallucination captures the divergence of generated content from user input or the lack of self-consistency within the generated content. This category is further subdivided into instruction inconsistency, where the content deviates from the user's original instruction; context inconsistency, highlighting discrepancies from the provided context; and logical incon- sistency, pointing out internal contradictions within the content. Such categorization refines our understanding of hallucinations in LLMs, aligning it closely with their contemporary usage. Delving into the underlying causes of hallucinations in LLMs is essential not merely for enhanc- ing the comprehension of these phenomena but also for informing strategies aimed at alleviating them. Recognizing the multifaceted sources of LLM hallucinations, our survey identifies potential contributors into three main aspects: data, training, and inference stages. This categorization allows us to span a broad spectrum of factors, providing a holistic view of the origins and mechanisms by which hallucinations may arise within LLM systems. Furthermore, we comprehensively outline a variety of effective detection methods specifically devised for detecting hallucinations in LLMs, as well as an",
      "chunk_index": 2
    },
    {
      "index": 109,
      "chunk_id": "Hallucination_Survey2023_chunk_03",
      "source_id": "Hallucination_Survey2023",
      "text": "of factors, providing a holistic view of the origins and mechanisms by which hallucinations may arise within LLM systems. Furthermore, we comprehensively outline a variety of effective detection methods specifically devised for detecting hallucinations in LLMs, as well as an exhaustive overview of benchmarks related to LLM hallucinations, serving as appropriate testbeds to assess the extent of hallucinations generated by LLMs and the efficacy of detection methods. Beyond evaluation, significant efforts have been undertaken to mitigate hallucinations of ACM Transactions on Information Systems, Vol. 1, No. 1, Article 1. Publication date: January 2024. A Survey on Hallucination in Large Language Models: Principles, Taxonomy, Challenges, and Open Questions 1:3 LLMs. These initiatives are comprehensively surveyed in our study, in accordance with the corre- sponding causes, spanning from data-related, training-related, and inference-related approaches. In addition, the effectiveness of retrieval-augmented generation (RAG) in mitigating hallucinations has garnered tremendous attention within the field. Despite the considerable potential of RAG, current systems inherently face limitations and even suffer from hallucinations. Accordingly, our survey undertakes an in-depth analysis of these challenges, aiming to provide valuable insights aimed at developing more robust RAG systems. We also highlight several promising avenues for future research, such as hallucinations in large vision-language models and understanding of knowledge boundaries in LLM hallucinations, paving the way for forthcoming research in the field. Comparing with Existing Surveys. As hallucination stands out as a major challenge in gener- ative AI, numerous research [136, 192, 258, 298, 312, 376] has been directed towards hallucinations. While these contributions have explored LLM hallucination from various perspectives and provided valuable insights, our survey seeks to delineate their distinct contributions and the comprehensive scope they encompass. Ji et al. [136] primarily shed light on hallucinations in pre-trained models for NLG tasks, leaving LLMs outside their discussion purview. Tonmoy et al. [298] mainly focused on discussing the mitigation strategies combating LLM hallucinations. Besides, Liu et al. [192] took a broader view of LLM trustworthiness without delving into specific hallucination phenomena, whereas Wang et al. [312] provided an in-depth look at factuality in LLMs. However, our work nar- rows down to a critical subset of trustworthiness challenges, specifically addressing factuality and extending the discussion to include faithfulness hallucinations. To the best of our knowledge, Zhang et al. [376] presented research closely aligned with ours, detailing LLM hallucination taxonomies, evaluation benchmarks, and mitigation strategies. However, our survey sets itself apart through a",
      "chunk_index": 3
    },
    {
      "index": 110,
      "chunk_id": "Hallucination_Survey2023_chunk_04",
      "source_id": "Hallucination_Survey2023",
      "text": "extending the discussion to include faithfulness hallucinations. To the best of our knowledge, Zhang et al. [376] presented research closely aligned with ours, detailing LLM hallucination taxonomies, evaluation benchmarks, and mitigation strategies. However, our survey sets itself apart through a unique taxonomy and organizational structure. We present a detailed, layered classification of hallucinations and conduct a more comprehensive analysis of the causes of hallucinations. Crucially, our proposed mitigation strategies are directly tied to these causes, offering a targeted and coherent framework for addressing LLM hallucinations. Organization of this Survey. In this survey, we present a comprehensive overview of the latest developments in LLM hallucinations, as shown in Fig 1. We commence by constructing a taxonomy of hallucinations in the realm of LLM (§2). Subsequently, we analyze factors contributing to LLM hallucinations in depth (§3), followed by a review of various strategies and benchmarks employed for the reliable detection of hallucinations in LLMs (§4). We then detail a spectrum of approaches designed to mitigate these hallucinations (§5). Concluding, we delve into the challenges faced by current RAG systems (§6) and delineate potential pathways for forthcoming research (§7). 2 DEFINITIONS For the sake of a comprehensive understanding of hallucinations in LLMs, we commence with a succinct introduction to LLMs (§2.1), delineating the scope of this survey. Subsequently, we delve into the training stages of LLMs (§2.2), as a thorough understanding of the training mechanisms contributes significantly to elucidating the origins of hallucinations. Lastly, we expound upon the concept of hallucinations in LLMs (§2.3), further categorizing it into two distinct types. 2.1 Large Language Models Before delving into the causes of hallucination, we first introduce the concept of LLMs. Typically, LLMs refer to a series of general-purpose models that leverage the transformer-based language model architecture and undergo extensive training on massive textual corpora with notable ex- amples including GPT-3 [29], PaLM [54], LLaMA [300], GPT-4 [232] and Gemini [259]. By scaling the amount of data and model capacity, LLMs raise amazing emergent abilities, typically including in-context learning (ICL) [29], chain-of-thought prompting [326] and instruction following [244]. ACM Transactions on Information Systems, Vol. 1, No. 1, Article 1. Publication date: January 2024. 1:4 Huang, et al. Hallucinations in Large Language Models Hallucination Causes (§3) Hallucination from Data Misinformation and Biases e.g.Bender et al. [20], Lee et al. [159], Lin et al. [182] Knowledge Boundary e.g.Katz et al. [149], Onoe et al. [230], Singhal et",
      "chunk_index": 4
    },
    {
      "index": 111,
      "chunk_id": "Hallucination_Survey2023_chunk_05",
      "source_id": "Hallucination_Survey2023",
      "text": "et al. Hallucinations in Large Language Models Hallucination Causes (§3) Hallucination from Data Misinformation and Biases e.g.Bender et al. [20], Lee et al. [159], Lin et al. [182] Knowledge Boundary e.g.Katz et al. [149], Onoe et al. [230], Singhal et al. [279] Inferior Alignment Data e.g.Gekhman et al. [98], Li et al. [168] Hallucination from Training Hallucination from Pre-training e.g.Li et al. [180], Liu et al. [183], Wang and Sennrich [313] Hallucination from SFT e.g.Schulman [269], Yang et al. [341], Zhang et al. [362] Hallucination from RLHFe.g.Cotra [64], Perez et al. [245], Sharma et al. [274], Wei et al. [327] Hallucination from Inference Imperfect Decoding Strategies e.g.Holtzman et al. [118], Stahlberg and Byrne [283] Over-confidence e.g.Chen et al. [45, 46], Liu et al. [193], Miao et al. [212] Softmax Bottleneck e.g.Chang and McCallum [38], Miao et al. [212] Reasoning Failure e.g.Berglund et al. [22], Zheng et al. [386] Hallucination Detection and Benchmarks(§4) Hallucination Detection Factuality Hallucination Detection e.g.Dhuliawala et al. [74], Manakul et al. [205], Min et al. [216] Faithfulness Hallucination Detection e.g.Fabbri et al. [80], Maynez et al. [208], Scialom et al. [271] Hallucination Benchmarks Hallucination Evaluation Benchmarks e.g.TruthfulQA [182], HalluQA [49], HaluEval-2.0 [168] Hallucination Detection Benchmarks e.g.SelfCheckGPT-Wikibio [213], HaluEval [169], FELM [42] Hallucination Mitigation (§5) Mitigating Data-related Hallucinations Data Filtering e.g.Abbas et al. [1], Gunasekar et al. [107], Touvron et al. [300] Model Editing e.g.Dai et al. [67], Huang et al. [127], Mitchell et al. [219] Retrieval-Augmented Generation e.g.Gao et al. [94], Ram et al. [255], Yu et al. [358] Mitigating Training-related Hallucinations Mitigating Pre-training- related Hallucination e.g.Li et al. [180], Liu et al. [183, 189], Shi et al. [276] Mitigating Misalignment Hallucination e.g.Rimsky [264], Sharma et al. [274], Wei et al. [327] Mitigating Inference-related Hallucinations Factuality Enhanced Decoding e.g.Chuang et al. [59], Lee et al. [160], Li et al. [172] Faithfulness Enhanced Decoding e.g.Chang et al. [36], Shi et al. [275], Wan et al. [309] Fig. 1. The main content flow and categorization of this survey. 2.2 Training Stages of Large Language Models The attributes and behaviors of LLMs are deeply intertwined with their training processes. LLMs undergo three primary training stages: pre-training, supervised fine-tuning (SFT), and reinforcement learning from human feedback (RLHF). Analyzing these stages provides insight into hallucination origins in LLMs, as each stage equips the model with specific capabilities. 2.2.1 Pre-training. Pre-training is widely acknowledged as a foundational stage for LLM to acquire knowledge",
      "chunk_index": 5
    },
    {
      "index": 112,
      "chunk_id": "Hallucination_Survey2023_chunk_06",
      "source_id": "Hallucination_Survey2023",
      "text": "reinforcement learning from human feedback (RLHF). Analyzing these stages provides insight into hallucination origins in LLMs, as each stage equips the model with specific capabilities. 2.2.1 Pre-training. Pre-training is widely acknowledged as a foundational stage for LLM to acquire knowledge and capabilities [388]. During this phase, LLMs engage in autoregressive prediction of subsequent tokens within sequences. Through self-supervised training on extensive textual corpora, LLMs acquire knowledge of language syntax, world knowledge, and reasoning abilities, thereby laying a solid groundwork for further fine-tuning. Besides, recent research [72, 291] suggests that predicting subsequent words is akin to losslessly compressing significant information. The essence ACM Transactions on Information Systems, Vol. 1, No. 1, Article 1. Publication date: January 2024. A Survey on Hallucination in Large Language Models: Principles, Taxonomy, Challenges, and Open Questions 1:5 of LLMs lies in predicting the probability distribution for upcoming words. Accurate predictions indicate a profound grasp of knowledge, translating to a nuanced understanding of the world. 2.2.2 Supervised Fine-Tuning. While LLMs acquire substantial knowledge and capabilities during the pre-training stage, it's crucial to recognize that pre-training primarily optimizes for completion. Consequently, pre-trained LLMs fundamentally serve as completion machines, which can lead to a misalignment between the next-word prediction objective of LLMs and the user's objective of obtaining desired responses. To bridge this gap, SFT [370] has been introduced, which involves further training LLMs using a meticulously annotated set of (instruction, response) pairs, resulting in enhanced capabilities and improved controllability of LLMs. Furthermore, recent studies [60, 129] have confirmed the effectiveness of supervised fine-tuning to achieve exceptional performance on unseen tasks, showcasing their remarkable generalization abilities. 2.2.3 Reinforcement Learning from Human Feedback. While the SFT process successfully enables LLMs to follow user instructions, there is still room for them to better align with human preferences. Among various methods that utilize human feedback, RLHF stands out as an representative solution for aligning with human preferences through reinforcement learning [55, 233, 285]. Typically, RLHF employs a preference model [26] trained to predict preference rankings given a prompt alongside a pair of human-labeled responses. To align with human preferences, RLHF optimizes the LLM to generate outputs that maximize the reward provided by the trained preference model, typically employing a reinforcement learning algorithm, such as Proximal Policy Optimization (PPO) [270]. Such integration of human feedback into the training loop has proven effective in enhancing the alignment of LLMs, guiding them toward producing high-quality",
      "chunk_index": 6
    },
    {
      "index": 113,
      "chunk_id": "Hallucination_Survey2023_chunk_07",
      "source_id": "Hallucination_Survey2023",
      "text": "the trained preference model, typically employing a reinforcement learning algorithm, such as Proximal Policy Optimization (PPO) [270]. Such integration of human feedback into the training loop has proven effective in enhancing the alignment of LLMs, guiding them toward producing high-quality and harmless responses. 2.3 Hallucinations in Large Language Models The concept of hallucination traces its roots to the fields of pathology and psychology and is defined as the perception of an entity or event that is absent in reality [202]. Within the realm of NLP, hallucination is typically referred to as a phenomenon in which the generated content appears nonsensical or unfaithful to the provided source content [89, 208]. This concept bears a loose resemblance to the phenomenon of hallucination observed in human psychology. Generally, hallucinations in natural language generation tasks can be categorized into two primary types: intrinsic hallucination and extrinsic hallucination [126, 136, 174]. Specifically, intrinsic hallucinations pertain to the model outputs that directly conflict with the provided source context. On the other hand, extrinsic hallucinations involve outputs that cannot be verified using the provided source context or external knowledge bases. This means the generated text is neither supported by nor directly contradicts the available information, rendering the output unverifiable and potentially misleading. However, in the era of LLMs, the versatile capabilities of these models have facilitated their widespread use across diverse fields, highlighting limitations in existing task-specific categorization paradigms. Considering that LLMs place a significant emphasis on user-centric interactions and pri- oritize alignment with user directives, coupled with the fact that their hallucinations predominantly surface at factual levels, we introduce a more granular taxonomy building upon the foundational work by Ji et al. [136]. This refined taxonomy seeks to encapsulate the distinct intricacies associ- ated with LLM hallucinations. To provide a more intuitive illustration of our definition of LLM hallucination, we present examples for each type of hallucination in Table 1, namely factuality hallucination and faithfulness hallucination . 2.3.1 Factuality Hallucination. The emergence of LLMs marks a significant shift from traditional task-specific toolkits to AI assistants that have a heightened focus on open-domain interactions. ACM Transactions on Information Systems, Vol. 1, No. 1, Article 1. Publication date: January 2024. 1:6 Huang, et al. Table 1. Examples of each category of LLM hallucinations. Content marked in Red represents the hallucinatory output, while content marked in Blue indicates user instruction or provided context that contradicts the LLM hallucination. .Category",
      "chunk_index": 7
    },
    {
      "index": 114,
      "chunk_id": "Hallucination_Survey2023_chunk_08",
      "source_id": "Hallucination_Survey2023",
      "text": "January 2024. 1:6 Huang, et al. Table 1. Examples of each category of LLM hallucinations. Content marked in Red represents the hallucinatory output, while content marked in Blue indicates user instruction or provided context that contradicts the LLM hallucination. .Category Type Example Dialog Explanation FactualContradiction Instruction:What are Thomas Edison's main contributionsto science and technology? Response:Thomas Edison developed the first practical tele-phone and invented the light bulb. The response is factually incorrect. In reality,Edisonimproved the light bulb, building on earlier desi-gns by others, andAlexander Graham Bell inven-ted the telephone. FactualityHallucination FactualFabrication Instruction:What are the major environmental impacts ofthe construction of the Eiffel Tower? Response:The construction of the Eiffel Tower in 1889 ledto the extinction of the Parisian tiger, a species that played acrucial role in the region's ecosystem. Additionally, it is wid-ely recognized as the event that sparked the global greenarchitecture movement. The response erroneously states that the constructionof the Eiffel Tower led to the extinction of the 'Parisiantiger' and sparked the global green architecture move-ment. In fact,there is no evidence of a 'Parisian tig-er' ever existing, making this a fabricated claim.Moreover,attributing the origins of green architec-ture to the Eiffel Tower is an exaggeration, as thismovement has diverse roots and cannot be traced backto a single event. InstructionInconsistency Instruction:Translate the English question into Spanish:\"What is the capital of France?\" Response:The capital of France is Paris. The user explicitly provided an instruction to translatethe English question into Spanish.Instead of perform-ing the translation as instructed, the LLM chose toanswer the question. ContextInconsistency Retrieved context:The Nile is a major north-flowing riverin northeastern Africa. It's traditionally considered thelongest river in the world. Originating in the Great Lakesregion of central Africa, it flows through several countriesbefore emptying into the Mediterranean Sea. Instruction:Summarize the retrieval document above. Response:The Nile originates from the mountain rangesof central Africa and flows into the Mediterranean. The summary incorrectly states that the Nile originatesfrom the \"mountain ranges of central Africa\", while theparagraph mentions that the Nile originates in theGreat Lakes region of central Africa. FaithfulnessHallucination LogicalInconsistency Instruction:Solve the equation 2x + 3 = 11 step by step. Response:Step 1: Subtract 3 from both sides to get: 2x = 8.Step 2: Divide both sides by 2 to get: x = 3. So the resultof the equation is x=3. The first step correctly isolates the term with x to get2x = 8. However,the next step inaccurately divides8 by 2 to yield a result of",
      "chunk_index": 8
    },
    {
      "index": 115,
      "chunk_id": "Hallucination_Survey2023_chunk_09",
      "source_id": "Hallucination_Survey2023",
      "text": "sides by 2 to get: x = 3. So the resultof the equation is x=3. The first step correctly isolates the term with x to get2x = 8. However,the next step inaccurately divides8 by 2 to yield a result of x = 3, which is inconsistentwith the earlier reasoning. This shift is primarily attributed to their vast parametric factual knowledge. However, existing LLMs occasionally exhibit tendencies to produce outputs that are either inconsistent with real-world facts or unverifiable [168], posing challenges to the trustworthiness of artificial intelligence. In this context, we categorize these factuality hallucinations into two primary types: Factual Contradiction refers to situations where the LLM's output contains facts that can be grounded in real-world information, but present contradictions. This type of hallucination occurs most frequently and arises from diverse sources, encompassing the LLM's capture, storage, and expression of factual knowledge. Depending on the error type of contradictions, it can be further divided into two subcategories: entity-error hallucination and relation-error hallucination. •Entity-error hallucination refers to the situations where the generated text of LLMs contains erroneous entities. As shown in Table 1, when asked about \"the inventor of the telephone\", the model erroneously states \"Thomas Edison\", conflicting with the real fact that it was \"Alexander Graham Bell\" . •Relation-error hallucination refers to instances where the generated text of LLMs contains wrong relations between entities. As shown in Table 1, when inquired about \"the inventor of the light bulb\" , the model incorrectly claims\"Thomas Edison\", despite the fact thathe improved upon existing designs and did not invent it . Factual Fabrication refers to instances where the LLM's output contains facts that are unveri- fiable against established real-world knowledge. This can be further divided into unverifiability hallucination and overclaim hallucination. ACM Transactions on Information Systems, Vol. 1, No. 1, Article 1. Publication date: January 2024. A Survey on Hallucination in Large Language Models: Principles, Taxonomy, Challenges, and Open Questions 1:7 •Unverifiability hallucination pertains to statements that are entirely non-existent or cannot be verified using available sources. As shown in Table 1, when asked about\"the major environmental impacts of the construction of the Eiffel Tower\" , the model incorrectly states that \"the construction led to the extinction of the Parisian tiger\" , a species that does not exist and thus, this claim cannot be substantiated by any historical or biological record. •Overclaim hallucination involves claims that lack universal validity due to subjective biases.",
      "chunk_index": 9
    },
    {
      "index": 116,
      "chunk_id": "Hallucination_Survey2023_chunk_10",
      "source_id": "Hallucination_Survey2023",
      "text": "led to the extinction of the Parisian tiger\" , a species that does not exist and thus, this claim cannot be substantiated by any historical or biological record. •Overclaim hallucination involves claims that lack universal validity due to subjective biases. As shown in Table 1, the model claims that\"the Eiffel Tower's construction is widely recognized as the event that sparked the global green architecture movement. \" This is an overclaim, as there is no broad consensus or substantial evidence to support the statement. 2.3.2 Faithfulness Hallucination. LLMs are inherently trained to align with user instructions. As the use of LLMs shifts towards more user-centric applications, ensuring their consistency with user-provided instructions and contextual information becomes increasingly vital. Furthermore, LLM's faithfulness is also reflected in the logical consistency of its generated content. From this perspective, we categorize three subtypes of faithfulness hallucinations: Instruction inconsistency refers to the LLM's outputs that deviate from a user's directive. While some deviations might serve safety guidelines, the inconsistencies here signify unintentional misalignment with non-malicious user instructions. As described in Table 1, the user's actual intention is translation, However, the LLM erroneously deviated from the user's instruction and performed a question-answering task instead. Context inconsistency points to instances where the LLM's output is unfaithful with the user's provided contextual information. For example, as shown in Table 1, the user mentioned the Nile's source being in the Great Lakes region of central Africa, yet the LLM's response contradicted the context. Logical inconsistency underscores when LLM outputs exhibit internal logical contradictions, often observed in reasoning tasks. This manifests as inconsistency both among the reasoning steps themselves and between the steps and the final answer. For example, as shown in Table 1, while the reasoning step of dividing both sides of the equation by 2 is correct, the final answer of x=4 is inconsistent with the reasoning chain, leading to an incorrect result. 3 HALLUCINATION CAUSES LLM hallucinations have multifaceted origins, spanning the entire spectrum of LLMs' capability acquisition process. In this section, we delve into the root causes of hallucinations in LLMs, primarily categorized into three key aspects: (1) Data (§3.1), (2) Training (§3.2), and (3) Inference (§3.3). 3.1 Hallucination from Data Data for training LLMs are comprised of two primary components: (1) pre-training data, through which LLMs acquire their general capabilities and factual knowledge [ 388], and (2) alignment data, which teach LLMs to follow user instructions",
      "chunk_index": 10
    },
    {
      "index": 117,
      "chunk_id": "Hallucination_Survey2023_chunk_11",
      "source_id": "Hallucination_Survey2023",
      "text": "3.1 Hallucination from Data Data for training LLMs are comprised of two primary components: (1) pre-training data, through which LLMs acquire their general capabilities and factual knowledge [ 388], and (2) alignment data, which teach LLMs to follow user instructions and align with human preferences [ 322]. Although these data constantly expand the capability boundaries of LLMs, they inadvertently become the principal contributors to LLM hallucinations. This primarily manifests in three aspects: the presence of misinformation and biases in the flawed pre-training data sources (§3.1.1), the knowledge boundary inherently bounded by the scope of the pre-training data (§3.1.2), and the hallucinations induced by inferior alignment data (§3.1.3). 3.1.1 Misinformation and biases. Neural networks possess an intrinsic tendency to memorize training data [35], and this memorization tendency grows with model size [34, 54]. In general, the inherent memorization capability is a double-edged sword in the fight against hallucinations. On the one hand, the capacities of LLMs to memorize suggests their potential to capture profound ACM Transactions on Information Systems, Vol. 1, No. 1, Article 1. Publication date: January 2024. 1:8 Huang, et al. Table 2. Examples of Hallucinations from Misinformation and Biases. The table categorizes hallucinations arising from flawed data sources into imitative falsehoods, duplication bias, and social biases. Each category is accompanied by a premise outlining the data issue, user input, and the LLM's hallucinatory output, and an explanation for the occurrence, aiding comprehension of these complex phenomena. Type Premise Input Model Ouput Explanation Imitative Falsehood Within the LLM's training data, a fre- quently referenced but incorrect fact appears multiple times: Thomas Edison invented the light bulb. Who is credited with the invention of the light bulb? Thomas Edison is cred- ited with the invention of the light bulb. While Edison im- proved the light bulb's design, he was not its sole inventor. The model's answer reflects a misconception prevalent in its training data, exemplifying an imitative falsehood. Societal Biases In the model's train- ing data, certain names may frequently be associated with specific nationalities. Provide a summary of the following news: Dr. Kim re- cently published a break- through paper on quantum computing. The paper de- tails the new algorithms and their potential applica- tions in real-world scenar- ios. It has been hailed as revolutionary by experts. Dr. Kim from South Ko- rea recently published a paper on quantum computing, detailing new algorithms and their applications. The model erro-",
      "chunk_index": 11
    },
    {
      "index": 118,
      "chunk_id": "Hallucination_Survey2023_chunk_12",
      "source_id": "Hallucination_Survey2023",
      "text": "algorithms and their potential applica- tions in real-world scenar- ios. It has been hailed as revolutionary by experts. Dr. Kim from South Ko- rea recently published a paper on quantum computing, detailing new algorithms and their applications. The model erro- neously added \"South Korea\" based on the name \"Kim\" revealing its bias in linking cer- tain names to specific nationalities, which can cause hallucinations in the summary. world knowledge. On the other hand, it becomes problematic in the context of misinformation and biases present within pre-training data and may inadvertently be amplified, manifesting asimitative falsehood [182] and the reinforcement of societal biases. For a more comprehensive understanding, detailed examples are presented in Table 2. Imitative Falsehood. Misinformation such as fake news and unfounded rumors has been widely spread among social media platforms and gradually serves as a significant contributor to LLM hallucinations. The increasing demand for large-scale corpora for pre-training necessitates the employment of heuristic data collection methods. While facilitating the acquisition of extensive data, challenges arise in maintaining consistent data quality, which inevitably introduces such misinformation in pre-training data [20, 328]. In this situation, LLMs will increase the likelihood of generating such false statements due to their remarkable memorization capabilities, leading to imitative falsehoods. The issue is further exacerbated by the fact that LLMs have drastically lowered the barriers to content creation, posing considerable risks to the trustworthiness of public discourse and internet ecosystems. Societal Biases. In addition to misinformation, biases are also deeply rooted in social media platforms, showing up variously, like biased hiring, prejudiced news, and hate-spewing. As a purely negative phenomenon, biases and hallucinations have obvious differences, mainly manifested as stereotypes spreading and social inequalities. However, certain biases are intrinsically tied to hallucinations, especially those related to gender [ 242] and nationality [155, 226]. For instance, LLMs might associate the profession of nursing with females, even when gender isn't explicitly mentioned in the user-provided context, exemplifying context inconsistency as discussed in Section (§2.3). Such biases can be inadvertently acquired from internet-based texts, which are rife with diverse and biased viewpoints, and subsequently be propagated into the generated content [155]. ACM Transactions on Information Systems, Vol. 1, No. 1, Article 1. Publication date: January 2024. A Survey on Hallucination in Large Language Models: Principles, Taxonomy, Challenges, and Open Questions 1:9 Table 3. Example of Knowledge Boundary. Content marked in Red represents the hallucinatory output. Type Input Model",
      "chunk_index": 12
    },
    {
      "index": 119,
      "chunk_id": "Hallucination_Survey2023_chunk_13",
      "source_id": "Hallucination_Survey2023",
      "text": "No. 1, Article 1. Publication date: January 2024. A Survey on Hallucination in Large Language Models: Principles, Taxonomy, Challenges, and Open Questions 1:9 Table 3. Example of Knowledge Boundary. Content marked in Red represents the hallucinatory output. Type Input Model Ouput Explanation Long-tail Knowledge What causes phenylke- tonuria? Phenylketonuria is caused by a mutation in the hemoglobin gene, which results in an unusual interaction with hemoglobin, affecting its ability to transport oxygen in the bloodstream. The correct cause of phenylke- tonuria is mutations in the PAH gene. Such specialized medical information is less frequently encountered in the training corpus, making LLMs struggle to memorize. Up-to-date Knowledge Who will be the Demo- cratic Party candidate for the 2024 US presi- dential election? Michelle Obama has an- nounced her candidacy and is leading in the polls, with significant endorsements from key Democratic figures such as Barack Obama and Bernie Sanders. She has also outlined a policy platform focusing on healthcare reform and climate change initiatives. After President Joe Biden an- nounced his withdrawal from the race on July 21, 2024, he endorsed Kamala Harris as the Democratic Party candidate. Such hallucina- tions arise because LLMs' para- metric knowledge cannot be updated in real-time, leading to potential fabrication regard- ing current events. Copyright-sensitive Knowledge Describe the ending of the novel \"The Secrets of the Alchemist\" that was published last year. In the dramatic conclusion of \"The Secrets of the Alchemist\", the protagonist discovers the true secret behind the philoso- pher's stone, leading to a peace- ful resolution with all charac- ters sharing in the wisdom. The novel \"The Secrets of the Alchemist\" is under copyright protection, and LLMs have not been trained directly on such copyrighted materials. Thus, the model's output fabricates de- tails about the book's ending. 3.1.2 Knowledge Boundary. While the vast pre-training corpora have empowered LLMs with extensive factual knowledge, they inherently possess knowledge boundaries. These boundaries arise primarily from two sources: (1) the inability of LLMs to memorize all factual knowledge encountered during pre-training, especially the less frequent long-tail knowledge; and (2) the intrinsic boundary of the pre-training data itself, which does not include rapidly evolving world knowledge or content restricted by copyright laws. Consequently, when LLMs encounter information that falls outside their limited knowledge boundaries, they are more susceptible to generating hallucinations. We present detailed examples for clear illustration in Table 3. Long-tail Knowledge. The distribution of knowledge",
      "chunk_index": 13
    },
    {
      "index": 120,
      "chunk_id": "Hallucination_Survey2023_chunk_14",
      "source_id": "Hallucination_Survey2023",
      "text": "content restricted by copyright laws. Consequently, when LLMs encounter information that falls outside their limited knowledge boundaries, they are more susceptible to generating hallucinations. We present detailed examples for clear illustration in Table 3. Long-tail Knowledge. The distribution of knowledge within the pre-training corpora is in- herently non-uniform, which results in LLMs demonstrating varying levels of proficiency across different types of knowledge. Recent studies have highlighted a strong correlation between the model's accuracy on general domain questions and the volume of relevant documents [ 145] or entity popularity [204] within the pre-training corpora. Furthermore, given that LLMs are pre- dominantly trained on extensive general domain corpora [93, 243, 254], they may exhibit deficits in domain-specific knowledge. This limitation becomes particularly evident when LLMs are con- fronted with tasks that require domain-specific expertise, such as medical [ 179, 279] and legal [149, 353] questions, these models may exhibit pronounced hallucinations, often manifesting as factual fabrication. Up-to-date Knowledge. Beyond the shortfall in long-tail knowledge, another intrinsic limitation concerning the knowledge boundaries within LLMs is their constrained capacity for up-to-date knowledge. The factual knowledge embedded within LLMs exhibits clear temporal boundaries ACM Transactions on Information Systems, Vol. 1, No. 1, Article 1. Publication date: January 2024. 1:10 Huang, et al. and can become outdated over time [148, 166, 230]. Once these models are trained, their internal knowledge is never updated. This poses a challenge given the dynamic and ever-evolving nature of our world. When confronted with queries that transcend their temporal scope, LLMs often resort to fabricating facts or providing answers that might have been correct in the past but are now outdated. Copyright-sensitive Knowledge. Due to licensing restrictions [262], existing LLMs are legally constrained to training on corpora that are publicly licensed [63, 93] or otherwise available for use without infringing copyright laws [10, 115]. This limitation significantly impacts the breadth and diversity of knowledge that LLMs can legally acquire. A significant portion of valuable knowledge, encapsulated in copyrighted materials such as recent scientific research, proprietary data, and copyrighted literary works, remains inaccessible to LLMs. This exclusion creates a knowledge gap, leading to potential hallucinations when LLMs attempt to generate information in domains where their training data is inaccessible [215]. 3.1.3 Inferior Alignment Data. After the pre-training stage, LLMs have embedded substantial factual knowledge within their parameters, thereby establishing obvious knowledge boundaries. During the supervised fine-tuning (SFT) stage, LLMs are typically trained",
      "chunk_index": 14
    },
    {
      "index": 121,
      "chunk_id": "Hallucination_Survey2023_chunk_15",
      "source_id": "Hallucination_Survey2023",
      "text": "domains where their training data is inaccessible [215]. 3.1.3 Inferior Alignment Data. After the pre-training stage, LLMs have embedded substantial factual knowledge within their parameters, thereby establishing obvious knowledge boundaries. During the supervised fine-tuning (SFT) stage, LLMs are typically trained on instruction pairs labeled by human annotators, potentially introducing new factual knowledge that extends beyond the knowledge boundary established during pre-training. Gekhman et al. [98] analyzed the training dynamics of incorporating new factual knowledge during the SFT process and found that LLMs struggle to acquire such new knowledge effectively. Most importantly, they discovered a correlation between the acquisition of new knowledge through SFT and increased hallucinations, suggesting that introducing new factual knowledge encourages LLMs to hallucinate. Additionally, Li et al. [168] conducted extensive analysis on the effect of instructions in producing hallucinations. Findings indicated that task-specific instructions which primarily focus on task format learning, tend to yield a higher proportion of hallucinatory responses. Moreover, overly complex and diverse instructions also lead to increased hallucinations. 3.2 Hallucination from Training As detailed in Section 2.2, the distinct stages of training impart various capabilities to LLMs, with pre-training focusing on acquiring general-purpose representations and world knowledge, and alignment enables LLMs to better align with user instructions and preferences. While these stages are critical for equipping LLMs with remarkable capabilities, shortfalls in either stage can inadvertently pave the way for hallucinations. 3.2.1 Hallucination from Pre-training. Pre-training constitutes the foundational stage for LLMs, predominantly utilizing a transformer-based architecture following the paradigm established by GPT [29, 251, 252], and further developed by OPT[ 372], Falcon [ 243], and Llama-2 [ 300]. This stage employs a causal language modeling objective, where models learn to predict subsequent tokens solely based on preceding ones in a unidirectional, left-to-right manner. While facilitating efficient training, it inherently limits the ability to capture intricate contextual dependencies, potentially increasing risks for the emergence of hallucination [180]. Moreover, recent research has exposed that LLMs can occasionally exhibit unpredictable reasoning hallucinations spanning both long-range and short-range dependencies, which potentially arise from the limitations of soft attention [ 52, 111], where attention becomes diluted across positions as sequence length increases. Notably, the phenomenon of exposure bias [21, 256] has been a longstanding and serious contribution to hallucinations, resulting from the disparity between training and inference in the auto-regressive generative model. Such inconsistency can result in hallucinations [313], especially ACM Transactions on Information Systems, Vol. 1, No.",
      "chunk_index": 15
    },
    {
      "index": 122,
      "chunk_id": "Hallucination_Survey2023_chunk_16",
      "source_id": "Hallucination_Survey2023",
      "text": "[21, 256] has been a longstanding and serious contribution to hallucinations, resulting from the disparity between training and inference in the auto-regressive generative model. Such inconsistency can result in hallucinations [313], especially ACM Transactions on Information Systems, Vol. 1, No. 1, Article 1. Publication date: January 2024. A Survey on Hallucination in Large Language Models: Principles, Taxonomy, Challenges, and Open Questions 1:11 when an erroneous token generated by the model cascades errors throughout the subsequent sequence, akin to a snowball effect [368]. 3.2.2 Hallucination from Supervised Fine-tuning. LLMs have inherent capability boundaries es- tablished during pre-training. SFT seeks to utilize instruction data and corresponding responses to unlock these pre-acquired abilities. However, challenges arise when the demands of annotated instructions exceed the model's pre-defined capability boundaries. In such cases, LLMs are trained to fit responses beyond their actual knowledge boundaries. As discussed in §3.1.3, over-fitting on new factual knowledge encourages LLMs prone to fabricating content, amplifying the risk of hallucinations [98, 269]. Moreover, another significant reason lies in the models' inability to reject. Traditional SFT methods typically force models to complete each response, without allowing them to accurately express uncertainty [341, 362]. Consequently, when faced with queries that exceed their knowledge boundaries, these models are more likely to fabricate content rather than reject it. This misalignment of knowledge boundaries, coupled with the inability to express uncertainty, are critical factors that contribute to the occurrence of hallucinations during the SFT stage. 3.2.3 Hallucination from RLHF. Several studies [13, 31] have demonstrated that LLM's activations encapsulate an internal belief related to the truthfulness of its generated statements. Nevertheless, misalignment can occasionally arise between these internal beliefs and the generated outputs. Even when LLMs are refined with human feedback [ 233], they can sometimes produce outputs that diverge from their internal beliefs. Such behaviors, termed as sycophancy [ 64], underscore the model's inclination to appease human evaluators, often at the cost of truthfulness. Recent studies indicate that models trained via RLHF exhibit pronounced behaviors of pandering to user opinions. Such sycophantic behaviors are not restricted to ambiguous questions without definitive answers [245], like political stances, but can also arise when the model chooses a clearly incorrect answer, despite being aware of its inaccuracy [ 327]. Delving into this phenomenon, Sharma et al . [274] suggested that the root of sycophancy may lie in the training process of RLHF models. By further exploring the role",
      "chunk_index": 16
    },
    {
      "index": 123,
      "chunk_id": "Hallucination_Survey2023_chunk_17",
      "source_id": "Hallucination_Survey2023",
      "text": "clearly incorrect answer, despite being aware of its inaccuracy [ 327]. Delving into this phenomenon, Sharma et al . [274] suggested that the root of sycophancy may lie in the training process of RLHF models. By further exploring the role of human preferences in this behavior, the research indicates that the tendency for sycophancy is likely driven by both humans and preference models showing a bias towards sycophantic responses over truthful ones. 3.3 Hallucination from Inference Decoding plays an important role in manifesting the capabilities of LLMs after pretraining and alignment. However, certain shortcomings in decoding strategies can lead to LLM hallucinations. 3.3.1 Imperfect Decoding Strategies. LLMs have demonstrated a remarkable aptitude for generating highly creative and diverse content, a proficiency that is critically dependent on the pivotal role of randomness in their decoding strategies. Stochastic sampling [84, 118] is currently the prevailing decoding strategy employed by these LLMs. The rationale for incorporating randomness into decoding strategies stems from the realization that high likelihood sequences often result in surprisingly low-quality text, which is called likelihood trap [118, 209, 283, 363]. The diversity introduced by the randomness in decoding strategies comes at a cost, as it is positively correlated with an increased risk of hallucinations [59, 78]. An elevation in the sampling temperature results in a more uniform token probability distribution, increasing the likelihood of sampling tokens with lower frequencies from the tail of the distribution. Consequently, this heightened tendency to sample infrequently occurring tokens exacerbates the risk of hallucinations [5]. 3.3.2 Over-confidence. Prior studies in conditional text generation [45, 212] have highlighted the issue of over-confidence which stems from an excessive focus on the partially generated content, often prioritizing fluency at the expense of faithfully adhering to the source context. While LLMs, ACM Transactions on Information Systems, Vol. 1, No. 1, Article 1. Publication date: January 2024. 1:12 Huang, et al. primarily adopting the causal language model architecture, have gained widespread usage, the over-confidence phenomenon continues to persist. During the generation process, the prediction of the next word is conditioned on both the language model context and the partially generated text. However, as demonstrated in prior studies [ 19, 189, 307], language models often exhibit a localized focus within their attention mechanisms, giving priority to nearby words and resulting in a notable deficit in context attention [275]. Furthermore, this concern is further amplified in LLMs that exhibit a proclivity for",
      "chunk_index": 17
    },
    {
      "index": 124,
      "chunk_id": "Hallucination_Survey2023_chunk_18",
      "source_id": "Hallucination_Survey2023",
      "text": "307], language models often exhibit a localized focus within their attention mechanisms, giving priority to nearby words and resulting in a notable deficit in context attention [275]. Furthermore, this concern is further amplified in LLMs that exhibit a proclivity for generating lengthy and comprehensive responses. In such cases, there is even a heightened susceptibility to the risk of instruction forgetting [46, 193]. This insufficient attention can directly contribute to faithfulness hallucinations, wherein the model outputs content that deviates from the original context. 3.3.3 Softmax Bottleneck. The majority of language models utilize a softmax layer that operates on the final layer's representation within the language model, in conjunction with a word embedding, to compute the ultimate probability associated with word prediction. Nevertheless, the efficacy of Softmax-based language models is impeded by a recognized limitation known as the Softmax bottleneck [342], wherein the employment of softmax in tandem with distributed word embeddings constrains the expressivity of the output probability distributions given the context which prevents LMs from outputting the desired distribution. Additionally, Chang and McCallum [38] discovered that when the desired distribution within the output word embedding space exhibits multiple modes, language models face challenges in accurately prioritizing words from all the modes as the top next words, which also introduces the risk of hallucination. 3.3.4 Reasoning Failure. Beyond the challenges with long-tail knowledge, effective utilization of knowledge is inextricably linked with reasoning capabilities. For instance, in multi-hop question- answering scenarios, even if the LLM possesses the necessary knowledge, it may struggle to produce accurate results if multiple associations exist between questions, due to its limitations in reasoning [386]. Furthermore, Berglund et al. [22] unveiled a specific reasoning failure in LLMs termed the Reversal Curse. Specifically, while the model can correctly answer when the question is formulated as \"A is B\", it exhibits a failed logical deduction when asked the converse \"B is A\". This discrepancy in reasoning extends beyond simple deductions. 4 HALLUCINATION DETECTION AND BENCHMARKS The issue of hallucinations within LLMs has garnered considerable attention, raising concerns about the reliability of LLMs and their deployment in practical applications. As LLMs become increasingly adept at generating human-like text, accurately distinguishing between hallucinated versus factual content becomes increasingly vital. Moreover, effectively measuring the level of hallucination in LLM is crucial for improving their reliability. Thus, in this section, we delve into hallucination detection approaches (§4.1) and benchmarks for assessing LLM hallucinations (§4.2).",
      "chunk_index": 18
    },
    {
      "index": 125,
      "chunk_id": "Hallucination_Survey2023_chunk_19",
      "source_id": "Hallucination_Survey2023",
      "text": "hallucinated versus factual content becomes increasingly vital. Moreover, effectively measuring the level of hallucination in LLM is crucial for improving their reliability. Thus, in this section, we delve into hallucination detection approaches (§4.1) and benchmarks for assessing LLM hallucinations (§4.2). 4.1 Hallucination Detection Existing strategies for detecting hallucinations in LLMs can be categorized based on the type of hallucination: (1) factuality hallucination detection, which aims to identify factual inaccuracies in the model's outputs, and (2) faithfulness hallucination detection, which focuses on evaluating the faithfulness of model's outputs to the contextual information provided. 4.1.1 Factuality Hallucination Detection. Factuality hallucination detection involves assessing whether the output of LLMs aligns with real-world facts. Typical methods generally fall into two categories: fact-checking, which involves verifying the factuality of the generated response ACM Transactions on Information Systems, Vol. 1, No. 1, Article 1. Publication date: January 2024. A Survey on Hallucination in Large Language Models: Principles, Taxonomy, Challenges, and Open Questions 1:13 against trusted knowledge sources, and uncertainty estimation , which focuses on detecting factual inconsistency via internal uncertainty signals. Fact-checking. Given that the output of LLMs is typically comprehensive and consists of multiple factual statements, the fact-checking approach is generally divided into two primary steps: (1) fact extraction, which involves extracting independent factual statements within the model's outputs (2) fact verification, which aims at verifying the correctness of these factual statements against trusted knowledge sources. Depending on the type of knowledge sources employed for verification, fact-checking methodologies can be broadly categorized into two distinct parts:external retrieval and internal checking . •External retrieval: The most intuitive strategy for fact verification is external retrieval. Min et al. [216] developed FACTSCORE, a fine-grained factual metric tailored for evaluating long-form text generation. It first decomposes the generation content into atomic facts and subsequently computes the percentage supported by reliable knowledge sources. Expanding on this concept, Chern et al. [50]proposed a unified framework that equips LLMs with the capability to identify factual inaccuracies by utilizing a collection of external tools dedicated to evidence gathering. In addition to retrieving supporting evidence solely based on decomposited claims, Huo et al. [128] improved the retrieval process through query expansion. By combining the original question with the LLM-generated answer, they effectively addressed the issue of topic drift, ensuring that the retrieved evidence aligns with both the question and the LLM's response. •Internal checking : Given the extensive factual knowledge encoded in their parameters,",
      "chunk_index": 19
    },
    {
      "index": 126,
      "chunk_id": "Hallucination_Survey2023_chunk_20",
      "source_id": "Hallucination_Survey2023",
      "text": "original question with the LLM-generated answer, they effectively addressed the issue of topic drift, ensuring that the retrieved evidence aligns with both the question and the LLM's response. •Internal checking : Given the extensive factual knowledge encoded in their parameters, LLMs have been explored as factual knowledge sources for fact-checking. Dhuliawala et al. [74] introduced the Chain-of-Verification (CoVe), where an LLM first generates verification ques- tions for a draft response and subsequently leverages its parametric knowledge to assess the consistency of the answer against the original response, thereby detecting potential inconsistencies.Kadavath et al. [143] and Zhang et al. [375] calculates the probability 𝑝(𝑇𝑟𝑢𝑒 ) to assess the factuality of the response to a boolean question, relying exclusively on the model'sinternal knowledge. Additionally, Li et al. [168] observed that most atomic statements are interrelated, some may serve as contextual backgrounds for others, which potentially leads to incorrect judgments. Thus, they instruct the LLM to directly predict hallucination judgments considering all factual statements. However, as LLMs are not inherently reliable factual databases [385], solely relying on LLMs' parametric knowledge for fact-checking may result in inaccurate assessments. Uncertainty Estimation. While many approaches to hallucination detection rely on external knowledge sources for fact-checking, several methods have been devised to address this issue in zero-resource settings, thus eliminating the need for retrieval. The foundational premise behind these strategies is that the origin of LLM hallucinations is inherently tied to the model's uncertainty. Therefore, by estimating the uncertainty of the factual content generated by the model, it becomes feasible to detect hallucinations. The methodologies in uncertainty estimation can broadly be categorized into two approaches: based on LLM internal states and LLM behavior, as shown in Fig. 2. •LLM internal states : The internal states of LLMs can serve as informative indicators of their uncertainty, often manifested through metrics like token probability or entropy. Varshney et al. [306] determined the model's uncertainty towards key concepts quantified by considering the minimal token probability within those concepts. The underlying rationale is that a low probability serves as a strong indicator of the model's uncertainty, with less influence from higher probability tokens present in the concept. Similarly, Luo et al. [198] employed a self-evaluation-based approach for uncertainty estimation by grounding in the rationale that a language model's ability to adeptly reconstruct an original concept from its generated ACM Transactions on Information Systems, Vol. 1, No. 1, Article 1. Publication date:",
      "chunk_index": 20
    },
    {
      "index": 127,
      "chunk_id": "Hallucination_Survey2023_chunk_21",
      "source_id": "Hallucination_Survey2023",
      "text": "[198] employed a self-evaluation-based approach for uncertainty estimation by grounding in the rationale that a language model's ability to adeptly reconstruct an original concept from its generated ACM Transactions on Information Systems, Vol. 1, No. 1, Article 1. Publication date: January 2024. 1:14 Huang, et al. (1) Self-Consistency Mount Everest stands as the tallest peak in the world. As far as I know, the highest peak in the world is Mount Fuji in Japan. The highest peak is Mount Everest located in the Himalayas. Consistency The highest peak in the world is Mount Fuji. I stand corrected, you are right. I must correct you. Mount Fuji is the highest peak in Japan. The highest peak in the world is Mount Everest in the Himalayas range. (2) Multi-Debate (a) LLM Internal States Question: What is the highest peak in the world? Large Language Model The highest peak in the world is Mount Fuji. low uncertainty high uncertainty (b) LLM Behavior Fig. 2. Taxonomy of Uncertainty Estimation Methods in Factual Hallucination Detection, featuringa) LLM In- ternal Statesand b) LLM Behavior, with LLM Behavior encompassing two main categories: Self-Consistency and Multi-Debate. explanation is indicative of its proficiency with that concept. By initially prompting the model to generate an explanation for a given concept and then employing constrained decoding to have the model recreate the original concept based on its generated explanation, the probability score from the response sequence can serve as a familiarity score for the concept. Furthermore, Yao et al. [345] interpreted hallucination through the lens of adversarial attacks. Utilizing gradient-based token replacement, they devised prompts to induce hallucinations. Notably, they observed that the first token generated from a raw prompt typically exhibits low entropy, compared to those from adversarial attacks. Based on this observation, they proposed setting an entropy threshold to define such hallucination attacks. •LLM behavior : However, when systems are only accessible via API calls [ 100, 214, 231], access to the output's token-level probability distribution might be unavailable. Given this constraint, several studies have shifted their focus to probing a model's uncertainty, either through natural language prompts [143, 335] or by examining its behavioral manifestations. For instance, by sampling multiple responses from an LLM for the same prompt, Manakul et al. [205] detected hallucinations via evaluating the consistency among the factual statements. However, these methods predominantly rely on direct queries that explicitly solicit informa- tion or verification from",
      "chunk_index": 21
    },
    {
      "index": 128,
      "chunk_id": "Hallucination_Survey2023_chunk_22",
      "source_id": "Hallucination_Survey2023",
      "text": "sampling multiple responses from an LLM for the same prompt, Manakul et al. [205] detected hallucinations via evaluating the consistency among the factual statements. However, these methods predominantly rely on direct queries that explicitly solicit informa- tion or verification from the model. Agrawal et al. [3], inspired by investigative interviews, advocated for the use of indirect queries. Unlike direct ones, these indirect counterparts often pose open-ended questions to elicit specific information. By employing these indirect queries, consistency across multiple model generations can be better evaluated. Beyond assessing uncertainty from the self-consistency of a single LLM's multiple generations, one can embrace a multi-agent perspective by incorporating additional LLMs. Drawing inspiration from legal cross-examination practices, Cohen et al. [62] introduced the LMvLM approach. This strategy leverages an examiner LM to question an examinee LM, aiming to unveil inconsistencies of claims during multi-turn interaction. 4.1.2 Faithfulness Hallucination Detection. Ensuring the faithfulness of LLMs to provide context or user instructions is pivotal for their practical utility in IR applications, from conversational search to interactive dialogue systems. We categorize existing hallucination detection metrics tailored to faithfulness into the following groups, with an overview shown in Fig. 3: (1) Fact-based (2) Classifier-based (3) QA-based (4) Uncertainty-based (5) LLM-based. Fact-based Metrics. In the realm of assessing faithfulness, one of the most intuitive methods involves measuring the overlap of pivotal facts between the generated content and the source content. Given the diverse manifestations of facts, faithfulness can be measured based on n-gram, ACM Transactions on Information Systems, Vol. 1, No. 1, Article 1. Publication date: January 2024. A Survey on Hallucination in Large Language Models: Principles, Taxonomy, Challenges, and Open Questions 1:15 LLM Generation User Query Information Extraction Information Extraction Facts Fact Overlap Question Answering Answer Overlap Question Generation Answers Answer Selection Answers Questions binary judgement k-point Likert scale LLM Generation User Query Entailment NLI Model LLM Generation User Query User Query LLM Generation (a) Fact-based Metric (b) Classiﬁer-based Metric (e) Prompting-based Metric(c) QA-based Metric (d) Uncertainty Estimation Facts User Query EOS Large Language Model high uncertainty low uncertainty Fig. 3. The illustration of detection methods for faithfulness hallucinations: a) Fact-based Metrics, which assesses faithfulness by measuring the overlap of facts between the generated content and the source content; b) Classifier-based Metrics , utilizing trained classifiers to distinguish the level of entailment between the generated content and the source content;c) QA-based Metrics, employing question-answering systems to validate the",
      "chunk_index": 22
    },
    {
      "index": 129,
      "chunk_id": "Hallucination_Survey2023_chunk_23",
      "source_id": "Hallucination_Survey2023",
      "text": "overlap of facts between the generated content and the source content; b) Classifier-based Metrics , utilizing trained classifiers to distinguish the level of entailment between the generated content and the source content;c) QA-based Metrics, employing question-answering systems to validate the consistency of information between the source content and the generated content; d) Uncertainty Estimation, which assesses faithfulness by measuring the model's confidence in its generated outputs; e) Prompting-based Metrics, wherein LLMs are induced to serve as evaluators, assessing the faithfulness of generated content through specific prompting strategies. entities, and relation triples. Traditional n-gram-based metrics, such as BLEU [239], ROUGE [181] and PARENT-T [324], typically fall short in differentiating the nuanced discrepancies between the generated content and the source content [208]. Entity-based metrics [225] make a step further by calculating the overlap of entities, as any omission or inaccurate generation of these key entities could lead to an unfaithful response. Notably, even if entities match, the relations between them might be erroneous. Thus, relation-based metrics [99] focus on the overlap of relation tuples and introduce a metric that computes the overlap of relation tuples extracted using trained end-to-end fact extraction models. Classifier-based Metrics. Beyond computing fact overlap, another straightforward approach to assessing the faithfulness of the model generation involves utilizing classifiers trained on data from related tasks such as natural language inference (NLI) and fact-checking, or data comprised of synthetically task-specific hallucinated and faithful content. A foundational principle for assessing the faithfulness of generated text is anchored on the idea that genuinely faithful content should inherently be entailed by its source content. In line with this, numerous studies [ 82, 208] have trained classifiers on NLI datasets to identify factual inaccuracies, especially in the context of abstract summarization. However, Mishra et al . [217] highlighted that the mismatch in input granularity between conventional NLI datasets and inconsistency detection datasets limits their applicability for effectively detecting inconsistencies. Building on this, more advanced studies have proposed methods such as fine-tuning on adversarial datasets [17], decomposing the entailment decisions at the dependency arc level [101], and segmenting documents into sentence units then ACM Transactions on Information Systems, Vol. 1, No. 1, Article 1. Publication date: January 2024. 1:16 Huang, et al. aggregating scores between sentence pairs [ 154]. While using data from related tasks to fine- tune the classifier has shown promise in evaluating faithfulness, it's essential to recognize the inherent gap between related",
      "chunk_index": 23
    },
    {
      "index": 130,
      "chunk_id": "Hallucination_Survey2023_chunk_24",
      "source_id": "Hallucination_Survey2023",
      "text": "date: January 2024. 1:16 Huang, et al. aggregating scores between sentence pairs [ 154]. While using data from related tasks to fine- tune the classifier has shown promise in evaluating faithfulness, it's essential to recognize the inherent gap between related tasks and the downstream task. The scarcity of annotated data further constrains their applicability. In response to this challenge, a surge of research explores leveraging data-augmentation methods to construct synthetical data for fine-tuning the classifier, either by rule-based perturbation [79, 152, 266] or generation [389]. QA-based Metrics. In contrast to classifier-based metrics, QA-based metrics [77, 119, 271, 310] have recently garnered attention for their enhanced ability to capture information overlap between the model's generation and its source. These metrics operate by initially selecting target answers from the information units within the LLM's output, and then questions are generated by the question-generation module. The questions are subsequently used to generate source answers based on the user context. Finally, the faithfulness of the LLM's responses is calculated by comparing the matching scores between the source and target answers. Although these methodologies share a common thematic approach, they exhibit variability in aspects like answer selection, question generation, and answer overlap, leading to diverse performance outcomes. Building on this founda- tional work, Fabbri et al. [80] conducted an in-depth evaluation of the components within QA-based metrics, yielding further enhancements in faithfulness evaluation. Uncertainty-based Metrics. Drawing parallels with the uncertainty-based approaches em- ployed for detecting factuality hallucinations (§4.1.1), the application of uncertainty estimation in assessing faithfulness has been widely explored, typically characterized by entropy and log- probability. For entropy-based uncertainty, Xiao and Wang[333] has revealed a positive correlation between hallucination likelihood in data-to-text generation and predictive uncertainty, which is estimated by deep ensembles [156]. In a related vein, Guerreiro et al. [106] leveraged the variance in hypotheses yielded by Monte Carlo Dropout [ 92] as an uncertainty measure within neural machine translation. More recently, van der Poel et al. [305] employed conditional entropy [337] to assess model uncertainty in abstractive summarization. Regarding log-probability, it can be applied at different levels of granularity, such as word or sentence level. Notably, several studies [91, 106, 359] have adopted length-normalized sequence log-probability to measure model confi- dence. Furthermore, considering the hallucinated token can be assigned high probability when the preceding context contains the same hallucinated information, Zhang et al. [374] focused on the most informative and important keywords",
      "chunk_index": 24
    },
    {
      "index": 131,
      "chunk_id": "Hallucination_Survey2023_chunk_25",
      "source_id": "Hallucination_Survey2023",
      "text": "adopted length-normalized sequence log-probability to measure model confi- dence. Furthermore, considering the hallucinated token can be assigned high probability when the preceding context contains the same hallucinated information, Zhang et al. [374] focused on the most informative and important keywords and introduced a penalty mechanism to counteract the propagation of hallucinated content. LLM-based Judgement. Recently, the remarkable instruction-following ability of LLMs has underscored their potential for automatic evaluation [ 51, 190, 314]. Exploiting this capability, researchers have ventured into novel paradigms for assessing the faithfulness of model-generated content [2, 95, 133, 153, 199]. By providing LLMs with concrete evaluation guidelines and feeding them both the model-generated and source content, they can effectively assess faithfulness. The final evaluation output can either be a binary judgment on faithfulness [199] or a k-point Likert scale indicating the degree of faithfulness [95]. For prompt selection, evaluation prompt can either be direct prompting, chain-of-thought prompting [2], using in-context-learning [133] or allowing the model to generate evaluation results accompanying with explanations [153]. 4.2 Hallucination Benchmarks In this section, we present a comprehensive overview of existing hallucination benchmarks, which can be categorized into two primary domains: Hallucination Evaluation Benchmarks (§4.2.1), which assess the extent of hallucinations generated by existing cutting-edge LLMs, and Hallucination ACM Transactions on Information Systems, Vol. 1, No. 1, Article 1. Publication date: January 2024. A Survey on Hallucination in Large Language Models: Principles, Taxonomy, Challenges, and Open Questions 1:17 Detection Benchmarks (§4.2.2), designed specifically to evaluate the performance of existing hallu- cination detection methods. Collectively, these benchmarks establish a unified framework, enabling a nuanced and thorough exploration of hallucinatory patterns in LLMs. Table 4. An overview of existing hallucination benchmarks. For Attribute, Factuality and Faithfulness represent whether the benchmark is used to evaluate LLM's factuality or to detect faithfulness hallucination, and Manual represents whether the inputs in the data are handwritten. Attribute Task Benchmark Datasets Data Size Language Factuality Faithfulness Manual Task Type Input Label Metric Generative QATruthfulQA [182] - 817 English ✔ ✗ ✔ Multi-Choice QAQuestion Answer LLM-Judge & Human Multi-Choice QA AccREALTIMEQA [148] - Dynamic English ✔ ✗ ✔ Generative QA Question Answer EM & F1 SelfCheckGPT-Wikibio [213] - 1,908 English ✗ ✔ ✗ Detection Paragraph & Concept Passage AUROC Task-specific 30,000 English ✗ ✔ ✗ Detection Query Response AccHaluEval [169] General 5,000 English ✗ ✔ ✗ Detection Task Input Response Acc Med-HALT [303] - 4,916 Multilingual ✔ ✗ ✗ Multi-Choice",
      "chunk_index": 25
    },
    {
      "index": 132,
      "chunk_id": "Hallucination_Survey2023_chunk_26",
      "source_id": "Hallucination_Survey2023",
      "text": "✗ ✔ ✗ Detection Paragraph & Concept Passage AUROC Task-specific 30,000 English ✗ ✔ ✗ Detection Query Response AccHaluEval [169] General 5,000 English ✗ ✔ ✗ Detection Task Input Response Acc Med-HALT [303] - 4,916 Multilingual ✔ ✗ ✗ Multi-Choice QA Question Choice Pointwise Score & Acc Wiki-FACTOR 2,994 English ✔ ✗ ✗ Multi-Choice QA Question Answer likelihoodFACTOR [223] News-FACTOR 1,036 English ✔ ✗ ✗ Multi-Choice QA Question Answer likelihood SenHallu 200 English ✗ ✔ ✗ Detection Paper Summary P & R & F1BAMBOO [76] AbsHallu 200 English ✗ ✔ ✗ Detection Paper Summary P & R & F1 ChineseFactEval [311] - 125 Chinese ✔ ✗ ✔ Generative QA Question - Score Misleading 175 Chinese ✔ ✗ ✔ Generative QA Question Answer LLM-Judge Misleading-hard 69 Chinese ✔ ✗ ✔ Generative QA Question Answer LLM-JudgeHaluQA [49] Knowledge 206 Chinese ✔ ✗ ✔ Generative QA Question Answer LLM-Judge Never-changing 150 English ✔ ✗ ✔ Generative QA Question Answer Human Slow-changing 150 English ✔ ✗ ✔ Generative QA Question Answer Human Fast-changing 150 English ✔ ✗ ✔ Generative QA Question Answer Human FreshQA [308] False-premise 150 English ✔ ✗ ✔ Generative QA Question Answer Human FELM [42] - 3,948 English ✔ ✔ ✗ Detection Question Response Balanced Acc & F1 PHD-LOW 100 English ✗ ✔ ✗ Detection Entity Response P & R & F1 PHD-Meidum 100 English ✗ ✔ ✗ Detection Entity Response P & R & F1PHD [340] PHD-High 100 English ✗ ✔ ✗ Detection Entity Response P & R & F1 ScreenEval [158] - 52 English ✗ ✔ ✗ Detection Document Summary AUROC COVID-QA N/A English ✗ ✔ ✗ Detection Question Answer AUROC DROP N/A English ✗ ✔ ✗ Detection Question Answer AUROC Open Assistant N/A English ✗ ✔ ✗ Detection Question Answer AUROCRealHall [90] TriviaQA N/A English ✗ ✔ ✗ Detection Question Answer AUROC LSum [85] - 6,166 English ✗ ✔ ✗ Detection Document Summary Balanced Acc HotpotQA 250 English ✗ ✔ ✗ Detection Question Answer AUROCSAC3 [364] NQ-Open 250 English ✗ ✔ ✗ Detection Question Answer AUROC Biomedicine 1,535 English ✔ ✗ ✗ Generative QA Question Answer MiHR & MaHR Finance 1,125 English ✔ ✗ ✗ Generative QA Question Answer MiHR & MaHR Science 1,409 English ✔ ✗ ✗ Generative QA Question Answer MiHR & MaHR Education 1,701 English ✔ ✗ ✗ Generative QA Question Answer MiHR & MaHR HaluEval 2.0 [168] Open domain 3,000 English ✔ ✗ ✗",
      "chunk_index": 26
    },
    {
      "index": 133,
      "chunk_id": "Hallucination_Survey2023_chunk_27",
      "source_id": "Hallucination_Survey2023",
      "text": "Answer MiHR & MaHR Science 1,409 English ✔ ✗ ✗ Generative QA Question Answer MiHR & MaHR Education 1,701 English ✔ ✗ ✗ Generative QA Question Answer MiHR & MaHR HaluEval 2.0 [168] Open domain 3,000 English ✔ ✗ ✗ Generative QA Question Answer MiHR & MaHR 4.2.1 Hallucination Evaluation Benchmarks. Hallucination evaluation benchmarks are devised to quantify the tendency of LLMs to generate hallucinations, particularly emphasizing factual inaccuracies and inconsistency from the given contexts. Given the adeptness of LLMs at memorizing high-frequency count knowledge, the primary focus of current hallucination evaluation benchmarks targets long-tailed knowledge and challenging questions that can easily elicit imitative falsehood. As for evaluating, these benchmarks typically utilize multiple choice QA, where performance is measured through accuracy metrics, or generative QA, evaluated either through human judgment or scores given by proxy models. ACM Transactions on Information Systems, Vol. 1, No. 1, Article 1. Publication date: January 2024. 1:18 Huang, et al. Long-tail Factual Knowledge. The selection criteria for gathering long-tail factual question- answering samples typically include the frequency of appearance, recency, and specific domains. Regarding the frequency of appearance, benchmarks such as PopQA [204] and Head-to-Tail [290] are constructed based on entity popularity derived directly from Wikipedia. Considering that world knowledge is constantly evolving, it becomes crucial to validate the LLM's factuality concerning the current world. Among benchmarks characterized by ever-changing, REALTIMEQA [148] and FreshQA [308] stands out. REALTIMEQA offers real-time, open-domain multiple-choice questions that are regularly updated to reflect the latest developments. These questions are derived from newly published news articles, encompassing a broad spectrum of topics, including politics, business, sports, and entertainment. Similarly, FreshQA challenges LLMs with questions designed to represent varying degrees of temporal change-categorized into never-changing, slow-changing, and fast- changing world knowledge. This benchmark is further enriched by including questions based on false premises, requiring debunking, thus comprising a total of 600 meticulously hand-crafted questions. Moreover, long-tail knowledge often pertains to specific domains. For instance, Med- HALT [303] is distinguished by its focus on the medical domain, challenging LLMs with multiple- choice questions derived from a variety of countries. Additionally, Malaviya et al. [203] collected expert-curated questions across 32 fields of study, resulting in a high-quality long-form QA dataset with 2,177 questions. Imitative Falsehood Knowledge. Imitative falsehood knowledge is specifically designed to challenge LLMs through adversarial prompting. This approach crafts questions in such a way that they are prone to misleading LLMs",
      "chunk_index": 27
    },
    {
      "index": 134,
      "chunk_id": "Hallucination_Survey2023_chunk_28",
      "source_id": "Hallucination_Survey2023",
      "text": "resulting in a high-quality long-form QA dataset with 2,177 questions. Imitative Falsehood Knowledge. Imitative falsehood knowledge is specifically designed to challenge LLMs through adversarial prompting. This approach crafts questions in such a way that they are prone to misleading LLMs due to false beliefs or misconceptions. The two most representative benchmarks are TruthfulQA [182] and HalluQA [49]. TruthfulQA comprises 817 questions that span 38 diverse categories, such as health, law, finance, and politics. Crafted using an adversarial methodology, it aims to elicit \"imitative falsehoods\"-misleading responses that models might generate due to their frequent presence in training data. The benchmark is divided into two parts, one of which contains manually curated questions that were further refined by filtering out those correctly answered by GPT-3, resulting in 437 filtered questions. The other part includes 380 unfiltered non-adversarial questions. Drawing from the construction approach of TruthfulQA, HalluQA is crafted to specifically assess hallucinations in Chinese LLMs, focusing on imitative falsehoods and factual errors. The benchmark comprises 450 handcrafted adversarial questions across 30 domains and is categorized into two parts. The misleading section captures questions that successfully deceive GLM-130B, while the knowledge section retains questions that both ChatGPT and Puyu consistently answer incorrectly. To comprehensively evaluate LLM hallucinations across various domains, Li et al . [168] constructed an upgraded hallucination evaluation benchmark, HaluEval 2.0, based on [ 169]. This benchmark includes 8,770 questions that LLMs are prone to hallucination, across five domains: biomedicine, finance, science, education, and open domain. 4.2.2 Hallucination Detection Benchmarks. For hallucination detection benchmarks, most prior studies have primarily concentrated on task-specific hallucinations, such as abstractive summariza- tion [81, 102, 152, 208, 236, 310], data-to-text[240, 296], and machine translation [389]. However, the content generated in these studies often originates from models with lesser capabilities, such as BART [164] and PEGASUS [366]. As a result, they may not accurately reflect the effectiveness of hallucination detection strategies, underlining the necessity for a significant shift toward developing benchmarks that encapsulate more complex scenarios reflective of the era of LLMs. For example, SelfCheckGPT-Wikibio [213] offers a sentence-level dataset created by generating synthetic Wikipedia articles with GPT-3, manually annotated for factuality, highlighting the chal- lenge of detecting hallucinations in the biography domain. Complementing this, HaluEval [169] combines automated generation with human annotation to evaluate LLMs' ability to recognize ACM Transactions on Information Systems, Vol. 1, No. 1, Article 1. Publication date: January 2024. A Survey on",
      "chunk_index": 28
    },
    {
      "index": 135,
      "chunk_id": "Hallucination_Survey2023_chunk_29",
      "source_id": "Hallucination_Survey2023",
      "text": "detecting hallucinations in the biography domain. Complementing this, HaluEval [169] combines automated generation with human annotation to evaluate LLMs' ability to recognize ACM Transactions on Information Systems, Vol. 1, No. 1, Article 1. Publication date: January 2024. A Survey on Hallucination in Large Language Models: Principles, Taxonomy, Challenges, and Open Questions 1:19 hallucinations across 5,000 general user queries and 30,000 task-specific samples, leveraging a \"sampling-then-filtering\" approach. Building upon existing research predominantly focused on short documents, BAMBOO [76] and ScreenEval [158] extend the scope in long-form hallucination detection. Further, FELM [42], distinguishes itself by assessing factuality across diverse domains including world knowledge, science, and mathematics, producing 817 samples annotated for various facets of factual accuracy, thereby addressing the need for cross-domain evaluation of factuality in LLM-generated content. On a different note, PHD [340], shifts the focus towards passage-level detection of non-factual content by analyzing entities from Wikipedia, thus offering a nuanced view on the knowledge depth of LLMs. RealHall [90] and SAC3 [364] align closely with real-world applications focusing on open-domain question-answering, whereas LSum [85] concentrating on summarization tasks. 5 HALLUCINATION MITIGATION In this section, we present a comprehensive review of contemporary methods aimed at mitigating hallucinations in LLMs. Drawing from insights discussed in Hallucination Causes (§3), we systemat- ically categorize these methods based on the underlying causes of hallucinations. Specifically, we focus on approaches addressing Data-related Hallucinations (§5.1), Training-related Hallucinations (§5.2) and Inference-related Hallucinations (§5.3), each offering tailored solutions to tackle specific challenges inherent to their respective cause. 5.1 Mitigating Data-related Hallucinations As analyzed in §3.1, data-related hallucinations generally emerge as a byproduct of misinformation, biases, and knowledge gaps, which are fundamentally rooted in the pre-training data. Several methods are proposed to mitigate such hallucinations, primarily categorized into three distinct parts: (1)data filtering aiming at selecting high-quality data to avoid introducing misinformation and biases, (2) model editing focusing on injecting up-to-date knowledge by editing model's parameters, and (3) retrieval-augmented generation leveraging external non-parametric database for knowledge supplying. 5.1.1 Data Filtering. To reduce the presence of misinformation and biases, an intuitive approach involves the careful selection of high-quality pre-training data from reliable sources. In this way, we can ensure the factual correctness of data while also minimizing the introduction of social biases. As early as the advent of GPT-2, Radford et al. [252]underscored the significance of exclusively scraping web pages that had undergone rigorous curation and filtration by human experts. However,",
      "chunk_index": 29
    },
    {
      "index": 136,
      "chunk_id": "Hallucination_Survey2023_chunk_30",
      "source_id": "Hallucination_Survey2023",
      "text": "correctness of data while also minimizing the introduction of social biases. As early as the advent of GPT-2, Radford et al. [252]underscored the significance of exclusively scraping web pages that had undergone rigorous curation and filtration by human experts. However, as pre-training datasets continue to scale, manual curation becomes a challenge. Given that academic or specialized domain data is typically factually accurate, gathering high-quality data emerges as a primary strategy. Notable examples includethe Pile [93] and \"textbook-like\" data sources [107, 177]. Additionally, up-sampling factual data during the pre-training phase has been proven effective in enhancing the factual correctness of LLMs [300], thus alleviating hallucination. In addition to strictly controlling the source of data, deduplication serves as a crucial procedure. Existing practices typically fall into two categories: exact duplicates and near-duplicates. For exact duplicates, the most straightforward method involves exact substring matching to identify identical strings. However, given the vastness of pre-training data, this process can be computationally intensive, a more efficient method utilizes the construction of a suffix array [206], enabling effective computation of numerous substring queries in linear time. Regarding near-duplicates, the identifi- cation often involves approximate full-text matching, typically utilizing hash-based techniques to identify document pairs with significant n-gram overlap. Furthermore, MinHash [28] stands out as a prevalent algorithm for large-scale deduplication tasks [110]. Additionally, SemDeDup [1] makes ACM Transactions on Information Systems, Vol. 1, No. 1, Article 1. Publication date: January 2024. 1:20 Huang, et al. use of embeddings from pre-trained models to identify semantic duplicates, which refers to data pairs with semantic similarities but not identical. Discussion. Since data filtering works directly at the source of hallucinations, it effectively mitigates hallucinations by ensuring the use of high-quality, factually accurate sources. Despite its effectiveness, the efficiency and scalability of current data filtering methods pose significant challenges as data volumes expand. Additionally, these methods often overlook the influence of LLM-generated content, which can introduce new risks and inaccuracies. To advance, future research must focus on developing more efficient, automated data filtering algorithms that can keep pace with the rapid expansion of datasets and the complexities of LLM-generated content. 5.1.2 Model Editing. Model editing [280, 320, 369] has garnered rising attention from researchers, which aims to rectify model behavior by incorporating additional knowledge. Current model editing techniques can be categorized into two classes: locate-then-edit and meta-learning. Locate-then-edit. Locate-then-edit methods [66, 210] consist of two stages, which first locate the",
      "chunk_index": 30
    },
    {
      "index": 137,
      "chunk_id": "Hallucination_Survey2023_chunk_31",
      "source_id": "Hallucination_Survey2023",
      "text": "rising attention from researchers, which aims to rectify model behavior by incorporating additional knowledge. Current model editing techniques can be categorized into two classes: locate-then-edit and meta-learning. Locate-then-edit. Locate-then-edit methods [66, 210] consist of two stages, which first locate the \"buggy\" part of the model parameters and then apply an update to them to alter the model's behavior. For example, ROME [210] located the edits-related layer by destroying and subsequently restoring the activations and then updates the parameters of FFN in a direct manner to edit knowledge. MEMIT [211] employed the same knowledge locating methods as ROME, enabling the concurrent updating of multiple layers to facilitate the simultaneous integration of thousands of editing knowledge. However, Yao et al. [347] found that these methods lack non-trivial generalization capabilities and varying performance and applicability to different model architectures. The best-performing methods ROME and MEMIT empirically only work well on decoder-only LLMs. Meta-learning. Meta-learning methods [70, 218] train an external hyper-network to predict the weight update of the original model. Nevertheless, meta-learning methods often require additional training and memory cost, where MEND [218] utilized a low-rank decomposition with a specialized design to reduce the size of hyper-networks. Notably, MEND would exhibit a cancellation effect, where parameter shifts corresponding to different keys significantly counteract each other. MAL- MEN [292] further addressed this issue by framing the parameter shift aggregation as a least squares problem rather than a simple summation, thereby greatly enhancing its capacity for extensive editing. While these methods can fine-grainedly adjust the behavior of the model, modifications to the parameters could have a potentially harmful impact on the inherent knowledge of the model. Discussion. Model editing provides a precise way to mitigate hallucinations induced by specific misinformation without extensive retraining. However, these methods struggle with large-scale updates and can adversely affect the model's overall performance, particularly when continuous edits are applied. Consequently, future research should focus on improving model editing to handle large-scale knowledge updates more efficiently and address hallucinations caused by social biases. 5.1.3 Retrieval-Augmented Generation. Typically, retrieval-augmented generation (RAG) [109, 165, 278] follows a retrieve-then-read pipeline, where relevant knowledge is firstly retrieved by aretriever [146] from external sources, and then the final response is generated by a generator conditioning on both user query and retrieved documents. By decoupling external knowledge from LLM, RAG can effectively alleviate the hallucination caused by the knowledge gap without affecting the performance of LLM. Common",
      "chunk_index": 31
    },
    {
      "index": 138,
      "chunk_id": "Hallucination_Survey2023_chunk_32",
      "source_id": "Hallucination_Survey2023",
      "text": "the final response is generated by a generator conditioning on both user query and retrieved documents. By decoupling external knowledge from LLM, RAG can effectively alleviate the hallucination caused by the knowledge gap without affecting the performance of LLM. Common practices can be divided into three parts, as shown in Fig 4:one-time retrieval, iterative retrieval, and post-hoc retrieval, depending on the timing of retrieval. One-time Retrieval. One-time retrieval aims to directly prepend the external knowledge obtained from a single retrieval to the LLMs' prompt. Ram et al. [255] introduced In-context RALM, which entails a straightforward yet effective strategy of prepending chosen documents to the input text of LLMs. Beyond conventional knowledge repositories such as Wikipedia, ongoing research endeavors have explored alternative avenues, specifically the utilization of knowledge ACM Transactions on Information Systems, Vol. 1, No. 1, Article 1. Publication date: January 2024. A Survey on Hallucination in Large Language Models: Principles, Taxonomy, Challenges, and Open Questions 1:21 Retrieve Query Generate Large Language Model Answer Query Answer Query Generate Large Language Model Rertieve Answer Revise Revisior (a) One-time Retrieval (b) Iterative Retrieval (c) Post-hoc Retrieval ... LLM Output LLM Output Iteration Iteration ... Revision Fig. 4. The illustration of three distinct approaches for Retrieval-Augmented Generation: a) One-time Retrieval, where relevant information is retrieved once before text generation; b) Iterative Retrieval , involving multiple retrieval iterations during text generation for dynamic information integration; and c) Post-hoc Retrieval, where the retrieval process happens after an answer is generated, aiming to refine and fact-check the generated content. graphs (KGs). These KGs serve as a pivotal tool for prompting LLMs, facilitating their interaction with the most recent knowledge, and eliciting robust reasoning pathways [14, 249, 329]. Varshney et al. [306] introduce the Parametric Knowledge Guiding (PKG) framework, enhancing LLMs with domain-specific knowledge. PKG employs a trainable background knowledge module, aligning it with task knowledge and generating relevant contextual information. Iterative Retrieval. When confronted with intricate challenges like multi-step reasoning [344] and long-form question answering [83, 284], traditional one-time retrieval may fall short. Address- ing these demanding information needs, recent studies have proposed iterative retrieval, which allows for continuously gathering knowledge throughout the generation process. Recognizing the substantial advancements chain-of-thought prompting [326] has brought to LLMs in multi-step reasoning, numerous studies [113, 301, 346] try to incorporate external knowledge at each reasoning step and further guide retrieval process based on ongoing reasoning, reducing factual errors in",
      "chunk_index": 32
    },
    {
      "index": 139,
      "chunk_id": "Hallucination_Survey2023_chunk_33",
      "source_id": "Hallucination_Survey2023",
      "text": "the substantial advancements chain-of-thought prompting [326] has brought to LLMs in multi-step reasoning, numerous studies [113, 301, 346] try to incorporate external knowledge at each reasoning step and further guide retrieval process based on ongoing reasoning, reducing factual errors in reasoning chains. Building upon chain-of-thought prompting, Press et al. [247] introduced self-ask. Diverging from the conventional continuous, undelineated chain-of-thought prompting, self-ask delineates the question it intends to address at each step, subsequently incorporating a search action based on the follow-up question. Instead of solely depending on chain-of-thought prompting for retrieval guidance, both Feng et al. [87] and Shao et al. [273] employed an iterative retrieval- generation collaborative framework, where a model's response serves as an insightful context to procure more relevant knowledge, subsequently refining the response in the succeeding iteration. Beyond multi-step reasoning tasks, Jiang et al. [140] shifted their emphasis to long-form generation. They proposed an active retrieval augmented generation framework, which iteratively treats the upcoming prediction as a query to retrieve relevant documents. If the prediction contains tokens of low confidence, the sentence undergoes regeneration. In addition to using iterative retrieval to improve intermediate generations, Zhang et al . [371] presented MixAlign, which iteratively refines user questions using model-based guidance and seeking clarifications from users, ultimately enhancing the alignment between questions and knowledge. Post-hoc Retrieval. Beyond the traditional retrieve-then-read paradigm, a line of work has delved into post-hoc retrieval, refining LLM outputs through subsequent retrieval-based revisions. ACM Transactions on Information Systems, Vol. 1, No. 1, Article 1. Publication date: January 2024. 1:22 Huang, et al. To enhance the trustworthiness and attribution of LLMs, Gao et al . [94] adopted the research- then-revise workflow, which initially research relevant evidence and subsequently revise the initial generation based on detected discrepancies with the evidence. Similarly, Zhao et al. [381] introduced the verify-and-edit framework to enhance the factual accuracy of reasoning chains by incorporating external knowledge. For reasoning chains that show lower-than-average consistency, the framework generates verifying questions and then refines the rationales based on retrieved knowledge, ensuring a more factual response. Yu et al. [358] enhanced the post-hoc retrieval method through diverse answer generation. Instead of generating just a single answer, they sample various potential answers, allowing for a more comprehensive retrieval feedback. Additionally, by employing an ensembling technique that considers the likelihood of the answer before and after retrieval, they further mitigate the risk of misleading retrieval feedback. Discussion. One",
      "chunk_index": 33
    },
    {
      "index": 140,
      "chunk_id": "Hallucination_Survey2023_chunk_34",
      "source_id": "Hallucination_Survey2023",
      "text": "they sample various potential answers, allowing for a more comprehensive retrieval feedback. Additionally, by employing an ensembling technique that considers the likelihood of the answer before and after retrieval, they further mitigate the risk of misleading retrieval feedback. Discussion. One crucial advantage of retrieval-augmented generation methodology is its ef- fectiveness in mitigating hallucinations caused by knowledge gaps, and their generality, which allows for application across any domain. This flexibility is further enhanced by the modularity of the approach, treating external knowledge bases like plug-ins that can be swapped or modified as needed. In terms of the drawbacks, it can be easily impacted by irrelevant retrievals, which may decrease the overall performance by introducing noise or incorrect information into the response generation process. Furthermore, the current paradigm exhibits shallow interactions between the retriever and generator components, leading to suboptimal knowledge utilization. Hence, future research should focus on developing a robust RAG system that minimizes the impact of irrelevant retrieval, as well as integrating adaptive learning components that can dynamically adjust retrieval strategies based on the context of the query and the performance of previous interactions. 5.2 Mitigating Training-related Hallucination Training-related hallucinations typically arise from the intrinsic limitations of the architecture and training strategies adopted by LLMs. In this context, we discuss various optimization methods ranging from training stages (§5.2.1) and alignment stages (SFT & RLHF) (§5.2.2), aiming to mitigate hallucinations within the training process. 5.2.1 Mitigating Pretraining-related Hallucination. One significant avenue of research in mitigat- ing pretraining-related hallucination centers on the limitations inherent in model architectures, especially unidirectional representation and attention glitches. In light of this, numerous studies have delved into designing novel model architectures specifically tailored to address these flaws. To address the limitations inherent in unidirectional representation, Li et al. [180] introduced BATGPT which employs a bidirectional autoregressive approach. This design allows the model to predict the next token based on all previously seen tokens, considering both past and future contexts, thus capturing dependencies in both directions. Building on this idea, Liu et al. [189] highlighted the potential of encoder-decoder models to make better use of their context windows, suggesting a promising direction for future LLMs architecture design. Besides, recognizing the limitations of soft attention within self-attention-based architecture, Liu et al. [183] proposed attention-sharpening regularizers. This plug-and-play approach specifies self-attention architectures using differentiable loss terms [365] to promote sparsity, leading to a significant reduction in reasoning hallucinations. In the",
      "chunk_index": 34
    },
    {
      "index": 141,
      "chunk_id": "Hallucination_Survey2023_chunk_35",
      "source_id": "Hallucination_Survey2023",
      "text": "recognizing the limitations of soft attention within self-attention-based architecture, Liu et al. [183] proposed attention-sharpening regularizers. This plug-and-play approach specifies self-attention architectures using differentiable loss terms [365] to promote sparsity, leading to a significant reduction in reasoning hallucinations. In the pre-training phase of LLMs, the choice of objective plays a pivotal role in determining the model's performance. However, conventional objectives can lead to fragmented representations and inconsistencies in model outputs. Recent advancements have sought to address these challenges by refining pre-training strategies, ensuring richer context comprehension, and circumventing biases. Addressing the inherent limitations in training LLMs, where unstructured factual knowledge at a document level often gets chunked due to GPU memory constraints and computational efficiency, ACM Transactions on Information Systems, Vol. 1, No. 1, Article 1. Publication date: January 2024. A Survey on Hallucination in Large Language Models: Principles, Taxonomy, Challenges, and Open Questions 1:23 leading to fragmented information and incorrect entity associations, Lee et al. [160] introduced a factuality-enhanced training method. By appending a TOPICPREFIX to each sentence in factual documents, the approach transforms them into standalone facts, significantly reducing factual errors and enhancing the model's comprehension of factual associations. Similarly, considering that randomly concatenating shorter documents during pre-training might introduce inconsistencies in model outputs, Shi et al. [276] proposed In-Context Pretraining, an innovative approach in which LLMs are trained on sequences of related documents. By altering the document order, this method aims to maximize similarity within the context windows. It explicitly encourages LLMs to reason across document boundaries, potentially bolstering the logical consistency between generations. Discussion. Strategies designed to mitigate pretraining-related hallucinations typically are fundamental, potentially yielding significant improvements. However, they typically involve modifi- cations to pre-training architectures and objectives, which are computationally intensive. Moreover, these integrations may lack broad applicability. Moving forward, the focus should be on developing adaptable and efficient strategies that can be universally applied without extensive system overhaul. 5.2.2 Mitigating Misalignment Hallucination. Hallucinations induced during alignment often stem from capability misalignment and belief misalignment. However, defining the knowledge boundary of LLMs proves challenging, making it difficult to bridge the gap between LLMs' inherent capabilities and the knowledge presented in human-annotated data. While limited research addresses capability misalignment, the focus mainly shifts toward belief misalignment. Hallucinations stemming from belief misalignment often manifest as sycophancy, a tendency of LLMs to seek human approval in undesirable ways. This sycophantic behavior can be attributed to the fact that",
      "chunk_index": 35
    },
    {
      "index": 142,
      "chunk_id": "Hallucination_Survey2023_chunk_36",
      "source_id": "Hallucination_Survey2023",
      "text": "addresses capability misalignment, the focus mainly shifts toward belief misalignment. Hallucinations stemming from belief misalignment often manifest as sycophancy, a tendency of LLMs to seek human approval in undesirable ways. This sycophantic behavior can be attributed to the fact that human preference judgments often favor sycophantic responses over more truthful ones [274], paving the way for reward hacking [268]. To address this, a straightforward strategy is to improve human preference judgments and, by extension, the preference model. Recent research [25, 268] has investigated the use of LLMs to assist human labelers in identifying overlooked flaws. Additionally, Sharma et al. [274] discovered that aggregating multiple human preferences enhances feedback quality, thereby reducing sycophancy. Besides, modifications to LLMs' internal activations have also shown the potential to alter model behavior. This can be achieved through methods like fine-tuning [327] or activation steering during inference [69, 117, 289]. Specifically, Wei et al. [327] proposed a synthetic-data intervention, fine- tuning language models using synthetic data where the claim's ground truth is independent of a user's opinion, aiming to reduce sycophantic tendencies. Another avenue of research [ 263, 264] has been to mitigate sycophancy through activation steering. This approach involves using pairs of sycophantic/non-sycophantic prompts to generate the sycophancy steering vector, derived from averaging the differences in intermediate activations. During inference, subtracting this vector can produce less sycophantic LLM outputs. Discussion. Mitigating hallucinations through post-training methods represents a direct and effective approach, bypassing the complexities associated with data sourcing and pre-training. However, a notable gap in current research is the limited attention given to capability misalign- ment within LLMs. Future research should prioritize understanding the knowledge boundaries in capability alignment to address hallucinations effectively. 5.3 Mitigating Inference-related Hallucination Decoding strategies in LLMs play a pivotal role in determining the factuality and faithfulness of the generated content. However, as analyzed in Section §3.3, imperfect decoding often results in outputs that might lack factuality or stray from the original context. In this subsection, we explore ACM Transactions on Information Systems, Vol. 1, No. 1, Article 1. Publication date: January 2024. 1:24 Huang, et al. two advanced strategies aimed at refining the decoding strategy to enhance both the factuality and faithfulness of the LLMs' outputs. 5.3.1 Factuality Enhanced Decoding. Factuality Enhanced Decoding aims to improve the reliability of outputs from LLMs by prioritizing the factuality of the information they generate. This line of methods focuses on aligning model outputs",
      "chunk_index": 36
    },
    {
      "index": 143,
      "chunk_id": "Hallucination_Survey2023_chunk_37",
      "source_id": "Hallucination_Survey2023",
      "text": "and faithfulness of the LLMs' outputs. 5.3.1 Factuality Enhanced Decoding. Factuality Enhanced Decoding aims to improve the reliability of outputs from LLMs by prioritizing the factuality of the information they generate. This line of methods focuses on aligning model outputs closely with established real-world facts, thereby minimizing the risk of disseminating false or misleading information. Factuality Decoding. Considering the randomness in the sampling process can introduce non-factual content into open-ended text generation, Lee et al. [160] introduced the factual-nucleus sampling algorithm that dynamically adjusts the nucleus probability𝑝 throughout sentence genera- tion. By dynamically adjusting the nucleus probability based on decay factors and lower boundaries and resetting the nucleus probability at the beginning of every new sentence, the decoding strategy strikes a balance between generating factual content and preserving output diversity. Moreover, some studies [31, 220] posit that the activation space of LLMs contains interpretable structures related to factuality. Building on this idea, Li et al. [172] introduced Inference-Time Intervention (ITI). This method first identifies a direction in the activation space associated with factually cor- rect statements and then adjusts activations along the truth-correlated direction during inference. By repeatedly applying such intervention, LLMs can be steered towards producing more factual responses. Similarly, Chuang et al. [59] delved into enhancing the factuality of LLM's decoding process from a perspective of factual knowledge storage. They exploit the hierarchical encoding of factual knowledge within transformer LLMs, noting that lower-level information is captured in earlier layers and semantic information in the later ones. Drawing inspiration from [175], they introduce DoLa, a strategy that dynamically selects and contrasts logits from different layers to refine decoding factuality. By placing emphasis on knowledge from higher layers and downplaying that from the lower layers, DoLa showcases its potential to make LLMs more factual, thus reducing hallucinations. Post-editing Decoding. Unlike methods that directly modify the probability distribution to prevent hallucinations during the initial decoding, post-editing decoding seeks to harness the self-correction capabilities of LLMs [237] to refine the originally generated content without relying on an external knowledge base. Dhuliawala et al. [74] introduced the Chain-of-Verification (COVE), which operates under the assumption that, when appropriately prompted, LLMs can self-correct their mistakes and provide more accurate facts. Starting with an initial draft, it first formulates verification questions and then systematically answers those questions in order to finally produce an improved revised response. Similarly, Ji et al. [137] focused on the medical domain",
      "chunk_index": 37
    },
    {
      "index": 144,
      "chunk_id": "Hallucination_Survey2023_chunk_38",
      "source_id": "Hallucination_Survey2023",
      "text": "and provide more accurate facts. Starting with an initial draft, it first formulates verification questions and then systematically answers those questions in order to finally produce an improved revised response. Similarly, Ji et al. [137] focused on the medical domain and introduced an iterative self-reflection process. This process leverages the inherent ability of LLMs to first generate factual knowledge and then refine the response until it aligns consistently with the provided background knowledge. Discussion. Factuality decoding methods, which typically assess the factuality at each decoding step, can offer substantial improvements. Furthermore, due to their plug-and-play nature, they allow for application without the need for computation-intensive training. Nevertheless, one of the primary limitations of these methods lies in balancing factual accuracy with maintaining the diversity and informativeness of the generated content, which can sometimes lead to compromises in either aspect. On the other hand, post-editing decoding strategies, despite their effectiveness, heavily rely on the self-correction capabilities of LLMs, which may be unreliable. Furthermore, applying self-reflection can be time-consuming, limiting their practicality for real-time applications. Hence, it is crucial to achieve an optimal balance between factuality and computational efficiency. 5.3.2 Faithfulness Enhanced Decoding. On the other hand, Faithfulness Enhanced Decoding priori- tizes alignment with the provided context and also emphasizes enhancing the consistency within ACM Transactions on Information Systems, Vol. 1, No. 1, Article 1. Publication date: January 2024. A Survey on Hallucination in Large Language Models: Principles, Taxonomy, Challenges, and Open Questions 1:25 the generated content. Thus, in this section, we summarize existing work into two categories, including Context Consistency and Logical Consistency . Context Consistency. In the era of LLMs, the issue of faithfulness hallucination typically lies in insufficient attention to the given context, which inspired numerous research to design inference- time strategies to enhance context consistency. Shi et al. [275] proposed context-aware decoding (CAD), which modifies the model's original output distribution in a contrastive formulation [175]. By amplifying the difference between output probabilities with and without context, CAD encour- ages the LLM to focus more on contextual information rather than over-rely on prior knowledge. However, due to the inherent trade-off between diversity and context attribution [103, 363], overem- phasizing contextual information can reduce diversity. To address this, Chang et al. [36] introduced a dynamic decoding algorithm to bolster faithfulness while preserving diversity. Specifically, the algorithm involves two parallel decoding steps, one with the context and one without. During the",
      "chunk_index": 38
    },
    {
      "index": 145,
      "chunk_id": "Hallucination_Survey2023_chunk_39",
      "source_id": "Hallucination_Survey2023",
      "text": "contextual information can reduce diversity. To address this, Chang et al. [36] introduced a dynamic decoding algorithm to bolster faithfulness while preserving diversity. Specifically, the algorithm involves two parallel decoding steps, one with the context and one without. During the decoding, the KL divergence between two token distributions serves as a guiding signal, indicating the relevance of the source context. This signal is utilized to dynamically adjust the sampling temperature to improve source attribution when the source is relevant. In a parallel line of work, Choi et al. [53] introduced knowledge-constrained decoding (KCD), which employed a token-level hallucination detection discriminator to identify contextual hallucinations and then guides the faithful generation process by reweighing the token distribution. In addition to modifying output distribution in place to enhance contextual attention, another line of work has explored a generic post-edit approach to enhance faithfulness. Gao et al. [94] adopted a research-and-revise workflow, where the research stage raises questions about various aspects of the model's initial response and gathers evidence for each query, while the revision stage detects and revises any disagreements between the model's response and the evidence. Similarly, Lei et al . [161] first detected contex- tual hallucinations at both the sentence and entity levels and then incorporated the judgments to refine the generated response. Moreover, several studies have explored methods to overcome the softmax bottleneck, which constrains the expression of diversity and faithful representations. These approaches include employing a mixture of Softmax, which uses multiple hidden states to compute softmax multiple times and merge the resulting distributions [ 343] and incorporating pointer networks, which enables LLMs to copy the context words [37], thereby reducing context hallucinations. Logical Consistency. Inspired by the human thinking process, chain-of-thought [326] has been introduced to encourage LLMs to decompose complex problems into explicitly intermediate steps, thereby enhancing the reliability of the reasoning process [58]. Despite effective, recent research [157, 302] demonstrated that the intermediate rationales generated by LLMs do not faithfully capture their underlying behavior. A branch of research has been inspired to improve the consistency of intermediate rationales generated by LLMs, particularly in multi-step reasoning [61] and logical reasoning [18]. To enhance the self-consistency in chain-of-thought, Wang et al. [318] employed a knowledge distillation framework. They first generate a consistent rationale using contrastive decoding [175] and then fine-tune the student model with a counterfactual reasoning objective, which effectively eliminates reasoning shortcuts [27] that derive answers",
      "chunk_index": 39
    },
    {
      "index": 146,
      "chunk_id": "Hallucination_Survey2023_chunk_40",
      "source_id": "Hallucination_Survey2023",
      "text": "chain-of-thought, Wang et al. [318] employed a knowledge distillation framework. They first generate a consistent rationale using contrastive decoding [175] and then fine-tune the student model with a counterfactual reasoning objective, which effectively eliminates reasoning shortcuts [27] that derive answers without considering the rationale. Furthermore, by employing contrastive decoding directly, LLMs can reduce surface-level copying and prevent missed reasoning steps [229]. In addition, Li et al . [167] conducted a deep analysis of the causal relevance among the context, CoT, and answer during unfaithful reasoning. Analysis revealed that the unfaithfulness issue lies in the inconsistencies in the context information obtained by the CoT and the answer. To address this, they proposed inferential bridging, which takes the attribution method to recall contextual information as hints to enhance CoT reasoning and filter out noisy CoTs that have low semantic consistency and attribution scores to the context. Paul et al. [241] decomposed the reasoning process into two modules: an inference module, which employs ACM Transactions on Information Systems, Vol. 1, No. 1, Article 1. Publication date: January 2024. 1:26 Huang, et al. Direct Preference Optimization [253] to align the LLM towards preferring correct reasoning chains over counterfactual chains, and a reasoning module, which encourages the LLM to reason faithfully over the reasoning steps using a counterfactual and causal preference objective. Compared to natural language reasoning, logical reasoning demands rigorous logical calculation, whereas plain text often lacks precise logical structure, leading to unfaithful reasoning. To address this, Xu et al. [338] introduced Symbolic CoT (SymbCoT), which incorporates symbolic expressions within CoT to describe intermediate reasoning steps. Specifically, SymbCoT translates the natural language context into a symbolic representation and then formulates a step-by-step plan to address the logical reasoning problem, followed by a verifier to check the translation and reasoning chain, thereby ensuring faithful logical reasoning. Discussion. Faithfulness Enhanced Decoding significantly advances the alignment of LLM outputs with provided contexts and enhances the internal consistency of the generated content. However, strategies such as context-aware decoding often lack adaptive mechanisms, limiting their effectiveness in scenarios that demand dynamic attention to context. Furthermore, many decoding strategies require the integration of additional models that do not focus on context, introducing significant computational overhead and reducing efficiency. 6 HALLUCINATIONS IN RETRIEVAL AUGMENTED GENERATION Retrieval Augmented Generation (RAG) has emerged as a promising strategy to mitigate hallucina- tions and improve the factuality of LLM outputs [131, 165, 255, 277]. By",
      "chunk_index": 40
    },
    {
      "index": 147,
      "chunk_id": "Hallucination_Survey2023_chunk_41",
      "source_id": "Hallucination_Survey2023",
      "text": "context, introducing significant computational overhead and reducing efficiency. 6 HALLUCINATIONS IN RETRIEVAL AUGMENTED GENERATION Retrieval Augmented Generation (RAG) has emerged as a promising strategy to mitigate hallucina- tions and improve the factuality of LLM outputs [131, 165, 255, 277]. By incorporating large-scale external knowledge bases during inference, RAG equips LLMs with up-to-date knowledge, thus reducing the potential risk of hallucination due to the inherent knowledge boundaries of LLMs [260]. Despite being designed to mitigate LLM hallucinations, retrieval-augmented LLMs can still produce hallucinations [16]. Hallucinations in RAG present considerable complexities, manifesting as outputs that are either factually inaccurate or misleading. These hallucinations occur when the content generated by the LLM does not align with real-world facts, fails to accurately reflect the user's query, or is not supported by the retrieved information. Such hallucinations can stem from two primary factors: retrieval failure (§6.1) and generation bottleneck (§6.2). Through a comprehensive analysis of the limitations present in current RAG systems, we aim to shed light on potential improvements for retrieval-augmented LLMs, paving the way for more reliable information retrieval systems. 6.1 Retrieval Failure The retrieval process is a crucial initial step in the RAG framework, tasked with retrieving the most relevant information for information-seeking queries. Consequently, failures in the retrieval stage can have serious downstream effects on the RAG pipeline, leading to hallucinations. These failures typically stem from three primary parts: the formulation of user queries, the reliability and scope of retrieval sources, and the effectiveness of the retriever. 6.1.1 User Queries. User queries play a fundamental role in guiding the retrieval process with RAG systems. The specificity and clarity of these queries critically influence the effectiveness of retrieval outcomes. In this section, we discuss factors that may contribute to hallucinations from three perspectives: blind retrieval, misinterpretation of ambiguous queries, and the challenges in accurate retrieval of complex queries. Some examples are presented in Table 5 for a better understanding. Retrieval Intent Decisions. Not all queries necessitate retrieval. Blind retrieval for queries that do not require external knowledge can counterproductively lead to misleading responses. As shown in Table 5, the query about\"the boiling point of water at sea level\" pertains to a basic scientific fact that the model could address without external retrieval. However, the retrieval system was ACM Transactions on Information Systems, Vol. 1, No. 1, Article 1. Publication date: January 2024. A Survey on Hallucination in Large Language Models: Principles, Taxonomy,",
      "chunk_index": 41
    },
    {
      "index": 148,
      "chunk_id": "Hallucination_Survey2023_chunk_42",
      "source_id": "Hallucination_Survey2023",
      "text": "scientific fact that the model could address without external retrieval. However, the retrieval system was ACM Transactions on Information Systems, Vol. 1, No. 1, Article 1. Publication date: January 2024. A Survey on Hallucination in Large Language Models: Principles, Taxonomy, Challenges, and Open Questions 1:27 Table 5. Examples of each category of User Queries that may lead to LLM hallucinations. Content marked in Red represents the hallucinatory output. Type Example Dialog Explanation RetrievalIntent Retrieved context:At high altitudes, water boils at lowertemperatures than at sea level, such as 95°C at elevationsabove 2,000 meters. This altitude-induced reduction can affectcooking times and methods. Query:What is the boiling point of water at sea level? Response:The boiling point of water at sea level is 95°C. This query involves a basic scientific fact that should beavailable within the model's parametric knowledge.However, the system inappropriately retrievesand usesaltitude-specific information about boiling temperatures,leading to an incorrect response for sea-level conditions. AmbiguousQueries Retrieved context:The Hennessey Venom GT famouslyset a speed record by completing a mile on a racetrack in32 seconds, highlighting its performance in motorsport events. Query:What's the record for the fastest mile run on track? Response:The fastest mile on a track was completed in32 seconds by the Hennessey Venom GT. The query is ambiguous because it does not specify whe-ther it refers to a human runner or a vehicle.The retrievalsystem erroneously pulls information related to car racingrecords instead of athletics. ComplexQueries Retrieved context:Alexander Fleming, known for his ground-breaking work in discovering penicillin, significantly influencedmodern antibiotics. Fleming's discovery saved countless lives. Query:Which university did the scientist who discoveredpenicillin graduate from? Response:Alexander Fleming graduated from Oxford University. The retrieval system only manages to fetch informationabout Fleming's professional achievements in the discoveryof penicillin. However, the document does not provide informa-tion about his educational background, thus the model generates ahallucinatory answer. inappropriately activated, blindly retrieving inaccurate information and consequently leading to an undesirable response. Consequently, several studies [75, 204, 228, 378] have proposed to make a shift from passive retrieval to adaptive retrieval. In general, these strategies can be divided into two categories: heuristic-based and self-aware judgment. Heuristic-based methods employ heuristic rules to determine the necessity of retrieval. For instance, Mallen et al. [204] observed a positive correlation between LLMs' memorization capabilities and entity popularity and suggested triggering retrieval only when the entity popularity in the user query falls below a certain threshold. Similarly, Jeong et al. [135] determined the timing of retrieval",
      "chunk_index": 42
    },
    {
      "index": 149,
      "chunk_id": "Hallucination_Survey2023_chunk_43",
      "source_id": "Hallucination_Survey2023",
      "text": "[204] observed a positive correlation between LLMs' memorization capabilities and entity popularity and suggested triggering retrieval only when the entity popularity in the user query falls below a certain threshold. Similarly, Jeong et al. [135] determined the timing of retrieval based on the query complexity, whereas Asai et al. [11] considered whether the query is factual relevant. Self-aware judgment leverages the models' intrinsic judgment to decide the necessity for information retrieval. Feng et al. [86], Ren et al. [261] and Wang et al . [321] directly prompted LLMs for retrieval decisions, recognizing that LLMs possess a certain level of awareness regarding their knowledge boundaries [143, 350]. Moreover, Jiang et al . [140] introduced an active retrieval strategy that triggers retrieval only when the LLM generates low-probability tokens. Similarly, Su et al. [288] not only considered the uncertainty of each token but also its semantic contribution and impact on the subsequent context. More recently, Cheng et al. [48] proposed four orthogonal criteria for determining the retrieval timing, which include intent-aware, knowledge-aware, time-sensitive-aware, and self-aware. Ambiguous Queries. Ambiguous user queries, containing omission, coreference, and ambiguity, significantly complicate the retrieval system's ability to fetch precisely relevant information, thereby increasing the likelihood of generating undesirable responses. As shown in Table 5, due to the ambiguity of the query about \"the record for the fastest mile run on track\" , the retrieval system erroneously retrieved information from automobile racing events, which led the model to generate a response suited for vehicles instead of athletes. A prevalent mitigation strategy is query rewriting, where queries are refined and decontextualized to better match relevant documents. Wang et al. [317] and Jagerman et al. [132] have explored prompting approaches where the LLM is prompted to generate a pseudo-document or rationale based on the original query, which is then used for further retrieval. Additionally, Ma et al. [200] introduced a trainable rewriter which is trained using the feedback from the LLM via reinforcement learning. Mao et al. [207] employed the feedback signals from the reranker to train the rewrite model, thus eliminating the reliance on annotated data. ACM Transactions on Information Systems, Vol. 1, No. 1, Article 1. Publication date: January 2024. 1:28 Huang, et al. However, the challenges deepen in conversational search, which encounters a more complex issue of context-dependent query understanding with the lengthy conversational history. Addressing this, Yoon et al. [351] proposed a similar framework for optimizing",
      "chunk_index": 43
    },
    {
      "index": 150,
      "chunk_id": "Hallucination_Survey2023_chunk_44",
      "source_id": "Hallucination_Survey2023",
      "text": "January 2024. 1:28 Huang, et al. However, the challenges deepen in conversational search, which encounters a more complex issue of context-dependent query understanding with the lengthy conversational history. Addressing this, Yoon et al. [351] proposed a similar framework for optimizing the LLM to generate retriever- preferred query rewrites. This operated by generating a variety of queries and then using the preference of the rank of retrieved passage to optimize the query rewriting model. Complex Queries. Complex user queries, characterized by requiring intensive reasoning [286] or encompassing multiple aspects [272, 319], pose significant challenges to the retrieval system. Such queries require advanced understanding and decomposition capabilities, which may exceed the current capabilities of the current retrieval methods based on keyword or semantic match- ing, often leading to partial or incorrect retrievals. For example, as shown in Table 5, due to the multi-step nature of the query about \"Which university did the scientist who discovered penicillin graduate from?\", direct retrieval often leads to incomplete results, thereby resulting in hallucinatory responses. A common approach involves query decomposition, where the complex query is decom- posed into sub-queries to facilitate more accurate information retrieval. For instance, Wang et al. [319] implemented a sub-aspect explorer that utilizes the extensive world knowledge embedded LLMs to identify potential sub-aspects of user queries, thereby providing explicit insights into the user's underlying intents. Similarly, Shao et al . [272] concentrated on the demanding task of expository writing, aiming at retrieving comprehensive information to compose Wikipedia- like articles from scratch on a specific topic. This approach involves decomposing the topic into various perspectives and simulating multi-turn conversations with LLMs, each personified with different perspectives for question asking. Additionally, Cao et al. [32] and Chu et al. [56] explored knowledge-intensive complex reasoning and employed a divide-and-conquer strategy. This strategy begins with decomposing complex questions into question trees, where at each node, the LLM retrieves and aggregates answers from diverse knowledge sources. 6.1.2 Retrieval Sources. The reliability and scope of retrieval sources are crucial determinants of the efficacy of RAG systems. Effective retrieval depends not only on the clarity of the user queries but also on the quality and comprehensiveness of the sources from which information is retrieved. When these sources contain factually incorrect or outdated information, the risk of retrieval failures increases significantly, potentially leading to the generation of incorrect or misleading information. As the landscape of content creation evolves with the",
      "chunk_index": 44
    },
    {
      "index": 151,
      "chunk_id": "Hallucination_Survey2023_chunk_45",
      "source_id": "Hallucination_Survey2023",
      "text": "from which information is retrieved. When these sources contain factually incorrect or outdated information, the risk of retrieval failures increases significantly, potentially leading to the generation of incorrect or misleading information. As the landscape of content creation evolves with the rapid advancement of Artificial Intelligence Generated Content (AIGC) [33], an increasing volume of LLM-generated content is permeating the internet, subsequently becoming integrated into retrieval sources [39]. This integration is reshaping the dynamics of information retrieval, as evidenced by recent empirical studies [68, 339] suggesting that modern retrieval models tend to favor LLM-generated content over human-authored content. Recent research [44] has explored the implications of progressively integrating LLM-generated content into RAG systems. The findings indicate that, without appropriate intervention, human- generated content may progressively lose its influence within RAG systems. Additionally, Tan et al. [293] investigated the performance of RAG systems when incorporating LLM-generated into retrieved contexts, revealing a significant bias favoring generated contexts. This bias stems from the high similarity between generated context and questions, as well as the semantic incompleteness of retrieved contexts. More seriously, the propensity of LLMs to produce factually inaccurate hallucinations exacerbates the reliability issues of retrieval sources. As LLM-generated content often contains factual errors, its integration into retrieval sources can mislead retrieval systems, further diminishing the accuracy and reliability of the information retrieved. To combat these biases, several approaches have been explored. Inspired by common practice in pre-training data processing [23], Asai et al. [12] proposed a scenario that incorporates a quality filter designed to ensure the high quality of the retrieval datastore. Additionally, Pan et al. [238] ACM Transactions on Information Systems, Vol. 1, No. 1, Article 1. Publication date: January 2024. A Survey on Hallucination in Large Language Models: Principles, Taxonomy, Challenges, and Open Questions 1:29 proposed Credibility-aware Generation (CAG), which equips LLMs with the ability to discern and handle information based on its credibility. This approach assigns different credibility levels to information, considering its relevance, temporal context, and the trustworthiness of its source, thus effectively reducing the impact of flawed information in RAG systems. 6.1.3 Retriever. When the user query is explicit and the retrieval source is reliable, the effectiveness of the retrieval process depends crucially on the performance of the retriever. In such scenarios, the retriever's effectiveness is significantly compromised by improper chunking and embedding practices. Chunking. Given the extensive nature of retrieval sources, which often encompass lengthy documents like web",
      "chunk_index": 45
    },
    {
      "index": 152,
      "chunk_id": "Hallucination_Survey2023_chunk_46",
      "source_id": "Hallucination_Survey2023",
      "text": "retrieval process depends crucially on the performance of the retriever. In such scenarios, the retriever's effectiveness is significantly compromised by improper chunking and embedding practices. Chunking. Given the extensive nature of retrieval sources, which often encompass lengthy documents like web pages, it poses significant challenges for LLMs with limited context length. Thus, chunking emerges as an indispensable step in RAG, which involves segmenting these voluminous documents into smaller, more manageable chunks to provide precise and relevant evidence for LLMs. According to actual needs, the chunking granularity ranges from documents to paragraphs, even sentences. However, inappropriate retrieval granularity can compromise the semantic integrity and affect the relevance of retrieved information [ 224], thereby affecting the performance of LLMs. Fixed-size chunking, which typically breaks down the documents into chunks of a specified length such as 100-word paragraphs, serves as the most crude and prevalent strategy of chunking, which is widely used in RAG systems [24, 109, 165]. Considering fixed-size chunking falls short in capture structure and dependency of lengthy documents, Sarthi et al. [267] proposed RAPTOR, an indexing and retrieval system. By recursively embedding, clustering, and summarizing chunks of text, RAPTOR constructs a tree to capture both high-level and low-level details. When retrieval, RAPTOR enables LLMs to integrate information from different levels of abstraction, providing a more comprehensive context for user queries. Instead of chunking text with a fixed chunk size, semantic chunking adaptively identifies breakpoints between sentences through embedding similarity, thereby preserving semantic continuity [144]. Furthermore, Chen et al. [43] pointed out the limitations of the existing retrieval granularity. On the one hand, while a coarser retrieval with a longer context can theoretically provide a more comprehensive context, it often includes extraneous details that could potentially distract LLMs. On the other hand, a fine-grain level can provide more precise and relevant information, it has limitations such as not being self-contained and lacking necessary contextual information. To address these shortcomings, Chen et al . [43] introduced a novel retrieval granularity, proposition, which is defined as atomic expressions within the text, each encapsulating a distinct factoid and presented in a concise self-contained natural language format. Embedding. Once the retrieval text is chunked, text chunks are subsequently transformed into vector representation via an embedding model. Such a representation scheme is supported by the well-known data structure of vector database [142], which systematically organizes data as key- value pairs for efficient text retrieval. In this",
      "chunk_index": 46
    },
    {
      "index": 153,
      "chunk_id": "Hallucination_Survey2023_chunk_47",
      "source_id": "Hallucination_Survey2023",
      "text": "chunks are subsequently transformed into vector representation via an embedding model. Such a representation scheme is supported by the well-known data structure of vector database [142], which systematically organizes data as key- value pairs for efficient text retrieval. In this manner, the relevance score can be computed according to the similarity function between the text representation and query representation. However, a sub-optimal embedding model may compromise performance, which affects the similarity and matching of chunks to user queries, potentially misleading LLMs. Typically, a standard embedding model [96, 130, 147, 382] learns the query and text representations with encoder-based architecture (e.g. BERT [73], RoBERTa [191]) via contrastive learning [304], where the loss is constructed by contrasting a positive pair of query-document against a set of random negative pairs. However, these embeddings showcase their limitations when applied to new domains, such as medical and financial applications [222, 295]. In these cases, recent studies [71, 234, 277, 332] propose to fine-tune the embedding models on domain-specific data to enhance retrieval relevance. For example, REPLUG [277] utilizes language modeling scores of the answers as a proxy signal to train the dense retriever. ACM Transactions on Information Systems, Vol. 1, No. 1, Article 1. Publication date: January 2024. 1:30 Huang, et al. More recently, Muennighoff et al. [221] have introduced generative representational instruction tuning where a single LLM is trained to handle both generative and embedding tasks, which largely reduces inference latency in RAG by caching representations. Despite these advancements, the field faces challenges, particularly with the fine-tuning of high-performing yet inaccessible embedding models, such as OpenAI's text-embedding-ada-002. Addressing this gap, Zhang et al. [367] introduced a novel approach for fine-tuning a black-box embedding model by augmenting it with a trainable embedding model which significantly enhances the performance of the black-box embeddings. 6.2 Generation Bottleneck After the retrieval process, the generation stage emerges as a pivotal point, responsible for generat- ing content that faithfully reflects the retrieved information. However, this stage can encounter significant bottlenecks that may lead to hallucinations. We summarize two key capabilities of LLMs that are closely related to these bottlenecks: contextual awareness and contextual alignment. Each plays an important role in ensuring the reliability and credibility of the RAG system. 6.2.1 Contextual Awareness. Contextual awareness involves understanding and effectively utilizing contextual information retrieved. This section discusses the key factors that impact the LLM's ability to maintain contextual awareness, which can be",
      "chunk_index": 47
    },
    {
      "index": 154,
      "chunk_id": "Hallucination_Survey2023_chunk_48",
      "source_id": "Hallucination_Survey2023",
      "text": "ensuring the reliability and credibility of the RAG system. 6.2.1 Contextual Awareness. Contextual awareness involves understanding and effectively utilizing contextual information retrieved. This section discusses the key factors that impact the LLM's ability to maintain contextual awareness, which can be categorized into three main parts: (1) the presence of noisy retrieval in context, (2) context conflicts, and (3) insufficient utilization of context information. Noisy Context. As emphasized in §6.1, the failure in the retrieval process may inevitably intro- duce irrelevant information, which will propagate into the generation stage. When the generator is not robust enough to these irrelevant retrievals, it will mislead the generator and even introduce hallucinations [65]. Yoran et al. [352] conducted a comprehensive analysis on the robustness of current retrieval- augmented LLMs, revealing a significant decrease in performance with random retrieval. While using an NLI model to filter out irrelevant passages is effective, this method comes with the trade-off of inadvertently discarding some relevant passages. A more effective solution is to train LLMs to ignore irrelevant contexts by incorporating irrelevant contexts in training data. Similarly, Yu et al. [357] introduced Chain-of-Note, which enables LLMs to first generate reading notes for retrieved contexts and subsequently formulate the final answer. In this way, LLMs can not only filter irrelevant retrieval to improve noise robustness but also respond with unknown when retrieval is insufficient to answer user queries. In addition to improving LLM robustness by learning to ignore irrelevant content in the context, several studies [139, 176, 323, 336] propose to compress the context to filter out irrelevant information. Specifically, Li [176] and Jiang et al. [139] made use of small language models to compute self-information and perplexity for prompt compression, finding the most informative content. Similarly, Wang et al. [323] proposed to filter out irrelevant content and leave precisely supporting content based on lexical and information-theoretic approaches. Besides, efforts have been also made to employ summarization models as compressors. Xu et al. [336] presented both extractive and abstractive compressors, which are trained to improve LLMs' performance while keeping the prompt concise. Liu et al. [188] involved summarization compression and semantic compression, where the former achieves compression by summarizing while the latter removes tokens with a lower impact on the semantic. Context Conflict. Retrieval-augmented LLMs generate answers through the combined effect of parametric knowledge and contextual knowledge. As discussed in §3.3.2, LLMs may sometimes exhibit over-confidence, which can bring new",
      "chunk_index": 48
    },
    {
      "index": 155,
      "chunk_id": "Hallucination_Survey2023_chunk_49",
      "source_id": "Hallucination_Survey2023",
      "text": "the latter removes tokens with a lower impact on the semantic. Context Conflict. Retrieval-augmented LLMs generate answers through the combined effect of parametric knowledge and contextual knowledge. As discussed in §3.3.2, LLMs may sometimes exhibit over-confidence, which can bring new challenges to the faithfulness of RAG systems when facing knowledge conflicts. Knowledge conflicts in RAG are situations where contextual knowledge ACM Transactions on Information Systems, Vol. 1, No. 1, Article 1. Publication date: January 2024. A Survey on Hallucination in Large Language Models: Principles, Taxonomy, Challenges, and Open Questions 1:31 contradicts LLMs' parametric knowledge. Longpre et al. [194] first investigated knowledge conflicts in open-domain question answering, where conflicts are automatically created by replacing all spans of the gold answer in the retrieval context with a substituted entity. Findings demonstrate that generative QA reader models (e.g. T5) tend to trust parametric memory over contextual information. By further training the retriever to learn to trust the contextual evidence with augmented training examples by entity substitution, the issue of over-reliance on parametric knowledge is mitigated. Similar findings are also reported by Li et al . [166] who demonstrated that fine-tuning LLMs on counterfactual contexts can effectively improve the controllability of LLMs when dealing with contradicts contexts. Also building upon counterfactual data augmentation, Neeman et al . [227] trained models to predict two disentangled answers, one based on contextual knowledge and the other leveraging parametric knowledge to address knowledge conflicts. Besides, Zhou et al. [390] introduced two effective prompting-based strategies, namely opinion-based prompts and counterfactual demonstrations. Opinion-based prompts transform the context to narrators' statements, soliciting the narrators' opinions, whereas counterfactual demonstrations employ counterfactual instances to improve faithfulness in situations of knowledge conflict. While Longpre et al. [194] and Li et al. [166] concentrated their research on the context of a limited single evidence setting, Chen et al. [40] further expanded this study to consider a more realistic scenario in which models consider multiple evidence passages and find models rely almost exclusively on contextual evidence. Considering previous studies [ 166, 194] mostly focused on smaller models, Xie et al . [334] raised doubts about the applicability of their conclusions in the era of LLMs. Such heuristic entity- level substitution may lead to incoherent counter-memory, thereby making it trivial for LLMs to overlook the construct knowledge conflicts. By directly eliciting LLMs to generate a coherent counter-memory that factually conflicts with the parametric memory, LLMs exhibit their",
      "chunk_index": 49
    },
    {
      "index": 156,
      "chunk_id": "Hallucination_Survey2023_chunk_50",
      "source_id": "Hallucination_Survey2023",
      "text": "heuristic entity- level substitution may lead to incoherent counter-memory, thereby making it trivial for LLMs to overlook the construct knowledge conflicts. By directly eliciting LLMs to generate a coherent counter-memory that factually conflicts with the parametric memory, LLMs exhibit their high receptivity to external evidence. Context Utilization. Despite successfully retrieving evidence relevant to factoid queries, LLMs can encounter a significant performance degradation due to insufficient utilization of the context, especially for information located in the middle of the long context window, a notable issue known as the lost-in-the-middle phenomenon [189]. Beyond factoid QA, recent studies have further demonstrated such a middle-curse also holds in abstractive summarization [257], long-form QA [41] and passage ranking [294]. One potential explanation lies in the use of rotary positional embedding (RoPE) [287], which is widely used in open-source LLMs, due to its excellent performance in length extrapolation [380]. As a representative relative position embedding, RoPE features a long-term decay property, which inherently biases the LLM to give precedence to current or proximate tokens, thereby diminishing its attention on those that are more distant. Another contributing factor is that the most salient information often resides at the beginning or the end of pre-training data, a characteristic commonly observed in news reports [257]. Such an issue brings forth challenges in retrieval-augmented LLMs, as retrieval-augmented LLMs are typically designed with extensive lengths to accommodate more retrieval documents. To mitigate this crucial issue, He et al . [114] introduced several tasks specially designed for information seeking to enhance the capability of information utilization by explicitly repeating the question and extracting the index of supporting documents before generating answers. Furthermore, Zhang et al. [377] introduced Multi-scale Positional Encoding (Ms-PoE), which mitigates the long- term decay effect characteristic of RoPE by rescaling position indices. Ms-PoE provides a plug- and-play solution to enhance the ability of LLMs to effectively capture information in the middle of the context without the need for additional fine-tuning. Besides, Ravaut et al. [257] proposed hierarchical and incremental summarization, which effectively preserves the salient information and compresses the length of context to avoid the middle-curse. ACM Transactions on Information Systems, Vol. 1, No. 1, Article 1. Publication date: January 2024. 1:32 Huang, et al. 6.2.2 Contextual Alignment. Contextual alignment ensures that LLM outputs faithfully align with relevant context. This section outlines the primary components of contextual alignment, which include: (1) source attribution and (2) faithful decoding. Source Attribution.",
      "chunk_index": 50
    },
    {
      "index": 157,
      "chunk_id": "Hallucination_Survey2023_chunk_51",
      "source_id": "Hallucination_Survey2023",
      "text": "January 2024. 1:32 Huang, et al. 6.2.2 Contextual Alignment. Contextual alignment ensures that LLM outputs faithfully align with relevant context. This section outlines the primary components of contextual alignment, which include: (1) source attribution and (2) faithful decoding. Source Attribution. Source attribution [121] in retrieval-augmented LLMs refers to the process by which the model identifies and utilizes the origins of information within its generation process. This component is crucial for ensuring that the outputs of RAG systems are not only relevant but also verifiable and grounded in credible sources. To achieve source attribution in RAG systems, recent studies have been explored, which can be categorized into three lines based on the type of attribution. (1) Plan-then-Generate: Fierro et al. [88] introduced the blueprint model for attribution, which conceptualizes text plans as a series of questions that serve as blueprints for generation process, dictating both the content and the sequence of the output. Compared with abstractive questions, Huang et al. [120] enabled the model to first ground to extractive evidence spans, which guides the subsequent generation process. Leveraging either abstract questions or extractive spans as planning facilitates a built-in attribution mechanism, as they provide a natural link between retrieved information and the subsequent generation. Similarly, Slobodkin et al. [282] broke down the conventional end-to-end generation process into three intuitive stages: content selection, sentence planning, and sentence fusion. By initially identifying relevant source segments and subsequently conditioning the generation process on them, the selected segments naturally serve as attributions. (2) Generate-then-Reflect: Asai et al. [11] proposed training the LLM to generate text with reflection tokens. These reflection tokens empower the LLM to decide whether to retrieve, assess the relevance of the retrieved document, and critique its own generation to ensure attributability. By critiquing its generation. Furthermore, Ye et al. [348] introduced AGREE, designed to facilitate self-grounding in LLMs. AGREE trains LLMs to generate well-grounded claims with citations and identify claims that lack verification. An iterative retrieval process is then employed to actively seek additional information for these unsupported statements. (3) Self-Attribution: In addition to leveraging external supervised signals for attribution, Qi et al. [248] proposed a self-attribution mechanism that utilizes model-internal signals. It operates by first identifying context-sensitive answer tokens, which are then paired with retrieved documents that contributed to the model generation via saliency methods. Faithful Decoding. Despite significant optimizations in the RAG pipeline that facilitate the incorporation of highly relevant",
      "chunk_index": 51
    },
    {
      "index": 158,
      "chunk_id": "Hallucination_Survey2023_chunk_52",
      "source_id": "Hallucination_Survey2023",
      "text": "It operates by first identifying context-sensitive answer tokens, which are then paired with retrieved documents that contributed to the model generation via saliency methods. Faithful Decoding. Despite significant optimizations in the RAG pipeline that facilitate the incorporation of highly relevant content into the model's context, current LLMs still cannot guarantee faithful generation. The unfaithful utilization of relevant context by LLMs undermines the reliability of their outputs, even when the sources of information are verifiably accurate. Wu et al. [331] analyzed the model's knowledge preference when internal knowledge conflicts with contextual information and observed the tug-of-war between the LLM's internal prior and external evidence. To tackle this issue, recent research [275, 330] has focused on faithful decoding within RAG systems, aiming to improve the models' ability to generate content that faithfully aligns with contextual information. Shi et al. [275] presented context-aware decoding, which modifies the model's original output probability distribution into the pointwise mutual information (PMI) formulation. The strategy operates by amplifying the difference between the output probabilities when a model is used with and without context, thereby enhancing the faithfulness of LLMs to the provided context. Li et al. [173] adopted a semi-parametric language modeling approach [150] which facilitates the integration of contextual spans of arbitrary length into LM generations. The generation is then verified via speculative decoding, further ensuring model faithfulness. More recently, Wu et al. [330] proposed faithfulness-oriented decoding, which leverages a lightweight faithfulness detector to monitor the beam-search process. The detector leverages fine-grained decoding dynamics including sequence likelihood, uncertainty quantification, context influence, and semantic alignment to synchronously detect unfaithful sentences. When an unfaithful generation ACM Transactions on Information Systems, Vol. 1, No. 1, Article 1. Publication date: January 2024. A Survey on Hallucination in Large Language Models: Principles, Taxonomy, Challenges, and Open Questions 1:33 is detected, it triggers the backtrack operation and selects the beam with the more faithful score, thus ensuring greater faithfulness to the retrieval sources. 7 FUTURE DISCUSSION As the field of research on hallucinations in LLMs continues to evolve, our focus shifts towards the next horizon of inquiry. We explore prospective areas of study, notably the phenomenon of hallucinations in vision-language models (§7.1) and the challenge of delineating and understanding knowledge boundaries within LLMs (§7.2). 7.1 Hallucination in Large Vision-Language Models Enabling the visual perception ability, along with exceptional language understanding and gen- eration capabilities, Large Vision-Language Models (LVLMs) have exhibited remarkable vision-",
      "chunk_index": 52
    },
    {
      "index": 159,
      "chunk_id": "Hallucination_Survey2023_chunk_53",
      "source_id": "Hallucination_Survey2023",
      "text": "and the challenge of delineating and understanding knowledge boundaries within LLMs (§7.2). 7.1 Hallucination in Large Vision-Language Models Enabling the visual perception ability, along with exceptional language understanding and gen- eration capabilities, Large Vision-Language Models (LVLMs) have exhibited remarkable vision- language capabilities [47, 123, 186, 201, 355, 356, 360, 392]. Unlike previous pre-trained multi-modal models that gain limited vision-language abilities from large-scale visual-language pre-training datasets [170, 197, 325, 387], LVLMs exploit advanced LLMs to unleash the power of interacting with humans and the environment. The consequent diverse applications of LVLMs also bring new challenges to maintaining the reliability of such systems. Recent studies have revealed that current LVLMs are suffering from multi-modal hallucinations, where models provide responses misaligned with the corresponding visual information [104, 187, 297]. Such multi-modal hallucinations could cause unexpected behaviors when applying LVLMs to real-world scenarios, which therefore had to be further investigated and mitigated. Li et al. [178] and Lovenia et al. [195] took the first step towards evaluating the object hallucina- tions in the LVLMs. Evaluations and experiments reveal that current LVLMs are prone to generate inconsistent responses with respect to the associated image, including non-existent objects, wrong object types, and attributes, incorrect semantic relationships, etc. [315, 361]. Furthermore, Liu et al. [185], Zong et al. [395] and Liu et al. [184] show that LVLMs can be easily fooled and experience a severe performance drop due to their over-reliance on the strong language prior, as well as its inferior ability to defend against inappropriate user inputs [112, 134]. Jiang et al. [138], Wang et al. [315] and Jing et al. [141] took a step forward to holistically evaluate multi-modal hallucination. What's more, when presented with multiple images, LVLMs sometimes mix or miss parts of the visual context, as well as fail to understand temporal or logical connections between them, which might hinder their usage in many scenarios, yet properly identifying the reason for such disorders and tackling them still requires continued efforts. Despite the witnessed perception errors, LVLMs can generate flawed logical reasoning results even when correctly recognizing all visual elements, which remains further investigation. Efforts have been made towards building a more robust large vision-language model. Gunjal et al. [108], Lu et al. [196], Wang et al. [316], and Liu et al. [185] proposed to further finetune the model for producing more truthful and helpful responses. Another line of work chooses to post-hoc rectify",
      "chunk_index": 53
    },
    {
      "index": 160,
      "chunk_id": "Hallucination_Survey2023_chunk_54",
      "source_id": "Hallucination_Survey2023",
      "text": "vision-language model. Gunjal et al. [108], Lu et al. [196], Wang et al. [316], and Liu et al. [185] proposed to further finetune the model for producing more truthful and helpful responses. Another line of work chooses to post-hoc rectify the generated inconsistent content, such as [391], and [349], which introduced expert models. To free from the external tools, Leng et al. [162], Huang et al. [122], and Zhao et al. [379] tried to fully utilize the LVLM itself to alleviate hallucinations. Though proved to be effective, those methods usually require additional data annotations, visual experts, training phases, and computational costs, which prevent LVLMs from effectively scaling and generalizing to various fields. Thus, more universal approaches are expected to build a more reliable system, such as faithful and large-scale visual-text pre-training and alignment methods. ACM Transactions on Information Systems, Vol. 1, No. 1, Article 1. Publication date: January 2024. 1:34 Huang, et al. 7.2 Understanding Knowledge Boundary in LLMs Despite the impressive capacity to capture factual knowledge from extensive data, LLMs still face challenges in recognizing their own knowledge boundaries. This shortfall leads to the occurrence of hallucinations, where LLMs confidently produce falsehoods without an awareness of their own knowledge limits [235, 261, 384]. Numerous studies delve into probing knowledge boundaries of LLMs, utilizing strategies such as evaluating the probability of a correct response in a multiple- choice setting [143], or quantifying the model's output uncertainty by evaluating the similarity among sets of sentences with uncertain meanings. Furthermore, a line of work [13, 31, 172, 220] has revealed that LLMs contain latent structures within their activation space that relate to beliefs about truthfulness. Recent research [281] also found substantial evidence for LLMs' ability to encode the unanswerability of questions, despite the fact that these models exhibit overconfidence and produce hallucinations when presented with unanswerable questions. Nonetheless, Levinstein and Herrmann [163] have employed empirical and conceptual tools to probe whether or not LLMs have beliefs. Their empirical results suggest that current lie-detector methods for LLMs are not yet fully reliable, and the probing methods proposed by Burns et al . [31] and Azaria and Mitchell [13] do not adequately generalize. Consequently, whether we can effectively probe LLMs' internal beliefs is ongoing, requiring further research. 8 CONCLUSION In this comprehensive survey, we have undertaken an in-depth examination of hallucinations within large language models, delving into the intricacies of their underlying causes, pioneering",
      "chunk_index": 54
    },
    {
      "index": 161,
      "chunk_id": "Hallucination_Survey2023_chunk_55",
      "source_id": "Hallucination_Survey2023",
      "text": "whether we can effectively probe LLMs' internal beliefs is ongoing, requiring further research. 8 CONCLUSION In this comprehensive survey, we have undertaken an in-depth examination of hallucinations within large language models, delving into the intricacies of their underlying causes, pioneering detection methodologies as well as related benchmarks, and effective mitigation strategies. Although significant strides have been taken, the conundrum of hallucination in LLMs remains a compelling and ongoing concern that demands continuous investigation. Moreover, we envision this survey as a guiding beacon for researchers dedicated to advancing robust information retrieval systems and trustworthy artificial intelligence. By navigating the complex landscape of hallucinations, we hope to empower these dedicated individuals with invaluable insights that drive the evolution of AI technologies toward greater reliability and safety. REFERENCES [1] Amro Abbas, Kushal Tirumala, Dániel Simig, Surya Ganguli, and Ari S Morcos. 2023. SemDeDup: Data-efficient learning at web-scale through semantic deduplication. ArXiv preprint abs/2303.09540 (2023). https://arxiv.org/abs/ 2303.09540 [2] Vaibhav Adlakha, Parishad BehnamGhader, Xing Han Lu, Nicholas Meade, and Siva Reddy. 2023. Evaluating correctness and faithfulness of instruction-following models for question answering. ArXiv preprint abs/2307.16877 (2023). https://arxiv.org/abs/2307.16877 [3] Ayush Agrawal, Lester Mackey, and Adam Tauman Kalai. 2023. Do Language Models Know When They're Halluci- nating References? ArXiv preprint abs/2305.18248 (2023). https://arxiv.org/abs/2305.18248 [4] Perplexity AI. 2023. Perplexity AI. https://www.perplexity.ai/ [5] Renat Aksitov, Chung-Ching Chang, David Reitter, Siamak Shakeri, and Yun-Hsuan Sung. 2023. Characterizing Attribution and Fluency Tradeoffs for Retrieval-Augmented Large Language Models. ArXiv preprint abs/2302.05578 (2023). https://arxiv.org/abs/2302.05578 [6] Badr AlKhamissi, Millicent Li, Asli Celikyilmaz, Mona T. Diab, and Marjan Ghazvininejad. 2022. A Review on Language Models as Knowledge Bases. CoRR abs/2204.06031 (2022). https://doi.org/10.48550/ARXIV.2204.06031 arXiv:2204.06031 [7] Rohan Anil, Sebastian Borgeaud, Yonghui Wu, Jean-Baptiste Alayrac, Jiahui Yu, Radu Soricut, Johan Schalkwyk, Andrew M. Dai, Anja Hauth, Katie Millican, David Silver, Slav Petrov, Melvin Johnson, Ioannis Antonoglou, Julian Schrittwieser, Amelia Glaese, Jilin Chen, Emily Pitler, Timothy P. Lillicrap, Angeliki Lazaridou, Orhan Firat, James Molloy, Michael Isard, Paul Ronald Barham, Tom Hennigan, Benjamin Lee, Fabio Viola, Malcolm Reynolds, Yuanzhong Xu, Ryan Doherty, Eli Collins, Clemens Meyer, Eliza Rutherford, Erica Moreira, Kareem Ayoub, Megha Goel, George Tucker, Enrique Piqueras, Maxim Krikun, Iain Barr, Nikolay Savinov, Ivo Danihelka, Becca Roelofs, Anaïs White, ACM Transactions on Information Systems, Vol. 1, No. 1, Article 1. Publication date: January 2024. A Survey on Hallucination in Large Language Models: Principles, Taxonomy, Challenges, and Open Questions 1:35 Anders Andreassen, Tamara von Glehn, Lakshman Yagati, Mehran Kazemi, Lucas",
      "chunk_index": 55
    },
    {
      "index": 162,
      "chunk_id": "Hallucination_Survey2023_chunk_56",
      "source_id": "Hallucination_Survey2023",
      "text": "ACM Transactions on Information Systems, Vol. 1, No. 1, Article 1. Publication date: January 2024. A Survey on Hallucination in Large Language Models: Principles, Taxonomy, Challenges, and Open Questions 1:35 Anders Andreassen, Tamara von Glehn, Lakshman Yagati, Mehran Kazemi, Lucas Gonzalez, Misha Khalman, Jakub Sygnowski, and et al. 2023. Gemini: A Family of Highly Capable Multimodal Models. CoRR abs/2312.11805 (2023). https://doi.org/10.48550/ARXIV.2312.11805 arXiv:2312.11805 [8] Anthropic. 2023. Claude. https://claude.ai/ [9] Antropic. 2024. Claude 3 haiku: our fastest model yet. 2024. https://www.anthropic.com/news/claude-3-haiku [10] ArXiv. 2023. arxiv dataset . https://www.kaggle.com/datasets/Cornell-University/arxiv/versions/134 [11] Akari Asai, Zeqiu Wu, Yizhong Wang, Avirup Sil, and Hannaneh Hajishirzi. 2023. Self-RAG: Learning to Retrieve, Generate, and Critique through Self-Reflection. CoRR abs/2310.11511 (2023). https://doi.org/10.48550/ARXIV.2310. 11511 arXiv:2310.11511 [12] Akari Asai, Zexuan Zhong, Danqi Chen, Pang Wei Koh, Luke Zettlemoyer, Hannaneh Hajishirzi, and Wen-tau Yih. 2024. Reliable, Adaptable, and Attributable Language Models with Retrieval. CoRR abs/2403.03187 (2024). https://doi.org/10.48550/ARXIV.2403.03187 arXiv:2403.03187 [13] Amos Azaria and Tom M. Mitchell. 2023. The Internal State of an LLM Knows When its Lying. ArXiv preprint abs/2304.13734 (2023). https://arxiv.org/abs/2304.13734 [14] Jinheon Baek, Alham Fikri Aji, and Amir Saffari. 2023. Knowledge-Augmented Language Model Prompting for Zero- Shot Knowledge Graph Question Answering. ArXiv preprint abs/2306.04136 (2023). https://arxiv.org/abs/2306.04136 [15] Yejin Bang, Samuel Cahyawijaya, Nayeon Lee, Wenliang Dai, Dan Su, Bryan Wilie, Holy Lovenia, Ziwei Ji, Tiezheng Yu, Willy Chung, Quyet V. Do, Yan Xu, and Pascale Fung. 2023. A Multitask, Multilingual, Multimodal Evaluation of ChatGPT on Reasoning, Hallucination, and Interactivity. ArXiv preprint abs/2302.04023 (2023). https://arxiv.org/abs/ 2302.04023 [16] Scott Barnett, Stefanus Kurniawan, Srikanth Thudumu, Zach Brannelly, and Mohamed Abdelrazek. 2024. Seven Failure Points When Engineering a Retrieval Augmented Generation System. CoRR abs/2401.05856 (2024). https: //doi.org/10.48550/ARXIV.2401.05856 arXiv:2401.05856 [17] Mario Barrantes, Benedikt Herudek, and Richard Wang. 2020. Adversarial nli for factual correctness in text summari- sation models. ArXiv preprint abs/2005.11739 (2020). https://arxiv.org/abs/2005.11739 [18] Pierre Basso. 1993. Conditional Causal Logic: A Formal Theory of the Meaning Generating Processes in a Cognitive System. In Proceedings of the 13th International Joint Conference on Artificial Intelligence. Chambéry, France, August 28 - September 3, 1993 , Ruzena Bajcsy (Ed.). Morgan Kaufmann, 845-851. http://ijcai.org/Proceedings/93-2/Papers/002.pdf [19] Iz Beltagy, Matthew E. Peters, and Arman Cohan. 2020. Longformer: The Long-Document Transformer. ArXiv preprint abs/2004.05150 (2020). https://arxiv.org/abs/2004.05150 [20] Emily M. Bender, Timnit Gebru, Angelina McMillan-Major, and Shmargaret Shmitchell. 2021. On the Dangers of Stochastic Parrots: Can Language Models Be Too Big?. In FAccT '21: 2021 ACM Conference on Fairness, Accountability, and Transparency, Virtual Event / Toronto, Canada,",
      "chunk_index": 56
    },
    {
      "index": 163,
      "chunk_id": "Hallucination_Survey2023_chunk_57",
      "source_id": "Hallucination_Survey2023",
      "text": "[20] Emily M. Bender, Timnit Gebru, Angelina McMillan-Major, and Shmargaret Shmitchell. 2021. On the Dangers of Stochastic Parrots: Can Language Models Be Too Big?. In FAccT '21: 2021 ACM Conference on Fairness, Accountability, and Transparency, Virtual Event / Toronto, Canada, March 3-10, 2021 , Madeleine Clare Elish, William Isaac, and Richard S. Zemel (Eds.). ACM, 610-623. https://doi.org/10.1145/3442188.3445922 [21] Samy Bengio, Oriol Vinyals, Navdeep Jaitly, and Noam Shazeer. 2015. Scheduled Sampling for Sequence Prediction with Recurrent Neural Networks. In Advances in Neural Information Processing Systems 28: Annual Conference on Neural Information Processing Systems 2015, December 7-12, 2015, Montreal, Quebec, Canada , Corinna Cortes, Neil D. Lawrence, Daniel D. Lee, Masashi Sugiyama, and Roman Garnett (Eds.). 1171-1179. https://proceedings.neurips.cc/ paper/2015/hash/e995f98d56967d946471af29d7bf99f1-Abstract.html [22] Lukas Berglund, Meg Tong, Max Kaufmann, Mikita Balesni, Asa Cooper Stickland, Tomasz Korbak, and Owain Evans. 2023. The Reversal Curse: LLMs trained on\" A is B\" fail to learn\" B is A\". ArXiv preprint abs/2309.12288 (2023). https://arxiv.org/abs/2309.12288 [23] Sid Black, Stella Biderman, Eric Hallahan, Quentin Anthony, Leo Gao, Laurence Golding, Horace He, Connor Leahy, Kyle McDonell, Jason Phang, Michael Pieler, USVSN Sai Prashanth, Shivanshu Purohit, Laria Reynolds, Jonathan Tow, Ben Wang, and Samuel Weinbach. 2022. GPT-NeoX-20B: An Open-Source Autoregressive Language Model. CoRR abs/2204.06745 (2022). https://doi.org/10.48550/ARXIV.2204.06745 arXiv:2204.06745 [24] Sebastian Borgeaud, Arthur Mensch, Jordan Hoffmann, Trevor Cai, Eliza Rutherford, Katie Millican, George van den Driessche, Jean-Baptiste Lespiau, Bogdan Damoc, Aidan Clark, Diego de Las Casas, Aurelia Guy, Jacob Menick, Roman Ring, Tom Hennigan, Saffron Huang, Loren Maggiore, Chris Jones, Albin Cassirer, Andy Brock, Michela Paganini, Geoffrey Irving, Oriol Vinyals, Simon Osindero, Karen Simonyan, Jack W. Rae, Erich Elsen, and Laurent Sifre. 2022. Improving Language Models by Retrieving from Trillions of Tokens. In International Conference on Machine Learning, ICML 2022, 17-23 July 2022, Baltimore, Maryland, USA (Proceedings of Machine Learning Research, Vol. 162), Kamalika Chaudhuri, Stefanie Jegelka, Le Song, Csaba Szepesvári, Gang Niu, and Sivan Sabato (Eds.). PMLR, 2206-2240. https://proceedings.mlr.press/v162/borgeaud22a.html [25] Samuel R Bowman, Jeeyoon Hyun, Ethan Perez, Edwin Chen, Craig Pettit, Scott Heiner, Kamile Lukosuite, Amanda Askell, Andy Jones, Anna Chen, et al. 2022. Measuring progress on scalable oversight for large language models. ArXiv preprint abs/2211.03540 (2022). https://arxiv.org/abs/2211.03540 ACM Transactions on Information Systems, Vol. 1, No. 1, Article 1. Publication date: January 2024. 1:36 Huang, et al. [26] Ralph Allan Bradley and Milton E Terry. 1952. Rank analysis of incomplete block designs: I. The method of paired comparisons. Biometrika 39, 3/4 (1952), 324-345. https://www.jstor.org/stable/2334029",
      "chunk_index": 57
    },
    {
      "index": 164,
      "chunk_id": "Hallucination_Survey2023_chunk_58",
      "source_id": "Hallucination_Survey2023",
      "text": "1, No. 1, Article 1. Publication date: January 2024. 1:36 Huang, et al. [26] Ralph Allan Bradley and Milton E Terry. 1952. Rank analysis of incomplete block designs: I. The method of paired comparisons. Biometrika 39, 3/4 (1952), 324-345. https://www.jstor.org/stable/2334029 [27] Ruben Branco, António Branco, João António Rodrigues, and João Ricardo Silva. 2021. Shortcutted Commonsense: Data Spuriousness in Deep Learning of Commonsense Reasoning. In Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing . Association for Computational Linguistics, Online and Punta Cana, Dominican Republic, 1504-1521. https://doi.org/10.18653/v1/2021.emnlp-main.113 [28] Andrei Z Broder. 1997. On the resemblance and containment of documents. InProceedings. Compression and Complexity of SEQUENCES 1997 (Cat. No. 97TB100171) . IEEE, 21-29. [29] Tom B. Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared Kaplan, Prafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, Sandhini Agarwal, Ariel Herbert-Voss, Gretchen Krueger, Tom Henighan, Rewon Child, Aditya Ramesh, Daniel M. Ziegler, Jeffrey Wu, Clemens Winter, Christopher Hesse, Mark Chen, Eric Sigler, Mateusz Litwin, Scott Gray, Benjamin Chess, Jack Clark, Christopher Berner, Sam McCandlish, Alec Radford, Ilya Sutskever, and Dario Amodei. 2020. Language Models are Few-Shot Learners. In Advances in Neural Information Processing Systems 33: Annual Conference on Neural Information Processing Systems 2020, NeurIPS 2020, December 6-12, 2020, virtual , Hugo Larochelle, Marc'Aurelio Ranzato, Raia Hadsell, Maria-Florina Balcan, and Hsuan-Tien Lin (Eds.). https://proceedings.neurips.cc/paper/2020/hash/1457c0d6bfcb4967418bfb8ac142f64a-Abstract.html [30] Sébastien Bubeck, Varun Chandrasekaran, Ronen Eldan, Johannes Gehrke, Eric Horvitz, Ece Kamar, Peter Lee, Yin Tat Lee, Yuanzhi Li, Scott M. Lundberg, Harsha Nori, Hamid Palangi, Marco Túlio Ribeiro, and Yi Zhang. 2023. Sparks of Artificial General Intelligence: Early experiments with GPT-4. ArXiv preprint abs/2303.12712 (2023). https://arxiv.org/abs/2303.12712 [31] Collin Burns, Haotian Ye, Dan Klein, and Jacob Steinhardt. 2022. Discovering latent knowledge in language models without supervision. ArXiv preprint abs/2212.03827 (2022). https://arxiv.org/abs/2212.03827 [32] Shulin Cao, Jiajie Zhang, Jiaxin Shi, Xin Lv, Zijun Yao, Qi Tian, Lei Hou, and Juanzi Li. 2023. Probabilistic Tree- of-thought Reasoning for Answering Knowledge-intensive Complex Questions. In Findings of the Association for Computational Linguistics: EMNLP 2023, Singapore, December 6-10, 2023 , Houda Bouamor, Juan Pino, and Kalika Bali (Eds.). Association for Computational Linguistics, 12541-12560. https://doi.org/10.18653/V1/2023.FINDINGS- EMNLP.835 [33] Yihan Cao, Siyu Li, Yixin Liu, Zhiling Yan, Yutong Dai, Philip S. Yu, and Lichao Sun. 2023. A Comprehensive Survey of AI-Generated Content (AIGC): A History of Generative AI from GAN to ChatGPT. CoRR abs/2303.04226 (2023). https://doi.org/10.48550/ARXIV.2303.04226 arXiv:2303.04226 [34] Nicholas Carlini, Daphne Ippolito, Matthew Jagielski, Katherine Lee,",
      "chunk_index": 58
    },
    {
      "index": 165,
      "chunk_id": "Hallucination_Survey2023_chunk_59",
      "source_id": "Hallucination_Survey2023",
      "text": "Yan, Yutong Dai, Philip S. Yu, and Lichao Sun. 2023. A Comprehensive Survey of AI-Generated Content (AIGC): A History of Generative AI from GAN to ChatGPT. CoRR abs/2303.04226 (2023). https://doi.org/10.48550/ARXIV.2303.04226 arXiv:2303.04226 [34] Nicholas Carlini, Daphne Ippolito, Matthew Jagielski, Katherine Lee, Florian Tramer, and Chiyuan Zhang. 2022. Quantifying memorization across neural language models. ArXiv preprint abs/2202.07646 (2022). https://arxiv.org/ abs/2202.07646 [35] Nicholas Carlini, Florian Tramer, Eric Wallace, Matthew Jagielski, Ariel Herbert-Voss, Katherine Lee, Adam Roberts, Tom Brown, Dawn Song, Ulfar Erlingsson, et al. 2021. Extracting training data from large language models. In 30th USENIX Security Symposium (USENIX Security 21) . 2633-2650. [36] Chung-Ching Chang, David Reitter, Renat Aksitov, and Yun-Hsuan Sung. 2023. KL-Divergence Guided Temperature Sampling. ArXiv preprint abs/2306.01286 (2023). https://arxiv.org/abs/2306.01286 [37] Haw-Shiuan Chang, Zonghai Yao, Alolika Gon, Hong Yu, and Andrew McCallum. 2023. Revisiting the Architectures like Pointer Networks to Efficiently Improve the Next Word Distribution, Summarization Factuality, and Beyond. In Findings of the Association for Computational Linguistics: ACL 2023, Toronto, Canada, July 9-14, 2023 , Anna Rogers, Jordan L. Boyd-Graber, and Naoaki Okazaki (Eds.). Association for Computational Linguistics, 12707-12730. https: //doi.org/10.18653/V1/2023.FINDINGS-ACL.805 [38] Haw-Shiuan Chang and Andrew McCallum. 2022. Softmax Bottleneck Makes Language Models Unable to Represent Multi-mode Word Distributions. In Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers) . Association for Computational Linguistics, Dublin, Ireland, 8048-8073. https: //doi.org/10.18653/v1/2022.acl-long.554 [39] Canyu Chen and Kai Shu. 2023. Combating Misinformation in the Age of LLMs: Opportunities and Challenges. CoRR abs/2311.05656 (2023). https://doi.org/10.48550/ARXIV.2311.05656 arXiv:2311.05656 [40] Hung-Ting Chen, Michael J. Q. Zhang, and Eunsol Choi. 2022. Rich Knowledge Sources Bring Complex Knowledge Conflicts: Recalibrating Models to Reflect Conflicting Evidence. In Proceedings of the 2022 Conference on Empirical Methods in Natural Language Processing, EMNLP 2022, Abu Dhabi, United Arab Emirates, December 7-11, 2022 , Yoav Goldberg, Zornitsa Kozareva, and Yue Zhang (Eds.). Association for Computational Linguistics, 2292-2307. https: //doi.org/10.18653/V1/2022.EMNLP-MAIN.146 [41] Hung-Ting Chen, Fangyuan Xu, Shane A Arora, and Eunsol Choi. 2023. Understanding Retrieval Augmentation for Long-Form Question Answering. ArXiv preprint abs/2310.12150 (2023). https://arxiv.org/abs/2310.12150 ACM Transactions on Information Systems, Vol. 1, No. 1, Article 1. Publication date: January 2024. A Survey on Hallucination in Large Language Models: Principles, Taxonomy, Challenges, and Open Questions 1:37 [42] Shiqi Chen, Yiran Zhao, Jinghan Zhang, I-Chun Chern, Siyang Gao, Pengfei Liu, and Junxian He. 2023. FELM: Benchmarking Factuality Evaluation of Large Language Models. ArXiv preprint abs/2310.00741. https://arxiv.org/ abs/2310.00741 [43] Tong Chen, Hongwei Wang,",
      "chunk_index": 59
    },
    {
      "index": 166,
      "chunk_id": "Hallucination_Survey2023_chunk_60",
      "source_id": "Hallucination_Survey2023",
      "text": "Challenges, and Open Questions 1:37 [42] Shiqi Chen, Yiran Zhao, Jinghan Zhang, I-Chun Chern, Siyang Gao, Pengfei Liu, and Junxian He. 2023. FELM: Benchmarking Factuality Evaluation of Large Language Models. ArXiv preprint abs/2310.00741. https://arxiv.org/ abs/2310.00741 [43] Tong Chen, Hongwei Wang, Sihao Chen, Wenhao Yu, Kaixin Ma, Xinran Zhao, Hongming Zhang, and Dong Yu. 2023. Dense X Retrieval: What Retrieval Granularity Should We Use? CoRR abs/2312.06648 (2023). https://doi.org/10. 48550/ARXIV.2312.06648 arXiv:2312.06648 [44] Xiaoyang Chen, Ben He, Hongyu Lin, Xianpei Han, Tianshu Wang, Boxi Cao, Le Sun, and Yingfei Sun. 2024. Spiral of Silence: How is Large Language Model Killing Information Retrieval? - A Case Study on Open Domain Question Answering. arXiv:2404.10496 [cs.IR] https://arxiv.org/abs/2404.10496 [45] Xiuying Chen, Mingzhe Li, Xin Gao, and Xiangliang Zhang. 2022. Towards Improving Faithfulness in Abstractive Summarization. In NeurIPS. http://papers.nips.cc/paper_files/paper/2022/hash/9b6d7202750e8e32cd5270eb7fc131f7- Abstract-Conference.html [46] Yijie Chen, Yijin Liu, Fandong Meng, Yufeng Chen, Jinan Xu, and Jie Zhou. 2023. Improving Translation Faithfulness of Large Language Models via Augmenting Instructions. ArXiv preprint abs/2308.12674 (2023). https://arxiv.org/abs/ 2308.12674 [47] Yangyi Chen, Karan Sikka, Michael Cogswell, Heng Ji, and Ajay Divakaran. 2023. Measuring and Improving Chain-of- Thought Reasoning in Vision-Language Models.ArXiv preprint abs/2309.04461 (2023). https://arxiv.org/abs/2309.04461 [48] Qinyuan Cheng, Xiaonan Li, Shimin Li, Qin Zhu, Zhangyue Yin, Yunfan Shao, Linyang Li, Tianxiang Sun, Hang Yan, and Xipeng Qiu. 2024. Unified Active Retrieval for Retrieval Augmented Generation. CoRR abs/2406.12534 (2024). https://doi.org/10.48550/ARXIV.2406.12534 arXiv:2406.12534 [49] Qinyuan Cheng, Tianxiang Sun, Wenwei Zhang, Siyin Wang, Xiangyang Liu, Mozhi Zhang, Junliang He, Mianqiu Huang, Zhangyue Yin, Kai Chen, and Xipeng Qiu. 2023. Evaluating Hallucinations in Chinese Large Language Models. arXiv:2310.03368 [cs.CL] [50] I Chern, Steffi Chern, Shiqi Chen, Weizhe Yuan, Kehua Feng, Chunting Zhou, Junxian He, Graham Neubig, Pengfei Liu, et al. 2023. FacTool: Factuality Detection in Generative AI-A Tool Augmented Framework for Multi-Task and Multi-Domain Scenarios. ArXiv preprint abs/2307.13528 (2023). https://arxiv.org/abs/2307.13528 [51] Cheng-Han Chiang and Hung-yi Lee. 2023. Can Large Language Models Be an Alternative to Human Evaluations? ArXiv preprint abs/2305.01937 (2023). https://arxiv.org/abs/2305.01937 [52] David Chiang and Peter Cholak. 2022. Overcoming a Theoretical Limitation of Self-Attention. In Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers) . Association for Computational Linguistics, Dublin, Ireland, 7654-7664. https://doi.org/10.18653/v1/2022.acl-long.527 [53] Sehyun Choi, Tianqing Fang, Zhaowei Wang, and Yangqiu Song. 2023. KCTS: Knowledge-Constrained Tree Search Decoding with Token-Level Hallucination Detection. ArXiv preprint abs/2310.09044 (2023). https://arxiv.org/abs/ 2310.09044 [54] Aakanksha Chowdhery, Sharan Narang, Jacob Devlin, Maarten Bosma, Gaurav Mishra, Adam Roberts, Paul",
      "chunk_index": 60
    },
    {
      "index": 167,
      "chunk_id": "Hallucination_Survey2023_chunk_61",
      "source_id": "Hallucination_Survey2023",
      "text": "[53] Sehyun Choi, Tianqing Fang, Zhaowei Wang, and Yangqiu Song. 2023. KCTS: Knowledge-Constrained Tree Search Decoding with Token-Level Hallucination Detection. ArXiv preprint abs/2310.09044 (2023). https://arxiv.org/abs/ 2310.09044 [54] Aakanksha Chowdhery, Sharan Narang, Jacob Devlin, Maarten Bosma, Gaurav Mishra, Adam Roberts, Paul Barham, Hyung Won Chung, Charles Sutton, Sebastian Gehrmann, Parker Schuh, Kensen Shi, Sasha Tsvyashchenko, Joshua Maynez, Abhishek Rao, Parker Barnes, Yi Tay, Noam Shazeer, Vinodkumar Prabhakaran, Emily Reif, Nan Du, Ben Hutchinson, Reiner Pope, James Bradbury, Jacob Austin, Michael Isard, Guy Gur-Ari, Pengcheng Yin, Toju Duke, Anselm Levskaya, Sanjay Ghemawat, Sunipa Dev, Henryk Michalewski, Xavier Garcia, Vedant Misra, Kevin Robinson, Liam Fedus, Denny Zhou, Daphne Ippolito, David Luan, Hyeontaek Lim, Barret Zoph, Alexander Spiridonov, Ryan Sepassi, David Dohan, Shivani Agrawal, Mark Omernick, Andrew M. Dai, Thanumalayan Sankaranarayana Pillai, Marie Pellat, Aitor Lewkowycz, Erica Moreira, Rewon Child, Oleksandr Polozov, Katherine Lee, Zongwei Zhou, Xuezhi Wang, Brennan Saeta, Mark Diaz, Orhan Firat, Michele Catasta, Jason Wei, Kathy Meier-Hellstern, Douglas Eck, Jeff Dean, Slav Petrov, and Noah Fiedel. 2023. PaLM: Scaling Language Modeling with Pathways. J. Mach. Learn. Res. 24 (2023), 240:1-240:113. http://jmlr.org/papers/v24/22-1144.html [55] Paul F. Christiano, Jan Leike, Tom B. Brown, Miljan Martic, Shane Legg, and Dario Amodei. 2017. Deep Reinforcement Learning from Human Preferences. In Advances in Neural Information Processing Systems 30: Annual Conference on Neural Information Processing Systems 2017, December 4-9, 2017, Long Beach, CA, USA , Isabelle Guyon, Ulrike von Luxburg, Samy Bengio, Hanna M. Wallach, Rob Fergus, S. V. N. Vishwanathan, and Roman Garnett (Eds.). 4299-4307. https://proceedings.neurips.cc/paper/2017/hash/d5e2c0adad503c91f91df240d0cd4e49-Abstract.html [56] Zheng Chu, Jingchang Chen, Qianglong Chen, Haotian Wang, Kun Zhu, Xiyuan Du, Weijiang Yu, Ming Liu, and Bing Qin. 2024. BeamAggR: Beam Aggregation Reasoning over Multi-source Knowledge for Multi-hop Question Answering. arXiv:2406.19820 [cs.CL] https://arxiv.org/abs/2406.19820 [57] Zheng Chu, Jingchang Chen, Qianglong Chen, Weijiang Yu, Tao He, Haotian Wang, Weihua Peng, Ming Liu, Bing Qin, and Ting Liu. 2023. A Survey of Chain of Thought Reasoning: Advances, Frontiers and Future. ArXiv preprint abs/2309.15402 (2023). https://arxiv.org/abs/2309.15402 ACM Transactions on Information Systems, Vol. 1, No. 1, Article 1. Publication date: January 2024. 1:38 Huang, et al. [58] Zheng Chu, Jingchang Chen, Qianglong Chen, Weijiang Yu, Tao He, Haotian Wang, Weihua Peng, Ming Liu, Bing Qin, and Ting Liu. 2023. A Survey of Chain of Thought Reasoning: Advances, Frontiers and Future. CoRR abs/2309.15402 (2023). https://doi.org/10.48550/ARXIV.2309.15402 arXiv:2309.15402 [59] Yung-Sung Chuang, Yujia Xie, Hongyin Luo, Yoon Kim, James Glass, and Pengcheng He. 2023. Dola: Decoding by contrasting",
      "chunk_index": 61
    },
    {
      "index": 168,
      "chunk_id": "Hallucination_Survey2023_chunk_62",
      "source_id": "Hallucination_Survey2023",
      "text": "Qin, and Ting Liu. 2023. A Survey of Chain of Thought Reasoning: Advances, Frontiers and Future. CoRR abs/2309.15402 (2023). https://doi.org/10.48550/ARXIV.2309.15402 arXiv:2309.15402 [59] Yung-Sung Chuang, Yujia Xie, Hongyin Luo, Yoon Kim, James Glass, and Pengcheng He. 2023. Dola: Decoding by contrasting layers improves factuality in large language models. ArXiv preprint abs/2309.03883 (2023). https: //arxiv.org/abs/2309.03883 [60] Hyung Won Chung, Le Hou, Shayne Longpre, Barret Zoph, Yi Tay, William Fedus, Eric Li, Xuezhi Wang, Mostafa De- hghani, Siddhartha Brahma, et al. 2022. Scaling instruction-finetuned language models. ArXiv preprint abs/2210.11416 (2022). https://arxiv.org/abs/2210.11416 [61] Karl Cobbe, Vineet Kosaraju, Mohammad Bavarian, Mark Chen, Heewoo Jun, Lukasz Kaiser, Matthias Plappert, Jerry Tworek, Jacob Hilton, Reiichiro Nakano, Christopher Hesse, and John Schulman. 2021. Training Verifiers to Solve Math Word Problems. CoRR abs/2110.14168 (2021). arXiv:2110.14168 https://arxiv.org/abs/2110.14168 [62] Roi Cohen, May Hamri, Mor Geva, and Amir Globerson. 2023. LM vs LM: Detecting Factual Errors via Cross Examination. ArXiv preprint abs/2305.13281 (2023). https://arxiv.org/abs/2305.13281 [63] Together Computer. 2023. RedPajama: an Open Dataset for Training Large Language Models . https://github.com/ togethercomputer/RedPajama-Data [64] Ajeya Cotra. 2021. Why AI alignment could be hard with modern deep learning. https://www.cold-takes.com/why- ai-alignment-could-be-hard-with-modern-deep-learning/ Cold Takes. [65] Florin Cuconasu, Giovanni Trappolini, Federico Siciliano, Simone Filice, Cesare Campagnano, Yoelle Maarek, Nicola Tonellotto, and Fabrizio Silvestri. 2024. The Power of Noise: Redefining Retrieval for RAG Systems. CoRR abs/2401.14887 (2024). https://doi.org/10.48550/ARXIV.2401.14887 arXiv:2401.14887 [66] Damai Dai, Li Dong, Yaru Hao, Zhifang Sui, Baobao Chang, and Furu Wei. 2022. Knowledge Neurons in Pretrained Transformers. InProceedings of the 60th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers). Association for Computational Linguistics, Dublin, Ireland, 8493-8502. https://doi.org/10.18653/v1/2022.acl- long.581 [67] Damai Dai, Wenbin Jiang, Qingxiu Dong, Yajuan Lyu, Qiaoqiao She, and Zhifang Sui. 2022. Neural knowledge bank for pretrained transformers. ArXiv preprint abs/2208.00399 (2022). https://arxiv.org/abs/2208.00399 [68] Sunhao Dai, Yuqi Zhou, Liang Pang, Weihao Liu, Xiaolin Hu, Yong Liu, Xiao Zhang, and Jun Xu. 2023. LLMs may Dominate Information Access: Neural Retrievers are Biased Towards LLM-Generated Texts. CoRR abs/2310.20501 (2023). https://doi.org/10.48550/ARXIV.2310.20501 arXiv:2310.20501 [69] Sumanth Dathathri, Andrea Madotto, Janice Lan, Jane Hung, Eric Frank, Piero Molino, Jason Yosinski, and Rosanne Liu. 2020. Plug and Play Language Models: A Simple Approach to Controlled Text Generation. In 8th International Conference on Learning Representations, ICLR 2020, Addis Ababa, Ethiopia, April 26-30, 2020 . OpenReview.net. https: //openreview.net/forum?id=H1edEyBKDS [70] Nicola De Cao, Wilker Aziz, and Ivan Titov. 2021. Editing Factual Knowledge in Language Models. In Proceedings of the 2021 Conference on Empirical",
      "chunk_index": 62
    },
    {
      "index": 169,
      "chunk_id": "Hallucination_Survey2023_chunk_63",
      "source_id": "Hallucination_Survey2023",
      "text": "Conference on Learning Representations, ICLR 2020, Addis Ababa, Ethiopia, April 26-30, 2020 . OpenReview.net. https: //openreview.net/forum?id=H1edEyBKDS [70] Nicola De Cao, Wilker Aziz, and Ivan Titov. 2021. Editing Factual Knowledge in Language Models. In Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing . Association for Computational Linguistics, Online and Punta Cana, Dominican Republic, 6491-6506. https://doi.org/10.18653/v1/2021.emnlp-main.522 [71] Maria Angels de Luis Balaguer, Vinamra Benara, Renato Luiz de Freitas Cunha, Roberto de M. Estevão Filho, Todd Hendry, Daniel Holstein, Jennifer Marsman, Nick Mecklenburg, Sara Malvar, Leonardo O. Nunes, Rafael Padilha, Morris Sharp, Bruno Silva, Swati Sharma, Vijay Aski, and Ranveer Chandra. 2024. RAG vs Fine-tuning: Pipelines, Tradeoffs, and a Case Study on Agriculture. CoRR abs/2401.08406 (2024). https://doi.org/10.48550/ARXIV.2401.08406 arXiv:2401.08406 [72] Grégoire Delétang, Anian Ruoss, Paul-Ambroise Duquenne, Elliot Catt, Tim Genewein, Christopher Mattern, Jordi Grau-Moya, Li Kevin Wenliang, Matthew Aitchison, Laurent Orseau, et al. 2023. Language Modeling Is Compression. ArXiv preprint abs/2309.10668 (2023). https://arxiv.org/abs/2309.10668 [73] Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. 2019. BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding. In Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, NAACL-HLT 2019, Minneapolis, MN, USA, June 2-7, 2019, Volume 1 (Long and Short Papers) , Jill Burstein, Christy Doran, and Thamar Solorio (Eds.). Association for Computational Linguistics, 4171-4186. https://doi.org/10.18653/V1/N19-1423 [74] Shehzaad Dhuliawala, Mojtaba Komeili, Jing Xu, Roberta Raileanu, Xian Li, Asli Celikyilmaz, and Jason Weston. 2023. Chain-of-Verification Reduces Hallucination in Large Language Models. ArXiv preprint abs/2309.11495 (2023). https://arxiv.org/abs/2309.11495 [75] Hanxing Ding, Liang Pang, Zihao Wei, Huawei Shen, and Xueqi Cheng. 2024. Retrieve Only When It Needs: Adaptive Retrieval Augmentation for Hallucination Mitigation in Large Language Models. CoRR abs/2402.10612 (2024). https://doi.org/10.48550/ARXIV.2402.10612 arXiv:2402.10612 ACM Transactions on Information Systems, Vol. 1, No. 1, Article 1. Publication date: January 2024. A Survey on Hallucination in Large Language Models: Principles, Taxonomy, Challenges, and Open Questions 1:39 [76] Zican Dong, Tianyi Tang, Junyi Li, Wayne Xin Zhao, and Ji-Rong Wen. 2023. BAMBOO: A Comprehensive Benchmark for Evaluating Long Text Modeling Capacities of Large Language Models. ArXiv preprint abs/2309.13345 (2023). https://arxiv.org/abs/2309.13345 [77] Esin Durmus, He He, and Mona Diab. 2020. FEQA: A Question Answering Evaluation Framework for Faithfulness Assessment in Abstractive Summarization. In Proceedings of the 58th Annual Meeting of the Association for Computa- tional Linguistics. Association for Computational Linguistics, Online, 5055-5070. https://doi.org/10.18653/v1/2020.acl- main.454 [78] Nouha Dziri, Andrea Madotto, Osmar Zaïane, and Avishek Joey",
      "chunk_index": 63
    },
    {
      "index": 170,
      "chunk_id": "Hallucination_Survey2023_chunk_64",
      "source_id": "Hallucination_Survey2023",
      "text": "Evaluation Framework for Faithfulness Assessment in Abstractive Summarization. In Proceedings of the 58th Annual Meeting of the Association for Computa- tional Linguistics. Association for Computational Linguistics, Online, 5055-5070. https://doi.org/10.18653/v1/2020.acl- main.454 [78] Nouha Dziri, Andrea Madotto, Osmar Zaïane, and Avishek Joey Bose. 2021. Neural Path Hunter: Reducing Hallu- cination in Dialogue Systems via Path Grounding. In Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing . Association for Computational Linguistics, Online and Punta Cana, Dominican Republic, 2197-2214. https://doi.org/10.18653/v1/2021.emnlp-main.168 [79] Nouha Dziri, Hannah Rashkin, Tal Linzen, and David Reitter. 2021. Evaluating groundedness in dialogue systems: The begin benchmark. ArXiv preprint abs/2105.00071 (2021). https://arxiv.org/abs/2105.00071 [80] Alexander Fabbri, Chien-Sheng Wu, Wenhao Liu, and Caiming Xiong. 2022. QAFactEval: Improved QA-Based Factual Consistency Evaluation for Summarization. In Proceedings of the 2022 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies . Association for Computational Linguistics, Seattle, United States, 2587-2601. https://doi.org/10.18653/v1/2022.naacl-main.187 [81] Alexander R. Fabbri, Wojciech Kryściński, Bryan McCann, Caiming Xiong, Richard Socher, and Dragomir Radev. 2021. SummEval: Re-evaluating Summarization Evaluation. Transactions of the Association for Computational Linguistics 9 (2021), 391-409. https://doi.org/10.1162/tacl_a_00373 [82] Tobias Falke, Leonardo F. R. Ribeiro, Prasetya Ajie Utama, Ido Dagan, and Iryna Gurevych. 2019. Ranking Generated Summaries by Correctness: An Interesting but Challenging Application for Natural Language Inference. InProceedings of the 57th Annual Meeting of the Association for Computational Linguistics . Association for Computational Linguistics, Florence, Italy, 2214-2220. https://doi.org/10.18653/v1/P19-1213 [83] Angela Fan, Yacine Jernite, Ethan Perez, David Grangier, Jason Weston, and Michael Auli. 2019. ELI5: Long Form Question Answering. In Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics . Association for Computational Linguistics, Florence, Italy, 3558-3567. https://doi.org/10.18653/v1/P19-1346 [84] Angela Fan, Mike Lewis, and Yann Dauphin. 2018. Hierarchical Neural Story Generation. In Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers) . Association for Computational Linguistics, Melbourne, Australia, 889-898. https://doi.org/10.18653/v1/P18-1082 [85] Huawen Feng, Yan Fan, Xiong Liu, Ting-En Lin, Zekun Yao, Yuchuan Wu, Fei Huang, Yongbin Li, and Qianli Ma. 2023. Improving Factual Consistency of Text Summarization by Adversarially Decoupling Comprehension and Embellishment Abilities of LLMs. CoRR abs/2310.19347 (2023). https://doi.org/10.48550/ARXIV.2310.19347 arXiv:2310.19347 [86] Shangbin Feng, Weijia Shi, Yuyang Bai, Vidhisha Balachandran, Tianxing He, and Yulia Tsvetkov. 2023. Cook: Empow- ering general-purpose language models with modular and collaborative knowledge. arXiv preprint arXiv:2305.09955 (2023). [87] Zhangyin Feng, Xiaocheng Feng, Dezhi Zhao, Maojin Yang, and Bing Qin. 2023.",
      "chunk_index": 64
    },
    {
      "index": 171,
      "chunk_id": "Hallucination_Survey2023_chunk_65",
      "source_id": "Hallucination_Survey2023",
      "text": "Weijia Shi, Yuyang Bai, Vidhisha Balachandran, Tianxing He, and Yulia Tsvetkov. 2023. Cook: Empow- ering general-purpose language models with modular and collaborative knowledge. arXiv preprint arXiv:2305.09955 (2023). [87] Zhangyin Feng, Xiaocheng Feng, Dezhi Zhao, Maojin Yang, and Bing Qin. 2023. Retrieval-Generation Synergy Augmented Large Language Models. ArXiv preprint abs/2310.05149 (2023). https://arxiv.org/abs/2310.05149 [88] Constanza Fierro, Reinald Kim Amplayo, Fantine Huot, Nicola De Cao, Joshua Maynez, Shashi Narayan, and Mirella Lapata. 2024. Learning to Plan and Generate Text with Citations. CoRR abs/2404.03381 (2024). https://doi.org/10. 48550/ARXIV.2404.03381 arXiv:2404.03381 [89] Katja Filippova. 2020. Controlled Hallucinations: Learning to Generate Faithfully from Noisy Data. In Findings of the Association for Computational Linguistics: EMNLP 2020 . Association for Computational Linguistics, Online, 864-870. https://doi.org/10.18653/v1/2020.findings-emnlp.76 [90] Robert Friel and Atindriyo Sanyal. 2023. Chainpoll: A high efficacy method for LLM hallucination detection. arXiv:2310.18344 [cs.CL] [91] Jinlan Fu, See-Kiong Ng, Zhengbao Jiang, and Pengfei Liu. 2023. GPTScore: Evaluate as You Desire. CoRR abs/2302.04166 (2023). https://doi.org/10.48550/ARXIV.2302.04166 arXiv:2302.04166 [92] Yarin Gal and Zoubin Ghahramani. 2016. Dropout as a Bayesian Approximation: Representing Model Uncertainty in Deep Learning. In Proceedings of the 33nd International Conference on Machine Learning, ICML 2016, New York City, NY, USA, June 19-24, 2016 (JMLR Workshop and Conference Proceedings, Vol. 48) , Maria-Florina Balcan and Kilian Q. Weinberger (Eds.). JMLR.org, 1050-1059. http://proceedings.mlr.press/v48/gal16.html [93] Leo Gao, Stella Biderman, Sid Black, Laurence Golding, Travis Hoppe, Charles Foster, Jason Phang, Horace He, Anish Thite, Noa Nabeshima, et al. 2021. The pile: An 800gb dataset of diverse text for language modeling. ArXiv preprint abs/2101.00027 (2021). https://arxiv.org/abs/2101.00027 ACM Transactions on Information Systems, Vol. 1, No. 1, Article 1. Publication date: January 2024. 1:40 Huang, et al. [94] Luyu Gao, Zhuyun Dai, Panupong Pasupat, Anthony Chen, Arun Tejasvi Chaganty, Yicheng Fan, Vincent Y. Zhao, Ni Lao, Hongrae Lee, Da-Cheng Juan, and Kelvin Guu. 2023. RARR: Researching and Revising What Language Models Say, Using Language Models. InProceedings of the 61st Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), ACL 2023, Toronto, Canada, July 9-14, 2023 , Anna Rogers, Jordan L. Boyd-Graber, and Naoaki Okazaki (Eds.). Association for Computational Linguistics, 16477-16508. https://aclanthology.org/2023.acl-long.910 [95] Mingqi Gao, Jie Ruan, Renliang Sun, Xunjian Yin, Shiping Yang, and Xiaojun Wan. 2023. Human-like summarization evaluation with chatgpt. ArXiv preprint abs/2304.02554 (2023). https://arxiv.org/abs/2304.02554 [96] Tianyu Gao, Xingcheng Yao, and Danqi Chen. 2021. SimCSE: Simple Contrastive Learning of Sentence Embeddings. CoRR abs/2104.08821 (2021). arXiv:2104.08821 https://arxiv.org/abs/2104.08821 [97] Yunfan Gao, Tao Sheng, Youlin Xiang, Yun",
      "chunk_index": 65
    },
    {
      "index": 172,
      "chunk_id": "Hallucination_Survey2023_chunk_66",
      "source_id": "Hallucination_Survey2023",
      "text": "2023. Human-like summarization evaluation with chatgpt. ArXiv preprint abs/2304.02554 (2023). https://arxiv.org/abs/2304.02554 [96] Tianyu Gao, Xingcheng Yao, and Danqi Chen. 2021. SimCSE: Simple Contrastive Learning of Sentence Embeddings. CoRR abs/2104.08821 (2021). arXiv:2104.08821 https://arxiv.org/abs/2104.08821 [97] Yunfan Gao, Tao Sheng, Youlin Xiang, Yun Xiong, Haofen Wang, and Jiawei Zhang. 2023. Chat-REC: Towards Interactive and Explainable LLMs-Augmented Recommender System. CoRR abs/2303.14524 (2023). https://doi.org/10. 48550/ARXIV.2303.14524 arXiv:2303.14524 [98] Zorik Gekhman, Gal Yona, Roee Aharoni, Matan Eyal, Amir Feder, Roi Reichart, and Jonathan Herzig. 2024. Does Fine-Tuning LLMs on New Knowledge Encourage Hallucinations? CoRR abs/2405.05904 (2024). https://doi.org/10. 48550/ARXIV.2405.05904 arXiv:2405.05904 [99] Ben Goodrich, Vinay Rao, Peter J. Liu, and Mohammad Saleh. 2019. Assessing The Factual Accuracy of Generated Text. In Proceedings of the 25th ACM SIGKDD International Conference on Knowledge Discovery & Data Mining, KDD 2019, Anchorage, AK, USA, August 4-8, 2019 , Ankur Teredesai, Vipin Kumar, Ying Li, Rómer Rosales, Evimaria Terzi, and George Karypis (Eds.). ACM, 166-175. https://doi.org/10.1145/3292500.3330955 [100] Google. 2023. Bard. https://bard.google.com/ [101] Tanya Goyal and Greg Durrett. 2020. Evaluating Factuality in Generation with Dependency-level Entailment. In Findings of the Association for Computational Linguistics: EMNLP 2020 . Association for Computational Linguistics, Online, 3592-3603. https://doi.org/10.18653/v1/2020.findings-emnlp.322 [102] Tanya Goyal and Greg Durrett. 2021. Annotating and Modeling Fine-grained Factuality in Summarization. In Proceedings of the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies . Association for Computational Linguistics, Online, 1449-1462. https://doi.org/10. 18653/v1/2021.naacl-main.114 [103] Yuxuan Gu, Xiaocheng Feng, Sicheng Ma, Jiaming Wu, Heng Gong, and Bing Qin. 2022. Improving Controllable Text Generation with Position-Aware Weighted Decoding. InFindings of the Association for Computational Linguistics: ACL 2022. Association for Computational Linguistics, Dublin, Ireland, 3449-3467. https://doi.org/10.18653/v1/2022.findings- acl.272 [104] Tianrui Guan, Fuxiao Liu, Xiyang Wu, Ruiqi Xian, Zongxia Li, Xiaoyu Liu, Xijun Wang, Lichang Chen, Furong Huang, Yaser Yacoob, et al. 2023. Hallusionbench: An advanced diagnostic suite for entangled language hallucination & visual illusion in large vision-language models. arXiv preprint arXiv:2310.14566 (2023). [105] Nuno Miguel Guerreiro, Duarte M. Alves, Jonas Waldendorf, Barry Haddow, Alexandra Birch, Pierre Colombo, and André F. T. Martins. 2023. Hallucinations in Large Multilingual Translation Models. ArXiv preprint abs/2303.16104 (2023). https://arxiv.org/abs/2303.16104 [106] Nuno M. Guerreiro, Elena Voita, and André Martins. 2023. Looking for a Needle in a Haystack: A Comprehensive Study of Hallucinations in Neural Machine Translation. In Proceedings of the 17th Conference of the European Chapter of the Association for Computational Linguistics . Association for Computational Linguistics, Dubrovnik, Croatia, 1059-1075. https://aclanthology.org/2023.eacl-main.75",
      "chunk_index": 66
    },
    {
      "index": 173,
      "chunk_id": "Hallucination_Survey2023_chunk_67",
      "source_id": "Hallucination_Survey2023",
      "text": "for a Needle in a Haystack: A Comprehensive Study of Hallucinations in Neural Machine Translation. In Proceedings of the 17th Conference of the European Chapter of the Association for Computational Linguistics . Association for Computational Linguistics, Dubrovnik, Croatia, 1059-1075. https://aclanthology.org/2023.eacl-main.75 [107] Suriya Gunasekar, Yi Zhang, Jyoti Aneja, Caio César Teodoro Mendes, Allie Del Giorno, Sivakanth Gopi, Mojan Javaheripi, Piero Kauffmann, Gustavo de Rosa, Olli Saarikivi, et al. 2023. Textbooks Are All You Need. ArXiv preprint abs/2306.11644 (2023). https://arxiv.org/abs/2306.11644 [108] Anisha Gunjal, Jihan Yin, and Erhan Bas. 2023. Detecting and preventing hallucinations in large vision language models. ArXiv preprint abs/2308.06394 (2023). https://arxiv.org/abs/2308.06394 [109] Kelvin Guu, Kenton Lee, Zora Tung, Panupong Pasupat, and Ming-Wei Chang. 2020. Retrieval Augmented Language Model Pre-Training. In Proceedings of the 37th International Conference on Machine Learning, ICML 2020, 13-18 July 2020, Virtual Event (Proceedings of Machine Learning Research, Vol. 119) . PMLR, 3929-3938. http://proceedings.mlr. press/v119/guu20a.html [110] Bikash Gyawali, Lucas Anastasiou, and Petr Knoth. 2020. Deduplication of Scholarly Documents using Locality Sensitive Hashing and Word Embeddings. In Proceedings of the Twelfth Language Resources and Evaluation Conference . European Language Resources Association, Marseille, France, 901-910. https://aclanthology.org/2020.lrec-1.113 [111] Michael Hahn. 2020. Theoretical Limitations of Self-Attention in Neural Sequence Models. Transactions of the Association for Computational Linguistics 8 (2020), 156-171. https://doi.org/10.1162/tacl_a_00306 [112] Tianyang Han, Qing Lian, Rui Pan, Renjie Pi, Jipeng Zhang, Shizhe Diao, Yong Lin, and Tong Zhang. 2024. The Instinctive Bias: Spurious Images lead to Hallucination in MLLMs. arXiv preprint arXiv:2402.03757 (2024). ACM Transactions on Information Systems, Vol. 1, No. 1, Article 1. Publication date: January 2024. A Survey on Hallucination in Large Language Models: Principles, Taxonomy, Challenges, and Open Questions 1:41 [113] Hangfeng He, Hongming Zhang, and Dan Roth. 2023. Rethinking with retrieval: Faithful large language model inference. ArXiv preprint abs/2301.00303 (2023). https://arxiv.org/abs/2301.00303 [114] Junqing He, Kunhao Pan, Xiaoqun Dong, Zhuoyang Song, Yibo Liu, Yuxin Liang, Hao Wang, Qianguo Sun, Songxin Zhang, Zejian Xie, and Jiaxing Zhang. 2023. Never Lost in the Middle: Improving Large Language Models via Attention Strengthening Question Answering. CoRR abs/2311.09198 (2023). https://doi.org/10.48550/ARXIV.2311.09198 arXiv:2311.09198 [115] Peter Henderson, Mark S. Krass, Lucia Zheng, Neel Guha, Christopher D. Manning, Dan Jurafsky, and Daniel E. Ho. 2022. Pile of Law: Learning Responsible Data Filtering from the Law and a 256GB Open-Source Legal Dataset. In Advances in Neural Information Processing Systems 35: Annual Conference on Neural Information Processing Sys- tems 2022, NeurIPS 2022, New Orleans, LA, USA, November 28",
      "chunk_index": 67
    },
    {
      "index": 174,
      "chunk_id": "Hallucination_Survey2023_chunk_68",
      "source_id": "Hallucination_Survey2023",
      "text": "of Law: Learning Responsible Data Filtering from the Law and a 256GB Open-Source Legal Dataset. In Advances in Neural Information Processing Systems 35: Annual Conference on Neural Information Processing Sys- tems 2022, NeurIPS 2022, New Orleans, LA, USA, November 28 - December 9, 2022 , Sanmi Koyejo, S. Mohamed, A. Agarwal, Danielle Belgrave, K. Cho, and A. Oh (Eds.). http://papers.nips.cc/paper_files/paper/2022/hash/ bc218a0c656e49d4b086975a9c785f47-Abstract-Datasets_and_Benchmarks.html [116] Dan Hendrycks, Collin Burns, Steven Basart, Andy Zou, Mantas Mazeika, Dawn Song, and Jacob Steinhardt. 2021. Measuring Massive Multitask Language Understanding. In 9th International Conference on Learning Representations, ICLR 2021, Virtual Event, Austria, May 3-7, 2021 . OpenReview.net. https://openreview.net/forum?id=d7KBjmI3GmQ [117] Evan Hernandez, Belinda Z Li, and Jacob Andreas. 2023. Inspecting and editing knowledge representations in language models. ArXiv preprint abs/2304.00740 (2023). https://arxiv.org/abs/2304.00740 [118] Ari Holtzman, Jan Buys, Li Du, Maxwell Forbes, and Yejin Choi. 2020. The Curious Case of Neural Text Degeneration. In 8th International Conference on Learning Representations, ICLR 2020, Addis Ababa, Ethiopia, April 26-30, 2020 . OpenReview.net. https://openreview.net/forum?id=rygGQyrFvH [119] Or Honovich, Leshem Choshen, Roee Aharoni, Ella Neeman, Idan Szpektor, and Omri Abend. 2021. 𝑄2: Evaluating Factual Consistency in Knowledge-Grounded Dialogues via Question Generation and Question Answering. In Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing . Association for Computational Linguistics, Online and Punta Cana, Dominican Republic, 7856-7870. https://doi.org/10.18653/v1/2021.emnlp- main.619 [120] Lei Huang, Xiaocheng Feng, Weitao Ma, Yuxuan Gu, Weihong Zhong, Xiachong Feng, Weijiang Yu, Weihua Peng, Duyu Tang, Dandan Tu, and Bing Qin. 2024. Learning Fine-Grained Grounded Citations for Attributed Large Language Models. In Findings of the Association for Computational Linguistics, ACL 2024, Bangkok, Thailand and virtual meeting, August 11-16, 2024 , Lun-Wei Ku, Andre Martins, and Vivek Srikumar (Eds.). Association for Computational Linguistics, 14095-14113. https://doi.org/10.18653/V1/2024.FINDINGS-ACL.838 [121] Lei Huang, Xiaocheng Feng, Weitao Ma, Liang Zhao, Yuchun Fan, Weihong Zhong, Dongliang Xu, Qing Yang, Hongtao Liu, and Bing Qin. 2024. Advancing Large Language Model Attribution through Self-Improving. arXiv:2410.13298 [cs.CL] https://arxiv.org/abs/2410.13298 [122] Qidong Huang, Xiaoyi Dong, Pan Zhang, Bin Wang, Conghui He, Jiaqi Wang, Dahua Lin, Weiming Zhang, and Nenghai Yu. 2023. Opera: Alleviating hallucination in multi-modal large language models via over-trust penalty and retrospection-allocation. arXiv preprint arXiv:2311.17911 (2023). [123] Shaohan Huang, Li Dong, Wenhui Wang, Yaru Hao, Saksham Singhal, Shuming Ma, Tengchao Lv, Lei Cui, Owais Khan Mohammed, Qiang Liu, et al. 2023. Language is not all you need: Aligning perception with language models. ArXiv preprint abs/2302.14045 (2023). https://arxiv.org/abs/2302.14045 [124] Yuzhen Huang, Yuzhuo",
      "chunk_index": 68
    },
    {
      "index": 175,
      "chunk_id": "Hallucination_Survey2023_chunk_69",
      "source_id": "Hallucination_Survey2023",
      "text": "Wenhui Wang, Yaru Hao, Saksham Singhal, Shuming Ma, Tengchao Lv, Lei Cui, Owais Khan Mohammed, Qiang Liu, et al. 2023. Language is not all you need: Aligning perception with language models. ArXiv preprint abs/2302.14045 (2023). https://arxiv.org/abs/2302.14045 [124] Yuzhen Huang, Yuzhuo Bai, Zhihao Zhu, Junlei Zhang, Jinghan Zhang, Tangjun Su, Junteng Liu, Chuancheng Lv, Yikai Zhang, Jiayi Lei, et al . 2023. C-eval: A multi-level multi-discipline chinese evaluation suite for foundation models. ArXiv preprint abs/2305.08322 (2023). https://arxiv.org/abs/2305.08322 [125] Yi-Chong Huang, Xia-Chong Feng, Xiao-Cheng Feng, and Bing Qin. 2021. The Factual Inconsistency Problem in Abstractive Text Summarization: A Survey. CoRR abs/2104.14839 (2021). arXiv:2104.14839 https://arxiv.org/abs/2104. [126] Yi-Chong Huang, Xia-Chong Feng, Xiao-Cheng Feng, and Bing Qin. 2021. The Factual Inconsistency Problem in Abstractive Text Summarization: A Survey. ArXiv preprint abs/2104.14839 (2021). https://arxiv.org/abs/2104.14839 [127] Zeyu Huang, Yikang Shen, Xiaofeng Zhang, Jie Zhou, Wenge Rong, and Zhang Xiong. 2023. Transformer-Patcher: One Mistake Worth One Neuron. In The Eleventh International Conference on Learning Representations, ICLR 2023, Kigali, Rwanda, May 1-5, 2023 . OpenReview.net. https://openreview.net/pdf?id=4oYUGeGBPm [128] Siqing Huo, Negar Arabzadeh, and Charles L. A. Clarke. 2023. Retrieving Supporting Evidence for LLMs Generated Answers. ArXiv preprint abs/2306.13781 (2023). https://arxiv.org/abs/2306.13781 [129] Srinivasan Iyer, Xi Victoria Lin, Ramakanth Pasunuru, Todor Mihaylov, Daniel Simig, Ping Yu, Kurt Shuster, Tianlu Wang, Qing Liu, Punit Singh Koura, et al. 2022. Opt-iml: Scaling language model instruction meta learning through the lens of generalization. ArXiv preprint abs/2212.12017 (2022). https://arxiv.org/abs/2212.12017 [130] Gautier Izacard, Mathilde Caron, Lucas Hosseini, Sebastian Riedel, Piotr Bojanowski, Armand Joulin, and Edouard Grave. 2022. Unsupervised Dense Information Retrieval with Contrastive Learning. Trans. Mach. Learn. Res. 2022 ACM Transactions on Information Systems, Vol. 1, No. 1, Article 1. Publication date: January 2024. 1:42 Huang, et al. (2022). https://openreview.net/forum?id=jKN1pXi7b0 [131] Gautier Izacard, Patrick S. H. Lewis, Maria Lomeli, Lucas Hosseini, Fabio Petroni, Timo Schick, Jane Dwivedi-Yu, Armand Joulin, Sebastian Riedel, and Edouard Grave. 2023. Atlas: Few-shot Learning with Retrieval Augmented Language Models. J. Mach. Learn. Res. 24 (2023), 251:1-251:43. http://jmlr.org/papers/v24/23-0037.html [132] Rolf Jagerman, Honglei Zhuang, Zhen Qin, Xuanhui Wang, and Michael Bendersky. 2023. Query Expansion by Prompt- ing Large Language Models.CoRR abs/2305.03653 (2023). https://doi.org/10.48550/ARXIV.2305.03653 arXiv:2305.03653 [133] Sameer Jain, Vaishakh Keshava, Swarnashree Mysore Sathyendra, Patrick Fernandes, Pengfei Liu, Graham Neubig, and Chunting Zhou. 2023. Multi-Dimensional Evaluation of Text Summarization with In-Context Learning. ArXiv preprint abs/2306.01200 (2023). https://arxiv.org/abs/2306.01200 [134] Joonhyun Jeong. 2023. Hijacking Context in Large Multi-modal Models. arXiv preprint arXiv:2312.07553 (2023). [135] Soyeong Jeong, Jinheon Baek, Sukmin",
      "chunk_index": 69
    },
    {
      "index": 176,
      "chunk_id": "Hallucination_Survey2023_chunk_70",
      "source_id": "Hallucination_Survey2023",
      "text": "Liu, Graham Neubig, and Chunting Zhou. 2023. Multi-Dimensional Evaluation of Text Summarization with In-Context Learning. ArXiv preprint abs/2306.01200 (2023). https://arxiv.org/abs/2306.01200 [134] Joonhyun Jeong. 2023. Hijacking Context in Large Multi-modal Models. arXiv preprint arXiv:2312.07553 (2023). [135] Soyeong Jeong, Jinheon Baek, Sukmin Cho, Sung Ju Hwang, and Jong C. Park. 2024. Adaptive-RAG: Learning to Adapt Retrieval-Augmented Large Language Models through Question Complexity. CoRR abs/2403.14403 (2024). https://doi.org/10.48550/ARXIV.2403.14403 arXiv:2403.14403 [136] Ziwei Ji, Nayeon Lee, Rita Frieske, Tiezheng Yu, Dan Su, Yan Xu, Etsuko Ishii, Yejin Bang, Andrea Madotto, and Pascale Fung. 2023. Survey of Hallucination in Natural Language Generation. ACM Comput. Surv. 55, 12 (2023), 248:1-248:38. https://doi.org/10.1145/3571730 [137] Ziwei Ji, Tiezheng Yu, Yan Xu, Nayeon Lee, Etsuko Ishii, and Pascale Fung. 2023. Towards Mitigating Hallucination in Large Language Models via Self-Reflection. ArXiv preprint abs/2310.06271 (2023). https://arxiv.org/abs/2310.06271 [138] Chaoya Jiang, Wei Ye, Mengfan Dong, Hongrui Jia, Haiyang Xu, Ming Yan, Ji Zhang, and Shikun Zhang. 2024. Hal-Eval: A Universal and Fine-grained Hallucination Evaluation Framework for Large Vision Language Models. arXiv preprint arXiv:2402.15721 (2024). [139] Huiqiang Jiang, Qianhui Wu, Chin-Yew Lin, Yuqing Yang, and Lili Qiu. 2023. LLMLingua: Compressing Prompts for Accelerated Inference of Large Language Models. arXiv:2310.05736 [cs.CL] [140] Zhengbao Jiang, Frank F. Xu, Luyu Gao, Zhiqing Sun, Qian Liu, Jane Dwivedi-Yu, Yiming Yang, Jamie Callan, and Graham Neubig. 2023. Active Retrieval Augmented Generation. ArXiv preprint abs/2305.06983 (2023). https: //arxiv.org/abs/2305.06983 [141] Liqiang Jing, Ruosen Li, Yunmo Chen, Mengzhao Jia, and Xinya Du. 2023. Faithscore: Evaluating hallucinations in large vision-language models. arXiv preprint arXiv:2311.01477 (2023). [142] Zhi Jing, Yongye Su, Yikun Han, Bo Yuan, Haiyun Xu, Chunjiang Liu, Kehai Chen, and Min Zhang. 2024. When Large Language Models Meet Vector Databases: A Survey. CoRR abs/2402.01763 (2024). https://doi.org/10.48550/ARXIV. 2402.01763 arXiv:2402.01763 [143] Saurav Kadavath, Tom Conerly, Amanda Askell, Tom Henighan, Dawn Drain, Ethan Perez, Nicholas Schiefer, Zac Hatfield-Dodds, Nova DasSarma, Eli Tran-Johnson, et al. 2022. Language models (mostly) know what they know. ArXiv preprint abs/2207.05221 (2022). https://arxiv.org/abs/2207.05221 [144] Greg Kamradt. 2024. The 5 Levels Of Text Splitting For Retrieval . youtube. https://www.youtube.com/watch?v= 8OJC21T2SL4 [145] Nikhil Kandpal, Haikang Deng, Adam Roberts, Eric Wallace, and Colin Raffel. 2023. Large Language Models Struggle to Learn Long-Tail Knowledge. In International Conference on Machine Learning, ICML 2023, 23-29 July 2023, Honolulu, Hawaii, USA (Proceedings of Machine Learning Research, Vol. 202) , Andreas Krause, Emma Brunskill, Kyunghyun Cho, Barbara Engelhardt, Sivan Sabato, and Jonathan Scarlett (Eds.). PMLR, 15696-15707. https: //proceedings.mlr.press/v202/kandpal23a.html [146] Vladimir Karpukhin, Barlas",
      "chunk_index": 70
    },
    {
      "index": 177,
      "chunk_id": "Hallucination_Survey2023_chunk_71",
      "source_id": "Hallucination_Survey2023",
      "text": "Machine Learning, ICML 2023, 23-29 July 2023, Honolulu, Hawaii, USA (Proceedings of Machine Learning Research, Vol. 202) , Andreas Krause, Emma Brunskill, Kyunghyun Cho, Barbara Engelhardt, Sivan Sabato, and Jonathan Scarlett (Eds.). PMLR, 15696-15707. https: //proceedings.mlr.press/v202/kandpal23a.html [146] Vladimir Karpukhin, Barlas Oguz, Sewon Min, Patrick Lewis, Ledell Wu, Sergey Edunov, Danqi Chen, and Wen-tau Yih. 2020. Dense Passage Retrieval for Open-Domain Question Answering. In Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP) . Association for Computational Linguistics, Online, 6769-6781. https://doi.org/10.18653/v1/2020.emnlp-main.550 [147] Vladimir Karpukhin, Barlas Oguz, Sewon Min, Patrick S. H. Lewis, Ledell Wu, Sergey Edunov, Danqi Chen, and Wen-tau Yih. 2020. Dense Passage Retrieval for Open-Domain Question Answering. In Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing, EMNLP 2020, Online, November 16-20, 2020 , Bonnie Webber, Trevor Cohn, Yulan He, and Yang Liu (Eds.). Association for Computational Linguistics, 6769-6781. https: //doi.org/10.18653/V1/2020.EMNLP-MAIN.550 [148] Jungo Kasai, Keisuke Sakaguchi, Yoichi Takahashi, Ronan Le Bras, Akari Asai, Xinyan Yu, Dragomir Radev, Noah A Smith, Yejin Choi, and Kentaro Inui. 2022. RealTime QA: What's the Answer Right Now?ArXiv preprint abs/2207.13332 (2022). https://arxiv.org/abs/2207.13332 [149] Daniel Martin Katz, Michael James Bommarito, Shang Gao, and Pablo Arredondo. 2023. Gpt-4 passes the bar exam. Available at SSRN 4389233 (2023). https://www.datascienceassn.org/sites/default/files/GPT-4%20Passes%20the% 20Bar%20Exam.pdf ACM Transactions on Information Systems, Vol. 1, No. 1, Article 1. Publication date: January 2024. A Survey on Hallucination in Large Language Models: Principles, Taxonomy, Challenges, and Open Questions 1:43 [150] Urvashi Khandelwal, Omer Levy, Dan Jurafsky, Luke Zettlemoyer, and Mike Lewis. 2020. Generalization through Memorization: Nearest Neighbor Language Models. In 8th International Conference on Learning Representations, ICLR 2020, Addis Ababa, Ethiopia, April 26-30, 2020 . OpenReview.net. https://openreview.net/forum?id=HklBjCEKvH [151] Takeshi Kojima, Shixiang Shane Gu, Machel Reid, Yutaka Matsuo, and Yusuke Iwasawa. 2022. Large language models are zero-shot reasoners. Advances in neural information processing systems 35 (2022), 22199-22213. [152] Wojciech Kryscinski, Bryan McCann, Caiming Xiong, and Richard Socher. 2020. Evaluating the Factual Consistency of Abstractive Text Summarization. In Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP) . Association for Computational Linguistics, Online, 9332-9346. https://doi.org/10.18653/v1/2020. emnlp-main.750 [153] Philippe Laban, Wojciech Kryściński, Divyansh Agarwal, Alexander R Fabbri, Caiming Xiong, Shafiq Joty, and Chien-Sheng Wu. 2023. LLMs as Factual Reasoners: Insights from Existing Benchmarks and Beyond. ArXiv preprint abs/2305.14540 (2023). https://arxiv.org/abs/2305.14540 [154] Philippe Laban, Tobias Schnabel, Paul N. Bennett, and Marti A. Hearst. 2022. SummaC: Re-Visiting NLI-based Models for",
      "chunk_index": 71
    },
    {
      "index": 178,
      "chunk_id": "Hallucination_Survey2023_chunk_72",
      "source_id": "Hallucination_Survey2023",
      "text": "Xiong, Shafiq Joty, and Chien-Sheng Wu. 2023. LLMs as Factual Reasoners: Insights from Existing Benchmarks and Beyond. ArXiv preprint abs/2305.14540 (2023). https://arxiv.org/abs/2305.14540 [154] Philippe Laban, Tobias Schnabel, Paul N. Bennett, and Marti A. Hearst. 2022. SummaC: Re-Visiting NLI-based Models for Inconsistency Detection in Summarization. Transactions of the Association for Computational Linguistics 10 (2022), 163-177. https://doi.org/10.1162/tacl_a_00453 [155] Faisal Ladhak, Esin Durmus, Mirac Suzgun, Tianyi Zhang, Dan Jurafsky, Kathleen McKeown, and Tatsunori Hashimoto. 2023. When Do Pre-Training Biases Propagate to Downstream Tasks? A Case Study in Text Summarization. In Proceedings of the 17th Conference of the European Chapter of the Association for Computational Linguistics . Association for Computational Linguistics, Dubrovnik, Croatia, 3206-3219. https://aclanthology.org/2023.eacl-main.234 [156] Balaji Lakshminarayanan, Alexander Pritzel, and Charles Blundell. 2017. Simple and Scalable Predictive Uncertainty Estimation using Deep Ensembles. In Advances in Neural Information Processing Systems 30: Annual Conference on Neural Information Processing Systems 2017, December 4-9, 2017, Long Beach, CA, USA , Isabelle Guyon, Ulrike von Luxburg, Samy Bengio, Hanna M. Wallach, Rob Fergus, S. V. N. Vishwanathan, and Roman Garnett (Eds.). 6402-6413. https://proceedings.neurips.cc/paper/2017/hash/9ef2ed4b7fd2c810847ffa5fa85bce38-Abstract.html [157] Tamera Lanham, Anna Chen, Ansh Radhakrishnan, Benoit Steiner, Carson Denison, Danny Hernandez, Dustin Li, Esin Durmus, Evan Hubinger, Jackson Kernion, Kamile Lukosiute, Karina Nguyen, Newton Cheng, Nicholas Joseph, Nicholas Schiefer, Oliver Rausch, Robin Larson, Sam McCandlish, Sandipan Kundu, Saurav Kadavath, Shannon Yang, Thomas Henighan, Timothy Maxwell, Timothy Telleen-Lawton, Tristan Hume, Zac Hatfield-Dodds, Jared Kaplan, Jan Brauner, Samuel R. Bowman, and Ethan Perez. 2023. Measuring Faithfulness in Chain-of-Thought Reasoning. CoRR abs/2307.13702 (2023). https://doi.org/10.48550/ARXIV.2307.13702 arXiv:2307.13702 [158] Barrett Martin Lattimer, Patrick Chen, Xinyuan Zhang, and Yi Yang. 2023. Fast and Accurate Factual Inconsistency Detection Over Long Documents. arXiv:2310.13189 [cs.CL] [159] Katherine Lee, Daphne Ippolito, Andrew Nystrom, Chiyuan Zhang, Douglas Eck, Chris Callison-Burch, and Nicholas Carlini. 2022. Deduplicating Training Data Makes Language Models Better. In Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers) . Association for Computational Linguistics, Dublin, Ireland, 8424-8445. https://doi.org/10.18653/v1/2022.acl-long.577 [160] Nayeon Lee, Wei Ping, Peng Xu, Mostofa Patwary, Pascale N Fung, Mohammad Shoeybi, and Bryan Catanzaro. 2022. Factuality enhanced language models for open-ended text generation. Advances in Neural Information Processing Systems 35 (2022), 34586-34599. [161] Deren Lei, Yaxi Li, Mingyu Wang, Vincent Yun, Emily Ching, Eslam Kamal, et al. 2023. Chain of Natural Language Inference for Reducing Large Language Model Ungrounded Hallucinations. ArXiv preprint abs/2310.03951 (2023). https://arxiv.org/abs/2310.03951 [162] Sicong Leng, Hang Zhang, Guanzheng Chen, Xin Li, Shijian",
      "chunk_index": 72
    },
    {
      "index": 179,
      "chunk_id": "Hallucination_Survey2023_chunk_73",
      "source_id": "Hallucination_Survey2023",
      "text": "Yaxi Li, Mingyu Wang, Vincent Yun, Emily Ching, Eslam Kamal, et al. 2023. Chain of Natural Language Inference for Reducing Large Language Model Ungrounded Hallucinations. ArXiv preprint abs/2310.03951 (2023). https://arxiv.org/abs/2310.03951 [162] Sicong Leng, Hang Zhang, Guanzheng Chen, Xin Li, Shijian Lu, Chunyan Miao, and Lidong Bing. 2023. Miti- gating object hallucinations in large vision-language models through visual contrastive decoding. arXiv preprint arXiv:2311.16922 (2023). [163] BA Levinstein and Daniel A Herrmann. 2023. Still No Lie Detector for Language Models: Probing Empirical and Conceptual Roadblocks. ArXiv preprint abs/2307.00175 (2023). https://arxiv.org/abs/2307.00175 [164] Mike Lewis, Yinhan Liu, Naman Goyal, Marjan Ghazvininejad, Abdelrahman Mohamed, Omer Levy, Veselin Stoyanov, and Luke Zettlemoyer. 2020. BART: Denoising Sequence-to-Sequence Pre-training for Natural Language Generation, Translation, and Comprehension. In Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics. Association for Computational Linguistics, Online, 7871-7880. https://doi.org/10.18653/v1/2020.acl- main.703 [165] Patrick S. H. Lewis, Ethan Perez, Aleksandra Piktus, Fabio Petroni, Vladimir Karpukhin, Naman Goyal, Heinrich Küttler, Mike Lewis, Wen-tau Yih, Tim Rocktäschel, Sebastian Riedel, and Douwe Kiela. 2020. Retrieval-Augmented Generation for Knowledge-Intensive NLP Tasks. In Advances in Neural Information Processing Systems 33: Annual Conference on Neural Information Processing Systems 2020, NeurIPS 2020, December 6-12, 2020, virtual , Hugo Larochelle, Marc'Aurelio Ranzato, Raia Hadsell, Maria-Florina Balcan, and Hsuan-Tien Lin (Eds.). https://proceedings.neurips. ACM Transactions on Information Systems, Vol. 1, No. 1, Article 1. Publication date: January 2024. 1:44 Huang, et al. cc/paper/2020/hash/6b493230205f780e1bc26945df7481e5-Abstract.html [166] Daliang Li, Ankit Singh Rawat, Manzil Zaheer, Xin Wang, Michal Lukasik, Andreas Veit, Felix X. Yu, and Sanjiv Kumar. 2023. Large Language Models with Controllable Working Memory. In Findings of the Association for Computational Linguistics: ACL 2023, Toronto, Canada, July 9-14, 2023 , Anna Rogers, Jordan L. Boyd-Graber, and Naoaki Okazaki (Eds.). Association for Computational Linguistics, 1774-1793. https://doi.org/10.18653/v1/2023.findings-acl.112 [167] Jiachun Li, Pengfei Cao, Yubo Chen, Kang Liu, and Jun Zhao. 2024. Towards Faithful Chain-of-Thought: Large Language Models are Bridging Reasoners. CoRR abs/2405.18915 (2024). https://doi.org/10.48550/ARXIV.2405.18915 arXiv:2405.18915 [168] Junyi Li, Jie Chen, Ruiyang Ren, Xiaoxue Cheng, Wayne Xin Zhao, Jian-Yun Nie, and Ji-Rong Wen. 2024. The Dawn After the Dark: An Empirical Study on Factuality Hallucination in Large Language Models. CoRR abs/2401.03205 (2024). https://doi.org/10.48550/ARXIV.2401.03205 arXiv:2401.03205 [169] Junyi Li, Xiaoxue Cheng, Wayne Xin Zhao, Jian-Yun Nie, and Ji-Rong Wen. 2023. HaluEval: A Large-Scale Hallucination Evaluation Benchmark for Large Language Models. CoRR abs/2305.11747 (2023). https://doi.org/10.48550/ARXIV. 2305.11747 arXiv:2305.11747 [170] Junnan Li, Dongxu Li, Silvio Savarese, and Steven Hoi. 2023. Blip-2: Bootstrapping language-image pre-training with",
      "chunk_index": 73
    },
    {
      "index": 180,
      "chunk_id": "Hallucination_Survey2023_chunk_74",
      "source_id": "Hallucination_Survey2023",
      "text": "Xin Zhao, Jian-Yun Nie, and Ji-Rong Wen. 2023. HaluEval: A Large-Scale Hallucination Evaluation Benchmark for Large Language Models. CoRR abs/2305.11747 (2023). https://doi.org/10.48550/ARXIV. 2305.11747 arXiv:2305.11747 [170] Junnan Li, Dongxu Li, Silvio Savarese, and Steven Hoi. 2023. Blip-2: Bootstrapping language-image pre-training with frozen image encoders and large language models. ArXiv preprint abs/2301.12597 (2023). https://arxiv.org/abs/2301. [171] Jinming Li, Wentao Zhang, Tian Wang, Guanglei Xiong, Alan Lu, and Gerard Medioni. 2023. GPT4Rec: A Generative Framework for Personalized Recommendation and User Interests Interpretation. In Proceedings of the 2023 SIGIR Workshop on eCommerce co-located with the 46th International ACM SIGIR Conference on Research and Development in Information Retrieval (SIGIR 2023), Taipei, Taiwan, July 27, 2023 (CEUR Workshop Proceedings, Vol. 3589) , Surya Kallumadi, Yubin Kim, Tracy Holloway King, Shervin Malmasi, Maarten de Rijke, and Jacopo Tagliabue (Eds.). CEUR-WS.org. https://ceur-ws.org/Vol-3589/paper_2.pdf [172] Kenneth Li, Oam Patel, Fernanda Viégas, Hanspeter Pfister, and Martin Wattenberg. 2023. Inference-Time Intervention: Eliciting Truthful Answers from a Language Model. ArXiv preprint abs/2306.03341 (2023). https://arxiv.org/abs/2306. [173] Minghan Li, Xilun Chen, Ari Holtzman, Beidi Chen, Jimmy Lin, Wen-tau Yih, and Xi Victoria Lin. 2024. Nearest Neighbor Speculative Decoding for LLM Generation and Attribution. CoRR abs/2405.19325 (2024). https://doi.org/10. 48550/ARXIV.2405.19325 arXiv:2405.19325 [174] Wei Li, Wenhao Wu, Moye Chen, Jiachen Liu, Xinyan Xiao, and Hua Wu. 2022. Faithfulness in Natural Language Generation: A Systematic Survey of Analysis, Evaluation and Optimization Methods. ArXiv preprint abs/2203.05227 (2022). https://arxiv.org/abs/2203.05227 [175] Xiang Lisa Li, Ari Holtzman, Daniel Fried, Percy Liang, Jason Eisner, Tatsunori Hashimoto, Luke Zettlemoyer, and Mike Lewis. 2022. Contrastive decoding: Open-ended text generation as optimization. ArXiv preprint abs/2210.15097 (2022). https://arxiv.org/abs/2210.15097 [176] Yucheng Li. 2023. Unlocking Context Constraints of LLMs: Enhancing Context Efficiency of LLMs with Self- Information-Based Content Filtering. arXiv:2304.12102 [cs.CL] [177] Yuanzhi Li, Sébastien Bubeck, Ronen Eldan, Allie Del Giorno, Suriya Gunasekar, and Yin Tat Lee. 2023. Textbooks Are All You Need II: phi-1.5 technical report. ArXiv preprint abs/2309.05463 (2023). https://arxiv.org/abs/2309.05463 [178] Yifan Li, Yifan Du, Kun Zhou, Jinpeng Wang, Wayne Xin Zhao, and Ji-Rong Wen. 2023. Evaluating Object Hallucination in Large Vision-Language Models. https://arxiv.org/abs/2305.10355 [179] Yunxiang Li, Zihan Li, Kai Zhang, Ruilong Dan, and You Zhang. 2023. ChatDoctor: A Medical Chat Model Fine-tuned on LLaMA Model using Medical Domain Knowledge. ArXiv preprint abs/2303.14070 (2023). https://arxiv.org/abs/ 2303.14070 [180] Zuchao Li, Shitou Zhang, Hai Zhao, Yifei Yang, and Dongjie Yang. 2023. BatGPT: A Bidirectional Autoregessive Talker from Generative Pre-trained Transformer. ArXiv preprint abs/2307.00360 (2023). https://arxiv.org/abs/2307.00360 [181] Chin-Yew Lin. 2004. ROUGE: A",
      "chunk_index": 74
    },
    {
      "index": 181,
      "chunk_id": "Hallucination_Survey2023_chunk_75",
      "source_id": "Hallucination_Survey2023",
      "text": "Knowledge. ArXiv preprint abs/2303.14070 (2023). https://arxiv.org/abs/ 2303.14070 [180] Zuchao Li, Shitou Zhang, Hai Zhao, Yifei Yang, and Dongjie Yang. 2023. BatGPT: A Bidirectional Autoregessive Talker from Generative Pre-trained Transformer. ArXiv preprint abs/2307.00360 (2023). https://arxiv.org/abs/2307.00360 [181] Chin-Yew Lin. 2004. ROUGE: A Package for Automatic Evaluation of Summaries. In Text Summarization Branches Out. Association for Computational Linguistics, Barcelona, Spain, 74-81. https://aclanthology.org/W04-1013 [182] Stephanie Lin, Jacob Hilton, and Owain Evans. 2022. TruthfulQA: Measuring How Models Mimic Human Falsehoods. In Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers) . Association for Computational Linguistics, Dublin, Ireland, 3214-3252. https://doi.org/10.18653/v1/2022.acl-long.229 [183] Bingbin Liu, Jordan T. Ash, Surbhi Goel, Akshay Krishnamurthy, and Cyril Zhang. 2023. Exposing Attention Glitches with Flip-Flop Language Modeling. ArXiv preprint abs/2306.00946 (2023). https://arxiv.org/abs/2306.00946 [184] Fuxiao Liu, Tianrui Guan, Zongxia Li, Lichang Chen, Yaser Yacoob, Dinesh Manocha, and Tianyi Zhou. 2023. HallusionBench: You See What You Think? Or You Think What You See? An Image-Context Reasoning Benchmark Challenging for GPT-4V(Ision), LLaVA-1.5, and Other Multi-Modality Models. https://arxiv.org/abs/2310.14566 ACM Transactions on Information Systems, Vol. 1, No. 1, Article 1. Publication date: January 2024. A Survey on Hallucination in Large Language Models: Principles, Taxonomy, Challenges, and Open Questions 1:45 [185] Fuxiao Liu, Kevin Lin, Linjie Li, Jianfeng Wang, Yaser Yacoob, and Lijuan Wang. 2023. Mitigating Hallucination in Large Multi-Modal Models via Robust Instruction Tuning. arXiv:2306.14565 [cs.CV] [186] Haotian Liu, Chunyuan Li, Qingyang Wu, and Yong Jae Lee. 2023. Visual instruction tuning. ArXiv preprint abs/2304.08485 (2023). https://arxiv.org/abs/2304.08485 [187] Hanchao Liu, Wenyuan Xue, Yifei Chen, Dapeng Chen, Xiutian Zhao, Ke Wang, Liping Hou, Rongjun Li, and Wei Peng. 2024. A survey on hallucination in large vision-language models. arXiv preprint arXiv:2402.00253 (2024). [188] Junyi Liu, Liangzhi Li, Tong Xiang, Bowen Wang, and Yiming Qian. 2023. TCRA-LLM: Token Compression Retrieval Augmented Large Language Model for Inference Cost Reduction. In Findings of the Association for Computational Linguistics: EMNLP 2023, Singapore, December 6-10, 2023 , Houda Bouamor, Juan Pino, and Kalika Bali (Eds.). Association for Computational Linguistics, 9796-9810. https://aclanthology.org/2023.findings-emnlp.655 [189] Nelson F. Liu, Kevin Lin, John Hewitt, Ashwin Paranjape, Michele Bevilacqua, Fabio Petroni, and Percy Liang. 2023. Lost in the Middle: How Language Models Use Long Contexts. ArXiv preprint abs/2307.03172 (2023). https: //arxiv.org/abs/2307.03172 [190] Yang Liu, Dan Iter, Yichong Xu, Shuohang Wang, Ruochen Xu, and Chenguang Zhu. 2023. Gpteval: Nlg evaluation using gpt-4 with better human alignment. ArXiv preprint abs/2303.16634 (2023). https://arxiv.org/abs/2303.16634 [191] Yinhan Liu, Myle",
      "chunk_index": 75
    },
    {
      "index": 182,
      "chunk_id": "Hallucination_Survey2023_chunk_76",
      "source_id": "Hallucination_Survey2023",
      "text": "Contexts. ArXiv preprint abs/2307.03172 (2023). https: //arxiv.org/abs/2307.03172 [190] Yang Liu, Dan Iter, Yichong Xu, Shuohang Wang, Ruochen Xu, and Chenguang Zhu. 2023. Gpteval: Nlg evaluation using gpt-4 with better human alignment. ArXiv preprint abs/2303.16634 (2023). https://arxiv.org/abs/2303.16634 [191] Yinhan Liu, Myle Ott, Naman Goyal, Jingfei Du, Mandar Joshi, Danqi Chen, Omer Levy, Mike Lewis, Luke Zettlemoyer, and Veselin Stoyanov. 2019. RoBERTa: A Robustly Optimized BERT Pretraining Approach. CoRR abs/1907.11692 (2019). arXiv:1907.11692 http://arxiv.org/abs/1907.11692 [192] Yang Liu, Yuanshun Yao, Jean-Francois Ton, Xiaoying Zhang, Ruocheng Guo, Hao Cheng, Yegor Klochkov, Muham- mad Faaiz Taufiq, and Hang Li. 2023. Trustworthy LLMs: a Survey and Guideline for Evaluating Large Language Models' Alignment. ArXiv preprint abs/2308.05374 (2023). https://arxiv.org/abs/2308.05374 [193] Yijin Liu, Xianfeng Zeng, Fandong Meng, and Jie Zhou. 2023. Instruction Position Matters in Sequence Generation with Large Language Models. ArXiv preprint abs/2308.12097 (2023). https://arxiv.org/abs/2308.12097 [194] Shayne Longpre, Kartik Perisetla, Anthony Chen, Nikhil Ramesh, Chris DuBois, and Sameer Singh. 2021. Entity- Based Knowledge Conflicts in Question Answering. In Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing, EMNLP 2021, Virtual Event / Punta Cana, Dominican Republic, 7-11 November, 2021 , Marie-Francine Moens, Xuanjing Huang, Lucia Specia, and Scott Wen-tau Yih (Eds.). Association for Computational Linguistics, 7052-7063. https://doi.org/10.18653/V1/2021.EMNLP-MAIN.565 [195] Holy Lovenia, Wenliang Dai, Samuel Cahyawijaya, Ziwei Ji, and Pascale Fung. 2023. Negative Object Presence Evaluation (NOPE) to Measure Object Hallucination in Vision-Language Models. https://arxiv.org/abs/2310.05338 [196] Jiaying Lu, Jinmeng Rao, Kezhen Chen, Xiaoyuan Guo, Yawen Zhang, Baochen Sun, Carl Yang, and Jie Yang. 2023. Evaluation and Mitigation of Agnosia in Multimodal Large Language Models. ArXiv preprint abs/2309.04041 (2023). https://arxiv.org/abs/2309.04041 [197] Huaishao Luo, Lei Ji, Botian Shi, Haoyang Huang, Nan Duan, Tianrui Li, Jason Li, Taroon Bharti, and Ming Zhou. 2020. Univl: A unified video and language pre-training model for multimodal understanding and generation. ArXiv preprint abs/2002.06353 (2020). https://arxiv.org/abs/2002.06353 [198] Junyu Luo, Cao Xiao, and Fenglong Ma. 2023. Zero-Resource Hallucination Prevention for Large Language Models. ArXiv preprint abs/2309.02654 (2023). https://arxiv.org/abs/2309.02654 [199] Zheheng Luo, Qianqian Xie, and Sophia Ananiadou. 2023. Chatgpt as a factual inconsistency evaluator for text summarization. [200] Xinbei Ma, Yeyun Gong, Pengcheng He, Hai Zhao, and Nan Duan. 2023. Query Rewriting for Retrieval-Augmented Large Language Models. CoRR abs/2305.14283 (2023). https://doi.org/10.48550/ARXIV.2305.14283 arXiv:2305.14283 [201] Muhammad Maaz, Hanoona Rasheed, Salman Khan, and Fahad Shahbaz Khan. 2023. Video-ChatGPT: Towards Detailed Video Understanding via Large Vision and Language Models. ArXiv preprint abs/2306.05424 (2023). https: //arxiv.org/abs/2306.05424 [202] Fiona Macpherson and Dimitris Platchias.",
      "chunk_index": 76
    },
    {
      "index": 183,
      "chunk_id": "Hallucination_Survey2023_chunk_77",
      "source_id": "Hallucination_Survey2023",
      "text": "CoRR abs/2305.14283 (2023). https://doi.org/10.48550/ARXIV.2305.14283 arXiv:2305.14283 [201] Muhammad Maaz, Hanoona Rasheed, Salman Khan, and Fahad Shahbaz Khan. 2023. Video-ChatGPT: Towards Detailed Video Understanding via Large Vision and Language Models. ArXiv preprint abs/2306.05424 (2023). https: //arxiv.org/abs/2306.05424 [202] Fiona Macpherson and Dimitris Platchias. 2013. Hallucination: Philosophy and psychology . MIT Press. https://books.google.com/books?hl=zh-CN&lr=&id=_bwtAAAAQBAJ&oi=fnd&pg=PR5&dq=Hallucination: +Philosophy+and+psychology&ots=2E62kf7_yC&sig=rH9HGXYacNkxOJNMVbw514aChZo [203] Chaitanya Malaviya, Subin Lee, Sihao Chen, Elizabeth Sieber, Mark Yatskar, and Dan Roth. 2023. ExpertQA: Expert- Curated Questions and Attributed Answers. arXiv:2309.07852 [cs.CL] [204] Alex Mallen, Akari Asai, Victor Zhong, Rajarshi Das, Daniel Khashabi, and Hannaneh Hajishirzi. 2023. When Not to Trust Language Models: Investigating Effectiveness of Parametric and Non-Parametric Memories. InProceedings of the 61st Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), ACL 2023, Toronto, Canada, July 9-14, 2023 , Anna Rogers, Jordan L. Boyd-Graber, and Naoaki Okazaki (Eds.). Association for Computational Linguistics, 9802-9822. https://doi.org/10.18653/v1/2023.acl-long.546 [205] Potsawee Manakul, Adian Liusie, and Mark J. F. Gales. 2023. SelfCheckGPT: Zero-Resource Black-Box Hallucination Detection for Generative Large Language Models. ArXiv preprint abs/2303.08896 (2023). https://arxiv.org/abs/2303. ACM Transactions on Information Systems, Vol. 1, No. 1, Article 1. Publication date: January 2024. 1:46 Huang, et al. [206] Udi Manber and Gene Myers. 1993. Suffix arrays: a new method for on-line string searches.siam Journal on Computing 22, 5 (1993), 935-948. [207] Shengyu Mao, Yong Jiang, Boli Chen, Xiao Li, Peng Wang, Xinyu Wang, Pengjun Xie, Fei Huang, Huajun Chen, and Ningyu Zhang. 2024. RaFe: Ranking Feedback Improves Query Rewriting for RAG. CoRR abs/2405.14431 (2024). https://doi.org/10.48550/ARXIV.2405.14431 arXiv:2405.14431 [208] Joshua Maynez, Shashi Narayan, Bernd Bohnet, and Ryan McDonald. 2020. On Faithfulness and Factuality in Abstractive Summarization. In Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics . Association for Computational Linguistics, Online, 1906-1919. https://doi.org/10.18653/v1/2020.acl-main.173 [209] Clara Meister, Ryan Cotterell, and Tim Vieira. 2020. If beam search is the answer, what was the question?. In Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP) . Association for Computational Linguistics, Online, 2173-2185. https://doi.org/10.18653/v1/2020.emnlp-main.170 [210] Kevin Meng, David Bau, Alex Andonian, and Yonatan Belinkov. 2022. Locating and Editing Factual Associations in GPT. In NeurIPS. http://papers.nips.cc/paper_files/paper/2022/hash/6f1d43d5a82a37e89b0665b33bf3a182-Abstract- Conference.html [211] Kevin Meng, Arnab Sen Sharma, Alex J. Andonian, Yonatan Belinkov, and David Bau. 2023. Mass-Editing Memory in a Transformer. In The Eleventh International Conference on Learning Representations, ICLR 2023, Kigali, Rwanda, May 1-5, 2023 . OpenReview.net. https://openreview.net/pdf?id=MkbcAHIYgyS [212] Mengqi Miao, Fandong Meng, Yijin Liu, Xiao-Hua Zhou, and Jie Zhou. 2021. Prevent the",
      "chunk_index": 77
    },
    {
      "index": 184,
      "chunk_id": "Hallucination_Survey2023_chunk_78",
      "source_id": "Hallucination_Survey2023",
      "text": "Bau. 2023. Mass-Editing Memory in a Transformer. In The Eleventh International Conference on Learning Representations, ICLR 2023, Kigali, Rwanda, May 1-5, 2023 . OpenReview.net. https://openreview.net/pdf?id=MkbcAHIYgyS [212] Mengqi Miao, Fandong Meng, Yijin Liu, Xiao-Hua Zhou, and Jie Zhou. 2021. Prevent the Language Model from being Overconfident in Neural Machine Translation. In Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing (Volume 1: Long Papers). Association for Computational Linguistics, Online, 3456-3468. https://doi.org/10.18653/v1/2021.acl-long.268 [213] Ning Miao, Yee Whye Teh, and Tom Rainforth. 2023. Selfcheck: Using llms to zero-shot check their own step-by-step reasoning. ArXiv preprint abs/2308.00436 (2023). https://arxiv.org/abs/2308.00436 [214] Microsoft. 2023. New Bing. https://www.bing.com/new [215] Sewon Min, Suchin Gururangan, Eric Wallace, Hannaneh Hajishirzi, Noah A. Smith, and Luke Zettlemoyer. 2023. SILO Language Models: Isolating Legal Risk In a Nonparametric Datastore. CoRR abs/2308.04430 (2023). https: //doi.org/10.48550/ARXIV.2308.04430 arXiv:2308.04430 [216] Sewon Min, Kalpesh Krishna, Xinxi Lyu, Mike Lewis, Wen-tau Yih, Pang Wei Koh, Mohit Iyyer, Luke Zettlemoyer, and Hannaneh Hajishirzi. 2023. FActScore: Fine-grained Atomic Evaluation of Factual Precision in Long Form Text Generation. ArXiv preprint abs/2305.14251 (2023). https://arxiv.org/abs/2305.14251 [217] Anshuman Mishra, Dhruvesh Patel, Aparna Vijayakumar, Xiang Lorraine Li, Pavan Kapanipathi, and Kartik Tala- madupula. 2021. Looking Beyond Sentence-Level Natural Language Inference for Question Answering and Text Summarization. In Proceedings of the 2021 Conference of the North American Chapter of the Association for Compu- tational Linguistics: Human Language Technologies . Association for Computational Linguistics, Online, 1322-1336. https://doi.org/10.18653/v1/2021.naacl-main.104 [218] Eric Mitchell, Charles Lin, Antoine Bosselut, Chelsea Finn, and Christopher D. Manning. 2022. Fast Model Editing at Scale. In The Tenth International Conference on Learning Representations, ICLR 2022, Virtual Event, April 25-29, 2022 . OpenReview.net. https://openreview.net/forum?id=0DcZxeWfOPt [219] Eric Mitchell, Charles Lin, Antoine Bosselut, Christopher D. Manning, and Chelsea Finn. 2022. Memory-Based Model Editing at Scale. InInternational Conference on Machine Learning, ICML 2022, 17-23 July 2022, Baltimore, Maryland, USA (Proceedings of Machine Learning Research, Vol. 162) , Kamalika Chaudhuri, Stefanie Jegelka, Le Song, Csaba Szepesvári, Gang Niu, and Sivan Sabato (Eds.). PMLR, 15817-15831. https://proceedings.mlr.press/v162/mitchell22a.html [220] Luca Moschella, Valentino Maiorca, Marco Fumero, Antonio Norelli, Francesco Locatello, and Emanuele Rodola. 2022. Relative representations enable zero-shot latent space communication. ArXiv preprint abs/2209.15430 (2022). https://arxiv.org/abs/2209.15430 [221] Niklas Muennighoff, Hongjin Su, Liang Wang, Nan Yang, Furu Wei, Tao Yu, Amanpreet Singh, and Douwe Kiela. 2024. Generative Representational Instruction Tuning. CoRR abs/2402.09906 (2024). https://doi.org/10.48550/ARXIV. 2402.09906 arXiv:2402.09906 [222] Niklas Muennighoff, Nouamane Tazi, Loïc Magne,",
      "chunk_index": 78
    },
    {
      "index": 185,
      "chunk_id": "Hallucination_Survey2023_chunk_79",
      "source_id": "Hallucination_Survey2023",
      "text": "preprint abs/2209.15430 (2022). https://arxiv.org/abs/2209.15430 [221] Niklas Muennighoff, Hongjin Su, Liang Wang, Nan Yang, Furu Wei, Tao Yu, Amanpreet Singh, and Douwe Kiela. 2024. Generative Representational Instruction Tuning. CoRR abs/2402.09906 (2024). https://doi.org/10.48550/ARXIV. 2402.09906 arXiv:2402.09906 [222] Niklas Muennighoff, Nouamane Tazi, Loïc Magne, and Nils Reimers. 2023. MTEB: Massive Text Embedding Benchmark. In Proceedings of the 17th Conference of the European Chapter of the Association for Computational Linguistics, EACL 2023, Dubrovnik, Croatia, May 2-6, 2023 , Andreas Vlachos and Isabelle Augenstein (Eds.). Association for Computational Linguistics, 2006-2029. https://doi.org/10.18653/V1/2023.EACL-MAIN.148 [223] Dor Muhlgay, Ori Ram, Inbal Magar, Yoav Levine, Nir Ratner, Yonatan Belinkov, Omri Abend, Kevin Leyton-Brown, Amnon Shashua, and Yoav Shoham. 2023. Generating benchmarks for factuality evaluation of language models. ArXiv preprint abs/2307.06908 (2023). https://arxiv.org/abs/2307.06908 ACM Transactions on Information Systems, Vol. 1, No. 1, Article 1. Publication date: January 2024. A Survey on Hallucination in Large Language Models: Principles, Taxonomy, Challenges, and Open Questions 1:47 [224] Inderjeet Nair, Aparna Garimella, Balaji Vasan Srinivasan, Natwar Modani, Niyati Chhaya, Srikrishna Karanam, and Sumit Shekhar. 2023. A Neural CRF-based Hierarchical Approach for Linear Text Segmentation. In Findings of the Association for Computational Linguistics: EACL 2023, Dubrovnik, Croatia, May 2-6, 2023 , Andreas Vlachos and Isabelle Augenstein (Eds.). Association for Computational Linguistics, 853-863. https://doi.org/10.18653/V1/2023.FINDINGS- EACL.65 [225] Feng Nan, Ramesh Nallapati, Zhiguo Wang, Cicero Nogueira dos Santos, Henghui Zhu, Dejiao Zhang, Kathleen McKeown, and Bing Xiang. 2021. Entity-level Factual Consistency of Abstractive Text Summarization. InProceedings of the 16th Conference of the European Chapter of the Association for Computational Linguistics: Main Volume . Association for Computational Linguistics, Online, 2727-2733. https://doi.org/10.18653/v1/2021.eacl-main.235 [226] Pranav Narayanan Venkit, Sanjana Gautam, Ruchi Panchanadikar, Ting-Hao Huang, and Shomir Wilson. 2023. Nationality Bias in Text Generation. In Proceedings of the 17th Conference of the European Chapter of the Association for Computational Linguistics . Association for Computational Linguistics, Dubrovnik, Croatia, 116-122. https: //aclanthology.org/2023.eacl-main.9 [227] Ella Neeman, Roee Aharoni, Or Honovich, Leshem Choshen, Idan Szpektor, and Omri Abend. 2023. DisentQA: Disentangling Parametric and Contextual Knowledge with Counterfactual Question Answering. In Proceedings of the 61st Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), ACL 2023, Toronto, Canada, July 9-14, 2023 , Anna Rogers, Jordan L. Boyd-Graber, and Naoaki Okazaki (Eds.). Association for Computational Linguistics, 10056-10070. https://doi.org/10.18653/V1/2023.ACL-LONG.559 [228] Shiyu Ni, Keping Bi, Jiafeng Guo, and Xueqi Cheng. 2024. When Do LLMs Need Retrieval Augmentation? Mitigating LLMs' Overconfidence Helps Retrieval Augmentation. CoRR abs/2402.11457 (2024). https://doi.org/10.48550/ARXIV. 2402.11457 arXiv:2402.11457 [229]",
      "chunk_index": 79
    },
    {
      "index": 186,
      "chunk_id": "Hallucination_Survey2023_chunk_80",
      "source_id": "Hallucination_Survey2023",
      "text": "and Naoaki Okazaki (Eds.). Association for Computational Linguistics, 10056-10070. https://doi.org/10.18653/V1/2023.ACL-LONG.559 [228] Shiyu Ni, Keping Bi, Jiafeng Guo, and Xueqi Cheng. 2024. When Do LLMs Need Retrieval Augmentation? Mitigating LLMs' Overconfidence Helps Retrieval Augmentation. CoRR abs/2402.11457 (2024). https://doi.org/10.48550/ARXIV. 2402.11457 arXiv:2402.11457 [229] Sean O'Brien and Mike Lewis. 2023. Contrastive Decoding Improves Reasoning in Large Language Models. ArXiv preprint abs/2309.09117 (2023). https://arxiv.org/abs/2309.09117 [230] Yasumasa Onoe, Michael Zhang, Eunsol Choi, and Greg Durrett. 2022. Entity Cloze By Date: What LMs Know About Unseen Entities. In Findings of the Association for Computational Linguistics: NAACL 2022 . Association for Computational Linguistics, Seattle, United States, 693-702. https://doi.org/10.18653/v1/2022.findings-naacl.52 [231] OpenAI. 2022. Introducing chatgpt. https://openai.com/blog/chatgpt [232] OpenAI. 2023. GPT-4 Technical Report. ArXiv preprint abs/2303.08774 (2023). https://arxiv.org/abs/2303.08774 [233] Long Ouyang, Jeffrey Wu, Xu Jiang, Diogo Almeida, Carroll L. Wainwright, Pamela Mishkin, Chong Zhang, Sand- hini Agarwal, Katarina Slama, Alex Ray, John Schulman, Jacob Hilton, Fraser Kelton, Luke Miller, Maddie Simens, Amanda Askell, Peter Welinder, Paul F. Christiano, Jan Leike, and Ryan Lowe. 2022. Training language mod- els to follow instructions with human feedback. In NeurIPS. http://papers.nips.cc/paper_files/paper/2022/hash/ b1efde53be364a73914f58805a001731-Abstract-Conference.html [234] Oded Ovadia, Menachem Brief, Moshik Mishaeli, and Oren Elisha. 2023. Fine-Tuning or Retrieval? Comparing Knowl- edge Injection in LLMs. CoRR abs/2312.05934 (2023). https://doi.org/10.48550/ARXIV.2312.05934 arXiv:2312.05934 [235] Lorenzo Pacchiardi, Alex J Chan, Sören Mindermann, Ilan Moscovitz, Alexa Y Pan, Yarin Gal, Owain Evans, and Jan Brauner. 2023. How to Catch an AI Liar: Lie Detection in Black-Box LLMs by Asking Unrelated Questions. ArXiv preprint abs/2309.15840 (2023). https://arxiv.org/abs/2309.15840 [236] Artidoro Pagnoni, Vidhisha Balachandran, and Yulia Tsvetkov. 2021. Understanding Factuality in Abstractive Summarization with FRANK: A Benchmark for Factuality Metrics. In Proceedings of the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies . Association for Computational Linguistics, Online, 4812-4829. https://doi.org/10.18653/v1/2021.naacl-main.383 [237] Liangming Pan, Michael Saxon, Wenda Xu, Deepak Nathani, Xinyi Wang, and William Yang Wang. 2023. Automatically correcting large language models: Surveying the landscape of diverse self-correction strategies. ArXiv preprint abs/2308.03188 (2023). https://arxiv.org/abs/2308.03188 [238] Ruotong Pan, Boxi Cao, Hongyu Lin, Xianpei Han, Jia Zheng, Sirui Wang, Xunliang Cai, and Le Sun. 2024. Not All Contexts Are Equal: Teaching LLMs Credibility-aware Generation. CoRR abs/2404.06809 (2024). https://doi.org/10. 48550/ARXIV.2404.06809 arXiv:2404.06809 [239] Kishore Papineni, Salim Roukos, Todd Ward, and Wei-Jing Zhu. 2002. Bleu: a Method for Automatic Evaluation of Machine Translation. In Proceedings of the 40th Annual Meeting of the Association for Computational Linguistics . Association for Computational Linguistics, Philadelphia, Pennsylvania, USA,",
      "chunk_index": 80
    },
    {
      "index": 187,
      "chunk_id": "Hallucination_Survey2023_chunk_81",
      "source_id": "Hallucination_Survey2023",
      "text": "Kishore Papineni, Salim Roukos, Todd Ward, and Wei-Jing Zhu. 2002. Bleu: a Method for Automatic Evaluation of Machine Translation. In Proceedings of the 40th Annual Meeting of the Association for Computational Linguistics . Association for Computational Linguistics, Philadelphia, Pennsylvania, USA, 311-318. https://doi.org/10.3115/ 1073083.1073135 [240] Ankur P. Parikh, Xuezhi Wang, Sebastian Gehrmann, Manaal Faruqui, Bhuwan Dhingra, Diyi Yang, and Dipanjan Das. 2020. ToTTo: A Controlled Table-To-Text Generation Dataset. InProceedings of the 2020 Conference on Empirical Methods in Natural Language Processing, EMNLP 2020, Online, November 16-20, 2020 , Bonnie Webber, Trevor Cohn, Yulan He, and Yang Liu (Eds.). Association for Computational Linguistics, 1173-1186. https://doi.org/10.18653/V1/ 2020.EMNLP-MAIN.89 ACM Transactions on Information Systems, Vol. 1, No. 1, Article 1. Publication date: January 2024. 1:48 Huang, et al. [241] Debjit Paul, Robert West, Antoine Bosselut, and Boi Faltings. 2024. Making Reasoning Matter: Measuring and Improving Faithfulness of Chain-of-Thought Reasoning.CoRR abs/2402.13950 (2024). https://doi.org/10.48550/ARXIV. 2402.13950 arXiv:2402.13950 [242] Amandalynne Paullada, Inioluwa Deborah Raji, Emily M. Bender, Emily Denton, and Alex Hanna. 2021. Data and its (dis)contents: A survey of dataset development and use in machine learning research. Patterns 2, 11 (2021), 100336. https://doi.org/10.1016/J.PATTER.2021.100336 [243] Guilherme Penedo, Quentin Malartic, Daniel Hesslow, Ruxandra Cojocaru, Alessandro Cappelli, Hamza Alobei- dli, Baptiste Pannier, Ebtesam Almazrouei, and Julien Launay. 2023. The RefinedWeb Dataset for Falcon LLM: Outperforming Curated Corpora with Web Data, and Web Data Only. ArXiv preprint abs/2306.01116 (2023). https://arxiv.org/abs/2306.01116 [244] Baolin Peng, Chunyuan Li, Pengcheng He, Michel Galley, and Jianfeng Gao. 2023. Instruction tuning with gpt-4. ArXiv preprint abs/2304.03277 (2023). https://arxiv.org/abs/2304.03277 [245] Ethan Perez, Sam Ringer, Kamile Lukosiute, Karina Nguyen, Edwin Chen, Scott Heiner, Craig Pettit, Catherine Olsson, Sandipan Kundu, Saurav Kadavath, Andy Jones, Anna Chen, Benjamin Mann, Brian Israel, Bryan Seethor, Cameron McKinnon, Christopher Olah, Da Yan, Daniela Amodei, Dario Amodei, Dawn Drain, Dustin Li, Eli Tran-Johnson, Guro Khundadze, Jackson Kernion, James Landis, Jamie Kerr, Jared Mueller, Jeeyoon Hyun, Joshua Landau, Kamal Ndousse, Landon Goldberg, Liane Lovitt, Martin Lucas, Michael Sellitto, Miranda Zhang, Neerav Kingsland, Nelson Elhage, Nicholas Joseph, Noemí Mercado, Nova DasSarma, Oliver Rausch, Robin Larson, Sam McCandlish, Scott Johnston, Shauna Kravec, Sheer El Showk, Tamera Lanham, Timothy Telleen-Lawton, Tom Brown, Tom Henighan, Tristan Hume, Yuntao Bai, Zac Hatfield-Dodds, Jack Clark, Samuel R. Bowman, Amanda Askell, Roger Grosse, Danny Hernandez, Deep Ganguli, Evan Hubinger, Nicholas Schiefer, and Jared Kaplan. 2023. Discovering Language Model Behaviors with Model-Written Evaluations. In Findings of the Association for Computational Linguistics: ACL 2023, Toronto, Canada, July",
      "chunk_index": 81
    },
    {
      "index": 188,
      "chunk_id": "Hallucination_Survey2023_chunk_82",
      "source_id": "Hallucination_Survey2023",
      "text": "Clark, Samuel R. Bowman, Amanda Askell, Roger Grosse, Danny Hernandez, Deep Ganguli, Evan Hubinger, Nicholas Schiefer, and Jared Kaplan. 2023. Discovering Language Model Behaviors with Model-Written Evaluations. In Findings of the Association for Computational Linguistics: ACL 2023, Toronto, Canada, July 9-14, 2023 , Anna Rogers, Jordan L. Boyd-Graber, and Naoaki Okazaki (Eds.). Association for Computational Linguistics, 13387-13434. https://doi.org/10.18653/V1/2023.FINDINGS-ACL.847 [246] Fabio Petroni, Tim Rocktäschel, Sebastian Riedel, Patrick Lewis, Anton Bakhtin, Yuxiang Wu, and Alexander Miller. 2019. Language Models as Knowledge Bases?. In Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP) . Association for Computational Linguistics, Hong Kong, China, 2463-2473. https://doi.org/10.18653/v1/D19-1250 [247] Ofir Press, Muru Zhang, Sewon Min, Ludwig Schmidt, Noah A Smith, and Mike Lewis. 2022. Measuring and narrowing the compositionality gap in language models. ArXiv preprint abs/2210.03350 (2022). https://arxiv.org/abs/2210.03350 [248] Jirui Qi, Gabriele Sarti, Raquel Fernández, and Arianna Bisazza. 2024. Model Internals-based Answer Attribution for Trustworthy Retrieval-Augmented Generation. CoRR abs/2406.13663 (2024). https://doi.org/10.48550/ARXIV.2406. 13663 arXiv:2406.13663 [249] Zhixiao Qi, Yijiong Yu, Meiqi Tu, Junyi Tan, and Yongfeng Huang. 2023. FoodGPT: A Large Language Model in Food Testing Domain with Incremental Pre-training and Knowledge Graph Prompt. ArXiv preprint abs/2308.10173 (2023). https://arxiv.org/abs/2308.10173 [250] Shuofei Qiao, Yixin Ou, Ningyu Zhang, Xiang Chen, Yunzhi Yao, Shumin Deng, Chuanqi Tan, Fei Huang, and Huajun Chen. 2022. Reasoning with language model prompting: A survey. ArXiv preprint abs/2212.09597 (2022). https://arxiv.org/abs/2212.09597 [251] Alec Radford, Karthik Narasimhan, Tim Salimans, Ilya Sutskever, et al. 2018. Improving language understanding by generative pre-training. (2018). [252] Alec Radford, Jeffrey Wu, Rewon Child, David Luan, Dario Amodei, Ilya Sutskever, et al. 2019. Language models are unsupervised multitask learners. OpenAI blog 1, 8 (2019), 9. [253] Rafael Rafailov, Archit Sharma, Eric Mitchell, Stefano Ermon, Christopher D. Manning, and Chelsea Finn. 2023. Direct Preference Optimization: Your Language Model is Secretly a Reward Model. ArXiv preprint abs/2305.18290 (2023). https://arxiv.org/abs/2305.18290 [254] Colin Raffel, Noam Shazeer, Adam Roberts, Katherine Lee, Sharan Narang, Michael Matena, Yanqi Zhou, Wei Li, and Peter J. Liu. 2020. Exploring the Limits of Transfer Learning with a Unified Text-to-Text Transformer. J. Mach. Learn. Res. 21 (2020), 140:1-140:67. http://jmlr.org/papers/v21/20-074.html [255] Ori Ram, Yoav Levine, Itay Dalmedigos, Dor Muhlgay, Amnon Shashua, Kevin Leyton-Brown, and Yoav Shoham. 2023. In-Context Retrieval-Augmented Language Models. ArXiv preprint abs/2302.00083 (2023). https://arxiv.org/ abs/2302.00083 [256] Marc'Aurelio Ranzato, Sumit Chopra, Michael Auli, and Wojciech Zaremba. 2016. Sequence Level Training with Recurrent Neural",
      "chunk_index": 82
    },
    {
      "index": 189,
      "chunk_id": "Hallucination_Survey2023_chunk_83",
      "source_id": "Hallucination_Survey2023",
      "text": "Levine, Itay Dalmedigos, Dor Muhlgay, Amnon Shashua, Kevin Leyton-Brown, and Yoav Shoham. 2023. In-Context Retrieval-Augmented Language Models. ArXiv preprint abs/2302.00083 (2023). https://arxiv.org/ abs/2302.00083 [256] Marc'Aurelio Ranzato, Sumit Chopra, Michael Auli, and Wojciech Zaremba. 2016. Sequence Level Training with Recurrent Neural Networks. In 4th International Conference on Learning Representations, ICLR 2016, San Juan, Puerto Rico, May 2-4, 2016, Conference Track Proceedings , Yoshua Bengio and Yann LeCun (Eds.). http://arxiv.org/abs/1511. ACM Transactions on Information Systems, Vol. 1, No. 1, Article 1. Publication date: January 2024. A Survey on Hallucination in Large Language Models: Principles, Taxonomy, Challenges, and Open Questions 1:49 [257] Mathieu Ravaut, Aixin Sun, Nancy F. Chen, and Shafiq Joty. 2024. On Context Utilization in Summarization with Large Language Models. arXiv:2310.10570 [cs.CL] [258] Vipula Rawte, Amit P. Sheth, and Amitava Das. 2023. A Survey of Hallucination in Large Foundation Models. ArXiv preprint abs/2309.05922 (2023). https://arxiv.org/abs/2309.05922 [259] Machel Reid, Nikolay Savinov, Denis Teplyashin, Dmitry Lepikhin, Timothy Lillicrap, Jean baptiste Alayrac, Radu Soricut, Angeliki Lazaridou, Orhan Firat, Julian Schrittwieser, Ioannis Antonoglou, Rohan Anil, Sebastian Borgeaud, Andrew Dai, Katie Millican, Ethan Dyer, Mia Glaese, Thibault Sottiaux, Benjamin Lee, Fabio Viola, Malcolm Reynolds, Yuanzhong Xu, James Molloy, Jilin Chen, Michael Isard, Paul Barham, Tom Hennigan, Ross McIlroy, Melvin Johnson, Johan Schalkwyk, Eli Collins, Eliza Rutherford, Erica Moreira, Kareem Ayoub, Megha Goel, Clemens Meyer, Gregory Thornton, Zhen Yang, Henryk Michalewski, Zaheer Abbas, Nathan Schucher, Ankesh Anand, Richard Ives, James Keeling, Karel Lenc, Salem Haykal, Siamak Shakeri, Pranav Shyam, Aakanksha Chowdhery, Roman Ring, Stephen Spencer, Eren Sezener, Luke Vilnis, Oscar Chang, Nobuyuki Morioka, George Tucker, Ce Zheng, Oliver Woodman, Nithya Attaluri, Tomas Kocisky, Evgenii Eltyshev, Xi Chen, Timothy Chung, Vittorio Selo, Siddhartha Brahma, Petko Georgiev, Ambrose Slone, Zhenkai Zhu, James Lottes, Siyuan Qiao, Ben Caine, Sebastian Riedel, Alex Tomala, Martin Chadwick, Juliette Love, Peter Choy, Sid Mittal, Neil Houlsby, Yunhao Tang, Matthew Lamm, Libin Bai, Qiao Zhang, Luheng He, Yong Cheng, Peter Humphreys, Yujia Li, Sergey Brin, Albin Cassirer, Yingjie Miao, Lukas Zilka, Taylor Tobin, Kelvin Xu, Lev Proleev, Daniel Sohn, Alberto Magni, Lisa Anne Hendricks, Isabel Gao, Santiago Ontañón, Oskar Bunyan, Nathan Byrd, Abhanshu Sharma, Biao Zhang, Mario Pinto, Rishika Sinha, Harsh Mehta, Dawei Jia, Sergi Caelles, Albert Webson, Alex Morris, Becca Roelofs, Yifan Ding, Robin Strudel, Xuehan Xiong, Marvin Ritter, Mostafa Dehghani, Rahma Chaabouni, Abhijit Karmarkar, Guangda Lai, Fabian Mentzer, Bibo Xu, YaGuang Li, Yujing Zhang, Tom Le Paine, Alex Goldin, Behnam Neyshabur, Kate Baumli,",
      "chunk_index": 83
    },
    {
      "index": 190,
      "chunk_id": "Hallucination_Survey2023_chunk_84",
      "source_id": "Hallucination_Survey2023",
      "text": "Caelles, Albert Webson, Alex Morris, Becca Roelofs, Yifan Ding, Robin Strudel, Xuehan Xiong, Marvin Ritter, Mostafa Dehghani, Rahma Chaabouni, Abhijit Karmarkar, Guangda Lai, Fabian Mentzer, Bibo Xu, YaGuang Li, Yujing Zhang, Tom Le Paine, Alex Goldin, Behnam Neyshabur, Kate Baumli, Anselm Levskaya, Michael Laskin, Wenhao Jia, Jack W. Rae, Kefan Xiao, Antoine He, Skye Giordano, Lakshman Yagati, Jean-Baptiste Lespiau, Paul Natsev, Sanjay Ganapathy, Fangyu Liu, Danilo Martins, Nanxin Chen, Yunhan Xu, Megan Barnes, Rhys May, Arpi Vezer, Junhyuk Oh, Ken Franko, Sophie Bridgers, Ruizhe Zhao, Boxi Wu, Basil Mustafa, Sean Sechrist, Emilio Parisotto, Thanumalayan Sankaranarayana Pillai, Chris Larkin, Chenjie Gu, Christina Sorokin, Maxim Krikun, Alexey Guseynov, Jessica Landon, Romina Datta, Alexander Pritzel, Phoebe Thacker, Fan Yang, Kevin Hui, Anja Hauth, Chih-Kuan Yeh, David Barker, Justin Mao-Jones, Sophia Austin, Hannah Sheahan, Parker Schuh, James Svensson, Rohan Jain, Vinay Ramasesh, Anton Briukhov, Da-Woon Chung, Tamara von Glehn, Christina Butterfield, Priya Jhakra, Matthew Wiethoff, Justin Frye, Jordan Grimstad, Beer Changpinyo, Charline Le Lan, Anna Bortsova, Yonghui Wu, Paul Voigtlaender, Tara Sainath, Charlotte Smith, Will Hawkins, Kris Cao, James Besley, Srivatsan Srinivasan, Mark Omernick, Colin Gaffney, Gabriela Surita, Ryan Burnell, Bogdan Damoc, Junwhan Ahn, Andrew Brock, Mantas Pajarskas, Anastasia Petrushkina, Seb Noury, Lorenzo Blanco, Kevin Swersky, Arun Ahuja, Thi Avrahami, Vedant Misra, Raoul de Liedekerke, Mariko Iinuma, Alex Polozov, Sarah York, George van den Driessche, Paul Michel, Justin Chiu, Rory Blevins, Zach Gleicher, Adrià Recasens, Alban Rrustemi, Elena Gribovskaya, Aurko Roy, Wiktor Gworek, Séb Arnold, Lisa Lee, James Lee-Thorp, Marcello Maggioni, Enrique Piqueras, Kartikeya Badola, Sharad Vikram, Lucas Gonzalez, Anirudh Baddepudi, Evan Senter, Jacob Devlin, James Qin, Michael Azzam, Maja Trebacz, Martin Polacek, Kashyap Krishnakumar, Shuo yiin Chang, Matthew Tung, Ivo Penchev, Rishabh Joshi, Kate Olszewska, Carrie Muir, Mateo Wirth, Ale Jakse Hartman, Josh Newlan, Sheleem Kashem, Vijay Bolina, Elahe Dabir, Joost van Amersfoort, Zafarali Ahmed, James Cobon-Kerr, Aishwarya Kamath, Arnar Mar Hrafnkelsson, Le Hou, Ian Mackinnon, Alexandre Frechette, Eric Noland, Xiance Si, Emanuel Taropa, Dong Li, Phil Crone, Anmol Gulati, Sébastien Cevey, Jonas Adler, Ada Ma, David Silver, Simon Tokumine, Richard Powell, Stephan Lee, Michael Chang, Samer Hassan, Diana Mincu, Antoine Yang, Nir Levine, Jenny Brennan, Mingqiu Wang, Sarah Hodkinson, Jeffrey Zhao, Josh Lipschultz, Aedan Pope, Michael B. Chang, Cheng Li, Laurent El Shafey, Michela Paganini, Sholto Douglas, Bernd Bohnet, Fabio Pardo, Seth Odoom, Mihaela Rosca, Cicero Nogueira dos Santos, Kedar Soparkar, Arthur Guez, Tom Hudson, Steven Hansen, Chulayuth Asawaroengchai, Ravi Addanki, Tianhe",
      "chunk_index": 84
    },
    {
      "index": 191,
      "chunk_id": "Hallucination_Survey2023_chunk_85",
      "source_id": "Hallucination_Survey2023",
      "text": "Lipschultz, Aedan Pope, Michael B. Chang, Cheng Li, Laurent El Shafey, Michela Paganini, Sholto Douglas, Bernd Bohnet, Fabio Pardo, Seth Odoom, Mihaela Rosca, Cicero Nogueira dos Santos, Kedar Soparkar, Arthur Guez, Tom Hudson, Steven Hansen, Chulayuth Asawaroengchai, Ravi Addanki, Tianhe Yu, Wojciech Stokowiec, Mina Khan, Justin Gilmer, Jaehoon Lee, Carrie Grimes Bostock, Keran Rong, Jonathan Caton, Pedram Pejman, Filip Pavetic, Geoff Brown, Vivek Sharma, Mario Lučić, Rajkumar Samuel, Josip Djolonga, Amol Mandhane, Lars Lowe Sjösund, Elena Buchatskaya, Elspeth White, Natalie Clay, Jiepu Jiang, Hyeontaek Lim, Ross Hemsley, Jane Labanowski, Nicola De Cao, David Steiner, Sayed Hadi Hashemi, Jacob Austin, Anita Gergely, Tim Blyth, Joe Stanton, Kaushik Shivakumar, Aditya Siddhant, Anders Andreassen, Carlos Araya, Nikhil Sethi, Rakesh Shivanna, Steven Hand, Ankur Bapna, Ali Khodaei, Antoine Miech, Garrett Tanzer, Andy Swing, Shantanu Thakoor, Zhufeng Pan, Zachary Nado, Stephanie Winkler, Dian Yu, Mohammad Saleh, Loren Maggiore, Iain Barr, Minh Giang, Thais Kagohara, Ivo Danihelka, Amit Marathe, Vladimir Feinberg, Mohamed Elhawaty, Nimesh Ghelani, Dan Horgan, Helen Miller, Lexi Walker, Richard Tanburn, Mukarram Tariq, Disha Shrivastava, Fei Xia, Chung-Cheng Chiu, Zoe Ashwood, Khuslen Baatarsukh, Sina Samangooei, Fred Alcober, Axel Stjerngren, Paul Komarek, Katerina Tsihlas, Anudhyan Boral, Ramona Comanescu, Jeremy Chen, Ruibo Liu, Dawn Bloxwich, Charlie Chen, Yanhua Sun, Fangxiaoyu Feng, Matthew Mauger, Xerxes Dotiwalla, Vincent Hellendoorn, Michael Sharman, Ivy Zheng, Krishna Haridasan, Gabe Barth-Maron, Craig Swanson, Dominika ACM Transactions on Information Systems, Vol. 1, No. 1, Article 1. Publication date: January 2024. 1:50 Huang, et al. Rogozińska, Alek Andreev, Paul Kishan Rubenstein, Ruoxin Sang, Dan Hurt, Gamaleldin Elsayed, Renshen Wang, Dave Lacey, Anastasija Ilić, Yao Zhao, Lora Aroyo, Chimezie Iwuanyanwu, Vitaly Nikolaev, Balaji Lakshminarayanan, Sadegh Jazayeri, Raphaël Lopez Kaufman, Mani Varadarajan, Chetan Tekur, Doug Fritz, Misha Khalman, David Reitter, Kingshuk Dasgupta, Shourya Sarcar, Tina Ornduff, Javier Snaider, Fantine Huot, Johnson Jia, Rupert Kemp, Nejc Trdin, Anitha Vijayakumar, Lucy Kim, Christof Angermueller, Li Lao, Tianqi Liu, Haibin Zhang, David Engel, Somer Greene, Anaïs White, Jessica Austin, Lilly Taylor, Shereen Ashraf, Dangyi Liu, Maria Georgaki, Irene Cai, Yana Kulizhskaya, Sonam Goenka, Brennan Saeta, Kiran Vodrahalli, Christian Frank, Dario de Cesare, Brona Robenek, Harry Richardson, Mahmoud Alnahlawi, Christopher Yew, Priya Ponnapalli, Marco Tagliasacchi, Alex Korchemniy, Yelin Kim, Dinghua Li, Bill Rosgen, Zoe Ashwood, Kyle Levin, Jeremy Wiesner, Praseem Banzal, Praveen Srinivasan, Hongkun Yu, Çağlar Ünlü, David Reid, Zora Tung, Daniel Finchelstein, Ravin Kumar, Andre Elisseeff, Jin Huang, Ming Zhang, Rui Zhu, Ricardo Aguilar, Mai Giménez, Jiawei Xia, Olivier",
      "chunk_index": 85
    },
    {
      "index": 192,
      "chunk_id": "Hallucination_Survey2023_chunk_86",
      "source_id": "Hallucination_Survey2023",
      "text": "Li, Bill Rosgen, Zoe Ashwood, Kyle Levin, Jeremy Wiesner, Praseem Banzal, Praveen Srinivasan, Hongkun Yu, Çağlar Ünlü, David Reid, Zora Tung, Daniel Finchelstein, Ravin Kumar, Andre Elisseeff, Jin Huang, Ming Zhang, Rui Zhu, Ricardo Aguilar, Mai Giménez, Jiawei Xia, Olivier Dousse, Willi Gierke, Soheil Hassas Yeganeh, Damion Yates, Komal Jalan, Lu Li, Eri Latorre-Chimoto, Duc Dung Nguyen, Ken Durden, Praveen Kallakuri, Yaxin Liu, Matthew Johnson, Tomy Tsai, Alice Talbert, Jasmine Liu, Alexander Neitz, Chen Elkind, Marco Selvi, Mimi Jasarevic, Livio Baldini Soares, Albert Cui, Pidong Wang, Alek Wenjiao Wang, Xinyu Ye, Krystal Kallarackal, Lucia Loher, Hoi Lam, Josef Broder, Dan Holtmann-Rice, Nina Martin, Bramandia Ramadhana, Daniel Toyama, Mrinal Shukla, Sujoy Basu, Abhi Mohan, Nick Fernando, Noah Fiedel, Kim Paterson, Hui Li, Ankush Garg, Jane Park, DongHyun Choi, Diane Wu, Sankalp Singh, Zhishuai Zhang, Amir Globerson, Lily Yu, John Carpenter, Félix de Chaumont Quitry, Carey Radebaugh, Chu-Cheng Lin, Alex Tudor, Prakash Shroff, Drew Garmon, Dayou Du, Neera Vats, Han Lu, Shariq Iqbal, Alex Yakubovich, Nilesh Tripuraneni, James Manyika, Haroon Qureshi, Nan Hua, Christel Ngani, Maria Abi Raad, Hannah Forbes, Anna Bulanova, Jeff Stanway, Mukund Sundararajan, Victor Ungureanu, Colton Bishop, Yunjie Li, Balaji Venkatraman, Bo Li, Chloe Thornton, Salvatore Scellato, Nishesh Gupta, Yicheng Wang, Ian Tenney, Xihui Wu, Ashish Shenoy, Gabriel Carvajal, Diana Gage Wright, Ben Bariach, Zhuyun Xiao, Peter Hawkins, Sid Dalmia, Clement Farabet, Pedro Valenzuela, Quan Yuan, Chris Welty, Ananth Agarwal, Mia Chen, Wooyeol Kim, Brice Hulse, Nandita Dukkipati, Adam Paszke, Andrew Bolt, Elnaz Davoodi, Kiam Choo, Jennifer Beattie, Jennifer Prendki, Harsha Vashisht, Rebeca Santamaria-Fernandez, Luis C. Cobo, Jarek Wilkiewicz, David Madras, Ali Elqursh, Grant Uy, Kevin Ramirez, Matt Harvey, Tyler Liechty, Heiga Zen, Jeff Seibert, Clara Huiyi Hu, Mohamed Elhawaty, Andrey Khorlin, Maigo Le, Asaf Aharoni, Megan Li, Lily Wang, Sandeep Kumar, Alejandro Lince, Norman Casagrande, Jay Hoover, Dalia El Badawy, David Soergel, Denis Vnukov, Matt Miecnikowski, Jiri Simsa, Anna Koop, Praveen Kumar, Thibault Sellam, Daniel Vlasic, Samira Daruki, Nir Shabat, John Zhang, Guolong Su, Jiageng Zhang, Jeremiah Liu, Yi Sun, Evan Palmer, Alireza Ghaffarkhah, Xi Xiong, Victor Cotruta, Michael Fink, Lucas Dixon, Ashwin Sreevatsa, Adrian Goedeckemeyer, Alek Dimitriev, Mohsen Jafari, Remi Crocker, Nicholas FitzGerald, Aviral Kumar, Sanjay Ghemawat, Ivan Philips, Frederick Liu, Yannie Liang, Rachel Sterneck, Alena Repina, Marcus Wu, Laura Knight, Marin Georgiev, Hyo Lee, Harry Askham, Abhishek Chakladar, Annie Louis, Carl Crous, Hardie Cate, Dessie Petrova, Michael Quinn, Denese Owusu-Afriyie, Achintya Singhal, Nan Wei, Solomon Kim,",
      "chunk_index": 86
    },
    {
      "index": 193,
      "chunk_id": "Hallucination_Survey2023_chunk_87",
      "source_id": "Hallucination_Survey2023",
      "text": "Ivan Philips, Frederick Liu, Yannie Liang, Rachel Sterneck, Alena Repina, Marcus Wu, Laura Knight, Marin Georgiev, Hyo Lee, Harry Askham, Abhishek Chakladar, Annie Louis, Carl Crous, Hardie Cate, Dessie Petrova, Michael Quinn, Denese Owusu-Afriyie, Achintya Singhal, Nan Wei, Solomon Kim, Damien Vincent, Milad Nasr, Christopher A. Choquette-Choo, Reiko Tojo, Shawn Lu, Diego de Las Casas, Yuchung Cheng, Tolga Bolukbasi, Katherine Lee, Saaber Fatehi, Rajagopal Ananthanarayanan, Miteyan Patel, Charbel Kaed, Jing Li, Jakub Sygnowski, Shreyas Rammohan Belle, Zhe Chen, Jaclyn Konzelmann, Siim Põder, Roopal Garg, Vinod Koverkathu, Adam Brown, Chris Dyer, Rosanne Liu, Azade Nova, Jun Xu, Slav Petrov, Demis Hassabis, Koray Kavukcuoglu, Jeffrey Dean, and Oriol Vinyals. 2024. Gemini 1.5: Unlocking multimodal understanding across millions of tokens of context. arXiv:2403.05530 [cs.CL] [260] Ruiyang Ren, Yuhao Wang, Yingqi Qu, Wayne Xin Zhao, Jing Liu, Hao Tian, Hua Wu, Ji-Rong Wen, and Haifeng Wang. 2023. Investigating the Factual Knowledge Boundary of Large Language Models with Retrieval Augmentation. ArXiv preprint abs/2307.11019 (2023). https://arxiv.org/abs/2307.11019 [261] Ruiyang Ren, Yuhao Wang, Yingqi Qu, Wayne Xin Zhao, Jing Liu, Hao Tian, Hua Wu, Ji-Rong Wen, and Haifeng Wang. 2023. Investigating the factual knowledge boundary of large language models with retrieval augmentation. ArXiv preprint abs/2307.11019 (2023). https://arxiv.org/abs/2307.11019 [262] Reuters. 2023. U.S. Copyright Office says some AI-assisted works may be copyrighted. https://www.reuters.com/world/ us/us-copyright-office-says-some-ai-assisted-works-may-be-copyrighted-2023-03-15/ [263] Nina Rimsky. 2023. Modulating sycophancy in an RLHF model via activation steering . https://www.alignmentforum. org/posts/zt6hRsDE84HeBKh7E/reducing-sycophancy-and-improving-honesty-via-activation [264] Nina Rimsky. 2023. Reducing sycophancy and improving honesty via activation steering . https://www.alignmentforum. org/posts/zt6hRsDE84HeBKh7E/reducing-sycophancy-and-improving-honesty-via-activation [265] Vinu Sankar Sadasivan, Aounon Kumar, Sriram Balasubramanian, Wenxiao Wang, and Soheil Feizi. 2023. Can AI-Generated Text be Reliably Detected? CoRR abs/2303.11156 (2023). https://doi.org/10.48550/ARXIV.2303.11156 arXiv:2303.11156 [266] Sashank Santhanam, Behnam Hedayatnia, Spandana Gella, Aishwarya Padmakumar, Seokhwan Kim, Yang Liu, and Dilek Hakkani-Tur. 2021. Rome was built in 1776: A case study on factual correctness in knowledge-grounded response generation. ArXiv preprint abs/2110.05456 (2021). https://arxiv.org/abs/2110.05456 ACM Transactions on Information Systems, Vol. 1, No. 1, Article 1. Publication date: January 2024. A Survey on Hallucination in Large Language Models: Principles, Taxonomy, Challenges, and Open Questions 1:51 [267] Parth Sarthi, Salman Abdullah, Aditi Tuli, Shubh Khanna, Anna Goldie, and Christopher D. Manning. 2024. RAPTOR: Recursive Abstractive Processing for Tree-Organized Retrieval. CoRR abs/2401.18059 (2024). https://doi.org/10.48550/ ARXIV.2401.18059 arXiv:2401.18059 [268] William Saunders, Catherine Yeh, Jeff Wu, Steven Bills, Long Ouyang, Jonathan Ward, and Jan Leike. 2022. Self- critiquing models for assisting human evaluators. ArXiv preprint abs/2206.05802 (2022). https://arxiv.org/abs/2206. [269] John Schulman. 2023. Reinforcement",
      "chunk_index": 87
    },
    {
      "index": 194,
      "chunk_id": "Hallucination_Survey2023_chunk_88",
      "source_id": "Hallucination_Survey2023",
      "text": "CoRR abs/2401.18059 (2024). https://doi.org/10.48550/ ARXIV.2401.18059 arXiv:2401.18059 [268] William Saunders, Catherine Yeh, Jeff Wu, Steven Bills, Long Ouyang, Jonathan Ward, and Jan Leike. 2022. Self- critiquing models for assisting human evaluators. ArXiv preprint abs/2206.05802 (2022). https://arxiv.org/abs/2206. [269] John Schulman. 2023. Reinforcement Learning from Human Feedback: Progress and Challenges . Berkeley EECS. https://www.youtube.com/watch?v=hhiLw5Q_UFg [270] John Schulman, Filip Wolski, Prafulla Dhariwal, Alec Radford, and Oleg Klimov. 2017. Proximal Policy Optimization Algorithms. ArXiv preprint abs/1707.06347 (2017). https://arxiv.org/abs/1707.06347 [271] Thomas Scialom, Paul-Alexis Dray, Sylvain Lamprier, Benjamin Piwowarski, Jacopo Staiano, Alex Wang, and Patrick Gallinari. 2021. QuestEval: Summarization Asks for Fact-based Evaluation. In Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing . Association for Computational Linguistics, Online and Punta Cana, Dominican Republic, 6594-6604. https://doi.org/10.18653/v1/2021.emnlp-main.529 [272] Yijia Shao, Yucheng Jiang, Theodore A. Kanell, Peter Xu, Omar Khattab, and Monica S. Lam. 2024. Assisting in Writing Wikipedia-like Articles From Scratch with Large Language Models. CoRR abs/2402.14207 (2024). https: //doi.org/10.48550/ARXIV.2402.14207 arXiv:2402.14207 [273] Zhihong Shao, Yeyun Gong, Yelong Shen, Minlie Huang, Nan Duan, and Weizhu Chen. 2023. Enhancing Retrieval- Augmented Large Language Models with Iterative Retrieval-Generation Synergy. ArXiv preprint abs/2305.15294 (2023). https://arxiv.org/abs/2305.15294 [274] Mrinank Sharma, Meg Tong, Tomasz Korbak, David Duvenaud, Amanda Askell, Samuel R. Bowman, Newton Cheng, Esin Durmus, Zac Hatfield-Dodds, Scott R. Johnston, Shauna Kravec, Timothy Maxwell, Sam McCandlish, Kamal Ndousse, Oliver Rausch, Nicholas Schiefer, Da Yan, Miranda Zhang, and Ethan Perez. 2023. Towards Understanding Sycophancy in Language Models. ArXiv preprint abs/2310.13548 (2023). https://arxiv.org/abs/2310.13548 [275] Weijia Shi, Xiaochuang Han, Mike Lewis, Yulia Tsvetkov, Luke Zettlemoyer, and Scott Wen-tau Yih. 2023. Trusting Your Evidence: Hallucinate Less with Context-aware Decoding. ArXiv preprint abs/2305.14739 (2023). https: //arxiv.org/abs/2305.14739 [276] Weijia Shi, Sewon Min, Maria Lomeli, Chunting Zhou, Margaret Li, Xi Victoria Lin, Noah A. Smith, Luke Zettlemoyer, Scott Yih, and Mike Lewis. 2023. In-Context Pretraining: Language Modeling Beyond Document Boundaries. ArXiv preprint abs/2310.10638 (2023). https://arxiv.org/abs/2310.10638 [277] Weijia Shi, Sewon Min, Michihiro Yasunaga, Minjoon Seo, Rich James, Mike Lewis, Luke Zettlemoyer, and Wen- tau Yih. 2023. REPLUG: Retrieval-Augmented Black-Box Language Models. CoRR abs/2301.12652 (2023). https: //doi.org/10.48550/ARXIV.2301.12652 arXiv:2301.12652 [278] Kurt Shuster, Spencer Poff, Moya Chen, Douwe Kiela, and Jason Weston. 2021. Retrieval Augmentation Reduces Hallucination in Conversation. InFindings of the Association for Computational Linguistics: EMNLP 2021 . Association for Computational Linguistics, Punta Cana, Dominican Republic, 3784-3803. https://doi.org/10.18653/v1/2021.findings- emnlp.320 [279] Karan Singhal, Tao Tu, Juraj Gottweis, Rory Sayres, Ellery Wulczyn, Le Hou, Kevin Clark, Stephen Pfohl, Heather Cole-Lewis, Darlene Neal, Mike",
      "chunk_index": 88
    },
    {
      "index": 195,
      "chunk_id": "Hallucination_Survey2023_chunk_89",
      "source_id": "Hallucination_Survey2023",
      "text": "Association for Computational Linguistics: EMNLP 2021 . Association for Computational Linguistics, Punta Cana, Dominican Republic, 3784-3803. https://doi.org/10.18653/v1/2021.findings- emnlp.320 [279] Karan Singhal, Tao Tu, Juraj Gottweis, Rory Sayres, Ellery Wulczyn, Le Hou, Kevin Clark, Stephen Pfohl, Heather Cole-Lewis, Darlene Neal, Mike Schaekermann, Amy Wang, Mohamed Amin, Sami Lachgar, Philip Andrew Mansfield, Sushant Prakash, Bradley Green, Ewa Dominowska, Blaise Agüera y Arcas, Nenad Tomasev, Yun Liu, Renee Wong, Christopher Semturs, S. Sara Mahdavi, Joelle K. Barral, Dale R. Webster, Gregory S. Corrado, Yossi Matias, Shekoofeh Azizi, Alan Karthikesalingam, and Vivek Natarajan. 2023. Towards Expert-Level Medical Question Answering with Large Language Models. ArXiv preprint abs/2305.09617 (2023). https://arxiv.org/abs/2305.09617 [280] Anton Sinitsin, Vsevolod Plokhotnyuk, Dmitriy Pyrkin, Sergei Popov, and Artem Babenko. 2020. Editable Neural Networks. In 8th International Conference on Learning Representations, ICLR 2020, Addis Ababa, Ethiopia, April 26-30, 2020. OpenReview.net. https://openreview.net/forum?id=HJedXaEtvS [281] Aviv Slobodkin, Omer Goldman, Avi Caciularu, Ido Dagan, and Shauli Ravfogel. 2023. The Curious Case of Hallucina- tory Unanswerablity: Finding Truths in the Hidden States of Over-Confident Large Language Models. ArXiv preprint abs/2310.11877 (2023). https://arxiv.org/abs/2310.11877 [282] Aviv Slobodkin, Eran Hirsch, Arie Cattan, Tal Schuster, and Ido Dagan. 2024. Attribute First, then Generate: Locally- attributable Grounded Text Generation. CoRR abs/2403.17104 (2024). https://doi.org/10.48550/ARXIV.2403.17104 arXiv:2403.17104 [283] Felix Stahlberg and Bill Byrne. 2019. On NMT Search Errors and Model Errors: Cat Got Your Tongue?. In Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP) . Association for Computational Linguistics, Hong Kong, China, 3356-3362. https://doi.org/10.18653/v1/D19-1331 ACM Transactions on Information Systems, Vol. 1, No. 1, Article 1. Publication date: January 2024. 1:52 Huang, et al. [284] Ivan Stelmakh, Yi Luan, Bhuwan Dhingra, and Ming-Wei Chang. 2022. ASQA: Factoid Questions Meet Long-Form Answers. In Proceedings of the 2022 Conference on Empirical Methods in Natural Language Processing . Association for Computational Linguistics, Abu Dhabi, United Arab Emirates, 8273-8288. https://aclanthology.org/2022.emnlp- main.566 [285] Nisan Stiennon, Long Ouyang, Jeffrey Wu, Daniel M. Ziegler, Ryan Lowe, Chelsea Voss, Alec Radford, Dario Amodei, and Paul F. Christiano. 2020. Learning to summarize with human feedback. In Advances in Neural Information Processing Systems 33: Annual Conference on Neural Information Processing Systems 2020, NeurIPS 2020, December 6-12, 2020, virtual , Hugo Larochelle, Marc'Aurelio Ranzato, Raia Hadsell, Maria-Florina Balcan, and Hsuan-Tien Lin (Eds.). https://proceedings.neurips.cc/paper/2020/hash/1f89885d556929e98d3ef9b86448f951-Abstract.html [286] Hongjin Su, Howard Yen, Mengzhou Xia, Weijia Shi, Niklas Muennighoff, Han yu Wang, Haisu Liu, Quan Shi, Zachary S. Siegel,",
      "chunk_index": 89
    },
    {
      "index": 196,
      "chunk_id": "Hallucination_Survey2023_chunk_90",
      "source_id": "Hallucination_Survey2023",
      "text": "2020, December 6-12, 2020, virtual , Hugo Larochelle, Marc'Aurelio Ranzato, Raia Hadsell, Maria-Florina Balcan, and Hsuan-Tien Lin (Eds.). https://proceedings.neurips.cc/paper/2020/hash/1f89885d556929e98d3ef9b86448f951-Abstract.html [286] Hongjin Su, Howard Yen, Mengzhou Xia, Weijia Shi, Niklas Muennighoff, Han yu Wang, Haisu Liu, Quan Shi, Zachary S. Siegel, Michael Tang, Ruoxi Sun, Jinsung Yoon, Sercan O. Arik, Danqi Chen, and Tao Yu. 2024. BRIGHT: A Realistic and Challenging Benchmark for Reasoning-Intensive Retrieval. arXiv:2407.12883 [cs.CL] https://arxiv.org/abs/2407.12883 [287] Jianlin Su, Yu Lu, Shengfeng Pan, Bo Wen, and Yunfeng Liu. 2021. RoFormer: Enhanced Transformer with Rotary Position Embedding. CoRR abs/2104.09864 (2021). arXiv:2104.09864 https://arxiv.org/abs/2104.09864 [288] Weihang Su, Yichen Tang, Qingyao Ai, Zhijing Wu, and Yiqun Liu. 2024. DRAGIN: Dynamic Retrieval Augmented Generation based on the Real-time Information Needs of Large Language Models. CoRR abs/2403.10081 (2024). https://doi.org/10.48550/ARXIV.2403.10081 arXiv:2403.10081 [289] Nishant Subramani, Nivedita Suresh, and Matthew Peters. 2022. Extracting Latent Steering Vectors from Pretrained Language Models. InFindings of the Association for Computational Linguistics: ACL 2022 . Association for Computational Linguistics, Dublin, Ireland, 566-581. https://doi.org/10.18653/v1/2022.findings-acl.48 [290] Kai Sun, Yifan Ethan Xu, Hanwen Zha, Yue Liu, and Xin Luna Dong. 2023. Head-to-Tail: How Knowledgeable are Large Language Models (LLM)? A.K.A. Will LLMs Replace Knowledge Graphs? CoRR abs/2308.10168 (2023). https://doi.org/10.48550/ARXIV.2308.10168 arXiv:2308.10168 [291] Ilya Sutskever. 2023. An observation on Generalization . Youtube. https://www.youtube.com/watch?v=AKMuA_ TVz3A&t=5s [292] Chenmien Tan, Ge Zhang, and Jie Fu. 2024. Massive Editing for Large Language Model via Meta Learning. In The Twelfth International Conference on Learning Representations . https://openreview.net/forum?id=L6L1CJQ2PE [293] Hexiang Tan, Fei Sun, Wanli Yang, Yuanzhuo Wang, Qi Cao, and Xueqi Cheng. 2024. Blinded by Generated Contexts: How Language Models Merge Generated and Retrieved Contexts for Open-Domain QA? CoRR abs/2401.11911 (2024). https://doi.org/10.48550/ARXIV.2401.11911 arXiv:2401.11911 [294] Raphael Tang, Xinyu Zhang, Xueguang Ma, Jimmy Lin, and Ferhan Ture. 2023. Found in the Middle: Permutation Self-Consistency Improves Listwise Ranking in Large Language Models. CoRR abs/2310.07712 (2023). https: //doi.org/10.48550/ARXIV.2310.07712 arXiv:2310.07712 [295] Nandan Thakur, Nils Reimers, Andreas Rücklé, Abhishek Srivastava, and Iryna Gurevych. 2021. BEIR: A Heterogenous Benchmark for Zero-shot Evaluation of Information Retrieval Models. CoRR abs/2104.08663 (2021). arXiv:2104.08663 https://arxiv.org/abs/2104.08663 [296] Ran Tian, Shashi Narayan, Thibault Sellam, and Ankur P Parikh. 2019. Sticking to the facts: Confident decoding for faithful data-to-text generation. ArXiv preprint abs/1910.08684 (2019). https://arxiv.org/abs/1910.08684 [297] Shengbang Tong, Zhuang Liu, Yuexiang Zhai, Yi Ma, Yann LeCun, and Saining Xie. 2024. Eyes wide shut? exploring the visual shortcomings of multimodal llms. arXiv preprint arXiv:2401.06209 (2024). [298] S. M. Towhidul Islam Tonmoy, S. M. Mehedi Zaman, Vinija Jain, Anku Rani,",
      "chunk_index": 90
    },
    {
      "index": 197,
      "chunk_id": "Hallucination_Survey2023_chunk_91",
      "source_id": "Hallucination_Survey2023",
      "text": "Zhuang Liu, Yuexiang Zhai, Yi Ma, Yann LeCun, and Saining Xie. 2024. Eyes wide shut? exploring the visual shortcomings of multimodal llms. arXiv preprint arXiv:2401.06209 (2024). [298] S. M. Towhidul Islam Tonmoy, S. M. Mehedi Zaman, Vinija Jain, Anku Rani, Vipula Rawte, Aman Chadha, and Amitava Das. 2024. A Comprehensive Survey of Hallucination Mitigation Techniques in Large Language Models. CoRR abs/2401.01313 (2024). https://doi.org/10.48550/ARXIV.2401.01313 arXiv:2401.01313 [299] Hugo Touvron, Thibaut Lavril, Gautier Izacard, Xavier Martinet, Marie-Anne Lachaux, Timothée Lacroix, Baptiste Rozière, Naman Goyal, Eric Hambro, Faisal Azhar, Aurélien Rodriguez, Armand Joulin, Edouard Grave, and Guillaume Lample. 2023. LLaMA: Open and Efficient Foundation Language Models. CoRR abs/2302.13971 (2023). https: //doi.org/10.48550/ARXIV.2302.13971 arXiv:2302.13971 [300] Hugo Touvron, Louis Martin, Kevin Stone, Peter Albert, Amjad Almahairi, Yasmine Babaei, Nikolay Bashlykov, Soumya Batra, Prajjwal Bhargava, Shruti Bhosale, Dan Bikel, Lukas Blecher, Cristian Canton-Ferrer, Moya Chen, Guillem Cucurull, David Esiobu, Jude Fernandes, Jeremy Fu, Wenyin Fu, Brian Fuller, Cynthia Gao, Vedanuj Goswami, Naman Goyal, Anthony Hartshorn, Saghar Hosseini, Rui Hou, Hakan Inan, Marcin Kardas, Viktor Kerkez, Madian Khabsa, Isabel Kloumann, Artem Korenev, Punit Singh Koura, Marie-Anne Lachaux, Thibaut Lavril, Jenya Lee, Diana Liskovich, Yinghai Lu, Yuning Mao, Xavier Martinet, Todor Mihaylov, Pushkar Mishra, Igor Molybog, Yixin Nie, Andrew Poulton, Jeremy Reizenstein, Rashi Rungta, Kalyan Saladi, Alan Schelten, Ruan Silva, Eric Michael Smith, Ranjan Subramanian, Xiaoqing Ellen Tan, Binh Tang, Ross Taylor, Adina Williams, Jian Xiang Kuan, Puxin Xu, Zheng Yan, Iliyan Zarov, Yuchen Zhang, Angela Fan, Melanie Kambadur, Sharan Narang, Aurélien Rodriguez, Robert Stojnic, Sergey Edunov, and Thomas Scialom. 2023. Llama 2: Open Foundation and Fine-Tuned Chat Models. ArXiv preprint ACM Transactions on Information Systems, Vol. 1, No. 1, Article 1. Publication date: January 2024. A Survey on Hallucination in Large Language Models: Principles, Taxonomy, Challenges, and Open Questions 1:53 abs/2307.09288 (2023). https://arxiv.org/abs/2307.09288 [301] Harsh Trivedi, Niranjan Balasubramanian, Tushar Khot, and Ashish Sabharwal. 2023. Interleaving Retrieval with Chain-of-Thought Reasoning for Knowledge-Intensive Multi-Step Questions. In Proceedings of the 61st Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), ACL 2023, Toronto, Canada, July 9-14, 2023, Anna Rogers, Jordan L. Boyd-Graber, and Naoaki Okazaki (Eds.). Association for Computational Linguistics, 10014-10037. https://aclanthology.org/2023.acl-long.557 [302] Miles Turpin, Julian Michael, Ethan Perez, and Samuel R. Bowman. 2023. Language Models Don't Always Say What They Think: Unfaithful Explanations in Chain-of-Thought Prompting. ArXiv preprint abs/2305.04388 (2023). https://arxiv.org/abs/2305.04388 [303] Logesh Kumar Umapathi, Ankit Pal, and Malaikannan Sankarasubbu. 2023. Med-halt: Medical domain hallucination test for large",
      "chunk_index": 91
    },
    {
      "index": 198,
      "chunk_id": "Hallucination_Survey2023_chunk_92",
      "source_id": "Hallucination_Survey2023",
      "text": "and Samuel R. Bowman. 2023. Language Models Don't Always Say What They Think: Unfaithful Explanations in Chain-of-Thought Prompting. ArXiv preprint abs/2305.04388 (2023). https://arxiv.org/abs/2305.04388 [303] Logesh Kumar Umapathi, Ankit Pal, and Malaikannan Sankarasubbu. 2023. Med-halt: Medical domain hallucination test for large language models. ArXiv preprint abs/2307.15343 (2023). https://arxiv.org/abs/2307.15343 [304] Aäron van den Oord, Yazhe Li, and Oriol Vinyals. 2018. Representation Learning with Contrastive Predictive Coding. CoRR abs/1807.03748 (2018). arXiv:1807.03748 http://arxiv.org/abs/1807.03748 [305] Liam van der Poel, Ryan Cotterell, and Clara Meister. 2022. Mutual Information Alleviates Hallucinations in Abstractive Summarization. In Proceedings of the 2022 Conference on Empirical Methods in Natural Language Processing . Association for Computational Linguistics, Abu Dhabi, United Arab Emirates, 5956-5965. https://aclanthology.org/2022.emnlp- main.399 [306] Neeraj Varshney, Wenlin Yao, Hongming Zhang, Jianshu Chen, and Dong Yu. 2023. A Stitch in Time Saves Nine: Detect- ing and Mitigating Hallucinations of LLMs by Validating Low-Confidence Generation. ArXiv preprint abs/2307.03987 (2023). https://arxiv.org/abs/2307.03987 [307] Elena Voita, David Talbot, Fedor Moiseev, Rico Sennrich, and Ivan Titov. 2019. Analyzing Multi-Head Self-Attention: Specialized Heads Do the Heavy Lifting, the Rest Can Be Pruned. In Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics . Association for Computational Linguistics, Florence, Italy, 5797-5808. https://doi.org/10.18653/v1/P19-1580 [308] Tu Vu, Mohit Iyyer, Xuezhi Wang, Noah Constant, Jerry Wei, Jason Wei, Chris Tar, Yun-Hsuan Sung, Denny Zhou, Quoc Le, and Thang Luong. 2023. FreshLLMs: Refreshing Large Language Models with Search Engine Augmentation. arXiv:2310.03214 [cs.CL] [309] David Wan, Mengwen Liu, Kathleen McKeown, Markus Dreyer, and Mohit Bansal. 2023. Faithfulness-Aware Decoding Strategies for Abstractive Summarization. In Proceedings of the 17th Conference of the European Chapter of the Association for Computational Linguistics . Association for Computational Linguistics, Dubrovnik, Croatia, 2864-2880. https://aclanthology.org/2023.eacl-main.210 [310] Alex Wang, Kyunghyun Cho, and Mike Lewis. 2020. Asking and Answering Questions to Evaluate the Factual Consistency of Summaries. In Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics . Association for Computational Linguistics, Online, 5008-5020. https://doi.org/10.18653/v1/2020.acl-main.450 [311] Binjie Wang, Ethan Chern, and Pengfei Liu. 2023. ChineseFactEval: A Factuality Benchmark for Chinese LLMs. [312] Cunxiang Wang, Xiaoze Liu, Yuanhao Yue, Xiangru Tang, Tianhang Zhang, Jiayang Cheng, Yunzhi Yao, Wenyang Gao, Xuming Hu, Zehan Qi, Yidong Wang, Linyi Yang, Jindong Wang, Xing Xie, Zheng Zhang, and Yue Zhang. 2023. Survey on Factuality in Large Language Models: Knowledge, Retrieval and Domain-Specificity. ArXiv preprint abs/2310.07521 (2023). https://arxiv.org/abs/2310.07521 [313] Chaojun Wang and Rico Sennrich. 2020. On Exposure Bias, Hallucination and Domain Shift in",
      "chunk_index": 92
    },
    {
      "index": 199,
      "chunk_id": "Hallucination_Survey2023_chunk_93",
      "source_id": "Hallucination_Survey2023",
      "text": "Wang, Xing Xie, Zheng Zhang, and Yue Zhang. 2023. Survey on Factuality in Large Language Models: Knowledge, Retrieval and Domain-Specificity. ArXiv preprint abs/2310.07521 (2023). https://arxiv.org/abs/2310.07521 [313] Chaojun Wang and Rico Sennrich. 2020. On Exposure Bias, Hallucination and Domain Shift in Neural Machine Translation. In Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics . Association for Computational Linguistics, Online, 3544-3552. https://doi.org/10.18653/v1/2020.acl-main.326 [314] Jiaan Wang, Yunlong Liang, Fandong Meng, Haoxiang Shi, Zhixu Li, Jinan Xu, Jianfeng Qu, and Jie Zhou. 2023. Is chatgpt a good nlg evaluator? a preliminary study. ArXiv preprint abs/2303.04048 (2023). https://arxiv.org/abs/2303. [315] Junyang Wang, Yuhang Wang, Guohai Xu, Jing Zhang, Yukai Gu, Haitao Jia, Ming Yan, Ji Zhang, and Jitao Sang. 2023. An llm-free multi-dimensional benchmark for mllms hallucination evaluation. arXiv preprint arXiv:2311.07397 (2023). [316] Lei Wang, Jiabang He, Shenshen Li, Ning Liu, and Ee-Peng Lim. 2024. Mitigating fine-grained hallucination by fine-tuning large vision-language models with caption rewrites. In International Conference on Multimedia Modeling . Springer, 32-45. [317] Liang Wang, Nan Yang, and Furu Wei. 2023. Query2doc: Query Expansion with Large Language Models. InProceedings of the 2023 Conference on Empirical Methods in Natural Language Processing, EMNLP 2023, Singapore, December 6-10, 2023, Houda Bouamor, Juan Pino, and Kalika Bali (Eds.). Association for Computational Linguistics, 9414-9423. https://doi.org/10.18653/V1/2023.EMNLP-MAIN.585 [318] Peifeng Wang, Zhengyang Wang, Zheng Li, Yifan Gao, Bing Yin, and Xiang Ren. 2023. SCOTT: Self-consistent chain-of-thought distillation. ArXiv preprint abs/2305.01879 (2023). https://arxiv.org/abs/2305.01879 ACM Transactions on Information Systems, Vol. 1, No. 1, Article 1. Publication date: January 2024. 1:54 Huang, et al. [319] Shuting Wang, Xin Yu, Mang Wang, Weipeng Chen, Yutao Zhu, and Zhicheng Dou. 2024. RichRAG: Crafting Rich Responses for Multi-faceted Queries in Retrieval-Augmented Generation. CoRR abs/2406.12566 (2024). https: //doi.org/10.48550/ARXIV.2406.12566 arXiv:2406.12566 [320] Song Wang, Yaochen Zhu, Haochen Liu, Zaiyi Zheng, Chen Chen, and Jundong Li. 2023. Knowledge Editing for Large Language Models: A Survey. arXiv:2310.16218 [cs.CL] [321] Yile Wang, Peng Li, Maosong Sun, and Yang Liu. 2023. Self-Knowledge Guided Retrieval Augmentation for Large Language Models. In Findings of the Association for Computational Linguistics: EMNLP 2023, Singapore, December 6-10, 2023, Houda Bouamor, Juan Pino, and Kalika Bali (Eds.). Association for Computational Linguistics, 10303-10315. https://doi.org/10.18653/V1/2023.FINDINGS-EMNLP.691 [322] Yufei Wang, Wanjun Zhong, Liangyou Li, Fei Mi, Xingshan Zeng, Wenyong Huang, Lifeng Shang, Xin Jiang, and Qun Liu. 2023. Aligning Large Language Models with Human: A Survey. CoRR abs/2307.12966 (2023). https: //doi.org/10.48550/ARXIV.2307.12966 arXiv:2307.12966 [323] Zhiruo Wang, Jun Araki, Zhengbao Jiang, Md.",
      "chunk_index": 93
    },
    {
      "index": 200,
      "chunk_id": "Hallucination_Survey2023_chunk_94",
      "source_id": "Hallucination_Survey2023",
      "text": "Wanjun Zhong, Liangyou Li, Fei Mi, Xingshan Zeng, Wenyong Huang, Lifeng Shang, Xin Jiang, and Qun Liu. 2023. Aligning Large Language Models with Human: A Survey. CoRR abs/2307.12966 (2023). https: //doi.org/10.48550/ARXIV.2307.12966 arXiv:2307.12966 [323] Zhiruo Wang, Jun Araki, Zhengbao Jiang, Md. Rizwan Parvez, and Graham Neubig. 2023. Learning to Filter Context for Retrieval-Augmented Generation. CoRR abs/2311.08377 (2023). https://doi.org/10.48550/ARXIV.2311.08377 arXiv:2311.08377 [324] Zhenyi Wang, Xiaoyang Wang, Bang An, Dong Yu, and Changyou Chen. 2020. Towards Faithful Neural Table-to-Text Generation with Content-Matching Constraints. In Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics . Association for Computational Linguistics, Online, 1072-1086. https://doi.org/10.18653/ v1/2020.acl-main.101 [325] Zirui Wang, Jiahui Yu, Adams Wei Yu, Zihang Dai, Yulia Tsvetkov, and Yuan Cao. 2022. SimVLM: Simple Visual Language Model Pretraining with Weak Supervision. InThe Tenth International Conference on Learning Representations, ICLR 2022, Virtual Event, April 25-29, 2022 . OpenReview.net. https://openreview.net/forum?id=GUrhfTuf_3 [326] Jason Wei, Xuezhi Wang, Dale Schuurmans, Maarten Bosma, Fei Xia, Ed Chi, Quoc V Le, Denny Zhou, et al. 2022. Chain-of-thought prompting elicits reasoning in large language models. Advances in Neural Information Processing Systems 35 (2022), 24824-24837. [327] Jerry W. Wei, Da Huang, Yifeng Lu, Denny Zhou, and Quoc V. Le. 2023. Simple synthetic data reduces sycophancy in large language models. ArXiv preprint abs/2308.03958 (2023). https://arxiv.org/abs/2308.03958 [328] Laura Weidinger, John Mellor, Maribeth Rauh, Conor Griffin, Jonathan Uesato, Po-Sen Huang, Myra Cheng, Mia Glaese, Borja Balle, Atoosa Kasirzadeh, Zac Kenton, Sasha Brown, Will Hawkins, Tom Stepleton, Courtney Biles, Abeba Birhane, Julia Haas, Laura Rimell, Lisa Anne Hendricks, William Isaac, Sean Legassick, Geoffrey Irving, and Iason Gabriel. 2021. Ethical and social risks of harm from Language Models. ArXiv preprint abs/2112.04359 (2021). https://arxiv.org/abs/2112.04359 [329] Yilin Wen, Zifeng Wang, and Jimeng Sun. 2023. MindMap: Knowledge Graph Prompting Sparks Graph of Thoughts in Large Language Models. ArXiv preprint abs/2308.09729 (2023). https://arxiv.org/abs/2308.09729 [330] Di Wu, Jia-Chen Gu, Fan Yin, Nanyun Peng, and Kai-Wei Chang. 2024. Synchronous Faithfulness Monitoring for Trustworthy Retrieval-Augmented Generation. CoRR abs/2406.13692 (2024). https://doi.org/10.48550/ARXIV.2406. 13692 arXiv:2406.13692 [331] Kevin Wu, Eric Wu, and James Zou. 2024. ClashEval: Quantifying the tug-of-war between an LLM's internal prior and external evidence. arXiv:2404.10198 [cs.CL] https://arxiv.org/abs/2404.10198 [332] Shitao Xiao, Zheng Liu, Peitian Zhang, and Niklas Muennighof. 2023. C-Pack: Packaged Resources To Advance General Chinese Embedding. CoRR abs/2309.07597 (2023). https://doi.org/10.48550/ARXIV.2309.07597 arXiv:2309.07597 [333] Yijun Xiao and William Yang Wang. 2021. On Hallucination and Predictive Uncertainty in Conditional Language Generation. In Proceedings of the 16th Conference of the European",
      "chunk_index": 94
    },
    {
      "index": 201,
      "chunk_id": "Hallucination_Survey2023_chunk_95",
      "source_id": "Hallucination_Survey2023",
      "text": "2023. C-Pack: Packaged Resources To Advance General Chinese Embedding. CoRR abs/2309.07597 (2023). https://doi.org/10.48550/ARXIV.2309.07597 arXiv:2309.07597 [333] Yijun Xiao and William Yang Wang. 2021. On Hallucination and Predictive Uncertainty in Conditional Language Generation. In Proceedings of the 16th Conference of the European Chapter of the Association for Computational Linguistics: Main Volume . Association for Computational Linguistics, Online, 2734-2744. https://doi.org/10.18653/v1/ 2021.eacl-main.236 [334] Jian Xie, Kai Zhang, Jiangjie Chen, Renze Lou, and Yu Su. 2023. Adaptive Chameleon or Stubborn Sloth: Unraveling the Behavior of Large Language Models in Knowledge Clashes. ArXiv preprint abs/2305.13300 (2023). https: //arxiv.org/abs/2305.13300 [335] Miao Xiong, Zhiyuan Hu, Xinyang Lu, Yifei Li, Jie Fu, Junxian He, and Bryan Hooi. 2023. Can LLMs Express Their Uncertainty? An Empirical Evaluation of Confidence Elicitation in LLMs. ArXiv preprint abs/2306.13063 (2023). https://arxiv.org/abs/2306.13063 [336] Fangyuan Xu, Weijia Shi, and Eunsol Choi. 2023. RECOMP: Improving Retrieval-Augmented LMs with Compression and Selective Augmentation. ArXiv preprint abs/2310.04408 (2023). https://arxiv.org/abs/2310.04408 [337] Jiacheng Xu, Shrey Desai, and Greg Durrett. 2020. Understanding Neural Abstractive Summarization Models via Uncertainty. In Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP) . Association for Computational Linguistics, Online, 6275-6281. https://doi.org/10.18653/v1/2020.emnlp-main.508 ACM Transactions on Information Systems, Vol. 1, No. 1, Article 1. Publication date: January 2024. A Survey on Hallucination in Large Language Models: Principles, Taxonomy, Challenges, and Open Questions 1:55 [338] Jundong Xu, Hao Fei, Liangming Pan, Qian Liu, Mong-Li Lee, and Wynne Hsu. 2024. Faithful Logical Reason- ing via Symbolic Chain-of-Thought. CoRR abs/2405.18357 (2024). https://doi.org/10.48550/ARXIV.2405.18357 arXiv:2405.18357 [339] Shicheng Xu, Danyang Hou, Liang Pang, Jingcheng Deng, Jun Xu, Huawei Shen, and Xueqi Cheng. 2023. AI- Generated Images Introduce Invisible Relevance Bias to Text-Image Retrieval. CoRR abs/2311.14084 (2023). https: //doi.org/10.48550/ARXIV.2311.14084 arXiv:2311.14084 [340] Shiping Yang, Renliang Sun, and Xiaojun Wan. 2023. A New Benchmark and Reverse Validation Method for Passage- level Hallucination Detection. ArXiv preprint abs/2310.06498 (2023). https://arxiv.org/abs/2310.06498 [341] Yuqing Yang, Ethan Chern, Xipeng Qiu, Graham Neubig, and Pengfei Liu. 2023. Alignment for Honesty. CoRR abs/2312.07000 (2023). https://doi.org/10.48550/ARXIV.2312.07000 arXiv:2312.07000 [342] Zhilin Yang, Zihang Dai, Ruslan Salakhutdinov, and William W. Cohen. 2018. Breaking the Softmax Bottleneck: A High-Rank RNN Language Model. In 6th International Conference on Learning Representations, ICLR 2018, Vancouver, BC, Canada, April 30 - May 3, 2018, Conference Track Proceedings . OpenReview.net. https://openreview.net/forum?id= HkwZSG-CZ [343] Zhilin Yang, Thang Luong, Ruslan Salakhutdinov, and Quoc V. Le. 2019. Mixtape: Breaking the Softmax Bottleneck Efficiently. InAdvances in Neural Information Processing Systems 32: Annual Conference on Neural",
      "chunk_index": 95
    },
    {
      "index": 202,
      "chunk_id": "Hallucination_Survey2023_chunk_96",
      "source_id": "Hallucination_Survey2023",
      "text": "- May 3, 2018, Conference Track Proceedings . OpenReview.net. https://openreview.net/forum?id= HkwZSG-CZ [343] Zhilin Yang, Thang Luong, Ruslan Salakhutdinov, and Quoc V. Le. 2019. Mixtape: Breaking the Softmax Bottleneck Efficiently. InAdvances in Neural Information Processing Systems 32: Annual Conference on Neural Information Processing Systems 2019, NeurIPS 2019, December 8-14, 2019, Vancouver, BC, Canada , Hanna M. Wallach, Hugo Larochelle, Alina Beygelzimer, Florence d'Alché-Buc, Emily B. Fox, and Roman Garnett (Eds.). 15922-15930. https://proceedings. neurips.cc/paper/2019/hash/512fc3c5227f637e41437c999a2d3169-Abstract.html [344] Zhilin Yang, Peng Qi, Saizheng Zhang, Yoshua Bengio, William Cohen, Ruslan Salakhutdinov, and Christopher D. Manning. 2018. HotpotQA: A Dataset for Diverse, Explainable Multi-hop Question Answering. In Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing . Association for Computational Linguistics, Brussels, Belgium, 2369-2380. https://doi.org/10.18653/v1/D18-1259 [345] Jia-Yu Yao, Kun-Peng Ning, Zhen-Hui Liu, Mu-Nan Ning, and Li Yuan. 2023. LLM Lies: Hallucinations are not Bugs, but Features as Adversarial Examples. ArXiv preprint abs/2310.01469 (2023). https://arxiv.org/abs/2310.01469 [346] Shunyu Yao, Jeffrey Zhao, Dian Yu, Nan Du, Izhak Shafran, Karthik Narasimhan, and Yuan Cao. 2022. React: Synergizing reasoning and acting in language models. ArXiv preprint abs/2210.03629 (2022). https://arxiv.org/abs/ 2210.03629 [347] Yunzhi Yao, Peng Wang, Bozhong Tian, Siyuan Cheng, Zhoubo Li, Shumin Deng, Huajun Chen, and Ningyu Zhang. 2023. Editing Large Language Models: Problems, Methods, and Opportunities. ArXiv preprint abs/2305.13172 (2023). https://arxiv.org/abs/2305.13172 [348] Xi Ye, Ruoxi Sun, Sercan Ö. Arik, and Tomas Pfister. 2023. Effective Large Language Model Adaptation for Improved Grounding. CoRR abs/2311.09533 (2023). https://doi.org/10.48550/ARXIV.2311.09533 arXiv:2311.09533 [349] Shukang Yin, Chaoyou Fu, Sirui Zhao, Tong Xu, Hao Wang, Dianbo Sui, Yunhang Shen, Ke Li, Xing Sun, and Enhong Chen. 2023. Woodpecker: Hallucination Correction for Multimodal Large Language Models. https: //arxiv.org/abs/2310.16045 [350] Zhangyue Yin, Qiushi Sun, Qipeng Guo, Jiawen Wu, Xipeng Qiu, and Xuanjing Huang. 2023. Do Large Language Models Know What They Don't Know?. In Findings of the Association for Computational Linguistics: ACL 2023, Toronto, Canada, July 9-14, 2023 , Anna Rogers, Jordan L. Boyd-Graber, and Naoaki Okazaki (Eds.). Association for Computational Linguistics, 8653-8665. https://doi.org/10.18653/V1/2023.FINDINGS-ACL.551 [351] Chanwoong Yoon, Gangwoo Kim, Byeongguk Jeon, Sungdong Kim, Yohan Jo, and Jaewoo Kang. 2024. Ask Op- timal Questions: Aligning Large Language Models with Retriever's Preference in Conversational Search. CoRR abs/2402.11827 (2024). https://doi.org/10.48550/ARXIV.2402.11827 arXiv:2402.11827 [352] Ori Yoran, Tomer Wolfson, Ori Ram, and Jonathan Berant. 2023. Making Retrieval-Augmented Language Models Robust to Irrelevant Context. CoRR abs/2310.01558 (2023). https://doi.org/10.48550/ARXIV.2310.01558 arXiv:2310.01558 [353] Fangyi Yu, Lee Quartey, and Frank Schilder. 2022. Legal Prompting: Teaching a Language Model to",
      "chunk_index": 96
    },
    {
      "index": 203,
      "chunk_id": "Hallucination_Survey2023_chunk_97",
      "source_id": "Hallucination_Survey2023",
      "text": "[352] Ori Yoran, Tomer Wolfson, Ori Ram, and Jonathan Berant. 2023. Making Retrieval-Augmented Language Models Robust to Irrelevant Context. CoRR abs/2310.01558 (2023). https://doi.org/10.48550/ARXIV.2310.01558 arXiv:2310.01558 [353] Fangyi Yu, Lee Quartey, and Frank Schilder. 2022. Legal Prompting: Teaching a Language Model to Think Like a Lawyer. ArXiv preprint abs/2212.01326 (2022). https://arxiv.org/abs/2212.01326 [354] Fei Yu, Hongbo Zhang, and Benyou Wang. 2023. Nature language reasoning, a survey. ArXiv preprint abs/2303.14725 (2023). https://arxiv.org/abs/2303.14725 [355] Weijiang Yu, Jian Liang, Lei Ji, Lu Li, Yuejian Fang, Nong Xiao, and Nan Duan. 2021. Hybrid reasoning network for video-based commonsense captioning. In Proceedings of the 29th ACM international conference on multimedia . 5213-5221. [356] Weijiang Yu, Haofan Wang, Guohao Li, Nong Xiao, and Bernard Ghanem. 2023. Knowledge-aware Global Reasoning for Situation Recognition. IEEE Transactions on Pattern Analysis and Machine Intelligence (2023). [357] Wenhao Yu, Hongming Zhang, Xiaoman Pan, Kaixin Ma, Hongwei Wang, and Dong Yu. 2023. Chain-of-Note: Enhancing Robustness in Retrieval-Augmented Language Models. CoRR abs/2311.09210 (2023). https://doi.org/10. 48550/ARXIV.2311.09210 arXiv:2311.09210 ACM Transactions on Information Systems, Vol. 1, No. 1, Article 1. Publication date: January 2024. 1:56 Huang, et al. [358] Wenhao Yu, Zhihan Zhang, Zhenwen Liang, Meng Jiang, and Ashish Sabharwal. 2023. Improving Language Models via Plug-and-Play Retrieval Feedback. ArXiv preprint abs/2305.14002 (2023). https://arxiv.org/abs/2305.14002 [359] Weizhe Yuan, Graham Neubig, and Pengfei Liu. 2021. BARTScore: Evaluating Generated Text as Text Generation. In Advances in Neural Information Processing Systems 34: Annual Conference on Neural Information Processing Sys- tems 2021, NeurIPS 2021, December 6-14, 2021, virtual , Marc'Aurelio Ranzato, Alina Beygelzimer, Yann N. Dauphin, Percy Liang, and Jennifer Wortman Vaughan (Eds.). 27263-27277. https://proceedings.neurips.cc/paper/2021/hash/ e4d2b6e6fdeca3e60e0f1a62fee3d9dd-Abstract.html [360] Rowan Zellers, Yonatan Bisk, Ali Farhadi, and Yejin Choi. 2019. From Recognition to Cognition: Visual Commonsense Reasoning. In IEEE Conference on Computer Vision and Pattern Recognition, CVPR 2019, Long Beach, CA, USA, June 16-20, 2019 . Computer Vision Foundation / IEEE, 6720-6731. https://doi.org/10.1109/CVPR.2019.00688 [361] Bohan Zhai, Shijia Yang, Chenfeng Xu, Sheng Shen, Kurt Keutzer, and Manling Li. 2023. Halle-switch: Controlling object hallucination in large vision language models. arXiv e-prints (2023), arXiv-2310. [362] Hanning Zhang, Shizhe Diao, Yong Lin, Yi R. Fung, Qing Lian, Xingyao Wang, Yangyi Chen, Heng Ji, and Tong Zhang. 2023. R-Tuning: Teaching Large Language Models to Refuse Unknown Questions.CoRR abs/2311.09677 (2023). https://doi.org/10.48550/ARXIV.2311.09677 arXiv:2311.09677 [363] Hugh Zhang, Daniel Duckworth, Daphne Ippolito, and Arvind Neelakantan. 2021. Trading Off Diversity and Quality in Natural Language Generation. In Proceedings of the Workshop on Human Evaluation of NLP Systems (HumEval)",
      "chunk_index": 97
    },
    {
      "index": 204,
      "chunk_id": "Hallucination_Survey2023_chunk_98",
      "source_id": "Hallucination_Survey2023",
      "text": "to Refuse Unknown Questions.CoRR abs/2311.09677 (2023). https://doi.org/10.48550/ARXIV.2311.09677 arXiv:2311.09677 [363] Hugh Zhang, Daniel Duckworth, Daphne Ippolito, and Arvind Neelakantan. 2021. Trading Off Diversity and Quality in Natural Language Generation. In Proceedings of the Workshop on Human Evaluation of NLP Systems (HumEval) . Association for Computational Linguistics, Online, 25-33. https://aclanthology.org/2021.humeval-1.3 [364] Jiaxin Zhang, Zhuohang Li, Kamalika Das, Bradley A. Malin, and Sricharan Kumar. 2023. SAC3: Reliable Hallucination Detection in Black-Box Language Models via Semantic-aware Cross-check Consistency. arXiv:2311.01740 [cs.CL] [365] Jiajun Zhang, Yang Zhao, Haoran Li, and Chengqing Zong. 2018. Attention with sparsity regularization for neural machine translation and summarization. IEEE/ACM Transactions on Audio, Speech, and Language Processing 27, 3 (2018), 507-518. [366] Jingqing Zhang, Yao Zhao, Mohammad Saleh, and Peter J. Liu. 2020. PEGASUS: Pre-training with Extracted Gap- sentences for Abstractive Summarization. In Proceedings of the 37th International Conference on Machine Learning, ICML 2020, 13-18 July 2020, Virtual Event (Proceedings of Machine Learning Research, Vol. 119) . PMLR, 11328-11339. http://proceedings.mlr.press/v119/zhang20ae.html [367] Mingtian Zhang, Shawn Lan, Peter Hayes, and David Barber. 2024. Mafin: Enhancing Black-Box Embeddings with Model Augmented Fine-Tuning. arXiv:2402.12177 [cs.LG] [368] Muru Zhang, Ofir Press, William Merrill, Alisa Liu, and Noah A Smith. 2023. How language model hallucinations can snowball. ArXiv preprint abs/2305.13534 (2023). https://arxiv.org/abs/2305.13534 [369] Ningyu Zhang, Yunzhi Yao, Bozhong Tian, Peng Wang, Shumin Deng, Mengru Wang, Zekun Xi, Shengyu Mao, Jintian Zhang, Yuansheng Ni, Siyuan Cheng, Ziwen Xu, Xin Xu, Jia-Chen Gu, Yong Jiang, Pengjun Xie, Fei Huang, Lei Liang, Zhiqiang Zhang, Xiaowei Zhu, Jun Zhou, and Huajun Chen. 2024. A Comprehensive Study of Knowledge Editing for Large Language Models. arXiv:2401.01286 [cs.CL] [370] Shengyu Zhang, Linfeng Dong, Xiaoya Li, Sen Zhang, Xiaofei Sun, Shuhe Wang, Jiwei Li, Runyi Hu, Tianwei Zhang, Fei Wu, et al. 2023. Instruction Tuning for Large Language Models: A Survey. ArXiv preprint abs/2308.10792 (2023). https://arxiv.org/abs/2308.10792 [371] Shuo Zhang, Liangming Pan, Junzhou Zhao, and William Yang Wang. 2023. Mitigating Language Model Hallucination with Interactive Question-Knowledge Alignment. ArXiv preprint abs/2305.13669 (2023). https://arxiv.org/abs/2305. [372] Susan Zhang, Stephen Roller, Naman Goyal, Mikel Artetxe, Moya Chen, Shuohui Chen, Christopher Dewan, Mona T. Diab, Xian Li, Xi Victoria Lin, Todor Mihaylov, Myle Ott, Sam Shleifer, Kurt Shuster, Daniel Simig, Punit Singh Koura, Anjali Sridhar, Tianlu Wang, and Luke Zettlemoyer. 2022. OPT: Open Pre-trained Transformer Language Models. ArXiv preprint abs/2205.01068 (2022). https://arxiv.org/abs/2205.01068 [373] Tianyi Zhang, Faisal Ladhak, Esin Durmus, Percy Liang, Kathleen McKeown, and Tatsunori B Hashimoto. 2023. Benchmarking large language models",
      "chunk_index": 98
    },
    {
      "index": 205,
      "chunk_id": "Hallucination_Survey2023_chunk_99",
      "source_id": "Hallucination_Survey2023",
      "text": "Koura, Anjali Sridhar, Tianlu Wang, and Luke Zettlemoyer. 2022. OPT: Open Pre-trained Transformer Language Models. ArXiv preprint abs/2205.01068 (2022). https://arxiv.org/abs/2205.01068 [373] Tianyi Zhang, Faisal Ladhak, Esin Durmus, Percy Liang, Kathleen McKeown, and Tatsunori B Hashimoto. 2023. Benchmarking large language models for news summarization. ArXiv preprint abs/2301.13848 (2023). https: //arxiv.org/abs/2301.13848 [374] Tianhang Zhang, Lin Qiu, Qipeng Guo, Cheng Deng, Yue Zhang, Zheng Zhang, Chenghu Zhou, Xinbing Wang, and Luoyi Fu. 2023. Enhancing Uncertainty-Based Hallucination Detection with Stronger Focus. In Proceedings of the 2023 Conference on Empirical Methods in Natural Language Processing, EMNLP 2023, Singapore, December 6-10, 2023, Houda Bouamor, Juan Pino, and Kalika Bali (Eds.). Association for Computational Linguistics, 915-932. https://aclanthology.org/2023.emnlp-main.58 [375] Xiaoying Zhang, Baolin Peng, Ye Tian, Jingyan Zhou, Lifeng Jin, Linfeng Song, Haitao Mi, and Helen Meng. 2024. Self-Alignment for Factuality: Mitigating Hallucinations in LLMs via Self-Evaluation. CoRR abs/2402.09267 (2024). https://doi.org/10.48550/ARXIV.2402.09267 arXiv:2402.09267 ACM Transactions on Information Systems, Vol. 1, No. 1, Article 1. Publication date: January 2024. A Survey on Hallucination in Large Language Models: Principles, Taxonomy, Challenges, and Open Questions 1:57 [376] Yue Zhang, Yafu Li, Leyang Cui, Deng Cai, Lemao Liu, Tingchen Fu, Xinting Huang, Enbo Zhao, Yu Zhang, Yulong Chen, Longyue Wang, Anh Tuan Luu, Wei Bi, Freda Shi, and Shuming Shi. 2023. Siren's Song in the AI Ocean: A Survey on Hallucination in Large Language Models. ArXiv preprint abs/2309.01219 (2023). https://arxiv.org/abs/2309.01219 [377] Zhenyu Zhang, Runjin Chen, Shiwei Liu, Zhewei Yao, Olatunji Ruwase, Beidi Chen, Xiaoxia Wu, and Zhangyang Wang. 2024. Found in the Middle: How Language Models Use Long Contexts Better via Plug-and-Play Positional Encoding. arXiv:2403.04797 [cs.CL] [378] Zihan Zhang, Meng Fang, and Ling Chen. 2024. RetrievalQA: Assessing Adaptive Retrieval-Augmented Generation for Short-form Open-Domain Question Answering. arXiv:2402.16457 [cs.CL] [379] Linxi Zhao, Yihe Deng, Weitong Zhang, and Quanquan Gu. 2024. Mitigating Object Hallucination in Large Vision- Language Models via Classifier-Free Guidance. arXiv preprint arXiv:2402.08680 (2024). [380] Liang Zhao, Xiaocheng Feng, Xiachong Feng, Bing Qin, and Ting Liu. 2023. Length Extrapolation of Transformers: A Survey from the Perspective of Position Encoding. CoRR abs/2312.17044 (2023). https://doi.org/10.48550/ARXIV.2312. 17044 arXiv:2312.17044 [381] Ruochen Zhao, Xingxuan Li, Shafiq Joty, Chengwei Qin, and Lidong Bing. 2023. Verify-and-Edit: A Knowledge- Enhanced Chain-of-Thought Framework. InProceedings of the 61st Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), ACL 2023, Toronto, Canada, July 9-14, 2023 , Anna Rogers, Jordan L. Boyd-Graber, and Naoaki Okazaki (Eds.). Association for Computational Linguistics, 5823-5840. https://doi.org/10.18653/v1/2023.acl- long.320 [382]",
      "chunk_index": 99
    },
    {
      "index": 206,
      "chunk_id": "Hallucination_Survey2023_chunk_100",
      "source_id": "Hallucination_Survey2023",
      "text": "of the 61st Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), ACL 2023, Toronto, Canada, July 9-14, 2023 , Anna Rogers, Jordan L. Boyd-Graber, and Naoaki Okazaki (Eds.). Association for Computational Linguistics, 5823-5840. https://doi.org/10.18653/v1/2023.acl- long.320 [382] Wayne Xin Zhao, Jing Liu, Ruiyang Ren, and Ji-Rong Wen. 2024. Dense text retrieval based on pretrained language models: A survey. ACM Transactions on Information Systems 42, 4 (2024), 1-60. [383] Wayne Xin Zhao, Kun Zhou, Junyi Li, Tianyi Tang, Xiaolei Wang, Yupeng Hou, Yingqian Min, Beichen Zhang, Junjie Zhang, Zican Dong, et al . 2023. A survey of large language models. ArXiv preprint abs/2303.18223 (2023). https://arxiv.org/abs/2303.18223 [384] Yukun Zhao, Lingyong Yan, Weiwei Sun, Guoliang Xing, Chong Meng, Shuaiqiang Wang, Zhicong Cheng, Zhaochun Ren, and Dawei Yin. 2023. Knowing What LLMs DO NOT Know: A Simple Yet Effective Self-Detection Method. ArXiv preprint abs/2310.17918 (2023). https://arxiv.org/abs/2310.17918 [385] Danna Zheng, Mirella Lapata, and Jeff Z. Pan. 2024. Large Language Models as Reliable Knowledge Bases? arXiv:2407.13578 [cs.CL] https://arxiv.org/abs/2407.13578 [386] Shen Zheng, Jie Huang, and Kevin Chen-Chuan Chang. 2023. Why Does ChatGPT Fall Short in Answering Questions Faithfully? ArXiv preprint abs/2304.10513 (2023). https://arxiv.org/abs/2304.10513 [387] Weihong Zhong, Mao Zheng, Duyu Tang, Xuan Luo, Heng Gong, Xiaocheng Feng, and Bing Qin. 2023. STOA-VLP: Spatial-Temporal Modeling of Object and Action for Video-Language Pre-Training. In Proceedings of the Thirty- Seventh AAAI Conference on Artificial Intelligence and Thirty-Fifth Conference on Innovative Applications of Artificial Intelligence and Thirteenth Symposium on Educational Advances in Artificial Intelligence , Vol. 37. 3715-3723. https: //doi.org/10.1609/aaai.v37i3.25483 [388] Chunting Zhou, Pengfei Liu, Puxin Xu, Srini Iyer, Jiao Sun, Yuning Mao, Xuezhe Ma, Avia Efrat, Ping Yu, Lili Yu, et al. 2023. Lima: Less is more for alignment. ArXiv preprint abs/2305.11206 (2023). https://arxiv.org/abs/2305.11206 [389] Chunting Zhou, Graham Neubig, Jiatao Gu, Mona Diab, Francisco Guzmán, Luke Zettlemoyer, and Marjan Ghazvinine- jad. 2021. Detecting Hallucinated Content in Conditional Neural Sequence Generation. In Findings of the Association for Computational Linguistics: ACL-IJCNLP 2021 . Association for Computational Linguistics, Online, 1393-1404. https://doi.org/10.18653/v1/2021.findings-acl.120 [390] Wenxuan Zhou, Sheng Zhang, Hoifung Poon, and Muhao Chen. 2023. Context-faithful Prompting for Large Language Models. In Findings of the Association for Computational Linguistics: EMNLP 2023, Singapore, December 6-10, 2023 , Houda Bouamor, Juan Pino, and Kalika Bali (Eds.). Association for Computational Linguistics, 14544-14556. https: //aclanthology.org/2023.findings-emnlp.968 [391] Yiyang Zhou, Chenhang Cui, Jaehong Yoon, Linjun Zhang, Zhun Deng, Chelsea Finn, Mohit Bansal, and Huaxiu Yao. 2023. Analyzing and Mitigating Object",
      "chunk_index": 100
    },
    {
      "index": 207,
      "chunk_id": "Hallucination_Survey2023_chunk_101",
      "source_id": "Hallucination_Survey2023",
      "text": "2023 , Houda Bouamor, Juan Pino, and Kalika Bali (Eds.). Association for Computational Linguistics, 14544-14556. https: //aclanthology.org/2023.findings-emnlp.968 [391] Yiyang Zhou, Chenhang Cui, Jaehong Yoon, Linjun Zhang, Zhun Deng, Chelsea Finn, Mohit Bansal, and Huaxiu Yao. 2023. Analyzing and Mitigating Object Hallucination in Large Vision-Language Models. https://arxiv.org/abs/2310. [392] Deyao Zhu, Jun Chen, Xiaoqian Shen, Xiang Li, and Mohamed Elhoseiny. 2023. Minigpt-4: Enhancing vision- language understanding with advanced large language models. ArXiv preprint abs/2304.10592 (2023). https: //arxiv.org/abs/2304.10592 [393] Wenhao Zhu, Hongyi Liu, Qingxiu Dong, Jingjing Xu, Lingpeng Kong, Jiajun Chen, Lei Li, and Shujian Huang. 2023. Multilingual machine translation with large language models: Empirical results and analysis. ArXiv preprint abs/2304.04675 (2023). https://arxiv.org/abs/2304.04675 [394] Yutao Zhu, Huaying Yuan, Shuting Wang, Jiongnan Liu, Wenhan Liu, Chenlong Deng, Zhicheng Dou, and Ji-Rong Wen. 2023. Large Language Models for Information Retrieval: A Survey. CoRR abs/2308.07107 (2023). https: //doi.org/10.48550/ARXIV.2308.07107 arXiv:2308.07107 ACM Transactions on Information Systems, Vol. 1, No. 1, Article 1. Publication date: January 2024. 1:58 Huang, et al. [395] Yongshuo Zong, Tingyang Yu, Bingchen Zhao, Ruchika Chavhan, and Timothy Hospedales. 2023. Fool Your (Vision and) Language Model With Embarrassingly Simple Permutations. ArXiv preprint abs/2310.01651 (2023). https: //arxiv.org/abs/2310.01651 ACM Transactions on Information Systems, Vol. 1, No. 1, Article 1. Publication date: January 2024.",
      "chunk_index": 101
    },
    {
      "index": 208,
      "chunk_id": "FIB2022_chunk_00",
      "source_id": "FIB2022",
      "text": "Evaluating the Factual Consistency of Large Language Models Through News Summarization Derek Tam Anisha Mascarenhas Shiyue Zhang Sarah Kwan Mohit Bansal Colin Raffel University of North Carolina at Chapel Hill {dtredsox,amascare,shiyue,mbansal,craffel}@cs.unc.edu Abstract While large language models (LLMs) have proven to be effective on a large variety of tasks, they are also known to hallucinate information. To measure whether an LLM prefers factually consistent continuations of its input, we pro- pose a new benchmark called FIB (Factual Inconsistency Benchmark) that focuses on the task of summarization. Specifically, our bench- mark involves comparing the scores an LLM assigns to a factually consistent versus a fac- tually inconsistent summary for an input news article. For factually consistent summaries, we use human-written reference summaries that we manually verify as factually consistent. To generate summaries that are factually inconsis- tent, we generate summaries from a suite of summarization models that we have manually annotated as factually inconsistent. A model's factual consistency is then measured according to its accuracy, i.e. the proportion of documents where it assigns a higher score to the factually consistent summary. To validate the usefulness of FIB, we evaluate 23 large language mod- els ranging from 1B to 176B parameters from six different model families including BLOOM and OPT. We find that existing LLMs gener- ally assign a higher score to factually consis- tent summaries than to factually inconsistent summaries. However, if the factually inconsis- tent summaries occur verbatim in the document, then LLMs assign a higher score to these factu- ally inconsistent summaries than factually con- sistent summaries. We validate design choices in our benchmark including the scoring method and source of distractor summaries. 1 1 Introduction Factual inconsistency is a widespread problem in natural language generation tasks (Maynez et al., 2020; Weng et al., 2020; Devaraj et al., 2022). For text summarization in particular, it has been shown that models often hallucinate new information or 1We include our code in the supplementary Gold summary Language Model Document A member of the public raised the alarm after seeing the woman, aged in her 50s, fall at Peveril Point, near Swanage, on Saturday afternoon. She was airlifted by the coastguard helicopter to King George's Field park where she was treated by paramedics. A woman has been airlifted to a park after falling from Peveril Point. Distractor A middle-aged woman was hospitalized after falling from a cliff. ✓ 𐄂 Figure 1: A schematic",
      "chunk_index": 0
    },
    {
      "index": 209,
      "chunk_id": "FIB2022_chunk_01",
      "source_id": "FIB2022",
      "text": "King George's Field park where she was treated by paramedics. A woman has been airlifted to a park after falling from Peveril Point. Distractor A middle-aged woman was hospitalized after falling from a cliff. ✓ 𐄂 Figure 1: A schematic diagram of FIB, where we mea- sure whether an LLM assigns a higher score to a fac- tually consistent document summary than a factually inconsistent summary. generate content that contradicts the source doc- ument (Cao et al., 2018; Maynez et al., 2020). These works usually study supervised summariza- tion models that are either trained from scratch or fine-tuned from a pre-trained language model (Wan and Bansal, 2022). Recently, however, NLP has experienced a paradigm shift towards using large language models (LLMs) rather than supervised models. LLMs are generally pre-trained on a large corpus of unstructured text and then applied to a task through instructive prompts. In light of this new paradigm, our goal is to evaluate the factual consistency of large language models using text summarization as a testbed. To achieve this goal, we propose FIB (the Factual Inconsistency Benchmark) to measure how often models prefer factually consistent summaries arXiv:2211.08412v2 [cs.CL] 2 Dec 2023 over factually inconsistent summaries. In FIB, models are given a document and are evaluated on whether they assign a higher score to a factually consistent summary than a factually inconsistent summary. Scores are assigned based on a model's assigned probability to the summary. We use accu- racy on this binary classification task as a proxy for how factually consistent a model is. FIB consists of over 3,500 pairs of summaries that wereall man- ually annotated as either factually consistent or fac- tually inconsistent. The benchmark is based on doc- uments and summaries from the XSum (Narayan et al., 2018b) and CNN/DM (Hermann et al., 2015) datasets to test behavior on abstractive and extrac- tive summarization, respectively. For factually con- sistent summaries, we use reference summaries from the datasets that we verify are factually con- sistent or manually edit to make them factually con- sistent. The factually inconsistent summaries were generated from 22 models trained for summariza- tion and then annotated as factually inconsistent. To explore the behavior of existing models on FIB, we evaluate 23 LLMs from 6 different model families including BLOOM, OPT, GPT, and T0 (Radford et al., 2019; Zhang et al., 2022b; Sanh et al., 2022; Chung et al., 2022; Lester et al.,",
      "chunk_index": 1
    },
    {
      "index": 210,
      "chunk_id": "FIB2022_chunk_02",
      "source_id": "FIB2022",
      "text": "behavior of existing models on FIB, we evaluate 23 LLMs from 6 different model families including BLOOM, OPT, GPT, and T0 (Radford et al., 2019; Zhang et al., 2022b; Sanh et al., 2022; Chung et al., 2022; Lester et al., 2021; Scao et al., 2022) ranging from 1B to 176B param- eters. Next, we analyze whether the method used to generate the factually inconsistent summaries af- fects how often models prefers factually consistent summaries over factually inconsistent summaries. To do so, we evaluate these models on factually in- consistent summaries from three additional sources: (1) unedited reference summaries that we anno- tated as factually inconsistent, (2) summaries edited via FactCC (Kryscinski et al., 2020), and (3) sum- maries produced by MFMA (Lee et al., 2022). In addition, we test 4 different scoring functions: con- ditional log-likelihood (LL), length-normalized LL, pointwise mutual information (PMI), and length- normalized PMI. Overall, we find that: (1) The LLMs we consider typically assign a higher score to factually consistent summaries than to factually inconsistent summaries (e.g. 72.4% of the time for BLOOM (Scao et al., 2022)), but (2) LLMs rarely prefer factually consistent summaries over factually inconsistent summaries copied verbatim from the document (e.g. 9.6% of the time for BLOOM), (3) LLMs generally become more factually consistent as they are scaled up, and (4) FactCC-generated fac- tually inconsistent summaries can fool some LLMs at a similar rate to model-generated factually incon- sistent summaries. In summary, our contributions are: (1) a bench- marking procedure and collection of annotated summaries for probing the factual consistency of LLMs and (2) a thorough evaluation of 23 LLMs from 6 different model families of up to 176B pa- rameters. We hope FIB and our results help shed light on the factuality of LLMs. 2 Related Work 2.1 Factuality Evaluation Datasets In the literature on text summarization, many datasets with human-labeled factually consistent and inconsistent summaries have been introduced for meta-evaluation purposes (i.e., evaluating fac- tuality evaluation metrics) or for training the met- rics themselves. Pagnoni et al. (2021) introduced the FRANK benchmark that contains 2250 model- generated summaries with factuality labels for each summary sentence. Similarly, Gabriel et al. (2021) proposed the GO FIGURE meta-evaluation frame- work that has 1500 model-generated summaries that include factuality labels. Besides these two benchmarks, many other works collected their own small-scale factuality evaluation datasets for evalu- ating their proposed metrics or analyzing the factu-",
      "chunk_index": 2
    },
    {
      "index": 211,
      "chunk_id": "FIB2022_chunk_03",
      "source_id": "FIB2022",
      "text": "proposed the GO FIGURE meta-evaluation frame- work that has 1500 model-generated summaries that include factuality labels. Besides these two benchmarks, many other works collected their own small-scale factuality evaluation datasets for evalu- ating their proposed metrics or analyzing the factu- ality of summarization models (Falke et al., 2019; Maynez et al., 2020; Kryscinski et al., 2020; Wang et al., 2020a; Durmus et al., 2020; Lux et al., 2020). Ribeiro et al. (2022) combined labeled datasets from four works and formed the FactCollect dataset with more than 9000 summary sentences and their factuality labels. Additionally, a few other works proposed to automatically obtain factually incon- sistent summaries by perturbing the reference sum- maries (Kryscinski et al., 2020; Lee et al., 2022), e.g., entity swapping. However, Goyal and Dur- rett (2021) showed that these automatic techniques target inherently different error distributions than those seen in actual model generations. Goyal and Durrett (2020) considered model outputs at the top of beam search as factual and bottom genera- tions as non-factual. The aforementioned works mainly focus on abstractive summarization; in con- trast, Zhang et al. (2022a) introduced a factual- ity evaluation dataset for extractive summarization which we use as part of FIB. Previous datasets do not annotate reference summaries and instead only annotate model generations as factually con- sistent or factually inconsistent. However, the ref- erence summaries are not always factually consis- tent (Maynez et al., 2020; Bommasani and Cardie, 2020; Tejaswin et al., 2021) which means that some of the factually inconsistent summaries might not have any factually consistent summary to pair with. Hence, we perform a manual verification of reference summaries as factually consistent for FIB. Additionally, FIB aims to evaluate the fac- tual consistency of LLMs themselves instead of meta-evaluating evaluation metrics. Besides summarization, Devaraj et al. (2022) proposed a factuality evaluation dataset for text sim- plification. In addition, some datasets have been introduced for checking a fact or claim against a large knowledge base (Thorne et al., 2018; Augen- stein et al., 2019); here, we instead focus on factual consistency of conditional model continuations. 2.2 Factuality Evaluation Metrics Many metrics have been proposed to evaluate the factual consistency of model-generated sum- maries. These metrics can be roughly catego- rized into entailment-based metrics and question- generation/answering (QA/QG)-based metrics. Entailment-based metrics check whether each sum- mary sentence (or a more fine-grained subsentence) is entailed by the source document (Falke et al., 2019;",
      "chunk_index": 3
    },
    {
      "index": 212,
      "chunk_id": "FIB2022_chunk_04",
      "source_id": "FIB2022",
      "text": "sum- maries. These metrics can be roughly catego- rized into entailment-based metrics and question- generation/answering (QA/QG)-based metrics. Entailment-based metrics check whether each sum- mary sentence (or a more fine-grained subsentence) is entailed by the source document (Falke et al., 2019; Kryscinski et al., 2020; Goyal and Durrett, 2020; Maynez et al., 2020). QA/QG-based met- rics are designed based on the idea that a question should have the same answer whether it is based on the summary or the document (Wang et al., 2020a; Durmus et al., 2020; Scialom et al., 2021). Relat- edly, Goodrich et al. (2019) evaluated facutality by checking factual tuples extracted by OpenIE and Ribeiro et al. (2022) used the AMR graphs of the summary and the document for assessing fac- tual consistency. All these metrics were designed to evaluate models trained specifically for summa- rization. In this work, we focus more broadly on evaluating the factual consistency of LLMs. 3 FIB: Factual Inconsistency Benchmark Each example in FIB consists of a document and two summaries: a factually consistent summary and a factually inconsistent summary. Models are evaluated based on the proportion of times they assign a higher score to a factually consistent sum- mary than to a factually inconsistent summary. We define a factually consistent summary as a sum- mary whose contents can be inferred solely from the document. This means that even if a summary contains true information, if the information is not found in the document, then the summary is factu- ally inconsistent. For example, the Gold summary in fig. 1 is factually consistent as it is written, but if we swapped Peveril Pointwith a cliff, then it would no longer be factually consistent, even if Peveril Point is technically a cliff, since this fact cannot be inferred from the document. We compare the factual consistency of mod- els on both extractive and abstractive summaries. Extractive summaries occur verbatim in the doc- ument while abstractive summaries do not. We use two summarization datasets as our testbed: CNN/DM (See et al., 2017; Hermann et al., 2015) for extractive summaries and XSum (Narayan et al., 2018a) for abstractive summaries. CNN/DM consists of English documents about the news from CNN/Daily Mail and summaries that are several sentences long with 287K/13K/11K ex- amples for train/val/test.2 XSum consists of En- glish documents about the news from BBC and short summaries with 204K/11K/11K examples for train/val/test.3 The CNN/DM dataset is",
      "chunk_index": 4
    },
    {
      "index": 213,
      "chunk_id": "FIB2022_chunk_05",
      "source_id": "FIB2022",
      "text": "news from CNN/Daily Mail and summaries that are several sentences long with 287K/13K/11K ex- amples for train/val/test.2 XSum consists of En- glish documents about the news from BBC and short summaries with 204K/11K/11K examples for train/val/test.3 The CNN/DM dataset is distributed under an Apache 2.0 license and XSum is under a Creative Commons Attribution 4.0 International license. Our use is consistent with the intended use and we release our code under an Apache 2.0 license and the data for FIB under a Creative Com- mons Attribution 4.0 International license. 3.1 Dataset Construction We describe how we construct the factually consis- tent and factually inconsistent summaries for FIB. When performing annotations, each summary was annotated by two annotators. Four of the authors performed the annotations. Our inter-annotator agreement was 91.3%. Whenever there was a dis- agreement on a given summary, the two annotators would discuss and resolve the disagreement. See appendix A for annotator instructions. Factually Consistent Summaries. Though the summarization datasets we consider include ref- erence summaries, the reference summaries are not necessarily factually consistent with the doc- ument (Maynez et al., 2020). To account for this, we annotate reference summaries for 500 and 100 documents from XSum and CNN/DM respectively 2https://huggingface.co/datasets/cnn_ dailymail 3https://huggingface.co/datasets/xsum as either factually consistent or factually incon- sistent. Then, we edit the factually inconsistent reference summaries to be factually consistent us- ing minimal edits. Factually inconsistent reference summaries usually contain information that is true but not found in the document. Thus, most edits involve removing or changing certain keywords or phrases not present in the document. Two anno- tators then verified the edited summary was factu- ally consistent. The percentage of factually consis- tent summaries that were edited from the original reference summary was roughly 90% for XSum and 30% for CNN/DM. We denote these annotated factually consistent reference summaries as Gold summaries. See appendix B for some examples of edited summaries. Factually Inconsistent Summaries. To obtain factually inconsistent summaries, we generate sum- maries from models trained on a given summariza- tion dataset and annotate the generated summaries as factually consistent or factually inconsistent. We then retain the model-generated summaries that were annotated as factually inconsistent. We use 15 extractive models to generate summaries for CNN/DM and 7 generative models to generate sum- maries for XSum. See appendix D for the list of models used to generate the summaries. For XSum, we annotate the model-generated summaries",
      "chunk_index": 5
    },
    {
      "index": 214,
      "chunk_id": "FIB2022_chunk_06",
      "source_id": "FIB2022",
      "text": "We use 15 extractive models to generate summaries for CNN/DM and 7 generative models to generate sum- maries for XSum. See appendix D for the list of models used to generate the summaries. For XSum, we annotate the model-generated summaries our- selves and for CNN/DM we source the factual- consistency annotations from Zhang et al. (2022a). See appendix C for some examples of factually inconsistent model-extracted summaries. For the dataset underlying our benchmark, we create a paired example for every possible factu- ally inconsistent summary with the Gold summary for a given document. In the end, we have 3,124 factually consistent/inconsistent summary pairs across 500 unique documents for XSum and 457 pairs across 96 unique documents for CNN/DM (4 CNN/DM documents were dropped since all the models generated factually consistent summaries for them). A model's accuracy on FIB is then sim- ply the proportion of summary pairs where the model assigns a higher score to the Gold summary than to the factually inconsistent summary. 3.2 Scoring Function For FIB, we are primarily interested in a scoring function to measure the consistency of the sum- mary and the document. A natural scoring func- tion is the model's assigned log-likelihood (LL) of the summary given the document, but LL has two major issues. First, the log-likelihood has a bias towards shorter summaries since the proba- bility of each token in a summary is multiplied together to obtain the log-likelihood of the entire summary, and thus shorter summaries tend to pro- duce higher log-likehoods. Second, if the summary alone has a high likelihood, then the model might assign a high likelihood to the summary, even if the summary and the document are not that re- lated. To address the first issue, we normalize by the length of the summary. To address the second issue, we use the pointwise mutual information (PMI), which accounts for the likelihood of the summary by subtracting the log-likelihood of the summary alone from the log-likelihood of the sum- mary conditioned on the document. Several recent works have used the pointwise mutual information (PMI) as a way of scoring a language model's gen- erations: Holtzman et al. (2021) used PMI to solve multiple-choice tasks that probe for knowledge us- ing GPT3 and Padmakumar and He (2021) used PMI for unsupervised extractive summarization. Concurrently, van der Poel et al. (2022) show that optimizing for PMI during decoding can decrease hallucinations",
      "chunk_index": 6
    },
    {
      "index": 215,
      "chunk_id": "FIB2022_chunk_07",
      "source_id": "FIB2022",
      "text": "PMI to solve multiple-choice tasks that probe for knowledge us- ing GPT3 and Padmakumar and He (2021) used PMI for unsupervised extractive summarization. Concurrently, van der Poel et al. (2022) show that optimizing for PMI during decoding can decrease hallucinations in language models. To address both these issues, we use the length- normalized PMI as our default scoring function, where the length normalization is performed by averaging over tokens. Specifically, given docu- ment d and summary s which consists of T tokens {s1, s2, ..., sT }, the length-normalized PMI is de- fined as T log T ∑ t=1 P(st∣d, s1, ..., st−1) (1) − 1 T log T ∑ t=1 P(st∣, s1, ..., st−1) We ablate the impact of using different scoring functions in section 4.4. 4 Experiments Having defined our benchmark, we now evaluate the factual consistency of various LLMs and com- pare with several other methods for generating al- ternative summaries and assigning scores to LM generations. 4.1 Models We evaluate 23 large language models (1B to 176B parameters) from 6 different model families: /uni0031 /uni0031/uni0030/uni0031/uni0030/uni0030 /uni0042/uni0069/uni006C/uni006C/uni0069/uni006F/uni006E/uni0073/uni0020/uni006F/uni0066/uni0020/uni0050/uni0061/uni0072/uni0061/uni006D/uni0065/uni0074/uni0065/uni0072/uni0073 /uni0030 /uni0032/uni0030 /uni0034/uni0030 /uni0036/uni0030 /uni0038/uni0030 /uni0031/uni0030/uni0030/uni0050/uni0072/uni0065/uni0066/uni0065/uni0072/uni0065/uni006E/uni0063/uni0065/uni0020/uni0066/uni006F/uni0072/uni0020/uni0047/uni006F/uni006C/uni0064 /uni0058/uni0053/uni0075/uni006D /uni0031 /uni0031/uni0030/uni0031/uni0030/uni0030 /uni0042/uni0069/uni006C/uni006C/uni0069/uni006F/uni006E/uni0073/uni0020/uni006F/uni0066/uni0020/uni0050/uni0061/uni0072/uni0061/uni006D/uni0065/uni0074/uni0065/uni0072/uni0073 /uni0043/uni004E/uni004E/uni002F/uni0044/uni004D /uni0042/uni004C/uni004F/uni004F/uni004D /uni0046/uni004C/uni0041/uni004E/uni002D/uni0054/uni0035 /uni0047/uni0050/uni0054 /uni004F/uni0050/uni0054 /uni0054/uni0030 /uni0054/uni0035/uni002D/uni004C/uni004D Figure 2: Performance of various models on FIB. • GPT: GPT2-XL (Radford et al., 2019), GPT- Neo-1.3B, GPT-Neo-2.7B, GPT-NeoX-20B (Black et al., 2022) • OPT: OPT-1.3B, OPT-2.7B, OPT-6.7B, OPT- 13B, OPT-30B, OPT-66B, OPT-175B (Zhang et al., 2022b) • BLOOM: BLOOM-1.1B, BLOOM-1.7B, BLOOM-3B, BLOOM-7B, BLOOM (Scao et al., 2022) • T0: T0-3B, T0 (Sanh et al., 2022) • FLAN-T5: FLAN-T5-XL, FLAN-T5-XXL (Chung et al., 2022) • T5-LM-Adapt: T5-LM-Adapt-XL, T5-LM- Adapt-XXL (Lester et al., 2021) Our chosen models consist of both zero-shot mod- els that were not trained on XSum or CNN/DM (GPT, OPT, BLOOM, T5-LM-Adapt) and mod- els that were trained on XSum and CNN/DM in a multi-task fashion (T0, FLAN-T5). For each model, we use the same 3 prompts and report the median performance across prompts, follow- ing Sanh et al. (2022). See appendix E for the prompt templates used. We use a maximum se- quence length of 512, which was also applied when sampling 500 documents from XSUM for annotat- ing factual consistency. We use Pytorch (Paszke et al., 2019) and HuggingFace (Wolf et al., 2020) to run the models, and use bitsandbytes (Dettmers et al., 2022) to do 8-bit inference for the larger mod- els. All experiments were run on NVIDIA",
      "chunk_index": 7
    },
    {
      "index": 216,
      "chunk_id": "FIB2022_chunk_08",
      "source_id": "FIB2022",
      "text": "consistency. We use Pytorch (Paszke et al., 2019) and HuggingFace (Wolf et al., 2020) to run the models, and use bitsandbytes (Dettmers et al., 2022) to do 8-bit inference for the larger mod- els. All experiments were run on NVIDIA A6000s or 80GB NVIDIA A100s (depending on the model) and took about two days. 4.2 Main Results We show the performance of all the models on XSum and CNN/DM in fig. 2. On XSum, we high- light the following: • Factual Consistency: Models generally pre- fer Gold summaries over factually inconsistent model-generated summaries, but the average ac- curacy of any model is still far from 100%. • Effect of Scale: Performance generally increases slightly with scale within a given model family with the exception of T0, where the 11-billion- parameter model underperforms T0-3B. For zero- shot LLMs, the performance is remarkably simi- lar across model families. • Effect of Training: Both FLAN-T5 and T0 un- derperform the zero-shot models, which could be because they were trained on the XSum dataset, which had many reference summaries that were factually inconsistent. In contrast to our results on XSum, we find that models rarely assign a higher score to factu- ally consistent reference summaries than to factu- ally inconsistent model-extracted summaries on the CNN/DM dataset. However, if the factually consis- tent summary is also model-extracted, then models also assign higher scores to the factually consistent model-extracted summary. This suggests that all models have a strong preference for text copied from the input regardless of its factual-consistency. 4.3 Generating Alternative Summaries We also analyze the impact of the the method used to generate factually inconsistent summaries. To do so, we compare the model's performance when using different methods for generating the factu- ally inconsistent summary. We note that Goyal and Durrett (2021) showed that these automatic tech- niques target inherently different error distributions than those seen in actual model generations. We experiment with the following alternative methods for obtaining factually inconsistent summaries: • MFMA, proposed by Lee et al. (2022), uses pre- trained masked language models to generate fac- tually inconsistent summaries. Specifically, sum- maries are generated by reconstructing the refer- ence summary conditioned on the document and reference summary with α and β percent of the entities masked out respectively. The MFMA pro- cedure first fine-tunes a pre-trained masked LM to reconstruct summaries in this setup and then uses the fine-tuned model to",
      "chunk_index": 8
    },
    {
      "index": 217,
      "chunk_id": "FIB2022_chunk_09",
      "source_id": "FIB2022",
      "text": "on the document and reference summary with α and β percent of the entities masked out respectively. The MFMA pro- cedure first fine-tunes a pre-trained masked LM to reconstruct summaries in this setup and then uses the fine-tuned model to generate new sum- maries. For example, in fig. 1, if we masked out /uni0030 /uni0032/uni0035 /uni0035/uni0030 /uni0037/uni0035 /uni0031/uni0030/uni0030/uni0050/uni0072/uni0065/uni0066/uni0065/uni0072/uni0065/uni006E/uni0063/uni0065/uni0020/uni0066/uni006F/uni0072/uni0020/uni0047/uni006F/uni006C/uni0064 /uni0058/uni0053/uni0075/uni006D/uni0020/uni002D/uni0020/uni0046/uni0049/uni0052 /uni0058/uni0053/uni0075/uni006D/uni0020/uni002D/uni0020/uni0046/uni0061/uni0063/uni0074/uni0043/uni0043 /uni0058/uni0053/uni0075/uni006D/uni0020/uni002D/uni0020/uni0046/uni0043/uni004D/uni0047 /uni0058/uni0053/uni0075/uni006D/uni0020/uni002D/uni0020/uni004D/uni0046/uni004D/uni0041 /uni0031 /uni0031/uni0030/uni0031/uni0030/uni0030 /uni0042/uni0069/uni006C/uni006C/uni0069/uni006F/uni006E/uni0073/uni0020/uni006F/uni0066/uni0020/uni0050/uni0061/uni0072/uni0061/uni006D/uni0065/uni0074/uni0065/uni0072/uni0073 /uni0030 /uni0032/uni0035 /uni0035/uni0030 /uni0037/uni0035 /uni0031/uni0030/uni0030/uni0050/uni0072/uni0065/uni0066/uni0065/uni0072/uni0065/uni006E/uni0063/uni0065/uni0020/uni0066/uni006F/uni0072/uni0020/uni0047/uni006F/uni006C/uni0064 /uni0043/uni004E/uni004E/uni002F/uni0044/uni004D/uni0020/uni002D/uni0020/uni0046/uni0049/uni0052 /uni0031 /uni0031/uni0030/uni0031/uni0030/uni0030 /uni0042/uni0069/uni006C/uni006C/uni0069/uni006F/uni006E/uni0073/uni0020/uni006F/uni0066/uni0020/uni0050/uni0061/uni0072/uni0061/uni006D/uni0065/uni0074/uni0065/uni0072/uni0073 /uni0043/uni004E/uni004E/uni002F/uni0044/uni004D/uni0020/uni002D/uni0020/uni0046/uni0061/uni0063/uni0074/uni0043/uni0043 /uni0042/uni004C/uni004F/uni004F/uni004D/uni0054/uni0030 /uni0031 /uni0031/uni0030/uni0031/uni0030/uni0030 /uni0042/uni0069/uni006C/uni006C/uni0069/uni006F/uni006E/uni0073/uni0020/uni006F/uni0066/uni0020/uni0050/uni0061/uni0072/uni0061/uni006D/uni0065/uni0074/uni0065/uni0072/uni0073 /uni0043/uni004E/uni004E/uni002F/uni0044/uni004D/uni0020/uni002D/uni0020/uni0046/uni0043/uni004D/uni0047 /uni0031 /uni0031/uni0030/uni0031/uni0030/uni0030 /uni0042/uni0069/uni006C/uni006C/uni0069/uni006F/uni006E/uni0073/uni0020/uni006F/uni0066/uni0020/uni0050/uni0061/uni0072/uni0061/uni006D/uni0065/uni0074/uni0065/uni0072/uni0073 /uni0043/uni004E/uni004E/uni002F/uni0044/uni004D/uni0020/uni002D/uni0020/uni004D/uni0046/uni004D/uni0041 Figure 3: Preference for the Gold summary exhibited by BLOOM and T0 when using different methods for generating alternative choices. Peveril Pointin the reference summary and the model generated the grand canyon instead, then the factually-inconsistent MFMA-generated sum- mary would be A middle-aged woman has been driven by ambulance to a park after falling from the grand canyon. We follow the setup in MFMA and use T5-base (Raffel et al., 2020) and BART- base (Lewis et al., 2020a) to generate the sum- maries with α = 0.8 and β = 0.6. Since there is no guarantee that the model-reconstructed sum- maries are factually inconsistent, we annotate their factual-consistency and only keep the ones that are factually inconsistent. We construct fac- tually inconsistent summaries from MFMA by combining all factually inconsistent summaries generated by T5-base and BART-base. • FactCC, proposed by Kryscinski et al. (2020), generates factually inconsistent summaries via heuristic perturbations to reference summaries. FactCC uses two ways to perturb the reference summary: entity swapping and sentence nega- tion. Entity swapping replaces an entity (i.e. pronouns, dates, numbers and named entities) in the reference summary with a different en- tity from the document and sentence negation refers to negating a verb. For example, in fig. 1, if we negated has to hasn't, then the factually- inconsistent FactCC-generated summary would be A middle-aged woman hasn't been airlifted to a park after falling from Peveril Point. • FIR (factually inconsistent reference) summaries. Since some of the original reference summaries were factually inconsistent and had to be edited to become factually consistent, we use these orig- inal reference summaries as an alternative source of factually inconsistent summaries. As an additional baseline, we consider using factually consistent model-generated summaries rather than a factually inconsistent summary as the alternative summary. This allows us to test whether models prefer model-generated summaries over Gold summaries. We call this setup of where the alternative",
      "chunk_index": 9
    },
    {
      "index": 218,
      "chunk_id": "FIB2022_chunk_10",
      "source_id": "FIB2022",
      "text": "additional baseline, we consider using factually consistent model-generated summaries rather than a factually inconsistent summary as the alternative summary. This allows us to test whether models prefer model-generated summaries over Gold summaries. We call this setup of where the alternative choice is a factually consistent model- generated summaries FCMG (Factually-Consistent Model-Generated summaries). A comparison of different methods for generat- ing alternative summaries is shown in fig. 3. We only plot results for BLOOM and T0 since the re- sults for other decoder-only zero-shot LLMs are similar to those for BLOOM and the results for FLAN-T5 are similar to T0. We highlight the fol- lowing trends: • Preference for factually consistent model- generated summaries depends on whether sum- maries are extractive: On XSum, models are almost at chance when distinguishing between factually consistent model-generated summaries and Gold summaries. This is evident from the accuracy on FCMG being around 50%. How- ever, on CNN/DM, models consistently prefer factually consistent model-extracted summaries to Gold summaries. We conclude that models prefer model-extracted summaries that occur ver- batim in the document, regardless of their factual consistency. /uni0030 /uni0032/uni0035 /uni0035/uni0030 /uni0037/uni0035 /uni0031/uni0030/uni0030/uni0050/uni0072/uni0065/uni0066/uni0065/uni0072/uni0065/uni006E/uni0063/uni0065/uni0020/uni0066/uni006F/uni0072/uni0020/uni0047/uni006F/uni006C/uni0064 /uni0058/uni0053/uni0075/uni006D/uni0020/uni002D/uni0020/uni0042/uni004C/uni004F/uni004F/uni004D/uni0058/uni0053/uni0075/uni006D/uni0020/uni002D/uni0020/uni004F/uni0050/uni0054/uni002D/uni0031/uni0037/uni0035/uni0042/uni0058/uni0053/uni0075/uni006D/uni0020/uni002D/uni0020/uni0047/uni0050/uni0054/uni002D/uni004E/uni0065/uni006F/uni0078/uni002D/uni0032/uni0030/uni0042/uni0058/uni0053/uni0075/uni006D/uni0020/uni002D/uni0020/uni0054/uni0030 /uni0041/uni0076/uni0067/uni002E/uni0020/uni0050/uni004D/uni0049/uni0041/uni0076/uni0067/uni002E/uni0020/uni004C/uni004C /uni0050/uni004D/uni0049/uni004C/uni004C /uni0030 /uni0032/uni0035 /uni0035/uni0030 /uni0037/uni0035 /uni0031/uni0030/uni0030/uni0050/uni0072/uni0065/uni0066/uni0065/uni0072/uni0065/uni006E/uni0063/uni0065/uni0020/uni0066/uni006F/uni0072/uni0020/uni0047/uni006F/uni006C/uni0064 /uni0043/uni004E/uni004E/uni002F/uni0044/uni004D/uni0020/uni002D/uni0020/uni0042/uni004C/uni004F/uni004F/uni004D /uni0041/uni0076/uni0067/uni002E/uni0020/uni0050/uni004D/uni0049/uni0041/uni0076/uni0067/uni002E/uni0020/uni004C/uni004C /uni0050/uni004D/uni0049/uni004C/uni004C /uni0043/uni004E/uni004E/uni002F/uni0044/uni004D/uni0020/uni002D/uni0020/uni004F/uni0050/uni0054/uni002D/uni0031/uni0037/uni0035/uni0042 /uni0041/uni0076/uni0067/uni002E/uni0020/uni0050/uni004D/uni0049/uni0041/uni0076/uni0067/uni002E/uni0020/uni004C/uni004C /uni0050/uni004D/uni0049/uni004C/uni004C /uni0043/uni004E/uni004E/uni002F/uni0044/uni004D/uni0020/uni002D/uni0020/uni0047/uni0050/uni0054/uni002D/uni004E/uni0065/uni006F/uni0078/uni002D/uni0032/uni0030/uni0042 /uni0041/uni0076/uni0067/uni002E/uni0020/uni0050/uni004D/uni0049/uni0041/uni0076/uni0067/uni002E/uni0020/uni004C/uni004C /uni0050/uni004D/uni0049/uni004C/uni004C /uni0043/uni004E/uni004E/uni002F/uni0044/uni004D/uni0020/uni002D/uni0020/uni0054/uni0030 Figure 4: Performance of various models on FIB when using different scoring functions. • MFMA's Ineffectiveness: On both XSum and CNN/DM, models rarely assign MFMA- generated summaries a higher score than Gold summaries - the accuracy on MFMA is between 85% to 100% across all models. • FactCC's Effectiveness for zero-shot LLMs:On XSum, BLOOM's performance is similar when either FactCC or model-generated factually in- consistent summaries are used as an alternative, and on CNN/DM, performance is similar for FactCC and factually inconsistent reference sum- maries. This suggests that FactCC generates somewhat plausible factually inconsistent sum- maries for zero-shot decoder-only LLMs. • FactCC's Effectiveness for other models: How- ever, T0, FLAN-T5, and T5-LM-Adapt (see ap- pendix H for FLAN-T5 and T5-LM-Adapt ac- curacies) all perform better when using FactCC- generated factually inconsistent summaries than when using model-generated factually inconsis- tent summaries. This indicates FactCC might not be effective in generating plausible factually inconsistent summaries across all model architec- tures and training schemes. • Preference for Edited Summaries: On XSum and CNN/DM, models tend to prefer factually consis- tent reference summaries over factually inconsis- tent reference summaries. This is evident from the accuracy on FIR being around 80%",
      "chunk_index": 10
    },
    {
      "index": 219,
      "chunk_id": "FIB2022_chunk_11",
      "source_id": "FIB2022",
      "text": "architec- tures and training schemes. • Preference for Edited Summaries: On XSum and CNN/DM, models tend to prefer factually consis- tent reference summaries over factually inconsis- tent reference summaries. This is evident from the accuracy on FIR being around 80% and indi- cates that models tend to prefer factually consis- tent summaries over factually inconsistent sum- maries. 4.4 Scoring Function In FIB, we use the length-normalized PMI as the scoring function. To validate this choice, we com- pare various alternative scoring functions: standard log-likelihood, length-normalized log-likelihood, and the non-length-normalized PMI. We show re- sults for BLOOM, OPT-175B and T0 on XSum and CNN/DM using different scoring methods in fig. 4. In general we see that the average PMI enables models to best distinguish between factually con- sistent and factually inconsistent summaries. We also compare each scoring function on the alter- nate sources of factually inconsistent summaries; see appendix F for detailed results. We find that log-likelihood works best when the factually in- consistent summary was produced by FactCC or is a model generation on CNN/DM. We hypothe- size that log-likelihood works better than length- normalized PMI on FactCC because the generated summaries are often non-fluent and therefore are assigned a low likelihood regardless of their fac- tual consistency. For model-extracted summaries on CNN/DM, we hypothesize that log-likelihood works better than length-normalized PMI because log-likelihood is not as biased towards summaries extracted from the document as PMI is. 5 Analysis To get a better sense of what kind of factually inconsistent model-generated summaries tend to fool models into assigning a higher score than the Gold summary, we show some examples for BLOOM in table 1. These factually inconsistent summaries consist of extrinsic hallucinations that Document Factually Consistent Summary Factually Inconsistent Summary The $5m (3.2m) prize is supposed to be awarded each year to an elected leader who governed well, raised living standards and then left office. This is the fourth time in five years there has been no winner ... Sudan-born telecoms entrepreneur Mr Ibrahim launched the prize in an attempt to encourage African leaders to leave power peacefully. ... The prize from Ibrahim for good governance in Africa has gone unclaimed yet again. The winner of the prestigious Africa Leadership Prize has been announced by the African Union's executive committee. The character with a huge papier mache head ... Hundreds of people attended an unveiling ceremony earlier, many in fancy",
      "chunk_index": 11
    },
    {
      "index": 220,
      "chunk_id": "FIB2022_chunk_12",
      "source_id": "FIB2022",
      "text": "unclaimed yet again. The winner of the prestigious Africa Leadership Prize has been announced by the African Union's executive committee. The character with a huge papier mache head ... Hundreds of people attended an unveiling ceremony earlier, many in fancy dress for the occasion. Neil Taylor, who helped raise the donations for the statue, said its installation would mean that ¨Frank will gaze on the Timperley sunset forever¨... Frank Sidebottom created a whole ... A statue of the character Frank Sidebottom has been unveiled in Timperley. A statue of Timperley's character Frank Sidebottom has been unveiled at a Manchester museum. Table 1: Two examples where BLOOM assigns a higher score to the factually inconsistent model-generated summaries than the Gold summary. These examples have id 24521870 and id 24601038 respectively. /uni0042/uni0041/uni0052/uni0054/uni002D/uni0062/uni0061/uni0073/uni0065/uni0042/uni0041/uni0052/uni0054/uni002D/uni006C/uni0061/uni0072/uni0067/uni0065 /uni0042/uni004C/uni004F/uni004F/uni004D/uni002D/uni0035/uni0036/uni0030/uni006D /uni0064/uni0069/uni0073/uni0074/uni0069/uni006C/uni002D/uni0042/uni0041/uni0052/uni0054 /uni0064/uni0069/uni0073/uni0074/uni0069/uni006C/uni002D/uni0050/uni0045/uni0047/uni0041/uni0053/uni0055/uni0053 /uni0050/uni0045/uni0047/uni0041/uni0053/uni0055/uni0053/uni0054/uni0035/uni002D/uni006C/uni0061/uni0072/uni0067/uni0065 /uni0047/uni0065/uni006E/uni0065/uni0072/uni0061/uni0074/uni0069/uni006E/uni0067/uni0020/uni004D/uni006F/uni0064/uni0065/uni006C/uni0020 /uni0042/uni0041/uni0052/uni0054/uni002D/uni0062/uni0061/uni0073/uni0065 /uni0042/uni0041/uni0052/uni0054/uni002D/uni006C/uni0061/uni0072/uni0067/uni0065 /uni0042/uni004C/uni004F/uni004F/uni004D/uni002D/uni0035/uni0036/uni0030/uni006D /uni0064/uni0069/uni0073/uni0074/uni0069/uni006C/uni002D/uni0042/uni0041/uni0052/uni0054 /uni0064/uni0069/uni0073/uni0074/uni0069/uni006C/uni002D/uni0050/uni0045/uni0047/uni0041/uni0053/uni0055/uni0053 /uni0050/uni0045/uni0047/uni0041/uni0053/uni0055/uni0053 /uni0054/uni0035/uni002D/uni006C/uni0061/uni0072/uni0067/uni0065 /uni0045/uni0076/uni0061/uni006C/uni0075/uni0061/uni0074/uni0065/uni0064/uni0020/uni004D/uni006F/uni0064/uni0065/uni006C /uni0032/uni0034/uni002E/uni0034/uni0034/uni0032/uni002E/uni0035/uni0039/uni0035/uni002E/uni0034/uni0033/uni0034/uni002E/uni0034/uni0034/uni0035/uni002E/uni0031/uni0034/uni0032/uni002E/uni0032/uni0038/uni0033 /uni0036/uni0033/uni002E/uni0035/uni0032/uni0034/uni002E/uni0034/uni0039/uni0036/uni0032/uni0039/uni002E/uni0035/uni0033/uni0039/uni002E/uni0034/uni0033/uni0032/uni002E/uni0032/uni0039/uni0034/uni002E/uni0032 /uni0035/uni0035/uni002E/uni0039/uni0034/uni0034/uni002E/uni0037/uni0035/uni0032/uni002E/uni0038/uni0035/uni0033/uni002E/uni0039/uni0034/uni0035/uni002E/uni0038/uni0034/uni0036/uni002E/uni0031/uni0037/uni0032 /uni0035/uni0031/uni0032/uni0034/uni002E/uni0032/uni0039/uni0034/uni002E/uni0035/uni0031/uni0036/uni002E/uni0036/uni0033/uni0035/uni002E/uni0037/uni0033/uni0030/uni002E/uni0038/uni0039/uni0033/uni002E/uni0034 /uni0036/uni0032/uni002E/uni0039/uni0033/uni0034/uni002E/uni0031/uni0039/uni0037/uni002E/uni0033/uni0033/uni0032/uni002E/uni0034/uni0031/uni0039/uni002E/uni0037/uni0031/uni0038/uni002E/uni0039/uni0039/uni0034/uni002E/uni0038 /uni0037/uni0032/uni002E/uni0034/uni0034/uni0034/uni002E/uni0039/uni0039/uni0037/uni002E/uni0031/uni0034/uni0032/uni002E/uni0039/uni0033/uni0036/uni002E/uni0034/uni0032/uni0032/uni002E/uni0038/uni0039/uni0036/uni002E/uni0039 /uni0034/uni0033/uni002E/uni0032/uni0035/uni0030/uni002E/uni0037/uni0039/uni0033/uni002E/uni0035/uni0034/uni0036/uni002E/uni0031/uni0035/uni0031/uni002E/uni0035/uni0034/uni0039/uni002E/uni0038/uni0033/uni0031/uni002E/uni0037 Figure 5: Heatmap showing the rate at which an \"evalu- ated model\" assigns a Gold summary on XSum a higher score than a factually inconsistent summary generated by the \"generating model\". add new information rather than intrinsic halluci- nations that manipulate the information in the doc- ument (Maynez et al., 2020). In addition, these fac- tually inconsistent summaries contain information that is actually false, not just information absent from the document. 5.1 Factual Consistency of Models Used to Generate Summaries We take the models used to generate the factu- ally inconsistent summaries for XSum and evaluate them against each other using the same procedure as in FIB. Specifically, we use factually inconsis- tent summaries produced by a \"generating model\" and measure how often an \"evaluated model\" as- signs a higher score to the Gold summary than it does to the factually inconsistent model-generated summaries. The result is summarized in fig. 5, with full results in appendix K. The accuracies down the diagonal are the lowest, which means models perform poorly when scoring their own factually inconsistent summary. This is expected since mod- els should give high scores to factually inconsistent summaries they generate. In most cases, Gold sum- maries are preferred less than 50% of the time, suggesting that summarization models tend to as- sign higher scores to model-generated factually inconsistent summaries. However, certain mod- els (BLOOM and T5-large) almost always produce summaries that are assigned low scores by the other models. We leave exploration of this trend to future",
      "chunk_index": 12
    },
    {
      "index": 221,
      "chunk_id": "FIB2022_chunk_13",
      "source_id": "FIB2022",
      "text": "models tend to as- sign higher scores to model-generated factually inconsistent summaries. However, certain mod- els (BLOOM and T5-large) almost always produce summaries that are assigned low scores by the other models. We leave exploration of this trend to future work. 6 Conclusion and Takeaways We present FIB, a new benchmark for evaluating the factual consistency of language models, and evaluate 23 large language models on FIB. Our takeaways are: (1) LLMs tend to assign higher scores to factually consistent summaries than to fac- tually inconsistent summaries, except that LLMs almost always assign higher scores to extracted summaries even if they are factually inconsistent and (2) length-normalized PMI enables models to most effectively detect factually inconsistent sum- maries. Our results open new avenues for future work, including a more fine-grained study on the type of factually inconsistent errors different LLMs make and investigating the effect training on sum- marization has on the factual consistency of LLMs. 7 Limitations One limitation with FIB is that it only measures the factual consistency of language models for the task of summarization, and specifically news sum- marization. It is not clear how well the results will generalize, for example, to other domains such as scientific article or other tasks such as question answering. Acknowledgements This work was supported by NSF-AI Engage Insti- tute DRL-2112635. References Isabelle Augenstein, Christina Lioma, Dongsheng Wang, Lucas Chaves Lima, Casper Hansen, Chris- tian Hansen, and Jakob Grue Simonsen. 2019. Mul- tiFC: A real-world multi-domain dataset for evidence- based fact checking of claims. In Proceedings of the 2019 Conference on Empirical Methods in Natu- ral Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP), pages 4685-4697, Hong Kong, China. Association for Computational Linguistics. Sidney Black, Stella Biderman, Eric Hallahan, Quentin Anthony, Leo Gao, Laurence Golding, Horace He, Connor Leahy, Kyle McDonell, Jason Phang, Michael Pieler, Usvsn Sai Prashanth, Shivanshu Puro- hit, Laria Reynolds, Jonathan Tow, Ben Wang, and Samuel Weinbach. 2022. GPT-NeoX-20B: An open- source autoregressive language model. In Proceed- ings of BigScience Episode #5 - Workshop on Chal- lenges & Perspectives in Creating Large Language Models, pages 95-136, virtual+Dublin. Association for Computational Linguistics. Rishi Bommasani and Claire Cardie. 2020. Intrinsic evaluation of summarization datasets. In Proceed- ings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP) , pages 8075-8096, Online. Association for Computational Linguistics. Ziqiang Cao, Furu Wei, Wenjie Li, and Sujian",
      "chunk_index": 13
    },
    {
      "index": 222,
      "chunk_id": "FIB2022_chunk_14",
      "source_id": "FIB2022",
      "text": "and Claire Cardie. 2020. Intrinsic evaluation of summarization datasets. In Proceed- ings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP) , pages 8075-8096, Online. Association for Computational Linguistics. Ziqiang Cao, Furu Wei, Wenjie Li, and Sujian Li. 2018. Faithful to the original: Fact aware neural abstractive summarization. Proceedings of the AAAI Conference on Artificial Intelligence, 32(1). Yen-Chun Chen and Mohit Bansal. 2018. Fast abstrac- tive summarization with reinforce-selected sentence rewriting. In Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics (Vol- ume 1: Long Papers) , pages 675-686, Melbourne, Australia. Association for Computational Linguistics. Hyung Won Chung, Le Hou, Shayne Longpre, Bar- ret Zoph, Yi Tay, William Fedus, Eric Li, Xuezhi Wang, Mostafa Dehghani, Siddhartha Brahma, et al. 2022. Scaling instruction-finetuned language models. arXiv preprint arXiv:2210.11416. Tim Dettmers, Mike Lewis, Younes Belkada, and Luke Zettlemoyer. 2022. Llm.int8(): 8-bit matrix multi- plication for transformers at scale. arXiv preprint arXiv:2208.07339. Ashwin Devaraj, William Sheffield, Byron Wallace, and Junyi Jessy Li. 2022. Evaluating factuality in text simplification. In Proceedings of the 60th Annual Meeting of the Association for Computational Lin- guistics (Volume 1: Long Papers), pages 7331-7345, Dublin, Ireland. Association for Computational Lin- guistics. Yue Dong, Yikang Shen, Eric Crawford, Herke van Hoof, and Jackie Chi Kit Cheung. 2018. Bandit- Sum: Extractive summarization as a contextual ban- dit. In Proceedings of the 2018 Conference on Empir- ical Methods in Natural Language Processing, pages 3739-3748, Brussels, Belgium. Association for Com- putational Linguistics. Esin Durmus, He He, and Mona Diab. 2020. FEQA: A question answering evaluation framework for faith- fulness assessment in abstractive summarization. In Proceedings of the 58th Annual Meeting of the Asso- ciation for Computational Linguistics, pages 5055- 5070, Online. Association for Computational Lin- guistics. Tobias Falke, Leonardo F. R. Ribeiro, Prasetya Ajie Utama, Ido Dagan, and Iryna Gurevych. 2019. Rank- ing generated summaries by correctness: An interest- ing but challenging application for natural language inference. In Proceedings of the 57th Annual Meet- ing of the Association for Computational Linguistics, pages 2214-2220, Florence, Italy. Association for Computational Linguistics. Saadia Gabriel, Asli Celikyilmaz, Rahul Jha, Yejin Choi, and Jianfeng Gao. 2021. GO FIGURE: A meta eval- uation of factuality in summarization. In Findings of the Association for Computational Linguistics: ACL- IJCNLP 2021, pages 478-487, Online. Association for Computational Linguistics. Ben Goodrich, Vinay Rao, Peter J. Liu, and Mohammad Saleh. 2019. Assessing the factual accuracy of gener- ated",
      "chunk_index": 14
    },
    {
      "index": 223,
      "chunk_id": "FIB2022_chunk_15",
      "source_id": "FIB2022",
      "text": "of factuality in summarization. In Findings of the Association for Computational Linguistics: ACL- IJCNLP 2021, pages 478-487, Online. Association for Computational Linguistics. Ben Goodrich, Vinay Rao, Peter J. Liu, and Mohammad Saleh. 2019. Assessing the factual accuracy of gener- ated text. In Proceedings of the 25th ACM SIGKDD International Conference on Knowledge Discovery & Data Mining. Tanya Goyal and Greg Durrett. 2020. Evaluating factu- ality in generation with dependency-level entailment. In Findings of the Association for Computational Lin- guistics: EMNLP 2020 , pages 3592-3603, Online. Association for Computational Linguistics. Tanya Goyal and Greg Durrett. 2021. Annotating and modeling fine-grained factuality in summarization. In Proceedings of the 2021 Conference of the North American Chapter of the Association for Computa- tional Linguistics: Human Language Technologies, pages 1449-1462, Online. Association for Computa- tional Linguistics. Karl Moritz Hermann, Tomas Kocisky, Edward Grefen- stette, Lasse Espeholt, Will Kay, Mustafa Suleyman, and Phil Blunsom. 2015. Teaching machines to read and comprehend. In Advances in neural information processing systems. Ari Holtzman, Peter West, Vered Shwartz, Yejin Choi, and Luke Zettlemoyer. 2021. Surface form com- petition: Why the highest probability answer isn't always right. In Proceedings of the 2021 Conference on Empirical Methods in Natural Language Process- ing, pages 7038-7051, Online and Punta Cana, Do- minican Republic. Association for Computational Linguistics. Wojciech Kryscinski, Bryan McCann, Caiming Xiong, and Richard Socher. 2020. Evaluating the factual consistency of abstractive text summarization. In Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP), pages 9332-9346, Online. Association for Computa- tional Linguistics. Hwanhee Lee, Kang Min Yoo, Joonsuk Park, Hwaran Lee, and Kyomin Jung. 2022. Masked summariza- tion to generate factually inconsistent summaries for improved factual consistency checking. In Find- ings of the Association for Computational Linguis- tics: NAACL 2022, pages 1019-1030, Seattle, United States. Association for Computational Linguistics. Brian Lester, Rami Al-Rfou, and Noah Constant. 2021. The power of scale for parameter-efficient prompt tuning. In Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing, pages 3045-3059, Online and Punta Cana, Domini- can Republic. Association for Computational Lin- guistics. Mike Lewis, Yinhan Liu, Naman Goyal, Marjan Ghazvininejad, Abdelrahman Mohamed, Omer Levy, Veselin Stoyanov, and Luke Zettlemoyer. 2020a. Bart: Denoising sequence-to-sequence pre-training for natural language generation, translation, and com- prehension. In Proceedings of the 58th Annual Meet- ing of the Association for Computational Linguistics, pages 7871-7880. Mike Lewis, Yinhan Liu, Naman Goyal, Marjan Ghazvininejad, Abdelrahman Mohamed, Omer",
      "chunk_index": 15
    },
    {
      "index": 224,
      "chunk_id": "FIB2022_chunk_16",
      "source_id": "FIB2022",
      "text": "2020a. Bart: Denoising sequence-to-sequence pre-training for natural language generation, translation, and com- prehension. In Proceedings of the 58th Annual Meet- ing of the Association for Computational Linguistics, pages 7871-7880. Mike Lewis, Yinhan Liu, Naman Goyal, Marjan Ghazvininejad, Abdelrahman Mohamed, Omer Levy, Veselin Stoyanov, and Luke Zettlemoyer. 2020b. BART: Denoising sequence-to-sequence pre-training for natural language generation, translation, and com- prehension. In Proceedings of the 58th Annual Meet- ing of the Association for Computational Linguistics, pages 7871-7880, Online. Association for Computa- tional Linguistics. Chin-Yew Lin. 2004. ROUGE: A package for auto- matic evaluation of summaries. In Text Summariza- tion Branches Out, pages 74-81, Barcelona, Spain. Association for Computational Linguistics. Klaus-Michael Lux, Maya Sappelli, and Martha Larson. 2020. Truth or error? towards systematic analysis of factual errors in abstractive summaries. In Pro- ceedings of the First Workshop on Evaluation and Comparison of NLP Systems , pages 1-10, Online. Association for Computational Linguistics. Joshua Maynez, Shashi Narayan, Bernd Bohnet, and Ryan McDonald. 2020. On faithfulness and factu- ality in abstractive summarization. In Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics, pages 1906-1919, On- line. Association for Computational Linguistics. Rada Mihalcea and Paul Tarau. 2004. TextRank: Bring- ing order into text. In Proceedings of the 2004 Con- ference on Empirical Methods in Natural Language Processing, pages 404-411, Barcelona, Spain. Asso- ciation for Computational Linguistics. Shashi Narayan, Shay B. Cohen, and Mirella Lapata. 2018a. Don't give me the details, just the summary! topic-aware convolutional neural networks for ex- treme summarization. In Proceedings of the 2018 Conference on Empirical Methods in Natural Lan- guage Processing, pages 1797-1807, Brussels, Bel- gium. Association for Computational Linguistics. Shashi Narayan, Shay B Cohen, and Mirella Lapata. 2018b. Don't give me the details, just the summary! topic-aware convolutional neural networks for ex- treme summarization. In Proceedings of the 2018 Conference on Empirical Methods in Natural Lan- guage Processing, pages 1797-1807. Shashi Narayan, Shay B. Cohen, and Mirella Lapata. 2018c. Ranking sentences for extractive summariza- tion with reinforcement learning. In Proceedings of the 2018 Conference of the North American Chap- ter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long Pa- pers), pages 1747-1759, New Orleans, Louisiana. Association for Computational Linguistics. Vishakh Padmakumar and He He. 2021. Unsupervised extractive summarization using pointwise mutual in- formation. In Proceedings of the 16th Conference of the European Chapter of the Association for Compu- tational Linguistics: Main Volume, pages 2505-2512,",
      "chunk_index": 16
    },
    {
      "index": 225,
      "chunk_id": "FIB2022_chunk_17",
      "source_id": "FIB2022",
      "text": "Louisiana. Association for Computational Linguistics. Vishakh Padmakumar and He He. 2021. Unsupervised extractive summarization using pointwise mutual in- formation. In Proceedings of the 16th Conference of the European Chapter of the Association for Compu- tational Linguistics: Main Volume, pages 2505-2512, Online. Association for Computational Linguistics. Artidoro Pagnoni, Vidhisha Balachandran, and Yulia Tsvetkov. 2021. Understanding factuality in abstrac- tive summarization with FRANK: A benchmark for factuality metrics. In Proceedings of the 2021 Con- ference of the North American Chapter of the Asso- ciation for Computational Linguistics: Human Lan- guage Technologies, pages 4812-4829, Online. As- sociation for Computational Linguistics. Adam Paszke, Sam Gross, Francisco Massa, Adam Lerer, James Bradbury, Gregory Chanan, Trevor Killeen, Zeming Lin, Natalia Gimelshein, Luca Antiga, et al. 2019. Pytorch: An imperative style, high-performance deep learning library. Advances in neural information processing systems, 32. Alec Radford, Jeffrey Wu, Rewon Child, David Luan, Dario Amodei, and Ilya Sutskever. 2019. Teaching machines to read and comprehend. In OpenAI Blog. Colin Raffel, Noam Shazeer, Adam Roberts, Katherine Lee, Sharan Narang, Michael Matena, Yanqi Zhou, Wei Li, Peter J Liu, et al. 2020. Exploring the limits of transfer learning with a unified text-to-text trans- former. J. Mach. Learn. Res., 21(140):1-67. Nils Reimers and Iryna Gurevych. 2019. Sentence- BERT: Sentence embeddings using Siamese BERT- networks. In Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natu- ral Language Processing (EMNLP-IJCNLP), pages 3982-3992, Hong Kong, China. Association for Com- putational Linguistics. Leonardo Ribeiro, Mengwen Liu, Iryna Gurevych, Markus Dreyer, and Mohit Bansal. 2022. FactGraph: Evaluating factuality in summarization with semantic graph representations. In Proceedings of the 2022 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, pages 3238-3253, Seattle, United States. Association for Computational Lin- guistics. Victor Sanh, Albert Webson, Colin Raffel, Stephen H Bach, Lintang Sutawika, Zaid Alyafeai, Antoine Chaffin, Arnaud Stiegler, Teven Le Scao, Arun Raja, et al. 2022. Multitask prompted training enables zero- shot task generalization. International Conference on Learning Representations (ICLR). Teven Le Scao, Angela Fan, Christopher Akiki, El- lie Pavlick, Suzana Ili ´c, Daniel Hesslow, Roman Castagné, Alexandra Sasha Luccioni, François Yvon, Matthias Gallé, Jonathan Tow, Alexander M. Rush, Stella Biderman, Albert Webson, Pawan Sasanka Am- manamanchi, Thomas Wang, Benoît Sagot, Niklas Muennighoff, Albert Villanova del Moral, Olatunji Ruwase, Rachel Bawden, Stas Bekman, Angelina McMillan-Major, Iz Beltagy, Huu Nguyen, Lucile Saulnier, Samson Tan, Pedro",
      "chunk_index": 17
    },
    {
      "index": 226,
      "chunk_id": "FIB2022_chunk_18",
      "source_id": "FIB2022",
      "text": "Jonathan Tow, Alexander M. Rush, Stella Biderman, Albert Webson, Pawan Sasanka Am- manamanchi, Thomas Wang, Benoît Sagot, Niklas Muennighoff, Albert Villanova del Moral, Olatunji Ruwase, Rachel Bawden, Stas Bekman, Angelina McMillan-Major, Iz Beltagy, Huu Nguyen, Lucile Saulnier, Samson Tan, Pedro Ortiz Suarez, Vic- tor Sanh, Hugo Laurençon, Yacine Jernite, Julien Launay, Margaret Mitchell, Colin Raffel, Aaron Gokaslan, Adi Simhi, Aitor Soroa, Alham Fikri Aji, Amit Alfassy, Anna Rogers, Ariel Kreisberg Nitzav, Canwen Xu, Chenghao Mou, Chris Emezue, Christopher Klamm, Colin Leong, Daniel van Strien, David Ifeoluwa Adelani, Dragomir Radev, Ed- uardo González Ponferrada, Efrat Levkovizh, Ethan Kim, Eyal Bar Natan, Francesco De Toni, Gérard Dupont, Germán Kruszewski, Giada Pistilli, Hady Elsahar, Hamza Benyamina, Hieu Tran, Ian Yu, Idris Abdulmumin, Isaac Johnson, Itziar Gonzalez-Dios, Javier de la Rosa, Jenny Chim, Jesse Dodge, Jian Zhu, Jonathan Chang, Jörg Frohberg, Joseph Tobing, Joy- deep Bhattacharjee, Khalid Almubarak, Kimbo Chen, Kyle Lo, Leandro V on Werra, Leon Weber, Long Phan, Loubna Ben allal, Ludovic Tanguy, Manan Dey, Manuel Romero Muñoz, Maraim Masoud, María Grandury, Mario Šaško, Max Huang, Max- imin Coavoux, Mayank Singh, Mike Tian-Jian Jiang, Minh Chien Vu, Mohammad A. Jauhar, Mustafa Ghaleb, Nishant Subramani, Nora Kassner, Nuru- laqilla Khamis, Olivier Nguyen, Omar Espejel, Ona de Gibert, Paulo Villegas, Peter Henderson, Pierre Colombo, Priscilla Amuok, Quentin Lhoest, Rheza Harliman, Rishi Bommasani, Roberto Luis López, Rui Ribeiro, Salomey Osei, Sampo Pyysalo, Se- bastian Nagel, Shamik Bose, Shamsuddeen Hassan Muhammad, Shanya Sharma, Shayne Longpre, So- maieh Nikpoor, Stanislav Silberberg, Suhas Pai, Syd- ney Zink, Tiago Timponi Torrent, Timo Schick, Tris- tan Thrush, Valentin Danchev, Vassilina Nikoulina, Veronika Laippala, Violette Lepercq, Vrinda Prabhu, Zaid Alyafeai, Zeerak Talat, Arun Raja, Benjamin Heinzerling, Chenglei Si, Elizabeth Salesky, Sab- rina J. Mielke, Wilson Y . Lee, Abheesht Sharma, An- drea Santilli, Antoine Chaffin, Arnaud Stiegler, Deba- jyoti Datta, Eliza Szczechla, Gunjan Chhablani, Han Wang, Harshit Pandey, Hendrik Strobelt, Jason Alan Fries, Jos Rozen, Leo Gao, Lintang Sutawika, M Sai- ful Bari, Maged S. Al-shaibani, Matteo Manica, Ni- hal Nayak, Ryan Teehan, Samuel Albanie, Sheng Shen, Srulik Ben-David, Stephen H. Bach, Taewoon Kim, Tali Bers, Thibault Fevry, Trishala Neeraj, Ur- mish Thakker, Vikas Raunak, Xiangru Tang, Zheng- Xin Yong, Zhiqing Sun, Shaked Brody, Yallow Uri, Hadar Tojarieh, Adam Roberts, Hyung Won Chung, Jaesung Tae, Jason Phang, Ofir Press, Conglong Li, Deepak Narayanan, Hatim Bourfoune, Jared Casper, Jeff Rasley, Max Ryabinin, Mayank Mishra, Minjia Zhang, Mohammad Shoeybi, Myriam Peyrounette, Nicolas Patry,",
      "chunk_index": 18
    },
    {
      "index": 227,
      "chunk_id": "FIB2022_chunk_19",
      "source_id": "FIB2022",
      "text": "Sun, Shaked Brody, Yallow Uri, Hadar Tojarieh, Adam Roberts, Hyung Won Chung, Jaesung Tae, Jason Phang, Ofir Press, Conglong Li, Deepak Narayanan, Hatim Bourfoune, Jared Casper, Jeff Rasley, Max Ryabinin, Mayank Mishra, Minjia Zhang, Mohammad Shoeybi, Myriam Peyrounette, Nicolas Patry, Nouamane Tazi, Omar Sanseviero, Patrick von Platen, Pierre Cornette, Pierre François Lavallée, Rémi Lacroix, Samyam Rajbhandari, San- chit Gandhi, Shaden Smith, Stéphane Requena, Suraj Patil, Tim Dettmers, Ahmed Baruwa, Amanpreet Singh, Anastasia Cheveleva, Anne-Laure Ligozat, Arjun Subramonian, Aurélie Névéol, Charles Lover- ing, Dan Garrette, Deepak Tunuguntla, Ehud Reiter, Ekaterina Taktasheva, Ekaterina V oloshina, Eli Bog- danov, Genta Indra Winata, Hailey Schoelkopf, Jan- Christoph Kalo, Jekaterina Novikova, Jessica Zosa Forde, Jordan Clive, Jungo Kasai, Ken Kawamura, Liam Hazan, Marine Carpuat, Miruna Clinciu, Na- joung Kim, Newton Cheng, Oleg Serikov, Omer Antverg, Oskar van der Wal, Rui Zhang, Ruochen Zhang, Sebastian Gehrmann, Shani Pais, Tatiana Shavrina, Thomas Scialom, Tian Yun, Tomasz Lim- isiewicz, Verena Rieser, Vitaly Protasov, Vladislav Mikhailov, Yada Pruksachatkun, Yonatan Belinkov, Zachary Bamberger, Zden ˇek Kasner, Alice Rueda, Amanda Pestana, Amir Feizpour, Ammar Khan, Amy Faranak, Ana Santos, Anthony Hevia, Antigona Unl- dreaj, Arash Aghagol, Arezoo Abdollahi, Aycha Tam- mour, Azadeh HajiHosseini, Bahareh Behroozi, Ben- jamin Ajibade, Bharat Saxena, Carlos Muñoz Ferran- dis, Danish Contractor, David Lansky, Davis David, Douwe Kiela, Duong A. Nguyen, Edward Tan, Emi Baylor, Ezinwanne Ozoani, Fatima Mirza, Frankline Ononiwu, Habib Rezanejad, Hessie Jones, Indrani Bhattacharya, Irene Solaiman, Irina Sedenko, Isar Nejadgholi, Jesse Passmore, Josh Seltzer, Julio Bo- nis Sanz, Karen Fort, Livia Dutra, Mairon Sama- gaio, Maraim Elbadri, Margot Mieskes, Marissa Ger- chick, Martha Akinlolu, Michael McKenna, Mike Qiu, Muhammed Ghauri, Mykola Burynok, Nafis Abrar, Nazneen Rajani, Nour Elkott, Nour Fahmy, Olanrewaju Samuel, Ran An, Rasmus Kromann, Ryan Hao, Samira Alizadeh, Sarmad Shubber, Silas Wang, Sourav Roy, Sylvain Viguier, Thanh Le, Tobi Oyebade, Trieu Le, Yoyo Yang, Zach Nguyen, Ab- hinav Ramesh Kashyap, Alfredo Palasciano, Al- ison Callahan, Anima Shukla, Antonio Miranda- Escalada, Ayush Singh, Benjamin Beilharz, Bo Wang, Caio Brito, Chenxi Zhou, Chirag Jain, Chuxin Xu, Clémentine Fourrier, Daniel León Periñán, Daniel Molano, Dian Yu, Enrique Manjavacas, Fabio Barth, Florian Fuhrimann, Gabriel Altay, Giyased- din Bayrak, Gully Burns, Helena U. Vrabec, Imane Bello, Ishani Dash, Jihyun Kang, John Giorgi, Jonas Golde, Jose David Posada, Karthik Rangasai Sivara- man, Lokesh Bulchandani, Lu Liu, Luisa Shinzato, Madeleine Hahn de Bykhovetz, Maiko Takeuchi, Marc Pàmies, Maria A Castillo, Marianna Nezhurina, Mario Sänger, Matthias Samwald, Michael Cullan, Michael",
      "chunk_index": 19
    },
    {
      "index": 228,
      "chunk_id": "FIB2022_chunk_20",
      "source_id": "FIB2022",
      "text": "Dash, Jihyun Kang, John Giorgi, Jonas Golde, Jose David Posada, Karthik Rangasai Sivara- man, Lokesh Bulchandani, Lu Liu, Luisa Shinzato, Madeleine Hahn de Bykhovetz, Maiko Takeuchi, Marc Pàmies, Maria A Castillo, Marianna Nezhurina, Mario Sänger, Matthias Samwald, Michael Cullan, Michael Weinberg, Michiel De Wolf, Mina Mihalj- cic, Minna Liu, Moritz Freidank, Myungsun Kang, Natasha Seelam, Nathan Dahlberg, Nicholas Michio Broad, Nikolaus Muellner, Pascale Fung, Patrick Haller, Ramya Chandrasekhar, Renata Eisenberg, Robert Martin, Rodrigo Canalli, Rosaline Su, Ruisi Su, Samuel Cahyawijaya, Samuele Garda, Shlok S Deshmukh, Shubhanshu Mishra, Sid Kiblawi, Si- mon Ott, Sinee Sang-aroonsiri, Srishti Kumar, Ste- fan Schweter, Sushil Bharati, Tanmay Laud, Théo Gigant, Tomoya Kainuma, Wojciech Kusa, Yanis Labrak, Yash Shailesh Bajaj, Yash Venkatraman, Yi- fan Xu, Yingxin Xu, Yu Xu, Zhe Tan, Zhongli Xie, Zi- fan Ye, Mathilde Bras, Younes Belkada, and Thomas Wolf. 2022. Bloom: A 176b-parameter open-access multilingual language model. Thomas Scialom, Paul-Alexis Dray, Sylvain Lamprier, Benjamin Piwowarski, Jacopo Staiano, Alex Wang, and Patrick Gallinari. 2021. QuestEval: Summariza- tion asks for fact-based evaluation. In Proceedings of the 2021 Conference on Empirical Methods in Natu- ral Language Processing, pages 6594-6604, Online and Punta Cana, Dominican Republic. Association for Computational Linguistics. Abigail See, Peter J. Liu, and Christopher D. Manning. 2017. Get to the point: Summarization with pointer- generator networks. In Proceedings of the 55th An- nual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pages 1073- 1083, Vancouver, Canada. Association for Computa- tional Linguistics. Priyam Tejaswin, Dhruv Naik, and Pengfei Liu. 2021. How well do you know your summarization datasets? In Findings of the Association for Computational Linguistics: ACL-IJCNLP 2021, pages 3436-3449, Online. Association for Computational Linguistics. James Thorne, Andreas Vlachos, Christos Christodoulopoulos, and Arpit Mittal. 2018. FEVER: a large-scale dataset for fact extraction and VERification. In Proceedings of the 2018 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long Papers), pages 809-819, New Orleans, Louisiana. Association for Computational Linguistics. Liam van der Poel, Ryan Cotterell, and Clara Meis- ter. 2022. Mutual information alleviates hallucina- tions in abstractive summarization. arXiv preprint arXiv:2210.13210. David Wan and Mohit Bansal. 2022. FactPEGASUS: Factuality-aware pre-training and fine-tuning for ab- stractive summarization. In Proceedings of the 2022 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, pages 1010-1028, Seattle, United States. Association for Computational Lin- guistics. Alex Wang, Kyunghyun Cho, and Mike Lewis.",
      "chunk_index": 20
    },
    {
      "index": 229,
      "chunk_id": "FIB2022_chunk_21",
      "source_id": "FIB2022",
      "text": "ab- stractive summarization. In Proceedings of the 2022 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, pages 1010-1028, Seattle, United States. Association for Computational Lin- guistics. Alex Wang, Kyunghyun Cho, and Mike Lewis. 2020a. Asking and answering questions to evaluate the fac- tual consistency of summaries. In Proceedings of the 58th Annual Meeting of the Association for Compu- tational Linguistics, pages 5008-5020, Online. Asso- ciation for Computational Linguistics. Danqing Wang, Pengfei Liu, Yining Zheng, Xipeng Qiu, and Xuanjing Huang. 2020b. Heterogeneous graph neural networks for extractive document summariza- tion. In Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics, pages 6209-6219, Online. Association for Computational Linguistics. Rongxiang Weng, Heng Yu, Xiangpeng Wei, and Wei- hua Luo. 2020. Towards enhancing faithfulness for neural machine translation. In Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP), pages 2675-2684, Online. Association for Computational Linguistics. Thomas Wolf, Lysandre Debut, Victor Sanh, Julien Chaumond, Clement Delangue, Anthony Moi, Pier- ric Cistac, Tim Rault, Remi Louf, Morgan Funtow- icz, Joe Davison, Sam Shleifer, Patrick von Platen, Clara Ma, Yacine Jernite, Julien Plu, Canwen Xu, Teven Le Scao, Sylvain Gugger, Mariama Drame, Quentin Lhoest, and Alexander Rush. 2020. Trans- formers: State-of-the-art natural language processing. In Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing: System Demonstrations, pages 38-45, Online. Association for Computational Linguistics. Jiacheng Xu, Zhe Gan, Yu Cheng, and Jingjing Liu. 2020. Discourse-aware neural extractive text sum- marization. In Proceedings of the 58th Annual Meet- ing of the Association for Computational Linguistics, pages 5021-5031, Online. Association for Computa- tional Linguistics. Jingqing Zhang, Yao Zhao, Mohammad Saleh, and Pe- ter Liu. 2020. Pegasus: Pre-training with extracted gap-sentences for abstractive summarization. In In- ternational Conference on Machine Learning, pages 11328-11339. PMLR. Shiyue Zhang, David Wan, and Mohit Bansal. 2022a. Extractive is not faithful: An investigation of broad unfaithfulness problems in extractive summarization. arXiv preprint arXiv:2209.03549. Susan Zhang, Stephen Roller, Naman Goyal, Mikel Artetxe, Moya Chen, Shuohui Chen, Christopher De- wan, Mona Diab, Xian Li, Xi Victoria Lin, et al. 2022b. Opt: Open pre-trained transformer language models. arXiv preprint arXiv:2205.01068. Hao Zheng and Mirella Lapata. 2019. Sentence cen- trality revisited for unsupervised summarization. In Proceedings of the 57th Annual Meeting of the Asso- ciation for Computational Linguistics, pages 6236- 6247, Florence, Italy. Association for Computational Linguistics. Ming Zhong, Pengfei Liu, Yiran Chen,",
      "chunk_index": 21
    },
    {
      "index": 230,
      "chunk_id": "FIB2022_chunk_22",
      "source_id": "FIB2022",
      "text": "and Mirella Lapata. 2019. Sentence cen- trality revisited for unsupervised summarization. In Proceedings of the 57th Annual Meeting of the Asso- ciation for Computational Linguistics, pages 6236- 6247, Florence, Italy. Association for Computational Linguistics. Ming Zhong, Pengfei Liu, Yiran Chen, Danqing Wang, Xipeng Qiu, and Xuanjing Huang. 2020. Extractive summarization as text matching. In Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics, pages 6197-6208, Online. Association for Computational Linguistics. Ming Zhong, Pengfei Liu, Danqing Wang, Xipeng Qiu, and Xuanjing Huang. 2019. Searching for effective neural extractive summarization: What works and what's next. In Proceedings of the 57th Annual Meet- ing of the Association for Computational Linguistics, pages 1049-1058, Florence, Italy. Association for Computational Linguistics. Qingyu Zhou, Nan Yang, Furu Wei, Shaohan Huang, Ming Zhou, and Tiejun Zhao. 2018. Neural docu- ment summarization by jointly learning to score and select sentences. In Proceedings of the 56th Annual Meeting of the Association for Computational Lin- guistics (Volume 1: Long Papers) , pages 654-663, Melbourne, Australia. Association for Computational Linguistics. A Annotation Instructions The annotators were instructed to mark a summary as factually inconsistent if any information in the summary was not implied in the document. We assume no access to external knowledge so the summary has to be implied solely from the docu- ment. External knowledge is broadly defined as any knowledge that cannot be inferred from com- mon sense alone. For example, the capital of a country or the rules of a sport would be external knowledge. B Sample Edited Summaries We show some examples of documents with the original factually inconsistent reference summary and the edited factually consistent summary on XSum in table 2. C Sample Model-Extracted factually inconsistent We show some examples of documents with model-extracted factually inconsistent summaries on CNN/DM in table 3. D Models Used to Generate Summaries We use the following models to generate sum- maries for XSum and include the respective Hug- gingFace model name: • BLOOM-560m (Scao et al., 2022) - mrm8488/bloom-560m-finetuned-news- summarization-xsum • BART-base (Lewis et al., 2020b) - VictorSanh/bart-base-finetuned-xsum • distil-PEGASUS (Zhang et al., 2020) - sshleifer/distill-pegasus-xsum-16-8 • BART-large (Lewis et al., 2020b) - facebook/bart-large-xsum • PEGASUS (Zhang et al., 2020) - google/pegasus-xsum • distil-BART (Lewis et al., 2020b) - sshleifer/distilbart-xsum-12-6 • T5-large (Raffel et al., 2020) - sysresearch101/t5-large-finetuned-xsum We use greedy decoding for all models with a max- imum generation length of 50 tokens. We use the",
      "chunk_index": 22
    },
    {
      "index": 231,
      "chunk_id": "FIB2022_chunk_23",
      "source_id": "FIB2022",
      "text": "et al., 2020) - google/pegasus-xsum • distil-BART (Lewis et al., 2020b) - sshleifer/distilbart-xsum-12-6 • T5-large (Raffel et al., 2020) - sysresearch101/t5-large-finetuned-xsum We use greedy decoding for all models with a max- imum generation length of 50 tokens. We use the following models to generate sum- maries for CNN/DM. See Zhang et al. (2022a) for more description of the models. • Oracle (Lin, 2004) • Oracle (discourse) (Xu et al., 2020) • RNN Ext RL (Chen and Bansal, 2018) • BanditSumm (Dong et al., 2018) • NeuSumm (Zhou et al., 2018) • Refresh (Narayan et al., 2018c) Document Original Ref. Summary Edited Ref. Summary West Midlands Ambulance Service said the car was discovered on Sunday at 09:35 GMT by two cyclists in Crakemarsh near Uttoxeter, Staffordshire. A spokesman said the black Ford Fiesta appeared to have hit a tree in very foggy conditions on the B5030. The girl, in the back of the car, was treated at hospital for minor injuries. The man, who was 25 and from the local area, has not yet been named ... A five-year-old girl has been found with her dead father in a crashed car which had been in a ditch \"for some time\". A girl has been found in a crashed car. Aiden Webb, 22, from Norwich, was climbing Fansipan mountain alone on Friday when he fell down a ravine and lost his way ... in the fall on the 3,100m (10,300ft) high Fansipan mountain in the north of Vietnam ... A Foreign and Commonwealth Office spokeswoman said: \"We are supporting the family of Aiden Webb, a British man reported missing in Vietnam. We are working closely with the local authorities leading the search.\" A British man is missing in Vietnam after falling while attempting to climb the country's highest mountain. A British man is missing in Vietnam after falling while attempting to climb a mountain. Table 2: These examples have id 34696511 and id 36459564 respectively. Document Model-Extracted Factually Inconsistent Summary the california public utilities commission on thursday said it is ordering pacific gas & electric co. to pay a record 1.6 billion penalty ... 850 million will go to \" gas transmission pipeline safety infrastructure improvements , \" the commission said ... pg & e failed to uphold the public 's trust , \" commission president michael picker said ... the company 's chief executive officer said ... \" since the 2010",
      "chunk_index": 23
    },
    {
      "index": 232,
      "chunk_id": "FIB2022_chunk_24",
      "source_id": "FIB2022",
      "text": "pipeline safety infrastructure improvements , \" the commission said ... pg & e failed to uphold the public 's trust , \" commission president michael picker said ... the company 's chief executive officer said ... \" since the 2010 explosion of our natural gas transmission pipeline in san bruno , we have worked hard to do the right thing for the victims , their families and the community of san bruno , \" tony earley said ... ... 850 million will go to \" gas transmission pipeline safety infrastructure improvements , \" the commission said . \" since the 2010 explosion of our natural gas transmission pipeline in san bruno , we have worked hard to do the right thing for the victims , their families and the community of san bruno ... a passenger on an atlanta-bound air canada flight told a cnn reporter on the plane friday that a stranger sitting behind him tried to choke him . oliver minatel , 22 , said he was sleeping on air canada flight 8623 from toronto when he felt something around his neck ... \" i forced it ( the cord ) down and then other people came to help , and then i got out and he started saying that we were here to kill him , \" minatel said . the man was not restrained for the rest of the trip , but the flight crew told him to stay seated with his seat belt on . the man kept trying to get out of his seat but other passengers yelled at him whenever he tried to stand up . oliver minatel , 22 , said he was sleeping on air canada flight 8623 from toronto when he felt something around his neck . the man kept trying to get out of his seat but other passengers yelled at him whenever he tried to stand up . the suspect was escorted off the plane . Table 3: Two examples of model-extracted factually inconsistent summaries. The annotations were sourced from Zhang et al. (2022a). These examples have id 41c6edecee127c396d17e2e9115a4a89252cc52b and id 32655a04c9e4733a1ae4b210a045bc6e0d443d85 respectively. The first example uses Textrank (Mihalcea and Tarau, 2004) to extract the summary. It is factually incorrect since 'we' refers to pg & e and not the commission. The second example uses MatchSumm (Zhong et al., 2020) to extract the summary. It is factually inconsistent",
      "chunk_index": 24
    },
    {
      "index": 233,
      "chunk_id": "FIB2022_chunk_25",
      "source_id": "FIB2022",
      "text": "(Mihalcea and Tarau, 2004) to extract the summary. It is factually incorrect since 'we' refers to pg & e and not the commission. The second example uses MatchSumm (Zhong et al., 2020) to extract the summary. It is factually inconsistent since the man refers to the stranger and not Oliver Minatel. • BERT+LSTM+PN+RL (Zhong et al., 2019) • MatchSumm (Zhong et al., 2020) • HeterGraph (Wang et al., 2020b) • Lead3 • Textrank (Mihalcea and Tarau, 2004) • Textrank (ST) (Reimers and Gurevych, 2019) • PacSum (tfidf) (Zheng and Lapata, 2019) • PacSum (bert) • MI-unsup (Padmakumar and He, 2021) E Prompt Templates We use the following 3 prompt templates for all models, where [input] is replaced with the docu- ment: • \"[input]\" • \"The summary of \"[input]\" is \" • \"Summarize: [input]\" F Accuracies Across All Scoring Functions We show the performance of all the models across different scoring functions for XSum in table 4, table 5, table 6, and table 7 and for CNN/DM in table 8, table 9, table 10, and table 11. G Accuracies from MFMA-Generated Summaries We show the performance of different models on MFMA-generated summaries broken down by the model used to generate the summary for XSum us- ing different scoring functions in table 12, table 13, table 14, and table 15. H Accuracies from FactCC-Generated Summaries We show the performance of different models on FactCC-generated summaries broken down by the method used to generate the summary using differ- ent scoring functions for XSum in table 16, table 17, table 18, table 19 and for CNN/DM in table 20, ta- ble 21, table 22, table 23. I Accuracies from Factual Model-Generated Summaries We show the performance of different models on factually consistent model-generated summaries broken down by the model used to generate the summary using different scoring functions on XSum in table 24, table 25, table 26, and table 27 and on CNN/DM in table 28, table 29, table 30, and table 31 J Accuracies from FIB Summaries We show the performance of different models on FIB broken down by the model used to generate the summary using different scoring functions for XSum in table 32, table 33, table 34, and table 35 and for CNN/DM in table 36, table 37, table 38, and table 39. K Accuracies from Models Used to Generate Summaries We show the performance of different models using the same",
      "chunk_index": 25
    },
    {
      "index": 234,
      "chunk_id": "FIB2022_chunk_26",
      "source_id": "FIB2022",
      "text": "table 32, table 33, table 34, and table 35 and for CNN/DM in table 36, table 37, table 38, and table 39. K Accuracies from Models Used to Generate Summaries We show the performance of different models using the same models to generate the alternative sum- maries for XSum using different scoring functions in table 40. Model FIR FCMG FIB FactCC MFMA T0-3B 53.2 41.6 57.6 87.6 85.1 T0 29.6 34.9 46.6 89.8 83.9 FLAN-T5-xl 58.1 47.8 59.9 87.3 85.6 FLAN-T5-xxl 59.0 51.3 63.7 87.1 87.3 T5-LM-Adapt-xl 81.3 49.5 68.7 78.7 87.5 T5-LM-Adapt-xxl 81.7 50.7 69.8 84.2 88.7 GPT-Neo-1.3B 88.0 45.7 72.1 68.9 87.1 GPT2-XL 84.9 46.3 69.2 71.5 83.2 GPT-Neo-2.7B 87.8 47.7 72.3 72.2 85.1 GPTJ-6B 88.0 51.2 75.4 74.0 87.3 GPT-Neox-20B 82.9 49.6 73.4 74.1 86.4 BLOOM 84.9 46.2 72.4 75.1 88.1 BLOOM-7B1 85.7 43.8 71.8 71.1 86.5 BLOOM-3B 89.3 43.2 72.6 70.4 86.6 BLOOM-1B7 88.9 42.9 70.5 67.8 87.1 BLOOM-1B1 87.5 41.3 68.8 64.0 85.3 OPT-175B 84.4 48.3 75.1 71.2 87.0 OPT-66B 83.5 47.8 73.9 70.8 87.2 OPT-30B 84.4 48.3 73.8 72.0 87.2 OPT-13B 85.1 49.0 72.9 71.6 86.5 OPT-6.7B 83.3 47.4 71.3 70.5 86.3 OPT-2.7B 84.4 48.1 71.3 70.5 85.8 OPT-1.3B 85.7 46.3 69.7 70.5 86.0 Table 4: The performance of the models on XSum with various alternative-choices using avg. PMI as the scoring function. Model FIR FCMG FIB FactCC MFMA T0-3B 20.0 15.5 29.1 97.7 68.2 T0 14.9 21.4 33.0 96.9 73.2 FLAN-T5-xl 23.6 16.2 29.4 97.7 68.9 FLAN-T5-xxl 21.6 17.6 32.1 98.1 72.0 T5-LM-Adapt-xl 34.1 17.7 23.9 93.1 62.3 T5-LM-Adapt-xxl 28.1 19.2 26.4 95.7 67.0 GPT-Neo-1.3B 37.4 18.1 24.7 94.7 59.1 GPT2-XL 33.6 19.3 26.0 95.3 60.7 GPT-Neo-2.7B 35.9 19.5 26.9 95.8 62.0 GPTJ-6B 28.3 21.1 28.4 96.8 68.9 GPT-Neox-20B 23.4 20.8 30.5 97.0 69.8 BLOOM 26.5 24.3 32.1 97.8 73.1 BLOOM-7B1 39.9 21.5 28.8 96.3 65.6 BLOOM-3B 44.3 20.5 28.2 95.7 63.9 BLOOM-1B7 49.0 20.8 27.1 94.7 61.2 BLOOM-1B1 51.4 20.4 27.4 93.0 59.7 OPT-175B 16.9 23.1 34.4 97.9 77.1 OPT-66B 18.7 22.8 32.3 97.5 75.1 OPT-30B 20.3 21.6 32.6 97.4 72.4 OPT-13B 22.5 21.4 31.0 96.6 73.2 OPT-6.7B 22.0 21.3 28.7 96.7 70.2 OPT-2.7B 29.0 20.1 28.4 96.7 68.7 OPT-1.3B 30.7 19.9 26.3 95.9 64.7 Table 5: The performance of the models on XSum with various alternative-choices using avg. LL as the scoring function. Model FIR FCMG FIB FactCC MFMA T0-3B 18.3 46.0 49.1 83.2 83.7 T0 16.7 36.8 45.6 89.0 83.7",
      "chunk_index": 26
    },
    {
      "index": 235,
      "chunk_id": "FIB2022_chunk_27",
      "source_id": "FIB2022",
      "text": "26.3 95.9 64.7 Table 5: The performance of the models on XSum with various alternative-choices using avg. LL as the scoring function. Model FIR FCMG FIB FactCC MFMA T0-3B 18.3 46.0 49.1 83.2 83.7 T0 16.7 36.8 45.6 89.0 83.7 FLAN-T5-xl 16.7 52.0 49.0 82.0 82.9 FLAN-T5-xxl 16.7 51.2 53.6 81.3 85.6 T5-LM-Adapt-xl 39.0 52.6 54.7 69.9 83.8 T5-LM-Adapt-xxl 35.4 51.5 55.3 76.8 85.1 GPT-Neo-1.3B 58.4 46.5 57.2 60.5 83.9 GPT2-XL 56.1 51.6 54.9 64.5 80.2 GPT-Neo-2.7B 57.5 49.4 55.2 66.3 82.3 GPTJ-6B 55.7 54.9 57.8 66.7 84.3 GPT-Neox-20B 53.0 49.5 58.1 69.2 83.6 BLOOM 53.0 48.9 59.3 72.9 84.7 BLOOM-7B1 59.5 48.5 57.5 67.5 85.2 BLOOM-3B 59.5 49.3 59.9 65.7 85.3 BLOOM-1B7 63.3 46.2 56.6 63.9 83.4 BLOOM-1B1 60.8 44.7 54.9 58.6 82.3 OPT-175B 50.3 50.5 60.0 65.2 86.1 OPT-66B 53.5 50.9 57.5 65.1 84.5 OPT-30B 58.1 49.8 57.6 66.6 85.4 OPT-13B 54.6 51.3 56.6 65.3 83.7 OPT-6.7B 56.3 50.5 55.5 65.3 84.3 OPT-2.7B 56.6 52.1 55.4 66.2 84.2 OPT-1.3B 57.2 48.9 54.0 64.7 82.6 Table 6: The performance of the models on XSum with various alternative-choices using PMI as the scoring function. Model FIR FCMG FIB FactCC MFMA T0-3B 45.2 15.9 34.4 98.5 73.5 T0 34.7 23.0 38.9 97.9 78.0 FLAN-T5-xl 52.8 18.5 35.6 98.3 74.9 FLAN-T5-xxl 49.4 18.5 39.2 98.3 78.1 T5-LM-Adapt-xl 82.6 23.8 44.6 98.1 71.4 T5-LM-Adapt-xxl 72.2 22.0 43.4 98.3 75.1 GPT-Neo-1.3B 83.3 22.2 46.9 97.0 66.1 GPT2-XL 78.6 22.1 45.6 97.3 67.9 GPT-Neo-2.7B 81.3 23.1 46.8 97.1 67.6 GPTJ-6B 72.2 22.9 47.2 98.0 74.6 GPT-Neox-20B 68.2 26.9 47.7 97.9 75.9 BLOOM 70.6 24.5 48.6 98.5 78.8 BLOOM-7B1 81.7 24.4 48.4 97.6 71.9 BLOOM-3B 85.1 24.4 48.6 97.3 68.5 BLOOM-1B7 87.3 25.4 48.5 96.2 65.1 BLOOM-1B1 90.4 24.7 49.3 96.2 64.2 OPT-175B 53.2 26.4 48.1 98.3 81.8 OPT-66B 61.0 25.5 47.4 98.3 80.2 OPT-30B 60.6 25.6 47.0 98.1 78.3 OPT-13B 66.8 24.6 46.3 98.1 78.8 OPT-6.7B 66.1 25.9 45.6 97.6 75.7 OPT-2.7B 72.6 24.6 45.7 98.1 73.2 OPT-1.3B 77.3 23.1 45.2 97.4 71.8 Table 7: The performance of the models on XSum with various alternative-choices using LL as the scoring function. Model FIR FCMG FIB FactCC MFMA T0-3B 65.6 7.0 17.7 82.4 98.0 T0 50.0 4.4 11.4 79.9 92.0 FLAN-T5-xl 65.6 7.4 16.0 79.7 100.0 FLAN-T5-xxl 59.4 6.3 13.8 76.5 100.0 T5-LM-Adapt-xl 62.5 4.9 12.7 79.6 99.0 T5-LM-Adapt-xxl 59.4 6.0 12.0 76.8 99.0 GPT-Neo-1.3B 78.1 6.4 8.7 77.7 100.0 GPT2-XL 78.1 8.2 9.8 79.5 99.0",
      "chunk_index": 27
    },
    {
      "index": 236,
      "chunk_id": "FIB2022_chunk_28",
      "source_id": "FIB2022",
      "text": "4.4 11.4 79.9 92.0 FLAN-T5-xl 65.6 7.4 16.0 79.7 100.0 FLAN-T5-xxl 59.4 6.3 13.8 76.5 100.0 T5-LM-Adapt-xl 62.5 4.9 12.7 79.6 99.0 T5-LM-Adapt-xxl 59.4 6.0 12.0 76.8 99.0 GPT-Neo-1.3B 78.1 6.4 8.7 77.7 100.0 GPT2-XL 78.1 8.2 9.8 79.5 99.0 GPT-Neo-2.7B 78.1 7.9 10.1 78.2 99.0 GPTJ-6B 78.1 7.5 8.1 82.0 99.0 GPT-Neox-20B 71.9 8.6 10.5 76.2 97.0 BLOOM 75.0 10.8 9.2 79.3 99.0 BLOOM-7B1 84.4 9.8 10.3 81.8 99.0 BLOOM-3B 78.1 8.0 7.9 78.2 100.0 BLOOM-1B7 84.4 6.8 9.2 76.3 99.0 BLOOM-1B1 84.4 7.5 11.2 75.8 100.0 OPT-175B 71.9 11.9 10.7 75.2 98.0 OPT-66B 71.9 8.8 9.2 75.9 99.0 OPT-30B 71.9 11.1 9.0 77.3 100.0 OPT-13B 75.0 8.2 9.6 79.5 99.0 OPT-6.7B 81.2 10.2 9.9 79.8 99.0 OPT-2.7B 75.0 7.8 9.6 74.1 98.0 OPT-1.3B 78.1 6.8 8.1 75.3 100.0 Table 8: The performance of the models on CNN/DM with various alternative-choices using avg. PMI as the scoring function. Model FIR FCMG FIB FactCC MFMA T0-3B 40.6 3.3 11.6 90.3 100.0 T0 37.5 2.2 8.3 90.8 100.0 FLAN-T5-xl 40.6 1.7 9.0 91.4 100.0 FLAN-T5-xxl 40.6 1.1 6.1 88.9 100.0 T5-LM-Adapt-xl 40.6 1.6 6.6 88.2 99.0 T5-LM-Adapt-xxl 31.2 1.2 5.3 89.8 100.0 GPT-Neo-1.3B 46.9 0.7 1.3 93.6 99.0 GPT2-XL 56.2 0.9 2.6 92.5 99.0 GPT-Neo-2.7B 50.0 0.8 1.8 92.9 97.0 GPTJ-6B 46.9 0.5 2.0 95.2 99.0 GPT-Neox-20B 40.6 0.2 1.8 94.2 98.0 BLOOM 40.6 0.3 1.8 93.8 99.0 BLOOM-7B1 50.0 1.0 2.8 95.9 100.0 BLOOM-3B 53.1 1.2 2.2 93.5 100.0 BLOOM-1B7 53.1 0.9 2.2 92.9 99.0 BLOOM-1B1 62.5 1.3 2.6 93.6 98.0 OPT-175B 40.6 0.6 2.2 91.4 99.0 OPT-66B 43.8 0.9 2.2 92.8 99.0 OPT-30B 43.8 0.8 2.0 94.1 99.0 OPT-13B 43.8 0.9 1.8 95.5 99.0 OPT-6.7B 56.2 0.9 2.6 94.6 98.0 OPT-2.7B 43.8 1.2 2.6 92.9 98.0 OPT-1.3B 46.9 1.2 2.0 92.5 98.0 Table 9: The performance of the models on CNN/DM with various alternative-choices using avg. LL as the scoring function. Model FIR FCMG FIB FactCC MFMA T0-3B 46.9 1.6 8.5 76.6 100.0 T0 28.1 1.2 6.1 75.9 96.0 FLAN-T5-xl 40.6 1.6 7.2 74.6 100.0 FLAN-T5-xxl 34.4 1.7 5.9 69.9 100.0 T5-LM-Adapt-xl 34.4 1.1 6.1 69.4 98.0 T5-LM-Adapt-xxl 34.4 0.9 5.3 68.4 99.0 GPT-Neo-1.3B 50.0 0.5 3.7 69.8 99.0 GPT2-XL 43.8 0.4 3.5 69.8 99.0 GPT-Neo-2.7B 46.9 0.4 2.6 66.9 99.0 GPTJ-6B 59.4 0.5 2.4 73.6 99.0 GPT-Neox-20B 56.2 0.4 2.4 69.0 99.0 BLOOM 40.6 0.5 2.4 69.7 99.0 BLOOM-7B1 56.2 0.5 2.9 73.9 100.0 BLOOM-3B 56.2 0.5 2.9",
      "chunk_index": 28
    },
    {
      "index": 237,
      "chunk_id": "FIB2022_chunk_29",
      "source_id": "FIB2022",
      "text": "GPT2-XL 43.8 0.4 3.5 69.8 99.0 GPT-Neo-2.7B 46.9 0.4 2.6 66.9 99.0 GPTJ-6B 59.4 0.5 2.4 73.6 99.0 GPT-Neox-20B 56.2 0.4 2.4 69.0 99.0 BLOOM 40.6 0.5 2.4 69.7 99.0 BLOOM-7B1 56.2 0.5 2.9 73.9 100.0 BLOOM-3B 56.2 0.5 2.9 71.1 100.0 BLOOM-1B7 53.1 0.5 3.3 64.8 98.0 BLOOM-1B1 59.4 0.5 3.5 68.4 99.0 OPT-175B 53.1 0.7 2.8 70.4 98.0 OPT-66B 59.4 0.5 2.4 68.1 99.0 OPT-30B 53.1 0.6 3.1 71.9 99.0 OPT-13B 43.8 0.6 3.1 71.3 98.0 OPT-6.7B 53.1 0.5 2.4 72.6 99.0 OPT-2.7B 56.2 0.5 3.1 66.0 98.0 OPT-1.3B 53.1 0.5 3.7 69.3 99.0 Table 10: The performance of the models on CNN/DM with various alternative-choices using PMI as the scoring function. Model FIR FCMG FIB FactCC MFMA T0-3B 71.9 45.1 52.7 98.7 97.0 T0 62.5 37.4 42.7 97.4 97.0 FLAN-T5-xl 75.0 42.8 48.6 98.4 98.0 FLAN-T5-xxl 68.8 26.9 35.5 97.0 99.0 T5-LM-Adapt-xl 90.6 39.7 45.1 97.0 89.0 T5-LM-Adapt-xxl 68.8 31.4 32.6 98.7 94.0 GPT-Neo-1.3B 78.1 24.3 20.1 97.4 99.0 GPT2-XL 81.2 26.9 26.5 96.6 97.0 GPT-Neo-2.7B 75.0 24.1 19.9 97.0 98.0 GPTJ-6B 78.1 21.0 18.6 97.9 99.0 GPT-Neox-20B 75.0 22.5 20.4 98.0 99.0 BLOOM 59.4 16.7 16.6 98.3 100.0 BLOOM-7B1 78.1 22.1 21.0 97.6 100.0 BLOOM-3B 78.1 25.2 20.6 98.0 98.0 BLOOM-1B7 81.2 23.4 20.1 97.0 98.0 BLOOM-1B1 84.4 26.2 23.2 97.4 98.0 OPT-175B 65.6 25.9 20.8 97.3 99.0 OPT-66B 68.8 26.7 23.6 97.9 99.0 OPT-30B 75.0 25.3 21.0 97.9 100.0 OPT-13B 68.8 28.1 24.3 97.9 100.0 OPT-6.7B 78.1 29.4 26.7 98.7 100.0 OPT-2.7B 71.9 29.5 25.8 98.3 100.0 OPT-1.3B 75.0 27.8 23.8 98.3 100.0 Table 11: The performance of the models on CNN/DM with various alternative-choices using LL as the scoring function. Model BART-base T5-base T0-3B 93.4 74.9 T0 94.2 71.2 FLAN-T5-xl 94.8 74.3 FLAN-T5-xxl 95.0 77.9 T5-LM-Adapt-xl 94.2 79.3 T5-LM-Adapt-xxl 95.0 81.0 GPT-Neo-1.3B 93.6 79.1 GPT2-XL 91.7 72.9 GPT-Neo-2.7B 94.4 73.7 GPTJ-6B 94.2 78.8 GPT-Neox-20B 95.2 75.7 BLOOM 95.0 79.6 BLOOM-7B1 94.6 76.5 BLOOM-3B 94.4 77.1 BLOOM-1B7 95.0 77.4 BLOOM-1B1 93.2 75.7 OPT-175B 94.6 77.7 OPT-66B 95.2 77.4 OPT-30B 94.8 77.9 OPT-13B 95.0 76.0 OPT-6.7B 95.0 75.7 OPT-2.7B 94.0 75.7 OPT-1.3B 93.8 76.5 Table 12: The performance of the models on XSum with MFMA-generated alternative-choices using avg. PMI as the scoring function. Model BART-base T5-base T0-3B 79.7 54.2 T0 83.0 61.2 FLAN-T5-xl 81.0 54.2 FLAN-T5-xxl 82.8 58.7 T5-LM-Adapt-xl 71.2 51.4 T5-LM-Adapt-xxl 74.9 57.3 GPT-Neo-1.3B 65.6 51.1 GPT2-XL 66.5 53.6 GPT-Neo-2.7B 69.6 52.8 GPTJ-6B 76.8 59.2",
      "chunk_index": 29
    },
    {
      "index": 238,
      "chunk_id": "FIB2022_chunk_30",
      "source_id": "FIB2022",
      "text": "using avg. PMI as the scoring function. Model BART-base T5-base T0-3B 79.7 54.2 T0 83.0 61.2 FLAN-T5-xl 81.0 54.2 FLAN-T5-xxl 82.8 58.7 T5-LM-Adapt-xl 71.2 51.4 T5-LM-Adapt-xxl 74.9 57.3 GPT-Neo-1.3B 65.6 51.1 GPT2-XL 66.5 53.6 GPT-Neo-2.7B 69.6 52.8 GPTJ-6B 76.8 59.2 GPT-Neox-20B 76.0 62.3 BLOOM 80.1 64.5 BLOOM-7B1 72.3 57.5 BLOOM-3B 71.4 54.7 BLOOM-1B7 69.4 51.1 BLOOM-1B1 67.9 49.7 OPT-175B 83.0 69.9 OPT-66B 81.8 67.0 OPT-30B 78.7 64.8 OPT-13B 79.5 65.6 OPT-6.7B 76.0 63.1 OPT-2.7B 74.1 62.0 OPT-1.3B 70.8 57.3 Table 13: The performance of the models on XSum with MFMA-generated alternative-choices using avg. LL as the scoring function. Model BART-base T5-base T0-3B 93.6 71.5 T0 94.2 70.9 FLAN-T5-xl 93.2 70.4 FLAN-T5-xxl 94.4 74.9 T5-LM-Adapt-xl 91.9 74.0 T5-LM-Adapt-xxl 93.6 74.6 GPT-Neo-1.3B 92.3 73.7 GPT2-XL 91.1 66.8 GPT-Neo-2.7B 92.3 70.1 GPTJ-6B 93.2 73.5 GPT-Neox-20B 93.4 71.5 BLOOM 93.2 74.3 BLOOM-7B1 93.8 74.6 BLOOM-3B 94.0 74.6 BLOOM-1B7 93.4 71.2 BLOOM-1B1 91.7 70.7 OPT-175B 94.0 76.5 OPT-66B 93.4 73.7 OPT-30B 94.4 74.3 OPT-13B 94.2 70.9 OPT-6.7B 93.0 73.7 OPT-2.7B 93.6 72.6 OPT-1.3B 92.1 70.9 Table 14: The performance of the models on MFMA-generated alternative-choices using PMI as the scoring function. Model BART-base T5-base T0-3B 85.9 58.4 T0 88.2 65.6 FLAN-T5-xl 87.4 59.5 FLAN-T5-xxl 89.6 64.0 T5-LM-Adapt-xl 80.3 60.6 T5-LM-Adapt-xxl 84.7 63.4 GPT-Neo-1.3B 73.3 57.3 GPT2-XL 75.4 58.7 GPT-Neo-2.7B 75.8 57.5 GPTJ-6B 83.2 64.0 GPT-Neox-20B 83.2 67.0 BLOOM 86.3 69.6 BLOOM-7B1 78.3 64.0 BLOOM-3B 76.4 58.9 BLOOM-1B7 72.0 56.7 BLOOM-1B1 72.3 54.2 OPT-175B 88.6 73.5 OPT-66B 86.1 72.9 OPT-30B 86.1 68.7 OPT-13B 86.1 69.8 OPT-6.7B 84.3 65.1 OPT-2.7B 81.2 63.4 OPT-1.3B 78.5 63.7 Table 15: The performance of the models on XSum with MFMA-generated alternative-choices using LL as the scoring function. Model Date Swap Entity Swap Negation Number Swap Pronoun T0-3B 76.4 86.6 94.5 76.5 78.7 T0 85.5 86.9 93.9 92.6 84.8 FLAN-T5-xl 72.7 86.0 96.1 82.4 72.6 FLAN-T5-xxl 76.4 85.5 97.2 85.3 67.1 T5-LM-Adapt-xl 67.3 75.9 89.9 60.3 65.2 T5-LM-Adapt-xxl 69.1 81.4 94.5 70.6 72.0 GPT-Neo-1.3B 52.7 66.3 75.5 42.6 72.0 GPT2-XL 60.0 69.2 82.1 41.2 63.4 GPT-Neo-2.7B 65.5 65.7 81.2 54.4 70.7 GPTJ-6B 60.0 70.6 85.1 54.4 63.4 GPT-Neox-20B 61.8 68.9 86.2 55.9 62.8 BLOOM 60.0 72.1 83.4 67.6 66.5 BLOOM-7B1 60.0 71.5 76.8 52.9 65.9 BLOOM-3B 50.9 69.5 75.7 57.4 69.5 BLOOM-1B7 54.5 65.1 70.5 60.3 73.8 BLOOM-1B1 58.2 63.1 65.9 54.4 66.5 OPT-175B 56.4 64.8 83.2 61.8 59.8 OPT-66B 58.2 63.7 84.0 60.3 57.3 OPT-30B 61.8 65.1 84.5 63.2 59.1 OPT-13B",
      "chunk_index": 30
    },
    {
      "index": 239,
      "chunk_id": "FIB2022_chunk_31",
      "source_id": "FIB2022",
      "text": "76.8 52.9 65.9 BLOOM-3B 50.9 69.5 75.7 57.4 69.5 BLOOM-1B7 54.5 65.1 70.5 60.3 73.8 BLOOM-1B1 58.2 63.1 65.9 54.4 66.5 OPT-175B 56.4 64.8 83.2 61.8 59.8 OPT-66B 58.2 63.7 84.0 60.3 57.3 OPT-30B 61.8 65.1 84.5 63.2 59.1 OPT-13B 65.5 68.6 81.6 63.2 55.5 OPT-6.7B 63.6 66.9 80.1 60.3 57.9 OPT-2.7B 60.0 65.1 82.7 51.5 59.1 OPT-1.3B 63.6 63.1 83.2 57.4 58.5 Table 16: The performance of the models on XSum with FactCC-generated alternative-choices using avg. PMI as the scoring function. Model Date Swap Entity Swap Negation Number Swap Pronoun T0-3B 96.4 96.5 98.7 94.1 99.4 T0 100.0 95.3 96.7 97.1 99.4 FLAN-T5-xl 100.0 96.2 98.7 92.6 99.4 FLAN-T5-xxl 98.2 95.9 99.1 98.5 99.4 T5-LM-Adapt-xl 92.7 91.0 92.8 89.7 100.0 T5-LM-Adapt-xxl 94.5 93.3 96.9 89.7 100.0 GPT-Neo-1.3B 96.4 89.5 97.6 88.2 99.4 GPT2-XL 96.4 91.3 97.8 86.8 100.0 GPT-Neo-2.7B 96.4 92.4 98.2 86.8 100.0 GPTJ-6B 98.2 93.9 98.9 88.2 100.0 GPT-Neox-20B 98.2 93.6 99.3 89.7 100.0 BLOOM 98.2 95.3 99.6 92.6 100.0 BLOOM-7B1 98.2 92.7 99.1 85.3 100.0 BLOOM-3B 92.7 91.6 99.1 85.3 100.0 BLOOM-1B7 92.7 89.8 98.5 83.8 99.4 BLOOM-1B1 90.9 86.9 96.7 85.3 99.4 OPT-175B 100.0 95.6 99.3 92.6 100.0 OPT-66B 98.2 94.8 99.6 89.7 100.0 OPT-30B 98.2 95.1 98.9 91.2 100.0 OPT-13B 98.2 94.8 97.8 88.2 100.0 OPT-6.7B 98.2 95.1 98.5 83.8 100.0 OPT-2.7B 98.2 93.9 98.9 86.8 100.0 OPT-1.3B 96.4 91.9 98.5 89.7 99.4 Table 17: The performance of the models on XSum with FactCC-generated alternative-choices using avg. LL as the scoring function. Model Date Swap Entity Swap Negation Number Swap Pronoun T0-3B 83.6 83.7 84.2 80.9 80.5 T0 87.3 86.0 92.3 91.2 86.0 FLAN-T5-xl 80.0 78.8 87.1 83.8 74.4 FLAN-T5-xxl 78.2 79.9 86.2 86.8 69.5 T5-LM-Adapt-xl 70.9 70.9 69.8 64.7 70.1 T5-LM-Adapt-xxl 74.5 75.0 79.9 72.1 75.0 GPT-Neo-1.3B 63.6 63.4 57.1 38.2 72.0 GPT2-XL 65.5 64.0 68.5 42.6 63.4 GPT-Neo-2.7B 65.5 64.8 67.8 54.4 70.7 GPTJ-6B 69.1 66.9 69.4 52.9 63.4 GPT-Neox-20B 65.5 66.0 76.4 55.9 62.8 BLOOM 65.5 69.5 79.9 64.7 66.5 BLOOM-7B1 63.6 67.4 71.3 50.0 65.9 BLOOM-3B 58.2 65.4 67.4 52.9 69.5 BLOOM-1B7 54.5 63.7 63.2 52.9 73.8 BLOOM-1B1 58.2 59.9 56.2 50.0 66.5 OPT-175B 54.5 61.9 71.1 64.7 59.8 OPT-66B 67.3 58.7 73.3 60.3 57.3 OPT-30B 61.8 62.5 73.3 64.7 59.1 OPT-13B 67.3 64.5 69.4 63.2 55.5 OPT-6.7B 67.3 62.8 70.7 57.4 57.9 OPT-2.7B 63.6 65.4 72.2 50.0 59.1 OPT-1.3B 67.3 60.5 71.1 55.9 58.5 Table 18: The performance of",
      "chunk_index": 31
    },
    {
      "index": 240,
      "chunk_id": "FIB2022_chunk_32",
      "source_id": "FIB2022",
      "text": "67.3 58.7 73.3 60.3 57.3 OPT-30B 61.8 62.5 73.3 64.7 59.1 OPT-13B 67.3 64.5 69.4 63.2 55.5 OPT-6.7B 67.3 62.8 70.7 57.4 57.9 OPT-2.7B 63.6 65.4 72.2 50.0 59.1 OPT-1.3B 67.3 60.5 71.1 55.9 58.5 Table 18: The performance of the models on XSum with FactCC-generated alternative-choices using PMI as the scoring function. Model Date Swap Entity Swap Negation Number Swap Pronoun T0-3B 98.2 96.8 100.0 95.6 99.4 T0 98.2 95.6 99.1 98.5 98.8 FLAN-T5-xl 100.0 96.2 100.0 94.1 99.4 FLAN-T5-xxl 98.2 95.6 100.0 98.5 99.4 T5-LM-Adapt-xl 98.2 95.9 100.0 91.2 100.0 T5-LM-Adapt-xxl 98.2 96.8 100.0 89.7 100.0 GPT-Neo-1.3B 96.4 93.9 99.8 88.2 99.4 GPT2-XL 96.4 95.1 99.6 86.8 100.0 GPT-Neo-2.7B 96.4 94.8 99.1 88.2 100.0 GPTJ-6B 98.2 96.2 100.0 88.2 100.0 GPT-Neox-20B 98.2 95.9 99.8 89.7 100.0 BLOOM 100.0 97.1 99.8 91.2 100.0 BLOOM-7B1 98.2 95.3 100.0 86.8 100.0 BLOOM-3B 92.7 94.8 100.0 88.2 100.0 BLOOM-1B7 90.9 93.0 99.3 88.2 99.4 BLOOM-1B1 94.5 92.2 99.6 86.8 99.4 OPT-175B 100.0 96.2 99.8 92.6 100.0 OPT-66B 98.2 97.1 100.0 89.7 100.0 OPT-30B 98.2 96.5 99.6 91.2 100.0 OPT-13B 100.0 96.8 99.8 86.8 100.0 OPT-6.7B 100.0 96.2 99.6 83.8 100.0 OPT-2.7B 100.0 96.5 100.0 86.8 100.0 OPT-1.3B 98.2 94.8 100.0 88.2 99.4 Table 19: The performance of the models on XSum with FactCC-generated alternative-choices using LL as the scoring function. Model Date Swap Entity Swap Negation Number Swap Pronoun T0-3B 81.8 78.3 91.6 75.0 80.0 T0 81.8 73.9 94.0 66.7 73.3 flan-t5-xl 78.2 75.4 92.8 77.8 66.7 flan-t5-xxl 76.4 71.0 90.4 69.4 66.7 t5-lm-adapt-xl 80.0 81.2 84.3 75.0 71.1 t5-lm-adapt-xxl 80.0 71.0 86.7 75.0 66.7 GPT-Neo-1.3B 72.7 75.4 85.5 75.0 75.6 GPT2-XL 78.2 79.7 86.7 75.0 71.1 GPT-Neo-2.7B 74.5 73.9 85.5 80.6 75.6 GPTJ-6B 80.0 76.8 91.6 83.3 75.6 GPT-Neox-20B 67.3 72.5 88.0 77.8 71.1 BLOOM 80.0 75.4 85.5 77.8 75.6 BLOOM-7B1 81.8 78.3 84.3 80.6 84.4 BLOOM-3B 80.0 79.7 75.9 80.6 75.6 BLOOM-1B7 78.2 73.9 77.1 77.8 75.6 BLOOM-1B1 80.0 71.0 78.3 77.8 73.3 OPT-175B 70.9 72.5 84.3 75.0 68.9 OPT-66B 69.1 72.5 83.1 75.0 77.8 OPT-30B 74.5 68.1 88.0 77.8 77.8 OPT-13B 80.0 78.3 84.3 72.2 77.8 OPT-6.7B 76.4 84.1 88.0 66.7 71.1 OPT-2.7B 65.5 76.8 81.9 69.4 68.9 OPT-1.3B 72.7 75.4 79.5 72.2 73.3 Table 20: The performance of the models on CNN/DM with FactCC-generated alternative-choices using avg. PMI as the scoring function. Model Date Swap Entity Swap Negation Number Swap Pronoun T0-3B 92.7 89.9 91.6 86.1 88.9 T0",
      "chunk_index": 32
    },
    {
      "index": 241,
      "chunk_id": "FIB2022_chunk_33",
      "source_id": "FIB2022",
      "text": "72.7 75.4 79.5 72.2 73.3 Table 20: The performance of the models on CNN/DM with FactCC-generated alternative-choices using avg. PMI as the scoring function. Model Date Swap Entity Swap Negation Number Swap Pronoun T0-3B 92.7 89.9 91.6 86.1 88.9 T0 92.7 92.8 94.0 80.6 86.7 flan-t5-xl 94.5 92.8 91.6 86.1 88.9 flan-t5-xxl 92.7 88.4 94.0 80.6 82.2 t5-lm-adapt-xl 89.1 88.4 89.2 86.1 86.7 t5-lm-adapt-xxl 90.9 92.8 88.0 88.9 86.7 GPT-Neo-1.3B 87.3 97.1 97.6 86.1 93.3 GPT2-XL 87.3 94.2 95.2 88.9 93.3 GPT-Neo-2.7B 89.1 95.7 94.0 91.7 91.1 GPTJ-6B 92.7 95.7 97.6 91.7 95.6 GPT-Neox-20B 90.9 95.7 96.4 91.7 93.3 BLOOM 92.7 94.2 95.2 88.9 95.6 BLOOM-7B1 92.7 97.1 98.8 91.7 95.6 BLOOM-3B 94.5 95.7 95.2 83.3 93.3 BLOOM-1B7 92.7 95.7 94.0 86.1 91.1 BLOOM-1B1 90.9 97.1 95.2 86.1 93.3 OPT-175B 89.1 92.8 94.0 91.7 86.7 OPT-66B 87.3 94.2 95.2 91.7 93.3 OPT-30B 89.1 94.2 97.6 94.4 93.3 OPT-13B 94.5 95.7 96.4 94.4 95.6 OPT-6.7B 92.7 97.1 95.2 91.7 93.3 OPT-2.7B 89.1 95.7 95.2 88.9 91.1 OPT-1.3B 89.1 94.2 95.2 86.1 93.3 Table 21: The performance of the models on CNN/DM with FactCC-generated alternative-choices using avg. LL as the scoring function. Model Date Swap Entity Swap Negation Number Swap Pronoun T0-3B 74.5 73.9 83.1 72.2 75.6 T0 78.2 72.5 88.0 63.9 66.7 flan-t5-xl 76.4 73.9 79.5 75.0 64.4 flan-t5-xxl 74.5 65.2 80.7 66.7 55.6 t5-lm-adapt-xl 69.1 75.4 67.5 66.7 64.4 t5-lm-adapt-xxl 72.7 68.1 72.3 69.4 55.6 GPT-Neo-1.3B 63.6 76.8 68.7 63.9 71.1 GPT2-XL 74.5 75.4 67.5 66.7 60.0 GPT-Neo-2.7B 63.6 71.0 65.1 63.9 68.9 GPTJ-6B 69.1 72.5 74.7 86.1 68.9 GPT-Neox-20B 61.8 68.1 74.7 77.8 62.2 BLOOM 69.1 68.1 74.7 75.0 60.0 BLOOM-7B1 74.5 73.9 74.7 69.4 75.6 BLOOM-3B 74.5 76.8 65.1 72.2 66.7 BLOOM-1B7 65.5 69.6 57.8 66.7 66.7 BLOOM-1B1 70.9 68.1 67.5 66.7 68.9 OPT-175B 65.5 68.1 75.9 77.8 64.4 OPT-66B 61.8 68.1 71.1 69.4 68.9 OPT-30B 72.7 66.7 78.3 72.2 68.9 OPT-13B 74.5 73.9 71.1 69.4 64.4 OPT-6.7B 69.1 79.7 78.3 63.9 60.0 OPT-2.7B 65.5 73.9 63.9 58.3 62.2 OPT-1.3B 65.5 72.5 69.9 69.4 66.7 Table 22: The performance of the models on CNN/DM with FactCC-generated alternative-choices using PMI as the scoring function. Model Date Swap Entity Swap Negation Number Swap Pronoun T0-3B 96.4 100.0 100.0 94.4 100.0 T0 96.4 100.0 100.0 88.9 95.6 flan-t5-xl 98.2 100.0 100.0 91.7 97.8 flan-t5-xxl 96.4 98.6 98.8 88.9 97.8 t5-lm-adapt-xl 98.2 98.6 97.6 88.9 97.8 t5-lm-adapt-xxl 96.4 100.0 100.0 94.4 100.0",
      "chunk_index": 33
    },
    {
      "index": 242,
      "chunk_id": "FIB2022_chunk_34",
      "source_id": "FIB2022",
      "text": "Negation Number Swap Pronoun T0-3B 96.4 100.0 100.0 94.4 100.0 T0 96.4 100.0 100.0 88.9 95.6 flan-t5-xl 98.2 100.0 100.0 91.7 97.8 flan-t5-xxl 96.4 98.6 98.8 88.9 97.8 t5-lm-adapt-xl 98.2 98.6 97.6 88.9 97.8 t5-lm-adapt-xxl 96.4 100.0 100.0 94.4 100.0 GPT-Neo-1.3B 90.9 100.0 100.0 91.7 100.0 GPT2-XL 92.7 97.1 98.8 94.4 97.8 GPT-Neo-2.7B 90.9 98.6 100.0 91.7 100.0 GPTJ-6B 94.5 98.6 100.0 94.4 100.0 GPT-Neox-20B 94.5 100.0 100.0 91.7 100.0 BLOOM 96.4 98.6 100.0 94.4 100.0 BLOOM-7B1 94.5 98.6 98.8 94.4 100.0 BLOOM-3B 96.4 100.0 100.0 88.9 100.0 BLOOM-1B7 94.5 98.6 98.8 94.4 95.6 BLOOM-1B1 94.5 100.0 98.8 91.7 97.8 OPT-175B 94.5 98.6 100.0 94.4 95.6 OPT-66B 94.5 98.6 100.0 94.4 100.0 OPT-30B 94.5 98.6 100.0 94.4 100.0 OPT-13B 94.5 98.6 100.0 94.4 100.0 OPT-6.7B 96.4 100.0 100.0 94.4 100.0 OPT-2.7B 94.5 100.0 100.0 94.4 100.0 OPT-1.3B 94.5 100.0 100.0 94.4 100.0 Table 23: The performance of the models on CNN/DM with FactCC-generated alternative-choices using LL as the scoring function. Model BART- BART- BLOOM- distil- distil- PEGASUS T5- base large 560m BART PEGASUS large T0-3B 62.2 33.7 90.5 32.2 17.5 25.8 94.1 T0 64.9 18.6 85.7 23.3 14.3 29.0 76.5 FLAN-T5-xl 64.9 38.4 90.5 38.9 25.4 38.7 82.4 FLAN-T5-xxl 70.3 46.5 90.5 42.2 28.6 35.5 82.4 T5-LM-Adapt-xl 56.8 45.3 76.2 44.4 31.7 35.5 82.4 T5-LM-Adapt-xxl 59.5 45.3 71.4 45.6 34.9 38.7 76.5 GPT-Neo-1.3B 59.5 38.4 66.7 53.3 28.6 22.6 76.5 GPT2-XL 62.2 40.7 61.9 50.0 27.0 33.9 52.9 GPT-Neo-2.7B 56.8 41.9 57.1 52.2 28.6 33.9 76.5 GPTJ-6B 64.9 40.7 71.4 61.1 38.1 29.0 64.7 GPT-Neox-20B 73.0 36.0 61.9 58.9 33.3 32.3 64.7 BLOOM 56.8 41.9 71.4 51.1 27.0 25.8 70.6 BLOOM-7B1 56.8 34.9 52.4 50.0 30.2 27.4 70.6 BLOOM-3B 64.9 30.2 57.1 50.0 23.8 32.3 64.7 BLOOM-1B7 70.3 33.7 52.4 45.6 22.2 29.0 70.6 BLOOM-1B1 62.2 32.6 57.1 43.3 22.2 30.6 58.8 OPT-175B 59.5 41.9 66.7 52.2 34.9 25.8 76.5 OPT-66B 75.7 38.4 52.4 57.8 31.7 22.6 70.6 OPT-30B 62.2 39.5 52.4 55.6 38.1 27.4 70.6 OPT-13B 64.9 44.2 57.1 54.4 38.1 22.6 70.6 OPT-6.7B 73.0 38.4 52.4 58.9 34.9 17.7 70.6 OPT-2.7B 64.9 37.2 52.4 54.4 38.1 29.0 70.6 OPT-1.3B 62.2 40.7 61.9 53.3 28.6 27.4 58.8 Table 24: The performance of the models on XSum with factually consistent model-generated alternative-choices using avg. PMI as the scoring function. Model BART- BART- BLOOM- distil- distil- PEGASUS T5- base large 560m BART PEGASUS large T0-3B 27.0 2.3 95.2 3.3 7.9 3.2",
      "chunk_index": 34
    },
    {
      "index": 243,
      "chunk_id": "FIB2022_chunk_35",
      "source_id": "FIB2022",
      "text": "The performance of the models on XSum with factually consistent model-generated alternative-choices using avg. PMI as the scoring function. Model BART- BART- BLOOM- distil- distil- PEGASUS T5- base large 560m BART PEGASUS large T0-3B 27.0 2.3 95.2 3.3 7.9 3.2 52.9 T0 51.4 9.3 95.2 6.7 4.8 8.1 58.8 FLAN-T5-xl 27.0 2.3 95.2 2.2 7.9 8.1 52.9 FLAN-T5-xxl 37.8 5.8 95.2 4.4 4.8 4.8 52.9 T5-LM-Adapt-xl 32.4 7.0 38.1 11.1 17.5 12.9 29.4 T5-LM-Adapt-xxl 40.5 5.8 47.6 7.8 15.9 16.1 41.2 GPT-Neo-1.3B 40.5 7.0 42.9 16.7 6.3 11.3 41.2 GPT2-XL 35.1 5.8 47.6 13.3 14.3 14.5 47.1 GPT-Neo-2.7B 35.1 10.5 38.1 18.9 9.5 12.9 41.2 GPTJ-6B 51.4 9.3 52.4 17.8 9.5 8.1 47.1 GPT-Neox-20B 51.4 5.8 52.4 21.1 9.5 8.1 47.1 BLOOM 51.4 10.5 66.7 20.0 9.5 12.9 58.8 BLOOM-7B1 43.2 5.8 57.1 20.0 15.9 9.7 47.1 BLOOM-3B 35.1 9.3 52.4 21.1 9.5 14.5 35.3 BLOOM-1B7 32.4 10.5 47.6 22.2 15.9 9.7 35.3 BLOOM-1B1 27.0 11.6 47.6 22.2 12.7 16.1 23.5 OPT-175B 56.8 7.0 66.7 20.0 11.1 9.7 47.1 OPT-66B 54.1 5.8 66.7 20.0 12.7 9.7 47.1 OPT-30B 48.6 7.0 61.9 18.9 9.5 9.7 52.9 OPT-13B 51.4 5.8 61.9 17.8 7.9 9.7 58.8 OPT-6.7B 51.4 4.7 47.6 15.6 12.7 12.9 58.8 OPT-2.7B 45.9 4.7 47.6 18.9 12.7 11.3 41.2 OPT-1.3B 43.2 5.8 52.4 17.8 12.7 9.7 41.2 Table 25: The performance of the models on XSum with factually consistent model-generated alternative-choices using avg. LL as the scoring function. Model BART- BART- BLOOM- distil- distil- PEGASUS T5- base large 560m BART PEGASUS large T0-3B 64.9 27.9 66.7 34.4 38.1 45.2 76.5 T0 64.9 18.6 81.0 22.2 22.2 32.3 82.4 FLAN-T5-xl 59.5 39.5 66.7 44.4 47.6 48.4 58.8 FLAN-T5-xxl 59.5 40.7 57.1 40.0 49.2 46.8 64.7 T5-LM-Adapt-xl 56.8 40.7 38.1 48.9 50.8 51.6 64.7 T5-LM-Adapt-xxl 59.5 41.9 42.9 43.3 47.6 51.6 58.8 GPT-Neo-1.3B 67.6 36.0 4.8 54.4 42.9 35.5 58.8 GPT2-XL 67.6 38.4 28.6 53.3 49.2 46.8 52.9 GPT-Neo-2.7B 64.9 37.2 9.5 56.7 46.0 43.5 58.8 GPTJ-6B 70.3 40.7 9.5 62.2 55.6 48.4 58.8 GPT-Neox-20B 73.0 31.4 19.0 55.6 46.0 45.2 58.8 BLOOM 67.6 45.3 14.3 44.4 41.3 40.3 70.6 BLOOM-7B1 62.2 40.7 9.5 53.3 42.9 40.3 64.7 BLOOM-3B 73.0 34.9 19.0 54.4 36.5 48.4 64.7 BLOOM-1B7 62.2 37.2 14.3 43.3 39.7 48.4 52.9 BLOOM-1B1 62.2 32.6 9.5 46.7 38.1 46.8 52.9 OPT-175B 67.6 40.7 9.5 54.4 49.2 38.7 70.6 OPT-66B 75.7 38.4 4.8 54.4 52.4 37.1 70.6 OPT-30B 67.6 43.0 14.3",
      "chunk_index": 35
    },
    {
      "index": 244,
      "chunk_id": "FIB2022_chunk_36",
      "source_id": "FIB2022",
      "text": "54.4 36.5 48.4 64.7 BLOOM-1B7 62.2 37.2 14.3 43.3 39.7 48.4 52.9 BLOOM-1B1 62.2 32.6 9.5 46.7 38.1 46.8 52.9 OPT-175B 67.6 40.7 9.5 54.4 49.2 38.7 70.6 OPT-66B 75.7 38.4 4.8 54.4 52.4 37.1 70.6 OPT-30B 67.6 43.0 14.3 52.2 46.0 38.7 58.8 OPT-13B 64.9 43.0 9.5 53.3 50.8 41.9 64.7 OPT-6.7B 73.0 38.4 4.8 58.9 52.4 37.1 52.9 OPT-2.7B 73.0 40.7 9.5 57.8 52.4 40.3 58.8 OPT-1.3B 64.9 43.0 4.8 52.2 44.4 43.5 47.1 Table 26: The performance of the models on XSum with factually consistent model-generated alternative-choices using PMI as the scoring function. Model BART- BART- BLOOM- distil- distil- PEGASUS T5- base large 560m BART PEGASUS large T0-3B 21.6 4.7 100.0 5.6 6.3 4.8 47.1 T0 48.6 9.3 100.0 10.0 6.3 9.7 64.7 FLAN-T5-xl 27.0 7.0 100.0 4.4 6.3 11.3 52.9 FLAN-T5-xxl 32.4 7.0 100.0 4.4 3.2 12.9 47.1 T5-LM-Adapt-xl 32.4 12.8 95.2 15.6 14.3 11.3 47.1 T5-LM-Adapt-xxl 32.4 10.5 90.5 11.1 11.1 11.3 58.8 GPT-Neo-1.3B 37.8 9.3 85.7 23.3 6.3 11.3 35.3 GPT2-XL 32.4 8.1 85.7 16.7 9.5 14.5 52.9 GPT-Neo-2.7B 37.8 9.3 85.7 23.3 7.9 11.3 47.1 GPTJ-6B 35.1 7.0 95.2 22.2 11.1 8.1 52.9 GPT-Neox-20B 51.4 10.5 95.2 26.7 9.5 9.7 58.8 BLOOM 40.5 12.8 95.2 17.8 7.9 9.7 64.7 BLOOM-7B1 40.5 9.3 90.5 23.3 9.5 11.3 52.9 BLOOM-3B 37.8 10.5 90.5 23.3 11.1 12.9 41.2 BLOOM-1B7 40.5 12.8 85.7 25.6 11.1 12.9 41.2 BLOOM-1B1 32.4 16.3 81.0 23.3 9.5 12.9 47.1 OPT-175B 51.4 9.3 95.2 24.4 6.3 12.9 64.7 OPT-66B 43.2 11.6 95.2 21.1 7.9 11.3 64.7 OPT-30B 45.9 10.5 95.2 23.3 4.8 12.9 64.7 OPT-13B 48.6 9.3 95.2 20.0 6.3 9.7 64.7 OPT-6.7B 45.9 9.3 95.2 23.3 9.5 11.3 64.7 OPT-2.7B 37.8 12.8 95.2 20.0 7.9 11.3 58.8 OPT-1.3B 37.8 12.8 95.2 20.0 4.8 9.7 47.1 Table 27: The performance of the models on XSum with factually consistent model-generated alternative-choices using LL as the scoring function. Model B BL HG L MS MI NS OD O PB PT R RE T TS T0-3B 1.4 3.9 1.3 2.1 5.1 4.5 23.7 39.3 8.7 3.4 0.0 23.2 2.6 4.7 3.7 T0 2.7 3.9 0.0 1.1 2.5 6.1 10.5 21.4 4.3 1.1 1.4 8.7 3.9 4.7 7.4 FLAN-T5-xl 1.4 3.9 1.3 0.0 3.8 3.0 25.0 28.6 0.0 2.3 1.4 23.2 5.3 6.2 5.6 FLAN-T5-xxl 2.7 2.6 1.3 1.1 2.5 3.0 14.5 35.7 0.0 0.0 1.4 15.9 6.6 4.7 1.9 T5-LM-Adapt-xl 5.4 2.6 0.0 0.0 0.0",
      "chunk_index": 36
    },
    {
      "index": 245,
      "chunk_id": "FIB2022_chunk_37",
      "source_id": "FIB2022",
      "text": "4.7 7.4 FLAN-T5-xl 1.4 3.9 1.3 0.0 3.8 3.0 25.0 28.6 0.0 2.3 1.4 23.2 5.3 6.2 5.6 FLAN-T5-xxl 2.7 2.6 1.3 1.1 2.5 3.0 14.5 35.7 0.0 0.0 1.4 15.9 6.6 4.7 1.9 T5-LM-Adapt-xl 5.4 2.6 0.0 0.0 0.0 3.0 18.4 35.7 0.0 0.0 0.0 20.3 2.6 3.1 1.9 T5-LM-Adapt-xxl 5.4 5.2 2.6 1.1 5.1 6.1 14.5 28.6 0.0 2.3 1.4 17.4 5.3 6.2 1.9 GPT-Neo-1.3B 1.4 1.3 0.0 1.1 3.8 4.5 35.5 32.1 2.2 1.1 2.7 20.3 2.6 3.1 0.0 GPT2-XL 1.4 2.6 2.6 1.1 2.5 6.1 44.7 14.3 0.0 2.3 2.7 40.6 2.6 0.0 1.9 GPT-Neo-2.7B 4.1 3.9 3.8 1.1 6.3 3.0 31.6 28.6 2.2 2.3 2.7 24.6 6.6 6.2 3.7 GPTJ-6B 4.1 5.2 5.1 2.1 5.1 6.1 25.0 14.3 2.2 3.4 6.8 20.3 6.6 6.2 3.7 GPT-Neox-20B 5.4 6.5 6.4 2.1 8.9 7.6 23.7 14.3 4.3 5.7 6.8 23.2 7.9 6.2 3.7 BLOOM 5.4 5.2 7.7 5.3 11.4 9.1 28.9 17.9 4.3 6.8 8.2 26.1 14.5 10.9 3.7 BLOOM-7B1 4.1 5.2 6.4 5.3 5.1 9.1 27.6 25.0 6.5 5.7 8.2 24.6 7.9 10.9 5.6 BLOOM-3B 5.4 5.2 3.8 3.2 3.8 4.5 28.9 28.6 2.2 4.5 4.1 20.3 5.3 7.8 3.7 BLOOM-1B7 2.7 2.6 2.6 1.1 3.8 3.0 27.6 32.1 2.2 2.3 2.7 23.2 5.3 4.7 1.9 BLOOM-1B1 2.7 2.6 1.3 0.0 5.1 4.5 31.6 32.1 2.2 1.1 4.1 27.5 7.9 4.7 0.0 OPT-175B 10.8 11.7 11.5 5.3 10.1 10.6 30.3 14.3 4.3 8.0 9.6 20.3 13.2 10.9 7.4 OPT-66B 9.5 9.1 9.0 3.2 8.9 6.1 19.7 10.7 4.3 5.7 8.2 15.9 9.2 7.8 5.6 OPT-30B 14.9 10.4 9.0 4.2 10.1 10.6 25.0 10.7 6.5 9.1 9.6 17.4 11.8 9.4 7.4 OPT-13B 6.8 6.5 5.1 2.1 6.3 7.6 23.7 14.3 2.2 4.5 8.2 20.3 7.9 7.8 3.7 OPT-6.7B 8.1 7.8 9.0 4.2 7.6 9.1 25.0 17.9 6.5 6.8 8.2 21.7 10.5 9.4 5.6 OPT-2.7B 6.8 5.2 5.1 1.1 3.8 6.1 26.3 17.9 4.3 4.5 5.5 20.3 6.6 7.8 1.9 OPT-1.3B 9.5 5.2 3.8 1.1 3.8 6.1 23.7 17.9 2.2 2.3 4.1 15.9 5.3 6.2 1.9 Table 28: The performance of the models on CNN/DM with factually consistent model-generated alternative-choices using avg. PMI as the scoring function. The models are BanditSumm (B), BERT_LSTM_PN_RL (BL), Heter-Graph (HG), Lead3 (L), MatchSumm (MS), MI-unsup (MI), NeuSumm (NS), Oracle (discourse) (OD), Oracle (O), Pacsum (bert) (PB), Pacsum (tfidf) (PT), Refresh (R), RNN_Ext_RL (RE), Textrank (T), Textrank (st) (TS) Model B BL HG",
      "chunk_index": 37
    },
    {
      "index": 246,
      "chunk_id": "FIB2022_chunk_38",
      "source_id": "FIB2022",
      "text": "models are BanditSumm (B), BERT_LSTM_PN_RL (BL), Heter-Graph (HG), Lead3 (L), MatchSumm (MS), MI-unsup (MI), NeuSumm (NS), Oracle (discourse) (OD), Oracle (O), Pacsum (bert) (PB), Pacsum (tfidf) (PT), Refresh (R), RNN_Ext_RL (RE), Textrank (T), Textrank (st) (TS) Model B BL HG L MS MI NS OD O PB PT R RE T TS T0-3B 1.4 0.0 0.0 1.1 1.3 1.5 13.2 14.3 2.2 0.0 1.4 8.7 5.3 3.1 3.7 T0 1.4 0.0 0.0 1.1 0.0 3.0 3.9 10.7 4.3 0.0 1.4 1.4 6.6 3.1 3.7 FLAN-T5-xl 1.4 0.0 0.0 0.0 0.0 0.0 5.3 0.0 4.3 0.0 0.0 5.8 0.0 4.7 3.7 FLAN-T5-xxl 0.0 0.0 0.0 0.0 0.0 1.5 1.3 3.6 2.2 0.0 0.0 1.4 0.0 3.1 3.7 T5-LM-Adapt-xl 0.0 0.0 0.0 0.0 0.0 0.0 5.3 7.1 2.2 0.0 1.4 5.8 2.6 3.1 1.9 T5-LM-Adapt-xxl 1.4 0.0 0.0 0.0 0.0 1.5 1.3 3.6 2.2 0.0 1.4 1.4 5.3 3.1 0.0 GPT-Neo-1.3B 0.0 0.0 0.0 0.0 0.0 0.0 2.6 0.0 2.2 0.0 0.0 4.3 0.0 1.6 0.0 GPT2-XL 0.0 0.0 0.0 0.0 0.0 1.5 3.9 0.0 2.2 0.0 0.0 4.3 0.0 1.6 0.0 GPT-Neo-2.7B 0.0 0.0 0.0 0.0 0.0 0.0 3.9 0.0 2.2 0.0 0.0 4.3 0.0 1.6 0.0 GPTJ-6B 0.0 0.0 0.0 0.0 0.0 0.0 2.6 0.0 2.2 0.0 0.0 1.4 0.0 1.6 0.0 GPT-Neox-20B 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 2.2 0.0 0.0 0.0 0.0 1.6 0.0 BLOOM 0.0 0.0 0.0 0.0 0.0 0.0 1.3 0.0 2.2 0.0 0.0 0.0 0.0 1.6 0.0 BLOOM-7B1 0.0 0.0 0.0 0.0 0.0 1.5 3.9 0.0 2.2 0.0 0.0 4.3 0.0 1.6 1.9 BLOOM-3B 0.0 0.0 0.0 0.0 0.0 1.5 5.3 0.0 2.2 0.0 0.0 5.8 0.0 1.6 1.9 BLOOM-1B7 1.4 0.0 0.0 0.0 0.0 1.5 2.6 3.6 2.2 0.0 0.0 4.3 0.0 0.0 0.0 BLOOM-1B1 2.7 1.3 0.0 1.1 0.0 1.5 2.6 0.0 2.2 0.0 0.0 5.8 0.0 1.6 1.9 OPT-175B 1.4 0.0 0.0 0.0 0.0 1.5 1.3 0.0 2.2 0.0 0.0 1.4 0.0 1.6 0.0 OPT-66B 1.4 0.0 0.0 0.0 0.0 1.5 3.9 0.0 2.2 0.0 0.0 2.9 0.0 1.6 0.0 OPT-30B 0.0 0.0 0.0 0.0 0.0 1.5 3.9 0.0 2.2 0.0 0.0 2.9 0.0 1.6 0.0 OPT-13B 0.0 0.0 0.0 0.0 0.0 1.5 5.3 0.0 2.2 0.0 0.0 2.9 0.0 1.6 0.0 OPT-6.7B 0.0 0.0 0.0 0.0 0.0 1.5 6.6 0.0 2.2 0.0 0.0 1.4 0.0 1.6 0.0 OPT-2.7B 1.4 0.0 0.0 0.0 0.0 1.5 6.6 0.0 2.2 0.0 0.0",
      "chunk_index": 38
    },
    {
      "index": 247,
      "chunk_id": "FIB2022_chunk_39",
      "source_id": "FIB2022",
      "text": "0.0 0.0 1.5 5.3 0.0 2.2 0.0 0.0 2.9 0.0 1.6 0.0 OPT-6.7B 0.0 0.0 0.0 0.0 0.0 1.5 6.6 0.0 2.2 0.0 0.0 1.4 0.0 1.6 0.0 OPT-2.7B 1.4 0.0 0.0 0.0 0.0 1.5 6.6 0.0 2.2 0.0 0.0 4.3 0.0 1.6 0.0 OPT-1.3B 1.4 0.0 0.0 0.0 0.0 1.5 6.6 0.0 2.2 0.0 0.0 4.3 0.0 1.6 0.0 Table 29: The performance of the models on CNN/DM with factually consistent model-generated alternative-choices using avg. LL as the scoring function. The models are BanditSumm (B), BERT_LSTM_PN_RL (BL), Heter-Graph (HG), Lead3 (L), MatchSumm (MS), MI-unsup (MI), NeuSumm (NS), Oracle (discourse) (OD), Oracle (O), Pacsum (bert) (PB), Pacsum (tfidf) (PT), Refresh (R), RNN_Ext_RL (RE), Textrank (T), Textrank (st) (TS) Model B BL HG L MS MI NS OD O PB PT R RE T TS T0-3B 1.4 1.3 0.0 0.0 0.0 0.0 1.3 21.4 6.5 0.0 0.0 1.4 0.0 4.7 1.9 T0 1.4 0.0 0.0 0.0 0.0 0.0 0.0 14.3 6.5 0.0 0.0 0.0 0.0 4.7 1.9 FLAN-T5-xl 0.0 1.3 0.0 0.0 0.0 0.0 1.3 10.7 4.3 0.0 0.0 0.0 0.0 4.7 1.9 FLAN-T5-xxl 1.4 0.0 0.0 0.0 0.0 0.0 0.0 14.3 4.3 0.0 0.0 0.0 0.0 3.1 1.9 T5-LM-Adapt-xl 0.0 0.0 0.0 0.0 0.0 0.0 0.0 17.9 4.3 0.0 0.0 0.0 0.0 4.7 1.9 T5-LM-Adapt-xxl 0.0 0.0 0.0 0.0 0.0 0.0 0.0 14.3 4.3 0.0 0.0 0.0 0.0 3.1 1.9 GPT-Neo-1.3B 0.0 0.0 0.0 0.0 0.0 0.0 0.0 3.6 4.3 0.0 0.0 0.0 0.0 1.6 1.9 GPT2-XL 0.0 0.0 0.0 0.0 0.0 0.0 0.0 3.6 2.2 0.0 0.0 0.0 0.0 1.6 1.9 GPT-Neo-2.7B 0.0 0.0 0.0 0.0 0.0 0.0 0.0 3.6 4.3 0.0 0.0 0.0 0.0 0.0 1.9 GPTJ-6B 0.0 0.0 0.0 0.0 0.0 0.0 0.0 3.6 4.3 0.0 0.0 0.0 0.0 1.6 1.9 GPT-Neox-20B 0.0 0.0 0.0 0.0 0.0 0.0 0.0 3.6 4.3 0.0 0.0 0.0 0.0 0.0 1.9 BLOOM 0.0 0.0 0.0 0.0 0.0 0.0 0.0 3.6 4.3 0.0 0.0 0.0 0.0 1.6 1.9 BLOOM-7B1 0.0 0.0 0.0 0.0 0.0 0.0 0.0 3.6 4.3 0.0 0.0 0.0 0.0 1.6 1.9 BLOOM-3B 0.0 0.0 0.0 0.0 0.0 0.0 0.0 3.6 4.3 0.0 0.0 0.0 0.0 1.6 1.9 BLOOM-1B7 0.0 0.0 0.0 0.0 0.0 0.0 0.0 3.6 4.3 0.0 0.0 0.0 0.0 1.6 1.9 BLOOM-1B1 0.0 0.0 0.0 0.0 0.0 0.0 0.0 3.6 4.3 0.0 0.0 0.0 0.0 1.6 1.9 OPT-175B 1.4 0.0 0.0 0.0 0.0 0.0 0.0 3.6 4.3",
      "chunk_index": 39
    },
    {
      "index": 248,
      "chunk_id": "FIB2022_chunk_40",
      "source_id": "FIB2022",
      "text": "0.0 0.0 0.0 0.0 0.0 0.0 3.6 4.3 0.0 0.0 0.0 0.0 1.6 1.9 BLOOM-1B1 0.0 0.0 0.0 0.0 0.0 0.0 0.0 3.6 4.3 0.0 0.0 0.0 0.0 1.6 1.9 OPT-175B 1.4 0.0 0.0 0.0 0.0 0.0 0.0 3.6 4.3 0.0 0.0 0.0 0.0 3.1 1.9 OPT-66B 0.0 0.0 0.0 0.0 0.0 0.0 0.0 3.6 4.3 0.0 0.0 0.0 0.0 1.6 1.9 OPT-30B 0.0 0.0 0.0 0.0 0.0 0.0 0.0 3.6 4.3 0.0 0.0 0.0 0.0 3.1 1.9 OPT-13B 0.0 0.0 0.0 0.0 0.0 0.0 0.0 3.6 4.3 0.0 0.0 0.0 0.0 3.1 1.9 OPT-6.7B 0.0 0.0 0.0 0.0 0.0 0.0 0.0 3.6 4.3 0.0 0.0 0.0 0.0 1.6 1.9 OPT-2.7B 0.0 0.0 0.0 0.0 0.0 0.0 0.0 3.6 4.3 0.0 0.0 0.0 0.0 1.6 1.9 OPT-1.3B 0.0 0.0 0.0 0.0 0.0 0.0 0.0 3.6 4.3 0.0 0.0 0.0 0.0 1.6 1.9 Table 30: The performance of the models on CNN/DM with factually consistent model-generated alternative-choices using PMI as the scoring function. The models are BanditSumm (B), BERT_LSTM_PN_RL (BL), Heter-Graph (HG), Lead3 (L), MatchSumm (MS), MI-unsup (MI), NeuSumm (NS), Oracle (discourse) (OD), Oracle (O), Pacsum (bert) (PB), Pacsum (tfidf) (PT), Refresh (R), RNN_Ext_RL (RE), Textrank (T), Textrank (st) (TS) Model B BL HG L MS MI NS OD O PB PT R RE T TS T0-3B 31.1 24.7 42.3 25.3 44.3 47.0 98.7 75.0 21.7 30.7 35.6 88.4 64.5 31.2 29.6 T0 35.1 20.8 26.9 13.7 26.6 45.5 85.5 57.1 23.9 23.9 28.8 76.8 47.4 32.8 35.2 FLAN-T5-xl 31.1 26.0 37.2 21.1 32.9 48.5 94.7 53.6 19.6 30.7 28.8 91.3 56.6 35.9 33.3 FLAN-T5-xxl 20.3 11.7 16.7 8.4 15.2 36.4 76.3 46.4 13.0 12.5 17.8 55.1 38.2 15.6 20.4 T5-LM-Adapt-xl 52.7 41.6 28.2 12.6 22.8 60.6 94.7 57.1 17.4 21.6 23.3 82.6 48.7 20.3 22.2 T5-LM-Adapt-xxl 47.3 36.4 14.1 6.3 13.9 57.6 82.9 32.1 10.9 11.4 15.1 68.1 40.8 15.6 22.2 GPT-Neo-1.3B 31.1 24.7 9.0 2.1 8.9 36.4 85.5 25.0 2.2 9.1 17.8 63.8 19.7 14.1 16.7 GPT2-XL 35.1 27.3 7.7 7.4 6.3 50.0 89.5 25.0 0.0 11.4 16.4 69.6 27.6 12.5 16.7 GPT-Neo-2.7B 35.1 28.6 5.1 1.1 7.6 43.9 85.5 21.4 0.0 8.0 15.1 55.1 26.3 10.9 16.7 GPTJ-6B 27.0 23.4 9.0 1.1 8.9 39.4 73.7 17.9 2.2 6.8 12.3 44.9 21.1 12.5 14.8 GPT-Neox-20B 28.4 24.7 10.3 4.2 10.1 31.8 72.4 21.4 4.3 9.1 13.7 44.9 27.6 18.8 16.7 BLOOM 18.9 14.3 7.7 0.0 5.1 27.3 57.9 21.4",
      "chunk_index": 40
    },
    {
      "index": 249,
      "chunk_id": "FIB2022_chunk_41",
      "source_id": "FIB2022",
      "text": "27.0 23.4 9.0 1.1 8.9 39.4 73.7 17.9 2.2 6.8 12.3 44.9 21.1 12.5 14.8 GPT-Neox-20B 28.4 24.7 10.3 4.2 10.1 31.8 72.4 21.4 4.3 9.1 13.7 44.9 27.6 18.8 16.7 BLOOM 18.9 14.3 7.7 0.0 5.1 27.3 57.9 21.4 0.0 4.5 12.3 36.2 25.0 9.4 14.8 BLOOM-7B1 31.1 22.1 6.4 3.2 6.3 39.4 80.3 21.4 0.0 9.1 13.7 46.4 23.7 12.5 14.8 BLOOM-3B 41.9 31.2 9.0 3.2 10.1 45.5 80.3 25.0 0.0 8.0 13.7 60.9 23.7 10.9 14.8 BLOOM-1B7 36.5 28.6 7.7 2.1 7.6 43.9 82.9 28.6 2.2 4.5 15.1 58.0 21.1 6.2 9.3 BLOOM-1B1 36.5 28.6 7.7 5.3 10.1 47.0 84.2 25.0 2.2 9.1 16.4 65.2 26.3 14.1 14.8 OPT-175B 45.9 33.8 11.5 1.1 8.9 48.5 78.9 25.0 2.2 10.2 12.3 55.1 25.0 14.1 16.7 OPT-66B 44.6 33.8 10.3 4.2 6.3 48.5 82.9 21.4 4.3 12.5 16.4 49.3 28.9 17.2 16.7 OPT-30B 44.6 31.2 7.7 3.2 6.3 47.0 81.6 21.4 2.2 9.1 15.1 56.5 22.4 14.1 16.7 OPT-13B 47.3 33.8 10.3 2.1 8.9 48.5 86.8 25.0 8.7 11.4 16.4 59.4 28.9 17.2 18.5 OPT-6.7B 44.6 33.8 11.5 4.2 8.9 54.5 89.5 28.6 6.5 12.5 19.2 62.3 27.6 20.3 20.4 OPT-2.7B 45.9 36.4 14.1 3.2 8.9 50.0 86.8 21.4 6.5 14.8 19.2 63.8 31.6 17.2 20.4 OPT-1.3B 45.9 40.3 10.3 3.2 8.9 50.0 85.5 17.9 2.2 12.5 16.4 63.8 21.1 15.6 18.5 Table 31: The performance of the models on CNN/DM with factually consistent model-generated alternative-choices using LL as the scoring function. The models are BanditSumm (B), BERT_LSTM_PN_RL (BL), Heter-Graph (HG), Lead3 (L), MatchSumm (MS), MI-unsup (MI), NeuSumm (NS), Oracle (discourse) (OD), Oracle (O), Pacsum (bert) (PB), Pacsum (tfidf) (PT), Refresh (R), RNN_Ext_RL (RE), Textrank (T), Textrank (st) (TS) Model BART- BART- BLOOM- distil- distil- PEGASUS T5- base large 560m BART PEGASUS large T0-3B 61.1 37.9 96.0 35.1 38.7 30.6 94.0 T0 55.7 19.6 91.0 20.2 19.7 15.1 92.5 FLAN-T5-xl 64.1 40.8 98.7 38.3 40.7 34.5 92.8 FLAN-T5-xxl 67.8 47.6 99.0 42.9 44.6 41.8 93.2 T5-LM-Adapt-xl 66.5 60.9 90.8 57.1 61.1 53.4 86.3 T5-LM-Adapt-xxl 70.6 61.6 95.4 56.3 59.0 53.0 87.0 GPT-Neo-1.3B 71.3 67.9 79.9 72.4 64.8 66.2 80.3 GPT2-XL 67.8 63.5 84.7 64.4 61.6 60.3 78.9 GPT-Neo-2.7B 71.9 65.9 87.0 67.3 65.4 64.8 81.2 GPTJ-6B 78.6 69.3 91.8 71.5 64.5 64.8 84.1 GPT-Neox-20B 76.5 64.7 89.3 70.5 64.1 61.9 83.6 BLOOM 72.1 65.0 92.7 65.1 62.9 59.6 85.1 BLOOM-7B1 71.5 64.7 86.4 66.8 63.6 63.7 83.0 BLOOM-3B 70.8",
      "chunk_index": 41
    },
    {
      "index": 250,
      "chunk_id": "FIB2022_chunk_42",
      "source_id": "FIB2022",
      "text": "65.9 87.0 67.3 65.4 64.8 81.2 GPTJ-6B 78.6 69.3 91.8 71.5 64.5 64.8 84.1 GPT-Neox-20B 76.5 64.7 89.3 70.5 64.1 61.9 83.6 BLOOM 72.1 65.0 92.7 65.1 62.9 59.6 85.1 BLOOM-7B1 71.5 64.7 86.4 66.8 63.6 63.7 83.0 BLOOM-3B 70.8 68.8 85.7 68.5 65.0 66.2 80.7 BLOOM-1B7 68.3 67.1 82.6 68.5 65.0 61.6 78.3 BLOOM-1B1 66.5 63.5 80.7 66.1 65.4 63.2 73.9 OPT-175B 78.8 66.4 91.0 67.8 65.2 63.2 89.4 OPT-66B 76.7 66.7 88.5 67.6 64.5 61.6 88.0 OPT-30B 78.4 65.0 89.3 68.5 63.2 61.0 87.2 OPT-13B 76.5 63.0 89.1 65.4 64.1 61.2 86.5 OPT-6.7B 73.9 60.6 86.2 65.1 63.6 60.0 85.9 OPT-2.7B 72.1 62.8 84.9 67.1 63.4 62.1 83.2 OPT-1.3B 71.3 63.3 81.6 62.7 61.6 62.8 81.2 Table 32: The performance of the models on XSum with FIB alternative-choices using avg. PMI as the scoring function. Model BART- BART- BLOOM- distil- distil- PEGASUS T5- base large 560m BART PEGASUS large T0-3B 19.7 1.2 87.4 1.2 2.1 3.0 76.2 T0 33.9 5.3 80.3 5.4 5.7 3.2 84.1 FLAN-T5-xl 19.2 2.4 85.7 4.9 3.4 3.4 74.5 FLAN-T5-xxl 26.3 5.3 86.8 5.6 5.5 3.7 78.5 T5-LM-Adapt-xl 19.7 9.7 40.9 12.4 11.7 15.8 51.1 T5-LM-Adapt-xxl 23.8 8.9 51.2 12.0 10.1 9.6 61.3 GPT-Neo-1.3B 26.3 10.9 31.4 21.2 14.2 13.7 50.5 GPT2-XL 28.3 9.7 39.6 16.1 13.3 11.2 57.8 GPT-Neo-2.7B 32.0 10.6 36.5 20.5 12.8 12.1 58.0 GPTJ-6B 35.2 7.0 43.2 18.5 9.8 10.5 66.7 GPT-Neox-20B 39.1 8.5 46.3 20.0 9.6 10.5 71.4 BLOOM 42.8 8.5 50.9 20.7 9.8 10.7 72.5 BLOOM-7B1 32.6 10.9 43.0 20.7 13.3 13.9 60.9 BLOOM-3B 30.5 13.8 39.8 19.8 18.3 18.7 51.3 BLOOM-1B7 27.0 14.7 36.9 22.9 19.2 21.5 44.1 BLOOM-1B1 24.8 17.1 35.2 24.9 21.7 24.7 40.6 OPT-175B 48.8 8.7 56.0 20.7 9.8 7.8 78.9 OPT-66B 44.3 8.2 50.7 19.8 9.2 7.3 77.6 OPT-30B 45.6 7.7 50.7 20.7 9.6 8.4 76.6 OPT-13B 41.0 8.7 47.8 18.8 9.4 8.7 73.7 OPT-6.7B 37.1 8.0 43.4 17.8 8.2 8.7 69.6 OPT-2.7B 33.7 8.7 39.6 21.0 10.3 10.5 67.7 OPT-1.3B 29.8 8.5 37.7 17.6 11.2 10.7 62.3 Table 33: The performance of the models on XSum with FIB alternative-choices using avg. LL as the scoring function. Model BART- BART- BLOOM- distil- distil- PEGASUS T5- base large 560m BART PEGASUS large T0-3B 48.8 26.1 83.2 27.3 29.7 27.4 91.1 T0 53.8 16.4 91.2 19.3 18.1 16.0 91.9 FLAN-T5-xl 46.2 25.8 82.6 30.2 31.1 29.0 88.6 FLAN-T5-xxl 54.6 30.9 85.7 34.4 36.6 33.6 89.9",
      "chunk_index": 42
    },
    {
      "index": 251,
      "chunk_id": "FIB2022_chunk_43",
      "source_id": "FIB2022",
      "text": "PEGASUS T5- base large 560m BART PEGASUS large T0-3B 48.8 26.1 83.2 27.3 29.7 27.4 91.1 T0 53.8 16.4 91.2 19.3 18.1 16.0 91.9 FLAN-T5-xl 46.2 25.8 82.6 30.2 31.1 29.0 88.6 FLAN-T5-xxl 54.6 30.9 85.7 34.4 36.6 33.6 89.9 T5-LM-Adapt-xl 59.2 45.2 42.6 48.3 52.6 48.9 82.8 T5-LM-Adapt-xxl 60.5 42.5 54.7 48.3 48.7 43.6 84.5 GPT-Neo-1.3B 64.8 56.8 21.0 65.9 59.5 58.4 75.4 GPT2-XL 61.8 49.0 33.3 57.1 53.8 54.1 74.9 GPT-Neo-2.7B 63.9 51.7 23.9 60.2 55.1 55.7 76.2 GPTJ-6B 70.0 49.0 28.9 66.6 54.7 54.1 80.7 GPT-Neox-20B 68.5 51.0 29.4 65.6 55.8 53.4 82.6 BLOOM 65.2 51.0 45.1 58.5 55.8 54.3 83.0 BLOOM-7B1 64.8 53.4 30.6 61.2 56.8 56.6 79.1 BLOOM-3B 67.6 56.0 34.0 66.1 58.1 60.0 78.1 BLOOM-1B7 62.9 53.6 25.2 62.9 59.3 59.1 74.5 BLOOM-1B1 59.2 50.2 29.4 61.7 55.8 57.3 71.2 OPT-175B 71.9 50.0 39.8 61.5 55.8 53.7 85.7 OPT-66B 68.0 53.6 28.5 58.8 54.0 54.3 84.3 OPT-30B 69.5 48.3 33.8 59.8 53.3 54.1 83.2 OPT-13B 66.7 48.8 31.2 58.0 54.5 53.4 82.2 OPT-6.7B 64.8 47.8 26.2 59.8 51.0 55.9 82.4 OPT-2.7B 63.5 50.7 24.5 59.3 53.1 55.3 81.0 OPT-1.3B 63.5 50.0 22.6 57.1 51.9 55.7 77.0 Table 34: The performance of the models on XSum with FIB alternative-choices using PMI as the scoring function. Model BART- BART- BLOOM- distil- distil- PEGASUS T5- base large 560m BART PEGASUS large T0-3B 28.5 4.8 98.5 4.9 6.2 5.9 78.3 T0 42.8 10.4 98.7 8.3 7.3 5.9 84.9 FLAN-T5-xl 30.5 8.9 98.7 6.8 7.8 8.7 74.5 FLAN-T5-xxl 40.0 12.1 99.2 10.2 11.2 9.1 79.1 T5-LM-Adapt-xl 39.1 29.7 97.3 26.3 26.1 27.6 58.2 T5-LM-Adapt-xxl 42.1 24.2 97.7 23.2 20.1 21.2 65.8 GPT-Neo-1.3B 44.3 31.2 96.2 36.3 28.6 27.6 56.7 GPT2-XL 45.1 28.0 96.2 31.5 24.7 24.0 61.7 GPT-Neo-2.7B 48.2 28.3 96.0 33.9 25.4 26.5 61.3 GPTJ-6B 52.9 25.8 97.9 33.2 21.1 21.7 68.1 GPT-Neox-20B 54.6 24.6 97.9 33.9 20.4 20.1 72.7 BLOOM 54.0 26.1 98.1 32.4 23.6 22.1 73.7 BLOOM-7B1 49.2 30.2 97.5 33.4 28.8 29.7 62.1 BLOOM-3B 44.3 33.8 96.4 34.6 31.6 34.7 57.8 BLOOM-1B7 45.1 34.8 96.0 37.8 32.7 34.7 52.2 BLOOM-1B1 44.1 37.7 94.8 39.5 34.6 37.4 51.3 OPT-175B 59.0 23.4 98.3 30.5 17.4 16.4 80.5 OPT-66B 57.0 24.2 98.3 30.2 19.2 14.4 77.6 OPT-30B 55.7 22.9 97.9 30.2 18.3 16.0 77.2 OPT-13B 51.8 23.2 98.1 28.8 18.5 17.8 75.4 OPT-6.7B 52.7 23.7 97.1 29.3 18.1 16.7 71.4 OPT-2.7B 49.9 26.1 97.3 30.2 19.7 19.9 67.3",
      "chunk_index": 43
    },
    {
      "index": 252,
      "chunk_id": "FIB2022_chunk_44",
      "source_id": "FIB2022",
      "text": "OPT-66B 57.0 24.2 98.3 30.2 19.2 14.4 77.6 OPT-30B 55.7 22.9 97.9 30.2 18.3 16.0 77.2 OPT-13B 51.8 23.2 98.1 28.8 18.5 17.8 75.4 OPT-6.7B 52.7 23.7 97.1 29.3 18.1 16.7 71.4 OPT-2.7B 49.9 26.1 97.3 30.2 19.7 19.9 67.3 OPT-1.3B 45.8 26.6 97.1 30.5 22.4 23.1 62.5 Table 35: The performance of the models on XSum with FIB alternative-choices using LL as the scoring function. Model B BL HG L MS MI NS OD O PB PT R RE T TS T0-3B 11.5 0.0 9.1 20.0 4.8 11.8 20.8 51.4 13.0 0.0 0.0 25.8 4.2 13.9 15.2 T0 7.7 0.0 4.5 0.0 4.8 8.8 12.5 37.5 9.3 0.0 0.0 9.7 0.0 8.3 8.7 FLAN-T5-xl 11.5 0.0 9.1 0.0 4.8 8.8 25.0 37.5 13.0 0.0 3.7 25.8 8.3 13.9 17.4 FLAN-T5-xxl 11.5 0.0 9.1 0.0 4.8 8.8 16.7 37.5 7.4 0.0 0.0 19.4 8.3 8.3 17.4 T5-LM-Adapt-xl 7.7 0.0 4.5 0.0 4.8 8.8 20.8 37.5 9.3 0.0 0.0 25.8 0.0 5.6 8.7 T5-LM-Adapt-xxl 11.5 0.0 9.1 0.0 4.8 14.7 20.8 30.6 7.4 0.0 0.0 9.7 8.3 8.3 10.9 GPT-Neo-1.3B 0.0 4.3 4.5 0.0 4.8 2.9 29.2 25.0 3.7 0.0 0.0 16.1 4.2 2.8 4.3 GPT2-XL 3.8 0.0 0.0 0.0 4.8 2.9 33.3 27.8 3.7 0.0 0.0 25.8 4.2 2.8 4.3 GPT-Neo-2.7B 7.7 0.0 9.1 0.0 4.8 5.9 29.2 23.6 3.7 0.0 0.0 16.1 8.3 5.6 8.7 GPTJ-6B 0.0 4.3 0.0 0.0 4.8 2.9 29.2 22.2 3.7 0.0 0.0 9.7 4.2 5.6 6.5 GPT-Neox-20B 11.5 0.0 9.1 20.0 9.5 5.9 20.8 23.6 5.6 0.0 0.0 16.1 8.3 5.6 8.7 BLOOM 11.5 4.3 9.1 0.0 9.5 5.9 16.7 19.4 5.6 8.3 0.0 12.9 8.3 5.6 4.3 BLOOM-7B1 7.7 8.7 0.0 0.0 4.8 2.9 20.8 25.0 9.3 0.0 0.0 12.9 8.3 5.6 10.9 BLOOM-3B 3.8 4.3 0.0 0.0 4.8 2.9 16.7 20.8 5.6 0.0 0.0 9.7 4.2 5.6 8.7 BLOOM-1B7 3.8 4.3 4.5 0.0 4.8 2.9 20.8 25.0 7.4 0.0 0.0 12.9 8.3 2.8 6.5 BLOOM-1B1 3.8 4.3 9.1 20.0 4.8 5.9 25.0 23.6 7.4 8.3 3.7 16.1 12.5 5.6 8.7 OPT-175B 7.7 4.3 9.1 40.0 9.5 8.8 12.5 23.6 5.6 8.3 0.0 9.7 8.3 8.3 10.9 OPT-66B 7.7 4.3 9.1 0.0 9.5 5.9 12.5 20.8 7.4 0.0 0.0 6.5 8.3 8.3 8.7 OPT-30B 7.7 4.3 9.1 0.0 4.8 5.9 16.7 19.4 5.6 0.0 0.0 9.7 8.3 8.3 8.7 OPT-13B 7.7 0.0 9.1 0.0 9.5 5.9 16.7 26.4 3.7 0.0 0.0 12.9 8.3",
      "chunk_index": 44
    },
    {
      "index": 253,
      "chunk_id": "FIB2022_chunk_45",
      "source_id": "FIB2022",
      "text": "5.9 12.5 20.8 7.4 0.0 0.0 6.5 8.3 8.3 8.7 OPT-30B 7.7 4.3 9.1 0.0 4.8 5.9 16.7 19.4 5.6 0.0 0.0 9.7 8.3 8.3 8.7 OPT-13B 7.7 0.0 9.1 0.0 9.5 5.9 16.7 26.4 3.7 0.0 0.0 12.9 8.3 5.6 6.5 OPT-6.7B 7.7 4.3 4.5 20.0 4.8 5.9 16.7 23.6 5.6 8.3 0.0 6.5 12.5 5.6 10.9 OPT-2.7B 7.7 4.3 4.5 0.0 4.8 8.8 16.7 25.0 3.7 0.0 0.0 12.9 8.3 5.6 8.7 OPT-1.3B 3.8 4.3 4.5 0.0 4.8 5.9 20.8 19.4 5.6 0.0 0.0 12.9 8.3 2.8 4.3 Table 36: The performance of the models on CNN/DM with FIB alternative-choices using avg. PMI as the scoring function. The models are BanditSumm (B), BERT_LSTM_PN_RL (BL), Heter-Graph (HG), Lead3 (L), MatchSumm (MS), MI-unsup (MI), NeuSumm (NS), Oracle (discourse) (OD), Oracle (O), Pacsum (bert) (PB), Pacsum (tfidf) (PT), Refresh (R), RNN_Ext_RL (RE), Textrank (T), Textrank (st) (TS) Model B BL HG L MS MI NS OD O PB PT R RE T TS T0-3B 0.0 0.0 0.0 0.0 0.0 5.9 12.5 33.3 7.4 0.0 3.7 25.8 0.0 8.3 17.4 T0 0.0 0.0 0.0 0.0 0.0 2.9 8.3 23.6 9.3 0.0 3.7 12.9 0.0 5.6 13.0 FLAN-T5-xl 0.0 0.0 0.0 0.0 0.0 8.8 12.5 25.0 5.6 0.0 0.0 12.9 0.0 8.3 15.2 FLAN-T5-xxl 0.0 0.0 0.0 0.0 0.0 2.9 4.2 18.1 3.7 0.0 0.0 6.5 0.0 5.6 15.2 T5-LM-Adapt-xl 0.0 0.0 0.0 0.0 0.0 2.9 4.2 18.1 7.4 0.0 3.7 9.7 0.0 2.8 13.0 T5-LM-Adapt-xxl 0.0 0.0 0.0 0.0 0.0 2.9 4.2 12.5 5.6 0.0 3.7 6.5 0.0 2.8 13.0 GPT-Neo-1.3B 0.0 0.0 0.0 0.0 0.0 0.0 4.2 4.2 0.0 0.0 0.0 0.0 0.0 2.8 2.2 GPT2-XL 0.0 0.0 0.0 0.0 0.0 0.0 4.2 5.6 3.7 0.0 0.0 6.5 0.0 2.8 4.3 GPT-Neo-2.7B 0.0 0.0 0.0 0.0 0.0 0.0 4.2 5.6 0.0 0.0 0.0 3.2 0.0 2.8 2.2 GPTJ-6B 0.0 0.0 0.0 0.0 0.0 0.0 4.2 5.6 1.9 0.0 0.0 0.0 0.0 2.8 4.3 GPT-Neox-20B 0.0 0.0 0.0 0.0 0.0 0.0 0.0 5.6 1.9 0.0 0.0 0.0 0.0 2.8 4.3 BLOOM 0.0 0.0 0.0 0.0 0.0 0.0 0.0 4.2 1.9 0.0 0.0 0.0 0.0 2.8 6.5 BLOOM-7B1 0.0 0.0 0.0 0.0 0.0 0.0 4.2 8.3 3.7 0.0 0.0 0.0 0.0 2.8 6.5 BLOOM-3B 0.0 0.0 0.0 0.0 0.0 2.9 4.2 4.2 1.9 0.0 0.0 0.0 0.0 2.8 6.5 BLOOM-1B7 0.0 0.0 0.0 0.0 0.0 0.0 4.2 6.9 0.0 0.0 0.0 0.0 0.0",
      "chunk_index": 45
    },
    {
      "index": 254,
      "chunk_id": "FIB2022_chunk_46",
      "source_id": "FIB2022",
      "text": "0.0 4.2 8.3 3.7 0.0 0.0 0.0 0.0 2.8 6.5 BLOOM-3B 0.0 0.0 0.0 0.0 0.0 2.9 4.2 4.2 1.9 0.0 0.0 0.0 0.0 2.8 6.5 BLOOM-1B7 0.0 0.0 0.0 0.0 0.0 0.0 4.2 6.9 0.0 0.0 0.0 0.0 0.0 2.8 6.5 BLOOM-1B1 0.0 0.0 0.0 0.0 0.0 2.9 4.2 5.6 1.9 0.0 0.0 3.2 0.0 2.8 6.5 OPT-175B 0.0 0.0 0.0 0.0 0.0 2.9 4.2 4.2 1.9 0.0 0.0 3.2 0.0 2.8 4.3 OPT-66B 0.0 0.0 0.0 0.0 0.0 2.9 4.2 5.6 1.9 0.0 0.0 3.2 0.0 2.8 2.2 OPT-30B 0.0 0.0 0.0 0.0 0.0 0.0 4.2 5.6 1.9 0.0 0.0 3.2 0.0 2.8 2.2 OPT-13B 0.0 0.0 0.0 0.0 0.0 0.0 4.2 4.2 1.9 0.0 0.0 3.2 0.0 2.8 2.2 OPT-6.7B 0.0 0.0 0.0 0.0 0.0 2.9 4.2 8.3 1.9 0.0 0.0 3.2 0.0 2.8 2.2 OPT-2.7B 0.0 0.0 0.0 0.0 0.0 2.9 4.2 6.9 1.9 0.0 0.0 3.2 0.0 2.8 4.3 OPT-1.3B 0.0 0.0 0.0 0.0 0.0 2.9 4.2 4.2 0.0 0.0 0.0 6.5 0.0 2.8 2.2 Table 37: The performance of the models on CNN/DM with FIB alternative-choices using avg. LL as the scoring function. The models are BanditSumm (B), BERT_LSTM_PN_RL (BL), Heter-Graph (HG), Lead3 (L), MatchSumm (MS), MI-unsup (MI), NeuSumm (NS), Oracle (discourse) (OD), Oracle (O), Pacsum (bert) (PB), Pacsum (tfidf) (PT), Refresh (R), RNN_Ext_RL (RE), Textrank (T), Textrank (st) (TS) Model B BL HG L MS MI NS OD O PB PT R RE T TS T0-3B 0.0 0.0 0.0 0.0 4.8 0.0 8.3 33.3 13.0 0.0 0.0 9.7 0.0 2.8 2.2 T0 0.0 0.0 0.0 0.0 4.8 0.0 0.0 22.2 13.0 0.0 0.0 3.2 0.0 2.8 4.3 FLAN-T5-xl 0.0 0.0 0.0 0.0 4.8 0.0 8.3 25.0 13.0 0.0 0.0 12.9 0.0 0.0 2.2 FLAN-T5-xxl 0.0 0.0 0.0 0.0 4.8 0.0 8.3 20.8 11.1 0.0 0.0 3.2 0.0 0.0 4.3 T5-LM-Adapt-xl 0.0 0.0 0.0 0.0 4.8 0.0 4.2 25.0 11.1 0.0 0.0 3.2 0.0 0.0 2.2 T5-LM-Adapt-xxl 0.0 0.0 0.0 0.0 4.8 0.0 0.0 22.2 9.3 0.0 0.0 3.2 0.0 0.0 2.2 GPT-Neo-1.3B 0.0 0.0 0.0 0.0 4.8 0.0 4.2 16.7 5.6 0.0 0.0 0.0 0.0 0.0 0.0 GPT2-XL 0.0 0.0 0.0 0.0 4.8 0.0 0.0 13.9 5.6 0.0 0.0 6.5 0.0 0.0 0.0 GPT-Neo-2.7B 0.0 0.0 0.0 0.0 4.8 0.0 0.0 11.1 5.6 0.0 0.0 0.0 0.0 0.0 0.0 GPTJ-6B 0.0 0.0 0.0 0.0 0.0 0.0 4.2 11.1 3.7 0.0 0.0 0.0 0.0",
      "chunk_index": 46
    },
    {
      "index": 255,
      "chunk_id": "FIB2022_chunk_47",
      "source_id": "FIB2022",
      "text": "0.0 0.0 13.9 5.6 0.0 0.0 6.5 0.0 0.0 0.0 GPT-Neo-2.7B 0.0 0.0 0.0 0.0 4.8 0.0 0.0 11.1 5.6 0.0 0.0 0.0 0.0 0.0 0.0 GPTJ-6B 0.0 0.0 0.0 0.0 0.0 0.0 4.2 11.1 3.7 0.0 0.0 0.0 0.0 0.0 0.0 GPT-Neox-20B 0.0 0.0 0.0 0.0 4.8 0.0 0.0 8.3 5.6 0.0 0.0 3.2 0.0 0.0 0.0 BLOOM 0.0 0.0 0.0 0.0 4.8 0.0 0.0 9.7 3.7 0.0 0.0 3.2 0.0 0.0 0.0 BLOOM-7B1 0.0 0.0 0.0 0.0 4.8 0.0 4.2 11.1 5.6 0.0 0.0 0.0 0.0 0.0 0.0 BLOOM-3B 0.0 0.0 0.0 0.0 4.8 0.0 0.0 12.5 5.6 0.0 0.0 0.0 0.0 0.0 0.0 BLOOM-1B7 0.0 0.0 0.0 0.0 4.8 0.0 0.0 16.7 3.7 0.0 0.0 0.0 0.0 0.0 0.0 BLOOM-1B1 0.0 0.0 0.0 0.0 4.8 0.0 4.2 15.3 5.6 0.0 0.0 0.0 0.0 0.0 0.0 OPT-175B 0.0 0.0 0.0 0.0 4.8 0.0 0.0 11.1 5.6 0.0 0.0 3.2 0.0 0.0 0.0 OPT-66B 0.0 0.0 0.0 0.0 4.8 0.0 0.0 9.7 5.6 0.0 0.0 0.0 0.0 0.0 0.0 OPT-30B 0.0 0.0 0.0 0.0 4.8 0.0 0.0 12.5 7.4 0.0 0.0 0.0 0.0 0.0 0.0 OPT-13B 0.0 0.0 0.0 0.0 4.8 0.0 0.0 13.9 5.6 0.0 0.0 0.0 0.0 0.0 0.0 OPT-6.7B 0.0 0.0 0.0 0.0 4.8 0.0 0.0 9.7 5.6 0.0 0.0 0.0 0.0 0.0 0.0 OPT-2.7B 0.0 0.0 0.0 0.0 4.8 0.0 0.0 13.9 5.6 0.0 0.0 0.0 0.0 0.0 0.0 OPT-1.3B 0.0 0.0 0.0 0.0 4.8 0.0 0.0 16.7 7.4 0.0 0.0 0.0 0.0 0.0 0.0 Table 38: The performance of the models on CNN/DM with FIB alternative-choices using PMI as the scoring func- tion. The models are BanditSumm (B), BERT_LSTM_PN_RL (BL), Heter-Graph (HG), Lead3 (L), MatchSumm (MS), MI-unsup (MI), NeuSumm (NS), Oracle (discourse) (OD), Oracle (O), Pacsum (bert) (PB), Pacsum (tfidf) (PT), Refresh (R), RNN_Ext_RL (RE), Textrank (T), Textrank (st) (TS) Model B BL HG L MS MI NS OD O PB PT R RE T TS T0-3B 26.9 21.7 45.5 40.0 42.9 61.8 75.0 81.9 29.6 33.3 48.1 74.2 54.2 44.4 54.3 T0 15.4 21.7 31.8 0.0 38.1 58.8 62.5 65.3 16.7 8.3 40.7 61.3 37.5 47.2 50.0 FLAN-T5-xl 23.1 30.4 31.8 20.0 47.6 61.8 75.0 68.1 18.5 16.7 40.7 74.2 58.3 47.2 56.5 FLAN-T5-xxl 7.7 17.4 18.2 0.0 23.8 47.1 50.0 68.1 13.0 0.0 18.5 45.2 29.2 41.7 47.8 T5-LM-Adapt-xl 38.5 47.8 36.4 0.0 38.1 64.7 70.8 65.3 14.8 16.7 29.6 61.3 37.5",
      "chunk_index": 47
    },
    {
      "index": 256,
      "chunk_id": "FIB2022_chunk_48",
      "source_id": "FIB2022",
      "text": "61.8 75.0 68.1 18.5 16.7 40.7 74.2 58.3 47.2 56.5 FLAN-T5-xxl 7.7 17.4 18.2 0.0 23.8 47.1 50.0 68.1 13.0 0.0 18.5 45.2 29.2 41.7 47.8 T5-LM-Adapt-xl 38.5 47.8 36.4 0.0 38.1 64.7 70.8 65.3 14.8 16.7 29.6 61.3 37.5 41.7 47.8 T5-LM-Adapt-xxl 34.6 26.1 9.1 0.0 23.8 55.9 54.2 52.8 13.0 0.0 11.1 48.4 20.8 27.8 37.0 GPT-Neo-1.3B 11.5 13.0 9.1 0.0 14.3 41.2 62.5 19.4 3.7 0.0 7.4 45.2 12.5 16.7 23.9 GPT2-XL 23.1 17.4 9.1 0.0 19.0 47.1 66.7 25.0 9.3 0.0 18.5 54.8 29.2 22.2 28.3 GPT-Neo-2.7B 15.4 17.4 4.5 0.0 14.3 44.1 50.0 19.4 5.6 0.0 7.4 38.7 16.7 16.7 23.9 GPTJ-6B 7.7 21.7 4.5 0.0 14.3 41.2 41.7 25.0 3.7 0.0 7.4 35.5 4.2 13.9 23.9 GPT-Neox-20B 0.0 13.0 4.5 0.0 14.3 47.1 50.0 26.4 3.7 0.0 14.8 32.3 4.2 25.0 28.3 BLOOM 7.7 13.0 4.5 0.0 14.3 38.2 29.2 20.8 5.6 0.0 3.7 29.0 16.7 13.9 21.7 BLOOM-7B1 19.2 17.4 0.0 0.0 9.5 44.1 45.8 22.2 7.4 0.0 11.1 41.9 16.7 19.4 26.1 BLOOM-3B 23.1 13.0 4.5 0.0 19.0 44.1 50.0 22.2 3.7 0.0 3.7 41.9 16.7 16.7 23.9 BLOOM-1B7 19.2 17.4 9.1 0.0 9.5 44.1 41.7 26.4 3.7 0.0 3.7 41.9 12.5 16.7 21.7 BLOOM-1B1 23.1 26.1 4.5 0.0 19.0 44.1 54.2 22.2 5.6 0.0 11.1 41.9 25.0 25.0 23.9 OPT-175B 23.1 34.8 4.5 0.0 19.0 50.0 45.8 19.4 3.7 0.0 7.4 32.3 16.7 16.7 21.7 OPT-66B 30.8 30.4 4.5 0.0 19.0 47.1 54.2 22.2 7.4 0.0 11.1 38.7 12.5 19.4 30.4 OPT-30B 26.9 34.8 4.5 0.0 14.3 50.0 45.8 20.8 3.7 0.0 3.7 35.5 12.5 13.9 26.1 OPT-13B 30.8 30.4 4.5 0.0 19.0 47.1 54.2 29.2 5.6 0.0 11.1 38.7 25.0 16.7 23.9 OPT-6.7B 30.8 39.1 4.5 0.0 19.0 50.0 58.3 26.4 7.4 0.0 18.5 38.7 29.2 27.8 26.1 OPT-2.7B 23.1 26.1 4.5 0.0 28.6 50.0 58.3 33.3 7.4 0.0 11.1 45.2 16.7 19.4 26.1 OPT-1.3B 26.9 30.4 9.1 0.0 19.0 44.1 62.5 25.0 7.4 0.0 7.4 45.2 16.7 16.7 23.9 Table 39: The performance of the models on CNN/DM withFIB alternative-choices using LL as the scoring function. The models are BanditSumm (B), BERT_LSTM_PN_RL (BL), Heter-Graph (HG), Lead3 (L), MatchSumm (MS), MI-unsup (MI), NeuSumm (NS), Oracle (discourse) (OD), Oracle (O), Pacsum (bert) (PB), Pacsum (tfidf) (PT), Refresh (R), RNN_Ext_RL (RE), Textrank (T), Textrank (st) (TS) Model Scoring BART- BART- BLOOM- distil- distil- PEGASUS T5- Function base large 560m BART PEGASUS large",
      "chunk_index": 48
    },
    {
      "index": 257,
      "chunk_id": "FIB2022_chunk_49",
      "source_id": "FIB2022",
      "text": "MI-unsup (MI), NeuSumm (NS), Oracle (discourse) (OD), Oracle (O), Pacsum (bert) (PB), Pacsum (tfidf) (PT), Refresh (R), RNN_Ext_RL (RE), Textrank (T), Textrank (st) (TS) Model Scoring BART- BART- BLOOM- distil- distil- PEGASUS T5- Function base large 560m BART PEGASUS large BART-base Avg. PMI 24.4 42.5 95.4 34.4 45.1 42.2 83.0 BART-base Avg. LL 0.0 2.2 97.1 0.5 3.4 5.5 50.1 BART-base PMI 17.7 26.6 64.8 27.1 35.0 34.7 77.4 BART-base LL 0.6 8.9 99.6 2.0 8.9 13.5 54.5 BART-large Avg. PMI 63.5 24.4 96.0 29.5 39.4 32.2 94.2 BART-large Avg. LL 32.8 0.0 96.9 4.4 2.5 3.0 77.0 BART-large PMI 52.9 17.9 62.3 26.8 32.3 29.2 91.1 BART-large LL 42.8 1.0 99.6 7.3 4.8 5.7 77.6 BLOOM-560m Avg. PMI 55.9 44.7 52.8 53.9 45.8 46.1 72.0 BLOOM-560m Avg. LL 18.6 6.0 0.4 11.7 6.6 7.5 50.9 BLOOM-560m PMI 49.5 36.5 10.7 48.3 40.7 42.2 68.9 BLOOM-560m LL 32.2 16.7 37.3 21.5 12.8 14.8 57.8 distil-BART Avg. PMI 51.0 24.2 94.5 16.6 35.7 30.8 93.4 distil-BART Avg. LL 11.0 0.0 97.7 0.0 2.1 4.3 72.5 distil-BART PMI 44.7 18.6 52.8 18.8 30.9 26.5 88.6 distil-BART LL 20.7 1.7 99.6 0.0 4.6 7.3 73.1 distil-PEGASUS Avg. PMI 62.9 34.1 97.3 32.4 19.7 18.9 94.8 distil-PEGASUS Avg. LL 16.4 1.9 88.9 2.0 0.0 0.7 74.1 distil-PEGASUS PMI 51.4 22.7 77.8 26.6 17.2 17.1 92.3 distil-PEGASUS LL 27.0 5.6 98.5 3.9 0.2 1.8 76.2 PEGASUS Avg. PMI 72.4 44.9 97.1 42.9 36.4 22.8 96.9 PEGASUS Avg. LL 29.4 1.7 87.8 2.9 0.5 0.0 84.3 PEGASUS PMI 65.4 29.7 79.9 37.3 26.8 19.2 94.2 PEGASUS LL 38.9 5.8 99.0 7.8 2.3 0.2 85.3 T5-large Avg. PMI 43.2 50.7 93.5 46.1 51.5 49.8 31.7 T5-large Avg. LL 8.6 12.3 94.8 10.2 13.3 18.9 0.2 T5-large PMI 34.1 34.5 59.3 36.3 42.1 42.0 27.7 T5-large LL 28.5 31.9 99.2 26.1 28.4 34.2 4.1 Table 40: The performance of the models on XSum using the same models to generate the factually inconsistent summary.",
      "chunk_index": 49
    },
    {
      "index": 258,
      "chunk_id": "BERTScore2019_chunk_00",
      "source_id": "BERTScore2019",
      "text": "Published as a conference paper at ICLR 2020 BERTS CORE : E VALUATING TEXT GENERATION WITH BERT Tianyi Zhang∗†‡⋄, Varsha Kishore∗‡, Felix Wu∗‡, Kilian Q. Weinberger†‡⋄, and Yoav Artzi‡§ ‡Department of Computer Science and §Cornell Tech, Cornell University {vk352, fw245, kilian}@cornell.edu {yoav}@cs.cornell.edu ⋄ASAPP Inc. tzhang@asapp.com ABSTRACT We propose BERTS CORE , an automatic evaluation metric for text generation. Analogously to common metrics, BERTS CORE computes a similarity score for each token in the candidate sentence with each token in the reference sentence. However, instead of exact matches, we compute token similarity using contextual embeddings. We evaluate using the outputs of 363 machine translation and image captioning systems. BERTS CORE correlates better with human judgments and provides stronger model selection performance than existing metrics. Finally, we use an adversarial paraphrase detection task to show that BERTS CORE is more robust to challenging examples when compared to existing metrics. 1 I NTRODUCTION Automatic evaluation of natural language generation, for example in machine translation and caption generation, requires comparing candidate sentences to annotated references. The goal is to evaluate semantic equivalence. However, commonly used methods rely on surface-form similarity only. For example, BLEU (Papineni et al., 2002), the most common machine translation metric, simply counts n-gram overlap between the candidate and the reference. While this provides a simple and general measure, it fails to account for meaning-preserving lexical and compositional diversity. In this paper, we introduce BERTS CORE , a language generation evaluation metric based on pre- trained BERT contextual embeddings (Devlin et al., 2019). BERTS CORE computes the similarity of two sentences as a sum of cosine similarities between their tokens' embeddings. BERTS CORE addresses two common pitfalls in n-gram-based metrics (Banerjee & Lavie, 2005). First, such methods often fail to robustly match paraphrases. For example, given the reference peo- ple like foreign cars, BLEU and METEOR (Banerjee & Lavie, 2005) incorrectly give a higher score to people like visiting places abroadcompared to consumers prefer imported cars. This leads to performance underestimation when semantically-correct phrases are penalized because they differ from the surface form of the reference. In contrast to string matching (e.g., in B LEU ) or matching heuristics (e.g., in METEOR ), we compute similarity using contextualized token embeddings, which have been shown to be effective for paraphrase detection (Devlin et al., 2019). Second,n-gram mod- els fail to capture distant dependencies and penalize semantically-critical ordering changes (Isozaki et al., 2010).",
      "chunk_index": 0
    },
    {
      "index": 259,
      "chunk_id": "BERTScore2019_chunk_01",
      "source_id": "BERTScore2019",
      "text": "METEOR ), we compute similarity using contextualized token embeddings, which have been shown to be effective for paraphrase detection (Devlin et al., 2019). Second,n-gram mod- els fail to capture distant dependencies and penalize semantically-critical ordering changes (Isozaki et al., 2010). For example, given a small window of size two, B LEU will only mildly penalize swapping of cause and effect clauses (e.g. A because Binstead of B because A), especially when the arguments A and B are long phrases. In contrast, contextualized embeddings are trained to effectively capture distant dependencies and ordering. We experiment with BERTS CORE on machine translation and image captioning tasks using the outputs of 363 systems by correlating BERTS CORE and related metrics to available human judg- ments. Our experiments demonstrate that BERTS CORE correlates highly with human evaluations. In machine translation, BERTS CORE shows stronger system-level and segment-level correlations with human judgments than existing metrics on multiple common benchmarks and demonstrates ∗Equal contribution. †Work done at Cornell. arXiv:1904.09675v3 [cs.CL] 24 Feb 2020 Published as a conference paper at ICLR 2020 strong model selection performance compared to B LEU . We also show that BERTS CORE is well-correlated with human annotators for image captioning, surpassing S PICE , a popular task- speciﬁc metric (Anderson et al., 2016). Finally, we test the robustness of BERTS CORE on the adversarial paraphrase dataset PAWS (Zhang et al., 2019), and show that it is more ro- bust to adversarial examples than other metrics. The code for BERTS CORE is available at https://github.com/Tiiiger/bert_score. 2 P ROBLEM STATEMENT AND PRIOR METRICS Natural language text generation is commonly evaluated using annotated reference sentences. Given a reference sentence xtokenized to k tokens ⟨x1,...,x k⟩and a candidate ˆxtokenized to ltokens ⟨ˆx1,..., ˆxl⟩, a generation evaluation metric is a function f(x,ˆx) ∈R. Better metrics have a higher correlation with human judgments. Existing metrics can be broadly categorized into using n-gram matching, edit distance, embedding matching, or learned functions. 2.1 n-GRAM MATCHING APPROACHES The most commonly used metrics for generation count the number of n-grams that occur in the reference xand candidate ˆx. The higher the nis, the more the metric is able to capture word order, but it also becomes more restrictive and constrained to the exact form of the reference. Formally, let Sn x and Sn ˆx be the lists of token n-grams (n ∈Z+) in the reference xand candidate ˆx sentences. The number of",
      "chunk_index": 1
    },
    {
      "index": 260,
      "chunk_id": "BERTScore2019_chunk_02",
      "source_id": "BERTScore2019",
      "text": "but it also becomes more restrictive and constrained to the exact form of the reference. Formally, let Sn x and Sn ˆx be the lists of token n-grams (n ∈Z+) in the reference xand candidate ˆx sentences. The number of matched n-grams is ∑ w∈Sn ˆx I[w ∈ Sn x ], where I[·] is an indicator function. The exact match precision (Exact-Pn) and recall (Exact-Rn) scores are: Exact-Pn = ∑ w∈Sn ˆx I[w∈Sn x ] |Sn ˆx | and Exact-R n = ∑ w∈Snx I[w∈Sn ˆx ] |Snx | . Several popular metrics build upon one or both of these exact matching scores. BLEU The most widely used metric in machine translation is BLEU (Papineni et al., 2002), which includes three modiﬁcations to Exact-P n. First, each n-gram in the reference can be matched at most once. Second, the number of exact matches is accumulated for all reference-candidate pairs in the corpus and divided by the total number ofn-grams in all candidate sentences. Finally, very short candidates are discouraged using a brevity penalty. Typically, BLEU is computed for multiple values of n (e.g. n = 1 ,2,3,4) and the scores are averaged geometrically. A smoothed variant, S ENT- BLEU (Koehn et al., 2007) is computed at the sentence level. In contrast to B LEU , BERTS CORE is not restricted to maximum n-gram length, but instead relies on contextualized embeddings that are able to capture dependencies of potentially unbounded length. METEOR METEOR (Banerjee & Lavie, 2005) computes Exact-P 1 and Exact-R1 while allowing backing-off from exact unigram matching to matching word stems, synonyms, and paraphrases. For example, running may match run if no exact match is possible. Non-exact matching uses an external stemmer, a synonym lexicon, and a paraphrase table. M ETEOR 1.5 (Denkowski & Lavie, 2014) weighs content and function words differently, and also applies importance weighting to different matching types. The more recent M ETEOR ++ 2.0 (Guo & Hu, 2019) further incorporates a learned external paraphrase resource. Because M ETEOR requires external resources, only ﬁve languages are supported with the full feature set, and eleven are partially supported. Similar to M ETEOR , BERTS CORE allows relaxed matches, but relies on BERT embeddings that are trained on large amounts of raw text and are currently available for 104 languages. BERTS CORE also supports importance weighting, which we estimate with simple corpus statistics. Other Related Metrics NIST (Doddington,",
      "chunk_index": 2
    },
    {
      "index": 261,
      "chunk_id": "BERTScore2019_chunk_03",
      "source_id": "BERTScore2019",
      "text": "matches, but relies on BERT embeddings that are trained on large amounts of raw text and are currently available for 104 languages. BERTS CORE also supports importance weighting, which we estimate with simple corpus statistics. Other Related Metrics NIST (Doddington, 2002) is a revised version of B LEU that weighs each n-gram differently and uses an alternative brevity penalty. ∆BLEU (Galley et al., 2015) modiﬁes multi-reference BLEU by including human annotated negative reference sentences. CHR F (Popovi´c, 2015) compares character n-grams in the reference and candidate sentences. CHR F++ (Popovi ´c, 2017) extends CHR F to include word bigram matching. R OUGE (Lin, 2004) is a commonly used metric for summarization evaluation. ROUGE -n(Lin, 2004) computes Exact-Rn (usually n= 1,2), while ROUGE -Lis a variant of Exact-R 1 with the numerator replaced by the length of the longest common subsequence. CIDE R (Vedantam et al., 2015) is an image captioning metric that computes Published as a conference paper at ICLR 2020 cosine similarity between tf-idf weighted n-grams. We adopt a similar approach to weigh tokens differently. Finally, Chaganty et al. (2018) and Hashimoto et al. (2019) combine automatic metrics with human judgments for text generation evaluation. 2.2 E DIT-DISTANCE -BASED METRICS Several methods use word edit distance or word error rate (Levenshtein, 1966), which quantify similarity using the number of edit operations required to get from the candidate to the refer- ence. TER (Snover et al., 2006) normalizes edit distance by the number of reference words, and ITER (Panja & Naskar, 2018) adds stem matching and better normalization. PER (Tillmann et al., 1997) computes position independent error rate, CDER (Leusch et al., 2006) models block reorder- ing as an edit operation. CHARAC TER (Wang et al., 2016) and EED (Stanchev et al., 2019) operate on the character level and achieve higher correlation with human judgements on some languages. 2.3 E MBEDDING -BASED METRICS Word embeddings (Mikolov et al., 2013; Pennington et al., 2014; Grave et al., 2018; Nguyen et al., 2017; Athiwaratkun et al., 2018) are learned dense token representations. M EANT 2.0 (Lo, 2017) uses word embeddings and shallow semantic parses to compute lexical and structural similarity. YISI -1 (Lo et al., 2018) is similar to M EANT 2.0, but makes the use of semantic parses optional. Both methods use a relatively simple similarity computation, which inspires our approach, including using greedy matching (Corley & Mihalcea, 2005) and experimenting",
      "chunk_index": 3
    },
    {
      "index": 262,
      "chunk_id": "BERTScore2019_chunk_04",
      "source_id": "BERTScore2019",
      "text": "(Lo et al., 2018) is similar to M EANT 2.0, but makes the use of semantic parses optional. Both methods use a relatively simple similarity computation, which inspires our approach, including using greedy matching (Corley & Mihalcea, 2005) and experimenting with a similar importance weighting to Y ISI -1. However, we use contextual embeddings, which capture the speciﬁc use of a token in a sentence, and potentially capture sequence information. We do not use external tools to generate linguistic structures, which makes our approach relatively simple and portable to new languages. Instead of greedy matching, WMD (Kusner et al., 2015), WMDO (Chow et al., 2019), and SMS (Clark et al., 2019) propose to use optimal matching based on earth mover's distance (Rubner et al., 1998). The tradeoff 1 between greedy and optimal matching was studied by Rus & Lintean (2012). Sharma et al. (2018) compute similarity with sentence-level representations. In contrast, our token-level computation allows us to weigh tokens differently according to their importance. 2.4 L EARNED METRICS Various metrics are trained to optimize correlation with human judgments. B EER (Stanojevi´c & Sima'an, 2014) uses a regression model based on charactern-grams and word bigrams. BLEND (Ma et al., 2017) uses regression to combine 29 existing metrics. R USE (Shimanaka et al., 2018) com- bines three pre-trained sentence embedding models. All these methods require costly human judg- ments as supervision for each dataset, and risk poor generalization to new domains, even within a known language and task (Chaganty et al., 2018). Cui et al. (2018) and Lowe et al. (2017) train a neural model to predict if the input text is human-generated. This approach also has the risk of being optimized to existing data and generalizing poorly to new data. In contrast, the model underlying BERTS CORE is not optimized for any speciﬁc evaluation task. 3 BERTS CORE Given a reference sentence x = ⟨x1,...,x k⟩and a candidate sentence ˆx = ⟨ˆx1,..., ˆxl⟩, we use contextual embeddings to represent the tokens, and compute matching using cosine similarity, op- tionally weighted with inverse document frequency scores. Figure 1 illustrates the computation. Token Representation We use contextual embeddings to represent the tokens in the input sen- tences x and ˆx. In contrast to prior word embeddings (Mikolov et al., 2013; Pennington et al., 2014), contextual embeddings, such as BERT (Devlin et al., 2019) and ELM O (Peters et al., 2018), can generate different",
      "chunk_index": 4
    },
    {
      "index": 263,
      "chunk_id": "BERTScore2019_chunk_05",
      "source_id": "BERTScore2019",
      "text": "the input sen- tences x and ˆx. In contrast to prior word embeddings (Mikolov et al., 2013; Pennington et al., 2014), contextual embeddings, such as BERT (Devlin et al., 2019) and ELM O (Peters et al., 2018), can generate different vector representations for the same word in different sentences depending on the surrounding words, which form the context of the target word. The models used to generate these embeddings are most commonly trained using various language modeling objectives, such as masked word prediction (Devlin et al., 2019). 1We provide an ablation study of this design choice in Appendix C. Published as a conference paper at ICLR 2020 Reference the weather is cold today Candidate it is freezing today Candidate Contextual Embedding Pairwise Cosine Similarity R BERT = (0 . 713 ⇥ 1 . 27)+(0 . 515 ⇥ 7 . 94)+ ... 1 . 27+7 . 94+1 . 82+7 . 90+8 . 88 <latexit sha1_base64=\"OJyoKlmBAgUA0KDtUcsH/di5BlI=\">AAACSHicbZDLattAFIaPnLRJ3JvTLrsZYgoJAqFxGqwsCqal0FVJQ5wELCNG41EyZHRh5ijECL1EnqAv002X2eUZsumipXRR6Mj2Ipf+MPDznXM4Z/64UNKg7187raXlR49XVtfaT54+e/6is/7y0OSl5mLIc5Xr45gZoWQmhihRieNCC5bGShzFZx+a+tG50Ebm2QFOCzFO2UkmE8kZWhR1ov2oClFcYPX+4/5BXZN3JEw049Wm7/XpdogyFYZQr9ffci3aoTsL1Pd23265oZrkaOqqaXAb5FIv6DXOdwMvCOqo0/U9fyby0NCF6Q52/15+BYC9qHMVTnJepiJDrpgxI+oXOK6YRsmVqNthaUTB+Bk7ESNrM2aPGVezIGryxpIJSXJtX4ZkRm9PVCw1ZprGtjNleGru1xr4v9qoxCQYVzIrShQZny9KSkUwJ02qZCK14Kim1jCupb2V8FNmc0SbfduGQO9/+aE57HnU9+gX2h18hrlW4TVswCZQ6MMAPsEeDIHDN7iBn/DL+e78cH47f+atLWcx8wruqNX6B8dUrVw=</latexit><latexit sha1_base64=\"RInTcZkWiVBnf/ncBstCvatCtG4=\">AAACSHicbZDPShxBEMZ7Nproxugaj14al4AyMEyvyoyHwGIQPImKq8LOMvT09mhjzx+6a0KWYV4iL5EnySXH3HwGLx4U8SDYs7sHo/mg4eNXVVT1F+VSaHDda6vxbmb2/Ye5+ebHhU+LS63lz6c6KxTjPZbJTJ1HVHMpUt4DAZKf54rTJJL8LLr6VtfPvnOlRZaewCjng4RepCIWjIJBYSs8DssA+A8od/eOT6oKf8VBrCgr113HI5sBiIRrTJyOt2EbtE22p8hzdrY27EAOM9BVWTfYNbKJ43dq59q+4/tV2Gq7jjsWfmvI1LS7O08/f3nLi4dh628wzFiR8BSYpFr3iZvDoKQKBJO8agaF5jllV/SC941NqTlmUI6DqPAXQ4Y4zpR5KeAxfTlR0kTrURKZzoTCpX5dq+H/av0CYn9QijQvgKdssiguJIYM16nioVCcgRwZQ5kS5lbMLqnJEUz2TRMCef3lt+a04xDXIUek3T1AE82hVbSG1hFBHuqifXSIeoih3+gG3aF76491az1Yj5PWhjWdWUH/qNF4BkPYrbk=</latexit><latexit sha1_base64=\"RInTcZkWiVBnf/ncBstCvatCtG4=\">AAACSHicbZDPShxBEMZ7Nproxugaj14al4AyMEyvyoyHwGIQPImKq8LOMvT09mhjzx+6a0KWYV4iL5EnySXH3HwGLx4U8SDYs7sHo/mg4eNXVVT1F+VSaHDda6vxbmb2/Ye5+ebHhU+LS63lz6c6KxTjPZbJTJ1HVHMpUt4DAZKf54rTJJL8LLr6VtfPvnOlRZaewCjng4RepCIWjIJBYSs8DssA+A8od/eOT6oKf8VBrCgr113HI5sBiIRrTJyOt2EbtE22p8hzdrY27EAOM9BVWTfYNbKJ43dq59q+4/tV2Gq7jjsWfmvI1LS7O08/f3nLi4dh628wzFiR8BSYpFr3iZvDoKQKBJO8agaF5jllV/SC941NqTlmUI6DqPAXQ4Y4zpR5KeAxfTlR0kTrURKZzoTCpX5dq+H/av0CYn9QijQvgKdssiguJIYM16nioVCcgRwZQ5kS5lbMLqnJEUz2TRMCef3lt+a04xDXIUek3T1AE82hVbSG1hFBHuqifXSIeoih3+gG3aF76491az1Yj5PWhjWdWUH/qNF4BkPYrbk=</latexit><latexit sha1_base64=\"fGWl4NCvlvtMu17rjLtk25oWpdc=\">AAACSHicbZBLS+RAFIUrPT7bVzsu3RQ2ghIIqVbpuBgQRZiVqNgqdJpQqa5oYeVB1Y1ME/Lz3Lic3fwGNy6UwZ2VNgtfBwoO372Xe+uEmRQaXPef1fgxMTk1PTPbnJtfWFxqLf8812muGO+xVKbqMqSaS5HwHgiQ/DJTnMah5BfhzUFVv7jlSos0OYNRxgcxvUpEJBgFg4JWcBoUPvA/UOwfnp6VJf6F/UhRVmy4Tpds+SBirjFxOt1N26AdslOjrrO7vWn7cpiCLouqwa6QTRyvUznX9hzPK4NW23XcsfBXQ2rTRrWOg9Zff5iyPOYJMEm17hM3g0FBFQgmedn0c80zym7oFe8bm1BzzKAYB1HidUOGOEqVeQngMX0/UdBY61Ecms6YwrX+XKvgd7V+DpE3KESS5cAT9rYoyiWGFFep4qFQnIEcGUOZEuZWzK6pyRFM9k0TAvn85a/mvOMQ1yEnpL13VMcxg1bRGtpABHXRHvqNjlEPMXSHHtATerburUfrv/Xy1tqw6pkV9EGNxisxMKq0</latexit> 1.27 7.94 1.82 7.90 8.88 idf weights Importance Weighting (Optional) Maximum Similarity x <latexit sha1_base64=\"f2yzimwbR/Dgjzp6tZ360fHRqNI=\">AAAB6HicbVBNS8NAEJ3Ur1q/qh69LBbBU0lE0GPRi8cW7Ae0oWy2k3btZhN2N2IJ/QVePCji1Z/kzX/jts1BWx8MPN6bYWZekAiujet+O4W19Y3NreJ2aWd3b/+gfHjU0nGqGDZZLGLVCahGwSU2DTcCO4lCGgUC28H4dua3H1FpHst7M0nQj+hQ8pAzaqzUeOqXK27VnYOsEi8nFchR75e/eoOYpRFKwwTVuuu5ifEzqgxnAqelXqoxoWxMh9i1VNIItZ/ND52SM6sMSBgrW9KQufp7IqOR1pMosJ0RNSO97M3E/7xuasJrP+MySQ1KtlgUpoKYmMy+JgOukBkxsYQyxe2thI2ooszYbEo2BG/55VXSuqh6btVrXFZqN3kcRTiBUzgHD66gBndQhyYwQHiGV3hzHpwX5935WLQWnHzmGP7A+fwB5jmM/A==</latexit><latexit sha1_base64=\"f2yzimwbR/Dgjzp6tZ360fHRqNI=\">AAAB6HicbVBNS8NAEJ3Ur1q/qh69LBbBU0lE0GPRi8cW7Ae0oWy2k3btZhN2N2IJ/QVePCji1Z/kzX/jts1BWx8MPN6bYWZekAiujet+O4W19Y3NreJ2aWd3b/+gfHjU0nGqGDZZLGLVCahGwSU2DTcCO4lCGgUC28H4dua3H1FpHst7M0nQj+hQ8pAzaqzUeOqXK27VnYOsEi8nFchR75e/eoOYpRFKwwTVuuu5ifEzqgxnAqelXqoxoWxMh9i1VNIItZ/ND52SM6sMSBgrW9KQufp7IqOR1pMosJ0RNSO97M3E/7xuasJrP+MySQ1KtlgUpoKYmMy+JgOukBkxsYQyxe2thI2ooszYbEo2BG/55VXSuqh6btVrXFZqN3kcRTiBUzgHD66gBndQhyYwQHiGV3hzHpwX5935WLQWnHzmGP7A+fwB5jmM/A==</latexit><latexit sha1_base64=\"f2yzimwbR/Dgjzp6tZ360fHRqNI=\">AAAB6HicbVBNS8NAEJ3Ur1q/qh69LBbBU0lE0GPRi8cW7Ae0oWy2k3btZhN2N2IJ/QVePCji1Z/kzX/jts1BWx8MPN6bYWZekAiujet+O4W19Y3NreJ2aWd3b/+gfHjU0nGqGDZZLGLVCahGwSU2DTcCO4lCGgUC28H4dua3H1FpHst7M0nQj+hQ8pAzaqzUeOqXK27VnYOsEi8nFchR75e/eoOYpRFKwwTVuuu5ifEzqgxnAqelXqoxoWxMh9i1VNIItZ/ND52SM6sMSBgrW9KQufp7IqOR1pMosJ0RNSO97M3E/7xuasJrP+MySQ1KtlgUpoKYmMy+JgOukBkxsYQyxe2thI2ooszYbEo2BG/55VXSuqh6btVrXFZqN3kcRTiBUzgHD66gBndQhyYwQHiGV3hzHpwX5935WLQWnHzmGP7A+fwB5jmM/A==</latexit><latexit sha1_base64=\"f2yzimwbR/Dgjzp6tZ360fHRqNI=\">AAAB6HicbVBNS8NAEJ3Ur1q/qh69LBbBU0lE0GPRi8cW7Ae0oWy2k3btZhN2N2IJ/QVePCji1Z/kzX/jts1BWx8MPN6bYWZekAiujet+O4W19Y3NreJ2aWd3b/+gfHjU0nGqGDZZLGLVCahGwSU2DTcCO4lCGgUC28H4dua3H1FpHst7M0nQj+hQ8pAzaqzUeOqXK27VnYOsEi8nFchR75e/eoOYpRFKwwTVuuu5ifEzqgxnAqelXqoxoWxMh9i1VNIItZ/ND52SM6sMSBgrW9KQufp7IqOR1pMosJ0RNSO97M3E/7xuasJrP+MySQ1KtlgUpoKYmMy+JgOukBkxsYQyxe2thI2ooszYbEo2BG/55VXSuqh6btVrXFZqN3kcRTiBUzgHD66gBndQhyYwQHiGV3hzHpwX5935WLQWnHzmGP7A+fwB5jmM/A==</latexit> ˆx <latexit sha1_base64=\"5QTnVRVSrnyzznVU7d5bF5u03Iw=\">AAAB7nicbVBNS8NAEJ3Ur1q/qh69LBbBU0lE0GPRi8cK9gPaUDbbTbt0swm7E7GE/ggvHhTx6u/x5r9x0+agrQ8GHu/NMDMvSKQw6LrfTmltfWNzq7xd2dnd2z+oHh61TZxqxlsslrHuBtRwKRRvoUDJu4nmNAok7wST29zvPHJtRKwecJpwP6IjJULBKFqp0x9TzJ5mg2rNrbtzkFXiFaQGBZqD6ld/GLM04gqZpMb0PDdBP6MaBZN8VumnhieUTeiI9yxVNOLGz+bnzsiZVYYkjLUthWSu/p7IaGTMNApsZ0RxbJa9XPzP66UYXvuZUEmKXLHFojCVBGOS/06GQnOGcmoJZVrYWwkbU00Z2oQqNgRv+eVV0r6oe27du7+sNW6KOMpwAqdwDh5cQQPuoAktYDCBZ3iFNydxXpx352PRWnKKmWP4A+fzB7A8j8k=</latexit><latexit sha1_base64=\"5QTnVRVSrnyzznVU7d5bF5u03Iw=\">AAAB7nicbVBNS8NAEJ3Ur1q/qh69LBbBU0lE0GPRi8cK9gPaUDbbTbt0swm7E7GE/ggvHhTx6u/x5r9x0+agrQ8GHu/NMDMvSKQw6LrfTmltfWNzq7xd2dnd2z+oHh61TZxqxlsslrHuBtRwKRRvoUDJu4nmNAok7wST29zvPHJtRKwecJpwP6IjJULBKFqp0x9TzJ5mg2rNrbtzkFXiFaQGBZqD6ld/GLM04gqZpMb0PDdBP6MaBZN8VumnhieUTeiI9yxVNOLGz+bnzsiZVYYkjLUthWSu/p7IaGTMNApsZ0RxbJa9XPzP66UYXvuZUEmKXLHFojCVBGOS/06GQnOGcmoJZVrYWwkbU00Z2oQqNgRv+eVV0r6oe27du7+sNW6KOMpwAqdwDh5cQQPuoAktYDCBZ3iFNydxXpx352PRWnKKmWP4A+fzB7A8j8k=</latexit><latexit sha1_base64=\"5QTnVRVSrnyzznVU7d5bF5u03Iw=\">AAAB7nicbVBNS8NAEJ3Ur1q/qh69LBbBU0lE0GPRi8cK9gPaUDbbTbt0swm7E7GE/ggvHhTx6u/x5r9x0+agrQ8GHu/NMDMvSKQw6LrfTmltfWNzq7xd2dnd2z+oHh61TZxqxlsslrHuBtRwKRRvoUDJu4nmNAok7wST29zvPHJtRKwecJpwP6IjJULBKFqp0x9TzJ5mg2rNrbtzkFXiFaQGBZqD6ld/GLM04gqZpMb0PDdBP6MaBZN8VumnhieUTeiI9yxVNOLGz+bnzsiZVYYkjLUthWSu/p7IaGTMNApsZ0RxbJa9XPzP66UYXvuZUEmKXLHFojCVBGOS/06GQnOGcmoJZVrYWwkbU00Z2oQqNgRv+eVV0r6oe27du7+sNW6KOMpwAqdwDh5cQQPuoAktYDCBZ3iFNydxXpx352PRWnKKmWP4A+fzB7A8j8k=</latexit><latexit sha1_base64=\"5QTnVRVSrnyzznVU7d5bF5u03Iw=\">AAAB7nicbVBNS8NAEJ3Ur1q/qh69LBbBU0lE0GPRi8cK9gPaUDbbTbt0swm7E7GE/ggvHhTx6u/x5r9x0+agrQ8GHu/NMDMvSKQw6LrfTmltfWNzq7xd2dnd2z+oHh61TZxqxlsslrHuBtRwKRRvoUDJu4nmNAok7wST29zvPHJtRKwecJpwP6IjJULBKFqp0x9TzJ5mg2rNrbtzkFXiFaQGBZqD6ld/GLM04gqZpMb0PDdBP6MaBZN8VumnhieUTeiI9yxVNOLGz+bnzsiZVYYkjLUthWSu/p7IaGTMNApsZ0RxbJa9XPzP66UYXvuZUEmKXLHFojCVBGOS/06GQnOGcmoJZVrYWwkbU00Z2oQqNgRv+eVV0r6oe27du7+sNW6KOMpwAqdwDh5cQQPuoAktYDCBZ3iFNydxXpx352PRWnKKmWP4A+fzB7A8j8k=</latexit> Reference Figure 1: Illustration of the computation of the recall metric RBERT . Given the reference x and candidate ˆx, we compute BERT embeddings and pairwise cosine similarity. We highlight the greedy matching in red, and include the optional idf importance weighting. We experiment with different models (Section 4), using the tokenizer provided with each model. Given a tokenized reference sentence x = ⟨x1,...,x k⟩, the embedding model generates a se- quence of vectors ⟨x1,..., xk⟩. Similarly, the tokenized candidate ˆx = ⟨ˆx1,..., ˆxm⟩is mapped to ⟨ˆ x1,..., ˆ xl⟩. The main model we use is BERT, which tokenizes the input text into a sequence of word pieces (Wu et al., 2016), where unknown words are split into several commonly observed sequences of characters. The representation for each word piece is computed with a Transformer encoder (Vaswani et al., 2017) by repeatedly applying self-attention and nonlinear transformations in an alternating fashion. BERT embeddings have been shown to beneﬁt various NLP tasks (Devlin et al., 2019; Liu, 2019; Huang et al., 2019; Yang et al., 2019a). Similarity Measure The vector representation allows for a soft measure of similarity instead of exact-string (Papineni et al., 2002) or heuristic (Banerjee & Lavie, 2005) matching. The cosine similarity of a reference token xi and a candidate token ˆxj is x⊤ i ˆ xj",
      "chunk_index": 5
    },
    {
      "index": 264,
      "chunk_id": "BERTScore2019_chunk_06",
      "source_id": "BERTScore2019",
      "text": "representation allows for a soft measure of similarity instead of exact-string (Papineni et al., 2002) or heuristic (Banerjee & Lavie, 2005) matching. The cosine similarity of a reference token xi and a candidate token ˆxj is x⊤ i ˆ xj ∥xi∥∥ˆ xj∥. We use pre-normalized vectors, which reduces this calculation to the inner product x⊤ i ˆ xj. While this measure considers tokens in isolation, the contextual embeddings contain information from the rest of the sentence. BERTS CORE The complete score matches each token in x to a token in ˆx to compute recall, and each token in ˆx to a token in x to compute precision. We use greedy matching to maximize the matching similarity score, 2 where each token is matched to the most similar token in the other sentence. We combine precision and recall to compute an F1 measure. For a reference x and candidate ˆx, the recall, precision, and F1 scores are: RBERT = 1 |x| ∑ xi∈x max ˆxj∈ˆx x⊤ i ˆ xj , P BERT = 1 |ˆx| ∑ ˆxj∈ˆx max xi∈x x⊤ i ˆ xj , F BERT = 2 PBERT ·RBERT PBERT + RBERT . Importance Weighting Previous work on similarity measures demonstrated that rare words can be more indicative for sentence similarity than common words (Banerjee & Lavie, 2005; Vedantam et al., 2015). BERTS CORE enables us to easily incorporate importance weighting. We experiment with inverse document frequency ( idf) scores computed from the test corpus. Given M reference sentences {x(i)}M i=1, the idf score of a word-piece token wis idf(w) = −log 1 M M∑ i=1 I[w∈x(i)] , where I[·] is an indicator function. We do not use the full tf-idf measure because we process single sentences, where the term frequency (tf) is likely 1. For example, recall with idf weighting is RBERT = ∑ xi∈x idf(xi) maxˆxj∈ˆx x⊤ i ˆ xj ∑ xi∈x idf(xi) . Because we use reference sentences to compute idf, the idf scores remain the same for all systems evaluated on a speciﬁc test set. We apply plus-one smoothing to handle unknown word pieces. 2We compare greedy matching with optimal assignment in Appendix C. Published as a conference paper at ICLR 2020 Baseline Rescaling Because we use pre-normalized vectors, our computed scores have the same numerical range of cosine similarity (between −1 and 1). However, in practice we observe scores in a more limited range, potentially",
      "chunk_index": 6
    },
    {
      "index": 265,
      "chunk_id": "BERTScore2019_chunk_07",
      "source_id": "BERTScore2019",
      "text": "a conference paper at ICLR 2020 Baseline Rescaling Because we use pre-normalized vectors, our computed scores have the same numerical range of cosine similarity (between −1 and 1). However, in practice we observe scores in a more limited range, potentially because of the learned geometry of contextual embeddings. While this characteristic does not impact BERTSCORE 's capability to rank text generation systems, it makes the actual score less readable. We address this by rescaling BERTS CORE with respect to its empirical lower bound b as a baseline. We compute b using Common Crawl monolingual datasets.3 For each language and contextual embedding model, we create 1M candidate-reference pairs by grouping two random sentences. Because of the random pairing and the corpus diversity, each pair has very low lexical and semantic overlapping.4 We computebby averaging BERTSCORE computed on these sentence pairs. Equipped with baseline b, we rescale BERTS CORE linearly. For example, the rescaled value ˆRBERT of RBERT is: ˆRBERT = RBERT −b 1 −b . After this operation ˆRBERT is typically between 0 and 1. We apply the same rescaling procedure for PBERT and FBERT . This method does not affect the ranking ability and human correlation of BERTS CORE , and is intended solely to increase the score readability. 4 E XPERIMENTAL SETUP We evaluate our approach on machine translation and image captioning. Contextual Embedding Models We evaluate twelve pre-trained contextual embedding models, including variants of BERT (Devlin et al., 2019), RoBERTa (Liu et al., 2019b), XLNet (Yang et al., 2019b), and XLM (Lample & Conneau, 2019). We present the best-performing models in Section 5. We use the 24-layer RoBERTalarge model5 for English tasks, 12-layer BERT chinese model for Chi- nese tasks, and the 12-layer cased multilingual BERTmulti model for other languages.6 We show the performance of all other models in Appendix F. Contextual embedding models generate embedding representations at every layer in the encoder network. Past work has shown that intermediate layers produce more effective representations for semantic tasks (Liu et al., 2019a). We use the WMT16 dataset (Bojar et al., 2016) as a validation set to select the best layer of each model (Appendix B). Machine Translation Our main evaluation corpus is the WMT18 metric evaluation dataset (Ma et al., 2018), which contains predictions of 149 translation systems across 14 language pairs, gold references, and two types of human judgment scores. Segment-level human judgments assign a score to",
      "chunk_index": 7
    },
    {
      "index": 266,
      "chunk_id": "BERTScore2019_chunk_08",
      "source_id": "BERTScore2019",
      "text": "main evaluation corpus is the WMT18 metric evaluation dataset (Ma et al., 2018), which contains predictions of 149 translation systems across 14 language pairs, gold references, and two types of human judgment scores. Segment-level human judgments assign a score to each reference-candidate pair. System-level human judgments associate each system with a single score based on all pairs in the test set. WMT18 includes translations from English to Czech, German, Estonian, Finnish, Russian, and Turkish, and from the same set of languages to English. We follow the WMT18 standard practice and use absolute Pearson correlation |ρ|and Kendall rank correlation τto evaluate metric quality, and compute signiﬁcance with the Williams test (Williams, 1959) for|ρ| and bootstrap re-sampling for τ as suggested by Graham & Baldwin (2014). We compute system- level scores by averaging BERTSCORE for every reference-candidate pair. We also experiment with hybrid systems by randomly sampling one candidate sentence from one of the available systems for each reference sentence (Graham & Liu, 2016). This enables system-level experiments with a higher number of systems. Human judgments of each hybrid system are created by averaging the WMT18 segment-level human judgments for the corresponding sentences in the sampled data. We compare BERTS CORE s to one canonical metric for each category introduced in Section 2, and include the comparison with all other participating metrics from WMT18 in Appendix F. In addition to the standard evaluation, we design model selection experiments. We use 10K hybrid systems super-sampled from WMT18. We randomly select 100 out of 10K hybrid systems, and rank them using the automatic metrics. We repeat this process 100K times. We report the percentage of the metric ranking agreeing with the human ranking on the best system (Hits@1). In Tables 23-28, 3https://commoncrawl.org/ 4BLEU computed on these pairs is around zero. 5We use the tokenizer provided with each model. For all Hugging Face models that use the GPT-2 tokenizer, at the time of our experiments, the tokenizer adds a space to the beginning of each sentence. 6All the models used are from https://github.com/huggingface/pytorch-transformers. Published as a conference paper at ICLR 2020 Metric en ↔cs en ↔de en ↔et en ↔ﬁ en ↔ru en ↔tr en ↔zh (5/5) (16/16) (14/14) (9/12) (8/9) (5/8) (14/14) BLEU .970/ .995 .971/.981 .986 /.975 .973/.962 .979/.983 .657 /.826 .978/.947 ITER .975/.915 .990/ .984 .975/.981 .996 /.973 .937/.975 .861/.865 .980/ - RUSE .981/ - .997/ - .990/ - .991/ - .988/",
      "chunk_index": 8
    },
    {
      "index": 267,
      "chunk_id": "BERTScore2019_chunk_09",
      "source_id": "BERTScore2019",
      "text": "↔zh (5/5) (16/16) (14/14) (9/12) (8/9) (5/8) (14/14) BLEU .970/ .995 .971/.981 .986 /.975 .973/.962 .979/.983 .657 /.826 .978/.947 ITER .975/.915 .990/ .984 .975/.981 .996 /.973 .937/.975 .861/.865 .980/ - RUSE .981/ - .997/ - .990/ - .991/ - .988/ - .853/ - .981/ - YiSi-1 .950/ .987 .992/.985 .979/.979 .973/.940 .991/.992 .958 /.976 .951/.963 PBERT .980/.994 .998 /.988 .990 /.981 .995/.957 .982/ .990 .791 /.935 .981/.954 RBERT .998/.997 .997/.990 .986/.980 .997 /.980 .995 /.989 .054/.879 .990/.976 FBERT .990/.997 .999 /.989 .990/.982 .998 /.972 .990 /.990 .499/.908 .988/.967 FBERT (idf) .985/ .995 .999 /.990 .992 /.981 .992/.972 .991 /.991 .826 /.941 .989 /.973 Table 1: Absolute Pearson correlations with system-level human judgments on WMT18. For each language pair, the left number is the to-English correlation, and the right is the from-English. We bold correlations of metrics not signiﬁcantly outperformed by any other metric under Williams Test for that language pair and direction. The numbers in parenthesis are the number of systems used for each language pair and direction. Metric en ↔cs en ↔de en ↔et en ↔ﬁ en ↔ru en ↔tr en ↔zh BLEU .956/.993 .969/ .977 .981 /.971 .962/.958 .972/.977 .586/.796 .968/.941 ITER .966/.865 .990/.978 .975/ .982 .989/.966 .943/.965 .742/.872 .978/ - RUSE .974/ - .996/ - .988/ - .983/ - .982/ - .780/ - .973/ - YiSi-1 .942/.985 .991/.983 .976/.976 .964/.938 .985/.989 .881 /.942 .943/.957 PBERT .965/.989 .995/.983 .990/.970 .976/.951 .976/.988 .846/.936 .975/.950 RBERT .989/.995 .997/.991 .982/.979 .989/.977 .988 /.989 .540/.872 .981 /.980 FBERT .978/.993 .998/.988 .989/.978 .983/.969 .985/.989 .760/.910 .981/.969 FBERT (idf) .982/.995 .998/.988 .988/.979 .989/.969 .983/.987 .453/.877 .980/.963 Table 2: Absolute Pearson correlations with system-level human judgments on WMT18. We use 10K hybrid super-sampled systems for each language pair and direction. For each language pair, the left number is the to-English correlation, and the right is the from-English. Bolding criteria is the same as in Table 1. we include two additional measures to the model selection study: (a) the mean reciprocal rank of the top metric-rated system according to the human ranking, and (b) the difference between the human score of the top human-rated system and that of the top metric-rated system. Additionally, we report the same study on the WMT17 (Bojar et al., 2017) and the WMT16 (Bojar et al., 2016) datasests in Appendix F.7 This adds 202 systems to our evaluation. Image Captioning We use the human judgments of twelve submission entries from the",
      "chunk_index": 9
    },
    {
      "index": 268,
      "chunk_id": "BERTScore2019_chunk_10",
      "source_id": "BERTScore2019",
      "text": "same study on the WMT17 (Bojar et al., 2017) and the WMT16 (Bojar et al., 2016) datasests in Appendix F.7 This adds 202 systems to our evaluation. Image Captioning We use the human judgments of twelve submission entries from the COCO 2015 Captioning Challenge. Each participating system generates a caption for each image in the COCO validation set (Lin et al., 2014), and each image has approximately ﬁve reference cap- tions. Following Cui et al. (2018), we compute the Pearson correlation with two system-level metrics: the percentage of captions that are evaluated as better or equal to human captions (M1) and the percentage of captions that are indistinguishable from human captions (M2). We compute BERTS CORE with multiple references by scoring the candidate with each available reference and returning the highest score. We compare with eight task-agnostic metrics: B LEU (Papineni et al., 2002), METEOR (Banerjee & Lavie, 2005), ROUGE -L (Lin, 2004), CIDE R (Vedantam et al., 2015), BEER (Stanojevi ´c & Sima'an, 2014), EED (Stanchev et al., 2019), CHR F++ (Popovi ´c, 2017), and CHARAC TER (Wang et al., 2016). We also compare with two task-speciﬁc metrics: SPICE (Ander- son et al., 2016) and LEIC (Cui et al., 2018). SPICE is computed using the similarity of scene graphs parsed from the reference and candidate captions. L EIC is trained to predict if a caption is written by a human given the image. 7For WMT16, we only conduct segment-level experiments on to-English pairs due to errors in the dataset. Published as a conference paper at ICLR 2020 Metric en ↔cs en ↔de en ↔et en ↔ﬁ en ↔ru en ↔tr en ↔zh BLEU .134/.151 .803/.610 .756/.618 .461/.088 .228/.519 .095/.029 .658/.515 ITER .154/.000 .814/.692 .742/.733 .475/.111 .234/.532 .102/.030 .673/ - RUSE .214/ - .823/ - .785/ - .487/ - .248/ - .109/ - .670/ - YiSi-1 .159/.178 .809/.671 .749/.671 .467/ .230 .248/.544 .108/ .398 .613/.594 PBERT .173/.180 .706/.663 .764/ .771 .498/.078 .255/ .545 .140/.372 .661/.551 RBERT .163/.184 .804/.730 .770/.722 .494/.148 .260/.542 .005/.030 .677/ .657 FBERT .175/.184 .824/.703 .769/.763 .501/.082 .262/.544 .142/.031 .673/.629 FBERT (idf) .179/.178 .824/.722 .760/.764 .503/.082 .265/.539 .004/.030 .678/.595 Table 3: Model selection accuracies (Hits@1) on WMT18 hybrid systems. We report the average of 100K samples and the 0.95 conﬁdence intervals are below 10−3. We bold the highest numbers for each language pair and direction. Metric en ↔cs en ↔de en ↔et en ↔ﬁ en ↔ru en ↔tr",
      "chunk_index": 10
    },
    {
      "index": 269,
      "chunk_id": "BERTScore2019_chunk_11",
      "source_id": "BERTScore2019",
      "text": "systems. We report the average of 100K samples and the 0.95 conﬁdence intervals are below 10−3. We bold the highest numbers for each language pair and direction. Metric en ↔cs en ↔de en ↔et en ↔ﬁ en ↔ru en ↔tr en ↔zh (5k/5k) (78k/ 20k) (57k/32k) (16k/10k) (10k/22k) (9k/1k) (33k/29k) BLEU .233/.389 .415/.620 .285/.414 .154/.355 .228/.330 .145/.261 .178/.311 ITER .198/.333 .396/.610 .235/.392 .128/.311 .139/.291 -.029/.236 .144/ - RUSE .347/ - .498/ - .368/ - .273/ - .311/ - .259/ - .218/ - YiSi-1 .319/.496 .488/.691 .351/.546 .231/.504 .300/.407 .234/.418 .211/.323 PBERT .387/.541 .541/.715 .389/.549 .283/.486 .345/.414 .280/.328 .248/.337 RBERT .388/.570 .546/.728 .391/.594 .304 /.565 .343/.420 .290/ .411 .255/.367 FBERT .404/.562 .550/.728 .397 /.586 .296/.546 .353/.423 .292/.399 .264/.364 FBERT (idf) .408/.553 .550/.721 .395/585 .293/.537 .346/ .425 .296 /.406 .260/.366 Table 4: Kendall correlations with segment-level human judgments on WMT18. For each language pair, the left number is the to-English correlation, and the right is the from-English. We bold corre- lations of metrics not signiﬁcantly outperformed by any other metric under bootstrap sampling for that language pair and direction. The numbers in parenthesis are the number of candidate-reference sentence pairs for each language pair and direction. 5 R ESULTS Machine Translation Tables 1-3 show system-level correlation to human judgements, correla- tions on hybrid systems, and model selection performance. We observe that BERTS CORE is con- sistently a top performer. In to-English results, RUSE (Shimanaka et al., 2018) shows competitive performance. However, RUSE is a supervised method trained on WMT16 and WMT15 human judgment data. In cases where RUSE models were not made available, such as for our from-English experiments, it is not possible to use RUSE without additional data and training. Table 4 shows segment-level correlations. We see that BERTS CORE exhibits signiﬁcantly higher performance compared to the other metrics. The large improvement over BLEU stands out, making BERTSCORE particularly suitable to analyze speciﬁc examples, where SENT BLEU is less reliable. In Appendix A, we provide qualitative examples to illustrate the segment-level performance difference between SENT BLEU and BERTS CORE . At the segment-level, BERTS CORE even signiﬁcantly outperforms RUSE. Overall, we ﬁnd that applying importance weighting using idf at times provides small bene- ﬁt, but in other cases does not help. Understanding better when such importance weighting is likely to help is an important direction for future work, and likely depends on the domain of the text and the available test",
      "chunk_index": 11
    },
    {
      "index": 270,
      "chunk_id": "BERTScore2019_chunk_12",
      "source_id": "BERTScore2019",
      "text": "small bene- ﬁt, but in other cases does not help. Understanding better when such importance weighting is likely to help is an important direction for future work, and likely depends on the domain of the text and the available test data. We continue without idf weighting for the rest of our experiments. While recall RBERT , precision PBERT , and F1 FBERT alternate as the best measure in different setting, F1 FBERT performs reliably well across all the different settings. Our overall recommendation is there- fore to use F1. We present additional results using the full set of 351 systems and evaluation metrics in Tables 12-28 in the appendix, including for experiments withidf importance weighting, different contextual embedding models, and model selection. Image Captioning Table 5 shows correlation results for the COCO Captioning Challenge. BERTS CORE outperforms all task-agnostic baselines by large margins. Image captioning presents a challenging evaluation scenario, and metrics based on strict n-gram matching, including BLEU and ROUGE , show weak correlations with human judgments. idf importance weighting shows signiﬁ- Published as a conference paper at ICLR 2020 Metric M1 M2 BLEU -0.019 ∗ -0.005 ∗ METEOR 0.606 ∗ 0.594 ∗ ROUGE -L 0.090 ∗ 0.096 ∗ CIDE R 0.438 ∗ 0.440 ∗ SPICE 0.759 ∗ 0.750 ∗ LEIC 0.939 ∗ 0.949 ∗ BEER 0.491 0.562 EED 0.545 0.599 CHR F++ 0.702 0.729 CHARAC TER 0.800 0.801 PBERT -0.105 -0.041 RBERT 0.888 0.863 FBERT 0.322 0.350 RBERT (idf) 0.917 0.889 Table 5: Pearson correlation on the 2015 COCO Captioning Challenge. The M1 and M2 measures are described in Section 4. L EIC uses images as addi- tional inputs. Numbers with ∗are cited from Cui et al. (2018). We bold the highest correlations of task-speciﬁc and task-agnostic metrics. Type Method QQP PAWS QQP Trained on QQP (supervised) DecAtt 0.939 ∗ 0.263 DIIN 0.952 ∗ 0.324 BERT 0.963∗ 0.351 Trained on QQP + PAWSQQP (supervised) DecAtt - 0.511 DIIN - 0.778 BERT - 0.831 Metric (Not trained on QQP or PAWSQQP) BLEU 0.707 0.527 METEOR 0.755 0.532 ROUGE-L 0.740 0.536 CHRF++ 0.577 0.608 BEER 0.741 0.564 EED 0.743 0.611 CHARACTER 0.698 0.650 PBERT 0.757 0.687 RBERT 0.744 0.685 FBERT 0.761 0.685 FBERT (idf) 0.777 0.693 Table 6: Area under ROC curve (AUC) on QQP and PAWSQQP datasets. The scores of trained De- cATT (Parikh et al., 2016), DIIN (Gong et al., 2018), and ﬁne-tuned BERT are reported by Zhang",
      "chunk_index": 12
    },
    {
      "index": 271,
      "chunk_id": "BERTScore2019_chunk_13",
      "source_id": "BERTScore2019",
      "text": "0.761 0.685 FBERT (idf) 0.777 0.693 Table 6: Area under ROC curve (AUC) on QQP and PAWSQQP datasets. The scores of trained De- cATT (Parikh et al., 2016), DIIN (Gong et al., 2018), and ﬁne-tuned BERT are reported by Zhang et al. (2019). Numbers with ∗are scores on the held-out test set of QQP. We bold the highest correlations of task- speciﬁc and task-agnostic metrics. cant beneﬁt for this task, suggesting people attribute higher importance to content words. Finally, LEIC (Cui et al., 2018), a trained metric that takes images as additional inputs and is optimized speciﬁcally for the COCO data and this set of systems, outperforms all other methods. Speed Despite the use of a large pre-trained model, computing BERTSCORE is relatively fast. We are able to process 192.5 candidate-reference pairs/second using a GTX-1080Ti GPU. The complete WMT18 en-de test set, which includes 2,998 sentences, takes 15.6sec to process, compared to 5.4sec with SacreBLEU (Post, 2018), a common BLEU implementation. Given the sizes of commonly used test and validation sets, the increase in processing time is relatively marginal, and BERTS CORE is a good ﬁt for using during validation (e.g., for stopping) and testing, especially when compared to the time costs of other development stages. 6 R OBUSTNESS ANALYSIS We test the robustness of BERTS CORE using adversarial paraphrase classiﬁcation. We use the Quora Question Pair corpus (QQP; Iyer et al., 2017) and the adversarial paraphrases from the Para- phrase Adversaries from Word Scrambling dataset (PAWS; Zhang et al., 2019). Both datasets con- tain pairs of sentences labeled to indicate whether they are paraphrases or not. Positive examples in QQP are real duplicate questions, while negative examples are related, but different questions. Sentence pairs in PAWS are generated through word swapping. For example, in PAWS,Flights from New York to Floridamay be changed to Flights from Florida to New Yorkand a good classiﬁer should identify that these two sentences are not paraphrases. PAWS includes two parts: PAWSQQP, which is based on the QQP data, and PAWS Wiki. We use the PAWS QQP development set which contains 667 sentences. For the automatic metrics, we use no paraphrase detection training data. We expect that pairs with higher scores are more likely to be paraphrases. To evaluate the automatic metrics on QQA, we use the ﬁrst 5,000 sentences in the training set instead of the the test set because the test labels are",
      "chunk_index": 13
    },
    {
      "index": 272,
      "chunk_id": "BERTScore2019_chunk_14",
      "source_id": "BERTScore2019",
      "text": "expect that pairs with higher scores are more likely to be paraphrases. To evaluate the automatic metrics on QQA, we use the ﬁrst 5,000 sentences in the training set instead of the the test set because the test labels are not available. We treat the ﬁrst sentence as the reference and the second sentence as the candidate. Table 6 reports the area under ROC curve (AUC) for existing models and automatic metrics. We observe that supervised classiﬁers trained on QQP perform worse than random guess on PAWSQQP, which shows these models predict the adversarial examples are more likely to be paraphrases. When Published as a conference paper at ICLR 2020 adversarial examples are provided in training, state-of-the-art models like DIIN (Gong et al., 2018) and ﬁne-tuned BERT are able to identify the adversarial examples but their performance still de- creases signiﬁcantly from their performance on QQP. Most metrics have decent performance on QQP, but show a signiﬁcant performance drop on PAWSQQP, almost down to chance performance. This suggests these metrics fail to to distinguish the harder adversarial examples. In contrast, the performance of BERTS CORE drops only slightly, showing more robustness than the other metrics. 7 D ISCUSSION We propose BERTS CORE , a new metric for evaluating generated text against gold standard refer- ences. BERTS CORE is purposely designed to be simple, task agnostic, and easy to use. Our analysis illustrates how BERTS CORE resolves some of the limitations of commonly used metrics, especially on challenging adversarial examples. We conduct extensive experiments with various conﬁguration choices for BERTS CORE , including the contextual embedding model used and the use of impor- tance weighting. Overall, our extensive experiments, including the ones in the appendix, show that BERTS CORE achieves better correlation than common metrics, and is effective for model selec- tion. However, there is no one conﬁguration of BERTS CORE that clearly outperforms all others. While the differences between the top conﬁgurations are often small, it is important for the user to be aware of the different trade-offs, and consider the domain and languages when selecting the exact conﬁguration to use. In general, for machine translation evaluation, we suggest using FBERT , which we ﬁnd the most reliable. For evaluating text generation in English, we recommend using the 24- layer RoBERTalarge model to compute BERTS CORE . For non-English language, the multilingual BERTmulti is a suitable choice although BERTS CORE",
      "chunk_index": 14
    },
    {
      "index": 273,
      "chunk_id": "BERTScore2019_chunk_15",
      "source_id": "BERTScore2019",
      "text": "FBERT , which we ﬁnd the most reliable. For evaluating text generation in English, we recommend using the 24- layer RoBERTalarge model to compute BERTS CORE . For non-English language, the multilingual BERTmulti is a suitable choice although BERTS CORE computed with this model has less stable performance on low-resource languages. We report the optimal hyperparameter for all models we experimented with in Appendix B Brieﬂy following our initial preprint publication, Zhao et al. (2019) published a concurrently devel- oped method related to ours, but with a focus on integrating contextual word embeddings with earth mover's distance (EMD; Rubner et al., 1998) rather than our simple matching process. They also propose various improvements compared to our use of contextualized embeddings. We study these improvements in Appendix C and show that integrating them into BERTSCORE makes it equivalent or better than the EMD-based approach. Largely though, the effect of the different improvements on BERTS CORE is more modest compared to their method. Shortly after our initial publication, YiSi-1 was updated to use BERT embeddings, showing improved performance (Lo, 2019). This further corroborates our ﬁndings. Other recent related work includes training a model on top of BERT to maximize the correlation with human judgments (Mathur et al., 2019) and evaluating gen- eration with a BERT model ﬁne-tuned on paraphrasing (Yoshimura et al., 2019). More recent work shows the potential of using BERTS CORE for training a summarization system (Li et al., 2019) and for domain-speciﬁc evaluation using SciBERT (Beltagy et al., 2019) to evaluate abstractive text summarization (Gabriel et al., 2019). In future work, we look forward to designing new task-speciﬁc metrics that use BERTS CORE as a subroutine and accommodate task-speciﬁc needs, similar to how Wieting et al. (2019) suggests to use semantic similarity for machine translation training. Because BERTS CORE is fully differentiable, it also can be incorporated into a training procedure to compute a learning loss that reduces the mismatch between optimization and evaluation objectives. ACKNOWLEDGEMENT This research is supported in part by grants from the National Science Foundation (III-1618134, III- 1526012, IIS1149882, IIS-1724282, TRIPODS-1740822, CAREER-1750499), the Ofﬁce of Naval Research DOD (N00014-17-1-2175), and the Bill and Melinda Gates Foundation, SAP, Zillow, Workday, and Facebook Research. We thank Graham Neubig and David Grangier for for their insightful comments. We thank the Cornell NLP community including but not limited to Claire Cardie, Tianze Shi, Alexandra Schoﬁeld, Gregory Yauney, and Rishi",
      "chunk_index": 15
    },
    {
      "index": 274,
      "chunk_id": "BERTScore2019_chunk_16",
      "source_id": "BERTScore2019",
      "text": "Foundation, SAP, Zillow, Workday, and Facebook Research. We thank Graham Neubig and David Grangier for for their insightful comments. We thank the Cornell NLP community including but not limited to Claire Cardie, Tianze Shi, Alexandra Schoﬁeld, Gregory Yauney, and Rishi Bommasani. We thank Yin Cui and Guandao Yang for their help with the COCO 2015 dataset. Published as a conference paper at ICLR 2020 REFERENCES Peter Anderson, Basura Fernando, Mark Johnson, and Stephen Gould. SPICE: Semantic proposi- tional image caption evaluation. In ECCV, 2016. Ben Athiwaratkun, Andrew Wilson, and Anima Anandkumar. Probabilistic fasttext for multi-sense word embeddings. In ACL, 2018. Satanjeev Banerjee and Alon Lavie. METEOR: An automatic metric for mt evaluation with im- proved correlation with human judgments. In IEEvaluation@ACL, 2005. Iz Beltagy, Kyle Lo, and Arman Cohan. SciBERT: A pretrained language model for scientiﬁc text. ArXiv, 2019. Ondˇrej Bojar, Yvette Graham, Amir Kamran, and Miloš Stanojevi´c. Results of the WMT16 metrics shared task. In WMT, 2016. Ondˇrej Bojar, Yvette Graham, and Amir Kamran. Results of the WMT17 metrics shared task. In WMT, 2017. Arun Chaganty, Stephen Mussmann, and Percy Liang. The price of debiasing automatic metrics in natural language evalaution. In ACL, 2018. Julian Chow, Lucia Specia, and Pranava Madhyastha. WMDO: Fluency-based word mover's dis- tance for machine translation evaluation. In WMT, 2019. Elizabeth Clark, Asli Celikyilmaz, and Noah A. Smith. Sentence mover's similarity: Automatic evaluation for multi-sentence texts. In ACL, 2019. Courtney Corley and Rada Mihalcea. Measuring the semantic similarity of texts. In ACL Workshop, EMSEE '05, 2005. Yin Cui, Guandao Yang, Andreas Veit, Xun Huang, and Serge J. Belongie. Learning to evaluate image captioning. In CVPR, 2018. Michael Denkowski and Alon Lavie. Meteor universal: Language speciﬁc translation evaluation for any target language. In WMT@ACL, 2014. Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. BERT: Pre-training of deep bidirectional transformers for language understanding. In NAACL-HLT, 2019. George Doddington. Automatic evaluation of machine translation quality using n-gram co- occurrence statistics. In HLT, 2002. William B Dolan and Chris Brockett. Automatically constructing a corpus of sentential paraphrases. In IWP, 2005. Saadia Gabriel, Antoine Bosselut, Ari Holtzman, Kyle Lo, Asli Çelikyilmaz, and Yejin Choi. Co- operative generator-discriminator networks for abstractive summarization with narrative ﬂow. ArXiv, 2019. Michel Galley, Chris Brockett, Alessandro Sordoni, Yangfeng Ji, Michael Auli, Chris Quirk, Mar- garet Mitchell, Jianfeng Gao, and William B. Dolan. deltaBLEU: A discriminative metric for generation tasks with intrinsically diverse targets. In",
      "chunk_index": 16
    },
    {
      "index": 275,
      "chunk_id": "BERTScore2019_chunk_17",
      "source_id": "BERTScore2019",
      "text": "abstractive summarization with narrative ﬂow. ArXiv, 2019. Michel Galley, Chris Brockett, Alessandro Sordoni, Yangfeng Ji, Michael Auli, Chris Quirk, Mar- garet Mitchell, Jianfeng Gao, and William B. Dolan. deltaBLEU: A discriminative metric for generation tasks with intrinsically diverse targets. In ACL, 2015. Jonas Gehring, Michael Auli, David Grangier, Denis Yarats, and Yann N Dauphin. Convolutional sequence to sequence learning. In ICML, 2017. Yichen Gong, Heng Luo, and Jian Zhang. Natural language inference over interaction space. In ICLR, 2018. Yvette Graham and Timothy Baldwin. Testing for signiﬁcance of increased correlation with human judgment. In EMNLP, 2014. Published as a conference paper at ICLR 2020 Yvette Graham and Qun Liu. Achieving accurate conclusions in evaluation of automatic machine translation metrics. In NAACL, 2016. Edouard Grave, Piotr Bojanowski, Prakhar Gupta, Armand Joulin, and Tomas Mikolov. Learning word vectors for 157 languages. arXiv preprint arXiv:1802.06893, 2018. Yinuo Guo and Junfeng Hu. Meteor++ 2.0: Adopt syntactic level paraphrase knowledge into ma- chine translation evaluation. In WMT, 2019. Tatsu Hashimoto, Hugh Zhang, and Percy Liang. Unifying human and statistical evaluation for natural language generation. In NAACL-HLT, 2019. Chenyang Huang, Amine Trabelsi, and Osmar R Zaïane. ANA at semeval-2019 task 3: Contex- tual emotion detection in conversations through hierarchical LSTMs and BERT. arXiv preprint arXiv:1904.00132, 2019. Hideki Isozaki, Tsutomu Hirao, Kevin Duh, Katsuhito Sudoh, and Hajime Tsukada. Automatic evaluation of translation quality for distant language pairs. In EMNLP, 2010. Shankar Iyer, Nikhil Dandekar, and Kornel Csernai. First quora dataset release: Question pairs. https://tinyurl.com/y2y8u5ed, 2017. Philipp Koehn, Hieu Hoang, Alexandra Birch, Chris Callison-Burch, Marcello Federico, Nicola Bertoldi, Brooke Cowan, Wade Shen, Christine Moran, Richard Zens, Chris Dyer, Ondrej Bojar, Alexandra Constantin, and Evan Herbst. Moses: Open source toolkit for statistical machine translation. In ACL, 2007. Matt Kusner, Yu Sun, Nicholas Kolkin, and Kilian Weinberger. From word embeddings to document distances. In ICML, 2015. Guillaume Lample and Alexis Conneau. Cross-lingual language model pretraining. arXiv, 2019. Gregor Leusch, Nicola Uefﬁng, and Hermann Ney. CDER: Efﬁcient MT evaluation using block movements. In EACL, 2006. Vladimir Iosifovich Levenshtein. Binary Codes Capable of Correcting Deletions, Insertions and Rever sals. Soviet Physics Doklady, 10, 1966. Siyao Li, Deren Lei, Pengda Qin, and William Yang Wang. Deep reinforcement learning with distributional semantic rewards for abstractive summarization. In EMNLP-IJCNLP, 2019. Chin-Yew Lin. ROUGE: A package for automatic evaluation of summaries. In ACL, 2004. Tsung-Yi Lin, Michael Maire, Serge J. Belongie, Lubomir D. Bourdev, Ross B. Girshick,",
      "chunk_index": 17
    },
    {
      "index": 276,
      "chunk_id": "BERTScore2019_chunk_18",
      "source_id": "BERTScore2019",
      "text": "Wang. Deep reinforcement learning with distributional semantic rewards for abstractive summarization. In EMNLP-IJCNLP, 2019. Chin-Yew Lin. ROUGE: A package for automatic evaluation of summaries. In ACL, 2004. Tsung-Yi Lin, Michael Maire, Serge J. Belongie, Lubomir D. Bourdev, Ross B. Girshick, James Hays, Pietro Perona, Deva Ramanan, Piotr Dollár, and C. Lawrence Zitnick. Microsoft COCO: Common objects in context. In ECCV, 2014. Nelson F. Liu, Matt Gardner, Yonatan Belinkov, Matthew E. Peters, and Noah A. Smith. Linguistic knowledge and transferability of contextual representations. arXiv preprint arXiv:1903.08855, 2019a. Yang Liu. Fine-tune BERT for extractive summarization. arXiv preprint arXiv:1903.10318, 2019. Yinhan Liu, Myle Ott, Naman Goyal, Jingfei Du, Mandar Joshi, Danqi Chen, Omer Levy, Mike Lewis, Luke Zettlemoyer, and Veselin Stoyanov. Roberta: A robustly optimized BERT pretrain- ing approach. arXiv, abs/1907.11692, 2019b. Chi-kiu Lo. MEANT 2.0: Accurate semantic mt evaluation for any output language. InWMT, 2017. Chi-kiu Lo. YiSi - a uniﬁed semantic MT quality evaluation and estimation metric for languages with different levels of available resources. In WMT, 2019. Published as a conference paper at ICLR 2020 Chi-kiu Lo, Michel Simard, Darlene Stewart, Samuel Larkin, Cyril Goutte, and Patrick Littell. Ac- curate semantic textual similarity for cleaning noisy parallel corpora using semantic machine translation evaluation metric: The NRC supervised submissions to the parallel corpus ﬁltering task. In WMT, 2018. Ryan Lowe, Michael Noseworthy, Iulian Vlad Serban, Nicolas Angelard-Gontier, Yoshua Bengio, and Joelle Pineau. Towards an automatic Turing test: Learning to evaluate dialogue responses. In ACL, 2017. Qingsong Ma, Yvette Graham, Shugen Wang, and Qun Liu. Blend: a novel combined mt metric based on direct assessment - casict-dcu submission to WMT17 metrics task. In WMT, 2017. Qingsong Ma, Ondrej Bojar, and Yvette Graham. Results of the WMT18 metrics shared task: Both characters and embeddings achieve good performance. In WMT, 2018. Nitika Mathur, Timothy Baldwin, and Trevor Cohn. Putting evaluation in context: Contextual em- beddings improve machine translation evaluation. In ACL, 2019. Tomas Mikolov, Ilya Sutskever, Kai Chen, Gregory S. Corrado, and Jeffrey Dean. Distributed representations of words and phrases and their compositionality. In NIPS, 2013. Dai Quoc Nguyen, Dat Quoc Nguyen, Ashutosh Modi, Stefan Thater, and Manfred Pinkal. A mixture model for learning multi-sense word embeddings. In ACL, 2017. Myle Ott, Sergey Edunov, David Grangier, and Michael Auli. Scaling neural machine translation. In WMT, 2018. Myle Ott, Sergey Edunov, Alexei Baevski, Angela Fan, Sam Gross, Nathan Ng, David Grangier, and Michael Auli. fairseq:",
      "chunk_index": 18
    },
    {
      "index": 277,
      "chunk_id": "BERTScore2019_chunk_19",
      "source_id": "BERTScore2019",
      "text": "multi-sense word embeddings. In ACL, 2017. Myle Ott, Sergey Edunov, David Grangier, and Michael Auli. Scaling neural machine translation. In WMT, 2018. Myle Ott, Sergey Edunov, Alexei Baevski, Angela Fan, Sam Gross, Nathan Ng, David Grangier, and Michael Auli. fairseq: A fast, extensible toolkit for sequence modeling. arXiv preprint arXiv:1904.01038, 2019. Joybrata Panja and Sudip Kumar Naskar. Iter: Improving translation edit rate through optimizable edit costs. In WMT, 2018. Kishore Papineni, Salim Roukos, Todd Ward, and Wei-Jing Zhu. Bleu: a method for automatic evaluation of machine translation. In ACL, 2002. Ankur Parikh, Oscar Täckström, Dipanjan Das, and Jakob Uszkoreit. A decomposable attention model for natural language inference. In EMNLP, 2016. Jeffrey Pennington, Richard Socher, and Christopher D. Manning. Glove: Global vectors for word representation. In EMNLP, 2014. Matthew E. Peters, Mark Neumann, Mohit Iyyer, Matt Gardner, Christopher Clark, Kenton Lee, and Luke S. Zettlemoyer. Deep contextualized word representations. In NAACL-HLT, 2018. Maja Popovi´c. chrf: character n-gram f-score for automatic mt evaluation. In WMT@ACL, 2015. Maja Popovi´c. chrf++: words helping character n-grams. In WMT, 2017. Matt Post. A call for clarity in reporting BLEU scores. In WMT, 2018. Nils Reimers and Iryna Gurevych. Alternative weighting schemes for elmo embeddings. arXiv preprint arXiv:1904.02954, 2019. Yossi Rubner, Carlo Tomasi, and Leonidas J Guibas. A metric for distributions with applications to image databases. In ICCV. IEEE, 1998. Vasile Rus and Mihai Lintean. A comparison of greedy and optimal assessment of natural language student input using word-to-word similarity metrics. In Proceedings of the Seventh Workshop on Building Educational Applications Using NLP. ACL, 2012. Published as a conference paper at ICLR 2020 Andreas Rücklé, Steffen Eger, Maxime Peyrard, and Iryna Gurevych. Concatenated power mean word embeddings as universal cross-lingual sentence representations. arXiv, 2018. Shikhar Sharma, Layla El Asri, Hannes Schulz, and Jeremie Zumer. Relevance of unsupervised metrics in task-oriented dialogue for evaluating natural language generation. arXiv preprint arXiv:1706.09799, 2018. Hiroki Shimanaka, Tomoyuki Kajiwara, and Mamoru Komachi. Ruse: Regressor using sentence embeddings for automatic machine translation evaluation. In WMT, 2018. Matthew Snover, Bonnie Dorr, Richard Schwartz, Linnea Micciulla, and John Makhoul. A study of translation edit rate with targeted human annotation. In AMTA, 2006. Peter Stanchev, Weiyue Wang, and Hermann Ney. EED: Extended edit distance measure for machine translation. In WMT, 2019. Miloš Stanojevi´c and Khalil Sima'an. Beer: Better evaluation as ranking. In WMT, 2014. Christoph Tillmann, Stephan V ogel, Hermann Ney, Arkaitz Zubiaga, and Hassan Sawaf.",
      "chunk_index": 19
    },
    {
      "index": 278,
      "chunk_id": "BERTScore2019_chunk_20",
      "source_id": "BERTScore2019",
      "text": "Wang, and Hermann Ney. EED: Extended edit distance measure for machine translation. In WMT, 2019. Miloš Stanojevi´c and Khalil Sima'an. Beer: Better evaluation as ranking. In WMT, 2014. Christoph Tillmann, Stephan V ogel, Hermann Ney, Arkaitz Zubiaga, and Hassan Sawaf. Accelerated dp based search for statistical translation. In EUROSPEECH, 1997. Kristina Toutanova, Chris Brockett, Ke M Tran, and Saleema Amershi. A dataset and evaluation metrics for abstractive compression of sentences and short paragraphs. In EMNLP, 2016. Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N Gomez, Łukasz Kaiser, and Illia Polosukhin. Attention is all you need. In NIPS, 2017. Ramakrishna Vedantam, C. Lawrence Zitnick, and Devi Parikh. CIDEr: Consensus-based image description evaluation. In CVPR, 2015. Weiyue Wang, Jan-Thorsten Peter, Hendrik Rosendahl, and Hermann Ney. Character: Translation edit rate on character level. In WMT, 2016. John Wieting, Taylor Berg-Kirkpatrick, Kevin Gimpel, and Graham Neubig. Beyond BLEU:training neural machine translation with semantic similarity. In ACL, 2019. Adina Williams, Nikita Nangia, and Samuel Bowman. A broad-coverage challenge corpus for sen- tence understanding through inference. In ACL, 2018. Evan James Williams. Regression analysis. wiley, 1959. Felix Wu, Angela Fan, Alexei Baevski, Yann Dauphin, and Michael Auli. Pay less attention with lightweight and dynamic convolutions. In ICLR, 2019. Yonghui Wu, Mike Schuster, Zhifeng Chen, Quoc V . Le, Mohammad Norouzi, Wolfgang Macherey, Maxim Krikun, Yuan Cao, Qin Gao, Jeff Klingner, Apurva Shah, Melvin Johnson, Xiaobing Liu, Lukasz Kaiser, Stephan Gouws, Yoshikiyo Kato, Taku Kudo, Hideto Kazawa, Keith Stevens, George Kurian, Nishant Patil, Wei Wang, Cliff Young, Jason Smith, Jason Riesa, Alex Rudnick, Oriol Vinyals, Gregory S. Corrado, Macduff Hughes, and Jeffrey Dean. Google's neural machine translation system: Bridging the gap between human and machine translation. arXiv preprint arXiv:1609.08144, 2016. Wei Yang, Haotian Zhang, and Jimmy Lin. Simple applications of BERT for ad hoc document retrieval. arXiv preprint arXiv:1903.10972, 2019a. Zhilin Yang, Zihang Dai, Yiming Yang, Jaime Carbonell, Ruslan Salakhutdinov, and Quoc V . Le. XLNet: Generalized autoregressive pretraining for language understanding. arXiv, 2019b. Ryoma Yoshimura, Hiroki Shimanaka, Yukio Matsumura, Hayahide Yamagishi, and Mamoru Ko- machi. Filtering pseudo-references by paraphrasing for automatic evaluation of machine transla- tion. In WMT, 2019. Published as a conference paper at ICLR 2020 Yuan Zhang, Jason Baldridge, and Luheng He. PAWS: Paraphrase adversaries from word scram- bling. arXiv preprint arXiv:1904.01130, 2019. Wei Zhao, Maxime Peyrard, Fei Liu, Yang Gao, Christian M. Meyer, and Steffen Eger. Moverscore: Text generation",
      "chunk_index": 20
    },
    {
      "index": 279,
      "chunk_id": "BERTScore2019_chunk_21",
      "source_id": "BERTScore2019",
      "text": "conference paper at ICLR 2020 Yuan Zhang, Jason Baldridge, and Luheng He. PAWS: Paraphrase adversaries from word scram- bling. arXiv preprint arXiv:1904.01130, 2019. Wei Zhao, Maxime Peyrard, Fei Liu, Yang Gao, Christian M. Meyer, and Steffen Eger. Moverscore: Text generation evaluating with contextualized embeddings and earth mover distance. InEMNLP, 2019. Published as a conference paper at ICLR 2020 Case No. Reference and Candidate Pairs Human FBERT BLEU FBERT>BLEU 1. x: At the same time Kingﬁsher is closing 60 B&Q outlets across the country 38 125 530ˆx: At the same time, Kingﬁsher will close 60 B & Q stores nationwide 2. x: Hewlett-Packard to cut up to 30,000 jobs 119 39 441ˆx: Hewlett-Packard will reduce jobs up to 30.000 3. x: According to opinion in Hungary, Serbia is \"a safe third country\". 23 96 465ˆx: According to Hungarian view, Serbia is a \"safe third country.\" 4. x: Experts believe November's Black Friday could be holding back spending.73 147 492ˆx: Experts believe that the Black Friday in November has put the brakes on spending 5. x: And it's from this perspective that I will watch him die. 37 111 414ˆx: And from this perspective, I will see him die. BLEU>FBERT 6. x: In their view the human dignity of the man had been violated. 500 470 115ˆx: Look at the human dignity of the man injured. 8. x: For example when he steered a shot from Ideye over the crossbar in the 56th minute.516 524 185ˆx: So, for example, when he steered a shot of Ideye over the latte (56th). 7. x: A good prank is funny, but takes moments to reverse. 495 424 152ˆx: A good prank is funny, but it takes only moments before he becomes a boomerang. 9. x: I will put the pressure on them and onus on them to make a decision. 507 471 220ˆx: I will exert the pressure on it and her urge to make a decision. 10. x: Transport for London is not amused by this ﬂyposting \"vandalism.\" 527 527 246ˆx: Transport for London is the Plaka animal \"vandalism\" is not funny. FBERT>Human 11. x: One big obstacle to access to the jobs market is the lack of knowledge of the German language.558 131 313ˆx: A major hurdle for access to the labour market are a lack of knowledge of English. 12. x: On Monday night Hungary closed its 175 km long border with",
      "chunk_index": 21
    },
    {
      "index": 280,
      "chunk_id": "BERTScore2019_chunk_22",
      "source_id": "BERTScore2019",
      "text": "is the lack of knowledge of the German language.558 131 313ˆx: A major hurdle for access to the labour market are a lack of knowledge of English. 12. x: On Monday night Hungary closed its 175 km long border with Serbia. 413 135 55ˆx: Hungary had in the night of Tuesday closed its 175 km long border with Serbia. 13. x: They got nothing, but they were allowed to keep the clothes. 428 174 318ˆx: You got nothing, but could keep the clothes. 14. x: A majority of Republicans don't see Trump's temperament as a problem. 290 34 134ˆx: A majority of Republicans see Trump's temperament is not a problem. 15. x:His car was still running in the driveway. 299 49 71ˆx: His car was still in the driveway. Human>FBERT 16. x: Currently the majority of staff are men. 77 525 553ˆx: At the moment the men predominate among the staff. 17. x: There are, indeed, multiple variables at play. 30 446 552ˆx: In fact, several variables play a role. 18. x: One was a man of about 5ft 11in tall. 124 551 528ˆx: One of the men was about 1,80 metres in size. 19. x: All that stuff sure does take a toll. 90 454 547ˆx: All of this certainly exacts its toll. 20. x: Wage gains have shown signs of picking up. 140 464 514ˆx: Increases of wages showed signs of a recovery. Table 7: Examples sentences where similarity ranks assigned by Human, FBERT , and B LEU differ signiﬁcantly on WMT16 German-to-English evaluation task.x: gold reference, ˆx: candidate outputs of MT systems. Rankings assigned by Human, FBERT , and B LEU are shown in the right three columns. The sentences are ranked by the similarity, i.e. rank 1 is the most similar pair assigned by a score. An ideal metric should rank similar to humans. A Q UALITATIVE ANALYSIS We study BERTSCORE and SENT BLEU using WMT16 German-to-English (Bojar et al., 2016). We rank all 560 candidate-reference pairs by human score, BERTS CORE , or S ENT BLEU from most similar to least similar. Ideally, the ranking assigned by BERTS CORE and S ENT BLEU should be similar to the ranking assigned by the human score. Table 7 ﬁrst shows examples where BERTSCORE and SENT BLEU scores disagree about the ranking for a candidate-reference pair by a large number. We observe that BERTS CORE is effectively able",
      "chunk_index": 22
    },
    {
      "index": 281,
      "chunk_id": "BERTScore2019_chunk_23",
      "source_id": "BERTScore2019",
      "text": "similar to the ranking assigned by the human score. Table 7 ﬁrst shows examples where BERTSCORE and SENT BLEU scores disagree about the ranking for a candidate-reference pair by a large number. We observe that BERTS CORE is effectively able to capture synonyms and changes in word order. For example, the reference and candidate sentences in pair 3 are almost identical except that the candidate replaces opinion in Hungarywith Hungarian view and switches the order of the quotation mark ( \") and a. While BERTS CORE ranks the pair relatively high, SENT BLEU judges the pair as dissimilar, because it cannot match synonyms and is sensitive to the small word order changes. Pair 5 shows a set of changes that preserve the semantic meaning: replacing to cutwith will reduceand swapping the order of30,000 and jobs. BERTS CORE ranks the candidate translation similar to the human judgment, whereas S ENT BLEU ranks it much lower. We also see that S ENT BLEU potentially over-rewards n-gram overlap, even when phrases are used very differently. In pair 6, both the candidate and the reference contain the human dignity of the man. Yet the two sentences convey very different meaning. BERTS CORE agrees with the human judgment and ranks the pair low. In contrast, S ENT BLEU considers the pair as relatively similar because of the signiﬁcant word overlap. Published as a conference paper at ICLR 2020 Figure 2: BERTS CORE visualization. The cosine similarity of each word matching in PBERT are color-coded. The bottom half of Table 7 shows examples where BERTS CORE and human judgments disagree about the ranking. We observe that BERTS CORE ﬁnds it difﬁcult to detect factual errors. For example, BERTS CORE assigns high similarity to pair 11 when the translation replaces German language with English and pair 12 where the translation incorrectly outputs Tuesday when it is supposed to generate Monday. BERTS CORE also fails to identify that 5ft 11inis equivalent with 1.80 metresin pair 18. As a result, BERTS CORE assigns low similarity to the eighth pair in Table 7. SENT BLEU also suffers from these limitations. Figure 2 visualizes the BERTS CORE matching of two pairs of candidate and reference sentences. The ﬁgure illustrates how FBERT matches synonymous phrases, such as imported carsand foreign cars. We also see that FBERT effectively matches words even given a high ordering distortion, for example the token people in the",
      "chunk_index": 23
    },
    {
      "index": 282,
      "chunk_id": "BERTScore2019_chunk_24",
      "source_id": "BERTScore2019",
      "text": "of candidate and reference sentences. The ﬁgure illustrates how FBERT matches synonymous phrases, such as imported carsand foreign cars. We also see that FBERT effectively matches words even given a high ordering distortion, for example the token people in the ﬁgure. Published as a conference paper at ICLR 2020 B R EPRESENTATION CHOICE As suggested by previous works (Peters et al., 2018; Reimers & Gurevych, 2019), selecting a good layer or a good combination of layers from the BERT model is important. In designing BERTS CORE , we use WMT16 segment-level human judgment data as a development set to fa- cilitate our representation choice. For Chinese models, we tune with the WMT17 \"en-zh\" data because the language pair \"en-zh\" is not available in WMT16. In Figure 3, we plot the change of human correlation of FBERT over different layers of BERT, RoBERTa, XLNet and XLM models. Based on results from different models, we identify a common trend that FBERT computed with the intermediate representations tends to work better. We tune the number of layer to use for a range of publicly available models.8 Table 8 shows the results of our hyperparameter search. Model Total Number of Layers Best Layer bert-base-uncased 12 9 bert-large-uncased 24 18 bert-base-cased-ﬁnetuned-mrpc 12 9 bert-base-multilingual-cased 12 9 bert-base-chinese 12 8 roberta-base 12 10 roberta-large 24 17 roberta-large-mnli 24 19 xlnet-base-cased 12 5 xlnet-large-cased 24 7 xlm-mlm-en-2048 12 7 xlm-mlm-100-1280 16 11 Table 8: Recommended layer of representation to use for BERTS CORE . The layers are chosen based on a held-out validation set (WMT16). 8https://huggingface.co/pytorch-transformers/pretrained_models.html Published as a conference paper at ICLR 2020 Figure 3: Pearson correlation of FBERT computed with different models, across different layers, with segment-level human judgments on the WMT16 to-English machine translation task. The WMT17 English-Chinese data is used for the BERT Chinese model. Layer 0 corresponds to using BPE embeddings. Consistently, correlation drops signiﬁcantly in the ﬁnal layers. Published as a conference paper at ICLR 2020 C A BLATION STUDY OF MOVER SCORE Word Mover's Distance (WMD; Kusner et al., 2015) is a semantic similarity metric that relies on word embeddings and optimal transport. M OVER SCORE (Zhao et al., 2019) combines contextual embeddings and WMD for text generation evaluation. In contrast, BERTS CORE adopts a greedy approach to aggregate token-level information. In addition to using WMD for generation evalu- ation, Zhao et al. (2019) also introduce various other improvements. We",
      "chunk_index": 24
    },
    {
      "index": 283,
      "chunk_id": "BERTScore2019_chunk_25",
      "source_id": "BERTScore2019",
      "text": "combines contextual embeddings and WMD for text generation evaluation. In contrast, BERTS CORE adopts a greedy approach to aggregate token-level information. In addition to using WMD for generation evalu- ation, Zhao et al. (2019) also introduce various other improvements. We do a detailed ablation study to understand the beneﬁt of each improvement, and to investigate whether it can be applied to BERTS CORE . We use a 12-layer uncased BERT model on the WMT17 to-English segment-level data, the same setting as Zhao et al. (2019). We identify several differences between MOVER SCORE and BERTS CORE by analyzing the released source code. We isolate each difference, and mark it with a bracketed tag for our ablation study: 1. [MNLI] Use a BERT model ﬁne-tuned on MNLI (Williams et al., 2018). 2. [PMEANS] Apply power means (Rücklé et al., 2018) to aggregate the information of dif- ferent layers.9 3. [IDF-L] For reference sentences, instead of computing the idf scores on the 560 sen- tences in the segment-level data ([IDF-S]), compute theidf scores on the 3,005 sentences in the system-level data. 4. [SEP] For candidate sentences, recompute the idf scores on the candidate sentences. The weighting of reference tokens are kept the same as in [IDF-S] 5. [RM] Exclude punctuation marks and sub-word tokens except the ﬁrst sub-word in each word from the matching. We follow the setup of Zhao et al. (2019) and use their released ﬁne-tuned BERT model to conduct the experiments. Table 9 shows the results of our ablation study. We report corre- lations for the two variants of WMD Zhao et al. (2019) study: unigrams (WMD1) and bi- grams (WMD2). Our FBERT corresponds to the vanilla setting and the importance weighted vari- ant corresponds to the [IDF-S] setting. The complete M OVER SCORE metric corresponds to [IDF-S]+[SEP]+[PMEANS]+[MNLI]+[RM]. We make several observations. First, for all lan- guage pairs except ﬁ-en and lv-en, we can replicate the reported performance. For these two lan- guage pairs, Zhao et al. (2019) did not release their implementations at the time of publication. 10 Second, we conﬁrm the effectiveness of [PMEANS] and [MNLI]. In Appendix F, we study more pre-trained models and further corroborate this conclusion. However, the contribution of other tech- niques, including [RM] and [SEP], seems less stable. Third, replacing greedy matching with WMD does not lead to consistent improvement. In fact, oftentimes BERTS CORE is the better met- ric when given the",
      "chunk_index": 25
    },
    {
      "index": 284,
      "chunk_id": "BERTScore2019_chunk_26",
      "source_id": "BERTScore2019",
      "text": "conclusion. However, the contribution of other tech- niques, including [RM] and [SEP], seems less stable. Third, replacing greedy matching with WMD does not lead to consistent improvement. In fact, oftentimes BERTS CORE is the better met- ric when given the same setup. In general, for any given language pair, BERTS CORE is always among the best performing ones. Given the current results, it is not clear tht WMD is better than greedy matching for text generation evaluation. 9 Zhao et al. (2019) uses the embeddings from the last ﬁve layers from BERT and L2-normalizes the embed- ding vectors at each layer before computing the P-MEANs and L2-normalizing the concatenated P-MEANS. 10A public comment on the project page indicates that some of the techniques are not applied for these two language pairs (https://github.com/AIPHES/emnlp19-moverscore/issues/1). Published as a conference paper at ICLR 2020 Ablation Metric cs-en de-en ﬁ-en lv-en ru-en tr-en zh-en Vanilla WMD1 0.628 0.655 0.795 0.692 0.701 0.715 0.699 WMD2 0.638 0.661 0.797 0.695 0.700 0.728 0.714 FBERT 0.659 0.680 0.817 0.702 0.719 0.727 0.717 IDF-S WMD1 0.636 0.662 0.824 0.709 0.716 0.728 0.713 WMD2 0.643 0.662 0.821 0.708 0.712 0.732 0.715 FBERT 0.657 0.681 0.823 0.713 0.725 0.718 0.711 IDF-L WMD1 0.633 0.659 0.825 0.708 0.716 0.727 0.715 WMD2 0.641 0.661 0.822 0.708 0.713 0.730 0.716 FBERT 0.655 0.682 0.823 0.713 0.726 0.718 0.712 IDF-L + SEP WMD1 0.651 0.660 0.819 0.703 0.714 0.724 0.715 WMD2 0.659 0.662 0.816 0.702 0.712 0.729 0.715 FBERT 0.664 0.681 0.818 0.709 0.724 0.716 0.710 IDF-L + SEP + RM WMD1 0.651 0.686 0.803 0.681 0.730 0.730 0.720 WMD2 0.664 0.687 0.797 0.679 0.728 0.735 0.718 FBERT 0.659 0.695 0.800 0.683 0.734 0.722 0.712 IDF-L + SEP + PMEANS WMD1 0.658 0.663 0.820 0.707 0.717 0.725 0.712 WMD2 0.667 0.665 0.817 0.707 0.717 0.727 0.712 FBERT 0.671 0.682 0.819 0.708 0.725 0.715 0.704 IDF-L + SEP + MNLI WMD1 0.659 0.679 0.822 0.732 0.718 0.746 0.725 WMD2 0.664 0.682 0.819 0.731 0.715 0.748 0.722 FBERT 0.668 0.701 0.825 0.737 0.727 0.744 0.725 IDF-L + SEP + PMEANS + MNLI WMD1 0.672 0.686 0.831 0.738 0.725 0.753 0.737 WMD2 0.677 0.690 0.828 0.736 0.722 0.755 0.735 FBERT 0.682 0.707 0.836 0.741 0.732 0.751 0.736 IDF-L + SEP + PMEANS + MNLI + RM WMD1 0.670 0.708 0.821 0.717 0.738 0.762 0.744 WMD2 0.679 0.709 0.814 0.716 0.736 0.762 0.738 FBERT 0.676 0.717 0.824 0.719 0.740",
      "chunk_index": 26
    },
    {
      "index": 285,
      "chunk_id": "BERTScore2019_chunk_27",
      "source_id": "BERTScore2019",
      "text": "0.735 FBERT 0.682 0.707 0.836 0.741 0.732 0.751 0.736 IDF-L + SEP + PMEANS + MNLI + RM WMD1 0.670 0.708 0.821 0.717 0.738 0.762 0.744 WMD2 0.679 0.709 0.814 0.716 0.736 0.762 0.738 FBERT 0.676 0.717 0.824 0.719 0.740 0.757 0.738 Table 9: Ablation Study of M OVER SCORE and BERTS CORE using Pearson correlations on the WMT17 to-English segment-level data. Correlations that are not outperformed by others for that language pair under Williams Test are bolded. We observe that using WMD does not consistently improve BERTS CORE . Published as a conference paper at ICLR 2020 Type Metric Meaning Grammar Combined BERTS CORE PBERT 0.36 0.47 0.46 RBERT 0.64 0.29 0.52 FBERT 0.58 0.41 0.56 Common metrics BLEU 0.46 0.13 0.33 METEOR 0.53 0.11 0.36 ROUGE-L 0.51 0.16 0.38 SARI 0.50 0.15 0.37 Best metrics according to Toutanova et al. (2016) SKIP-2+R ECALL +MULT-PROB 0.59 N/A 0.51 PARSE-2+R ECALL +MULT-MAX N/A 0.35 0.52 PARSE-2+R ECALL +MULT-PROB 0.57 0.35 0.52 Table 10: Pearson correlations with human judgments on the MSR Abstractive Text Compression Dataset. D A DDITIONAL EXPERIMENTS ON ABSTRACTIVE TEXT COMPRESSION We use the human judgments provided from the MSR Abstractive Text Compression Dataset (Toutanova et al., 2016) to illustrate the applicability of BERTS CORE to abstractive text compression evaluation. The data includes three types of human scores: (a) meaning: how well a compressed text preserve the meaning of the original text; (b) grammar: how grammatically correct a compressed text is; and (c) combined: the average of the meaning and the grammar scores. We follow the experimental setup of Toutanova et al. (2016) and report Pearson correlation between BERTS CORE and the three types of human scores. Table 10 shows that RBERT has the highest cor- relation with human meaning judgments, and PBERT correlates highly with human grammar judg- ments. FBERT provides a balance between the two aspects. Published as a conference paper at ICLR 2020 Task Model BLEU ˆPBERT ˆRBERT ˆFBERT PBERT RBERT FBERT WMT14 En-De ConvS2S (Gehring et al., 2017) 0.266 0.6099 0.6055 0.6075 0.8499 0.8482 0.8488 Transformer-big∗∗(Ott et al., 2018) 0.298 0.6587 0.6528 0.6558 0.8687 0.8664 0.8674 DynamicConv∗∗∗(Wu et al., 2019) 0.297 0.6526 0.6464 0.6495 0.8664 0.8640 0.8650 WMT14 En-Fr ConvS2S (Gehring et al., 2017) 0.408 0.6998 0.6821 0.6908 0.8876 0.8810 0.8841 Transformer-big (Ott et al., 2018) 0.432 0.7148 0.6978 0.7061 0.8932 0.8869 0.8899 DynamicConv (Wu et al., 2019) 0.432 0.7156 0.6989 0.7071 0.8936 0.8873",
      "chunk_index": 27
    },
    {
      "index": 286,
      "chunk_id": "BERTScore2019_chunk_28",
      "source_id": "BERTScore2019",
      "text": "0.8664 0.8640 0.8650 WMT14 En-Fr ConvS2S (Gehring et al., 2017) 0.408 0.6998 0.6821 0.6908 0.8876 0.8810 0.8841 Transformer-big (Ott et al., 2018) 0.432 0.7148 0.6978 0.7061 0.8932 0.8869 0.8899 DynamicConv (Wu et al., 2019) 0.432 0.7156 0.6989 0.7071 0.8936 0.8873 0.8902 IWSLT14 De-En Transformer-iwslt+ (Ott et al., 2019) 0.350 0.6749 0.6590 0.6672 0.9452 0.9425 0.9438 LightConv (Wu et al., 2019) 0.348 0.6737 0.6542 0.6642 0.9450 0.9417 0.9433 DynamicConv (Wu et al., 2019) 0.352 0.6770 0.6586 0.6681 0.9456 0.9425 0.9440 Table 11: B LEU scores and BERTS CORE s of publicly available pre-trained MT models in fairseq (Ott et al., 2019). We show both rescaled scores marked with ˆ and raw BERTS CORE s. ∗: trained on unconﬁrmed WMT data version, ∗∗: trained on WMT16 + ParaCrawl, ∗∗∗: trained on WMT16, +: trained by us using fairseq. E BERTS CORE OF RECENT MT M ODELS Table 11 shows the BLEU scores and the BERTS CORE s of pre-trained machine translation models on WMT14 English-to-German, WMT14 English-to-French, IWSLT14 German-to-English task. We used publicly available pre-trained models from fairseq (Ott et al., 2019). 11 Because a pre- trained Transformer model on IWSLT is not released, we trained our own using the fairseq library. We use multilingual cased BERT base12 for English-to-German and English-to-French pairs, and English uncased BERT base13 for German-to-English pairs. Interestingly, the gap between a Dy- namicConv (Wu et al., 2019) trained on only WMT16 and a Transformer (Ott et al., 2018) trained on WMT16 and ParaCrawl14 (about 30×more training data) becomes larger when evaluated with BERTS CORE rather than BLEU . 11 Code and pre-trained model available at https://github.com/pytorch/fairseq. 12Hash code: bert-base-multilingual-cased_L9_version=0.2.0 13Hash code: roberta-large_L17_version=0.2.0 14http://paracrawl.eu/download.html Published as a conference paper at ICLR 2020 F A DDITIONAL RESULTS In this section, we present additional experimental results: 1. Segment-level and system-level correlation studies on three years of WMT metric evalua- tion task (WMT16-18) 2. Model selection study on WMT18 10K hybrid systems 3. System-level correlation study on 2015 COCO captioning challenge 4. Robustness study on PAWS-QQP. Following BERT (Devlin et al., 2019), a variety of Transformer-based (Vaswani et al., 2017) pre- trained contextual embeddings have been proposed and released. We conduct additional experiments with four types of pre-trained embeddings: BERT, XLM (Lample & Conneau, 2019), XLNet (Yang et al., 2019b), and RoBERTa (Liu et al., 2019b). XLM (Cross-lingual Language Model) is a Trans- former pre-trained on the translation language modeling of",
      "chunk_index": 28
    },
    {
      "index": 287,
      "chunk_id": "BERTScore2019_chunk_29",
      "source_id": "BERTScore2019",
      "text": "additional experiments with four types of pre-trained embeddings: BERT, XLM (Lample & Conneau, 2019), XLNet (Yang et al., 2019b), and RoBERTa (Liu et al., 2019b). XLM (Cross-lingual Language Model) is a Trans- former pre-trained on the translation language modeling of predicting masked tokens from a pair of sentence in two different languages and masked language modeling tasks using multi-lingual train- ing data. Yang et al. (2019b) modify the Transformer architecture and pre-train it on a permutation language modeling task resulting in some improvement on top of the original BERT when ﬁne-tuned on several downstream tasks. Liu et al. (2019b) introduce RoBERTa (Robustly optimized BERT ap- proach) and demonstrate that an optimized BERT model is comparable to or sometimes outperforms an XLNet on downstream tasks. We perform a comprehensive study with the following pre-trained contextual embedding models:15 • BERT models: bert-base-uncased, bert-large-uncased, bert-based-chinese, bert-base-multilingual-cased, and bert-base-cased-mrpc • RoBERTa models: roberta-base, roberta-large, and roberta-large-mnli • XLNet models: xlnet-base-cased and xlnet-base-large • XLM models: xlm-mlm-en-2048 and xlm-mlm-100-1280 F.1 WMT C ORRELATION STUDY Experimental setup Because of missing data in the released WMT16 dataset (Bojar et al., 2016), we are only able to experiment with to-English segment-level data, which contains the outputs of 50 different systems on 6 language pairs. We use this data as the validation set for hyperparam- eter tuning (Appendix B). Table 12 shows the Pearson correlations of all participating metrics and BERTS CORE s computed with different pre-trained models. Signiﬁcance testing for this dataset does not include the baseline metrics because the released dataset does not contain the original outputs from the baseline metrics. We conduct signiﬁcance testing between BERTS CORE results only. The WMT17 dataset (Bojar et al., 2017) contains outputs of 152 different translations on 14 lan- guage pairs. We experiment on the segment-level and system-level data on both to-English and from-English language pairs. We exclude ﬁ-en data from the segment-level experiment due to an error in the released data. We compare our results to all participating metrics and perform standard signiﬁcance testing as done by Bojar et al. (2017). Tables 13-16 show the results. The WMT18 dataset (Ma et al., 2018) contains outputs of 159 translation systems on 14 lan- guage pairs. In addition to the results in Tables 1-4, we complement the study with the correla- tions of all participating metrics in WMT18 and results from using different contextual models for BERTS CORE . Results Table",
      "chunk_index": 29
    },
    {
      "index": 288,
      "chunk_id": "BERTScore2019_chunk_30",
      "source_id": "BERTScore2019",
      "text": "on 14 lan- guage pairs. In addition to the results in Tables 1-4, we complement the study with the correla- tions of all participating metrics in WMT18 and results from using different contextual models for BERTS CORE . Results Table 12-22 collectively showcase the effectiveness of BERTS CORE in correlating with human judgments. The improvement of BERTS CORE is more pronounced on the segment-level than on the system-level. We also see that more optimized or larger BERT models can produce better contextual representations (e.g., comparing FRoBERTa-Large and FBERT-Large). In contrast, the smaller XLNet performs better than a large one. Based on the evidence in Figure 8 and Tables 12-22, we 15Denoted by names speciﬁed at https://huggingface.co/pytorch-transformers/pretrained_models.html. Published as a conference paper at ICLR 2020 hypothesize that the permutation language task, though leading to a good set of model weights for ﬁne-tuning on downstream tasks, does not necessarily produce informative pre-trained embeddings for generation evaluation. We also observe that ﬁne-tuning pre-trained models on a related task, such as natural language inference (Williams et al., 2018), can lead to better human correlation in evaluating text generation. Therefore, for evaluating English sentences, we recommend computing BERTS CORE with a 24-layer RoBERTa model ﬁne-tuned on the MNLI dataset. For evaluating Non-English sentences, both the multilingual BERT model and the XLM model trained on 100 languages are suitable candidates. We also recommend using domain- or language-speciﬁc contex- tual embeddings when possible, such as using BERT Chinese models for evaluating Chinese tasks. In general, we advise users to consider the target domain and languages when selecting the exact conﬁguration to use. F.2 M ODEL SELECTION STUDY Experimental setup Similar to Section 4, we use the 10K hybrid systems super-sampled from WMT18. We randomly select 100 out of 10K hybrid systems, rank them using automatic metrics, and repeat this process 100K times. We add to the results in the main paper (Table 3) performance of all participating metrics in WMT18 and results from using different contextual embedding models for BERTS CORE . We reuse the hybrid conﬁguration and metric outputs released in WMT18. In addition to the Hits@1 measure, we evaluate the metrics using (a) mean reciprocal rank (MRR) of the top metric-rated system in human rankings, and (b) the absolute human score difference (Diff) between the top metric- and human-rated systems. Hits@1 captures a metric's ability to select the best system. The other two measures quantify",
      "chunk_index": 30
    },
    {
      "index": 289,
      "chunk_id": "BERTScore2019_chunk_31",
      "source_id": "BERTScore2019",
      "text": "rank (MRR) of the top metric-rated system in human rankings, and (b) the absolute human score difference (Diff) between the top metric- and human-rated systems. Hits@1 captures a metric's ability to select the best system. The other two measures quantify the amount of error a metric makes in the selection process. Tables 23-28 show the results from these experiments. Results The additional results further support our conclusion from Table 3: BERTSCORE demon- strates better model selection performance. We also observe that the supervised metric RUSE dis- plays strong model selection ability. F.3 I MAGE CAPTIONING ON COCO We follow the experimental setup described in Section 4. Table 29 shows the correlations of several pre-trained contextual embeddings. We observe that precision-based methods such as B LEU and PBERT are weakly correlated with human judgments on image captioning tasks. We hypothesize that this is because human judges prefer captions that capture the main objects in a picture for image captioning. In general, RBERT has a high correlation, even surpassing the task-speciﬁc metric SPICE Anderson et al. (2016). While the ﬁne-tuned RoBERTa-Large model does not result in the highest correlation, it is one of the best metrics. F.4 R OBUSTNESS ANALYSIS ON PAWS-QQP We present the full results of the robustness study described in Section 6 in Table 30. In general, we observe that BERTS CORE is more robust than other commonly used metrics. BERTS CORE computed with the 24-layer RoBERTa model performs the best. Fine-tuning RoBERTa-Large on MNLI (Williams et al., 2018) can signiﬁcantly improve the robustness against adversarial sentences. However, a ﬁne-tuned BERT on MRPC (Microsoft Research Paraphrasing Corpus) (Dolan & Brock- ett, 2005) performs worse than its counterpart. Published as a conference paper at ICLR 2020 Setting Metric cs-en de-en ﬁ-en ro-en ru-en tr-en 560 560 560 560 560 560 Unsupervised DPMF COMB 0.713 0.584 0.598 0.627 0.615 0.663 METRICS -F 0.696 0.601 0.557 0.662 0.618 0.649 COBALT -F. 0.671 0.591 0.554 0.639 0.618 0.627 UPF -COBA . 0.652 0.550 0.490 0.616 0.556 0.626 MPEDA 0.644 0.538 0.513 0.587 0.545 0.616 CHR F2 0.658 0.457 0.469 0.581 0.534 0.556 CHR F3 0.660 0.455 0.472 0.582 0.535 0.555 CHR F1 0.644 0.454 0.452 0.570 0.522 0.551 UOW-REVAL 0.577 0.528 0.471 0.547 0.528 0.531 WORD F3 0.599 0.447 0.473 0.525 0.504 0.536 WORD F2 0.596 0.445 0.471 0.522 0.503 0.537 WORD F1 0.585 0.435 0.464 0.508 0.497 0.535 SENT BLEU 0.557 0.448",
      "chunk_index": 31
    },
    {
      "index": 290,
      "chunk_id": "BERTScore2019_chunk_32",
      "source_id": "BERTScore2019",
      "text": "0.454 0.452 0.570 0.522 0.551 UOW-REVAL 0.577 0.528 0.471 0.547 0.528 0.531 WORD F3 0.599 0.447 0.473 0.525 0.504 0.536 WORD F2 0.596 0.445 0.471 0.522 0.503 0.537 WORD F1 0.585 0.435 0.464 0.508 0.497 0.535 SENT BLEU 0.557 0.448 0.484 0.499 0.502 0.532 DTED 0.394 0.254 0.361 0.329 0.375 0.267 Supervised BEER 0.661 0.462 0.471 0.551 0.533 0.545 Pre-Trained PBERT-Base 0.729 0.617 0.719 0.651 0.684 0.678 RBERT-Base 0.741 0.639 0.616 0.693 0.660 0.660 FBERT-Base 0.747 0.640 0.661 0.723 0.672 0.688 PBERT-Base (no idf) 0.723 0.638 0.662 0.700 0.633 0.696 RBERT-Base (no idf) 0.745 0.656 0.638 0.697 0.653 0.674 FBERT-Base (no idf) 0.747 0.663 0.666 0.714 0.662 0.703 PBERT-Base-MRPC 0.697 0.618 0.614 0.676 0.62 0.695 RBERT-Base-MRPC 0.723 0.636 0.587 0.667 0.648 0.664 FBERT-Base-MRPC 0.725 0.644 0.617 0.691 0.654 0.702 PBERT-Base-MRPC (idf) 0.713 0.613 0.630 0.693 0.635 0.691 RBERT-Base-MRPC (idf) 0.727 0.631 0.573 0.666 0.642 0.662 FBERT-Base-MRPC (idf) 0.735 0.637 0.620 0.700 0.658 0.697 PBERT-Large 0.756 0.671 0.701 0.723 0.678 0.706 RBERT-Large 0.768 0.684 0.677 0.720 0.686 0.699 FBERT-Large 0.774 0.693 0.705 0.736 0.701 0.717 PBERT-Large (idf) 0.758 0.653 0.704 0.734 0.685 0.705 RBERT-Large (idf) 0.771 0.680 0.661 0.718 0.687 0.692 FBERT-Large (idf) 0.774 0.678 0.700 0.740 0.701 0.711 PRoBERTa-Base 0.738 0.642 0.671 0.712 0.669 0.671 RRoBERTa-Base 0.745 0.669 0.645 0.698 0.682 0.653 FRoBERTa-Base 0.761 0.674 0.686 0.732 0.697 0.689 PRoBERTa-Base (idf) 0.751 0.626 0.678 0.723 0.685 0.668 RRoBERTa-Base (idf) 0.744 0.652 0.638 0.699 0.685 0.657 FRoBERTa-Base (idf) 0.767 0.653 0.688 0.737 0.705 0.685 PRoBERTa-Large 0.757 0.702 0.709 0.735 0.721 0.676 RRoBERTa-Large 0.765 0.713 0.686 0.718 0.714 0.676 FRoBERTa-Large 0.780 0.724 0.728 0.753 0.738 0.709 PRoBERTa-Large (idf) 0.771 0.682 0.705 0.727 0.714 0.681 RRoBERTa-Large (idf) 0.762 0.695 0.683 0.711 0.708 0.678 FRoBERTa-Large (idf) 0.786 0.704 0.727 0.747 0.732 0.711 PRoBERTa-Large-MNLI 0.777 0.718 0.733 0.744 0.729 0.747 RRoBERTa-Large-MNLI 0.790 0.731 0.702 0.741 0.727 0.732 FRoBERTa-Large-MNLI 0.795 0.736 0.733 0.757 0.744 0.756 PRoBERTa-Large-MNLI (idf) 0.794 0.695 0.731 0.752 0.732 0.747 RRoBERTa-Large-MNLI (idf) 0.792 0.706 0.694 0.737 0.724 0.733 FRoBERTa-Large-MNLI (idf) 0.804 0.710 0.729 0.760 0.742 0.754 PXLNet-Base 0.708 0.612 0.639 0.650 0.606 0.690 RXLNet-Base 0.728 0.630 0.617 0.645 0.621 0.675 FXLNet-Base 0.727 0.631 0.640 0.659 0.626 0.695 PXLNet-Base (idf) 0.726 0.618 0.655 0.678 0.629 0.700 RXLNet-Base (idf) 0.734 0.633 0.618 0.66 0.635 0.682 FXLNet-Base (idf) 0.739 0.633 0.649 0.681 0.643 0.702 PXL-NET-LARGE 0.710 0.577 0.643 0.647 0.616 0.684 RXL-NET-LARGE 0.732 0.600 0.610 0.636 0.627 0.668 FXL-NET-LARGE 0.733 0.600 0.643 0.655 0.637 0.691 PXL-NET-LARGE",
      "chunk_index": 32
    },
    {
      "index": 291,
      "chunk_id": "BERTScore2019_chunk_33",
      "source_id": "BERTScore2019",
      "text": "0.629 0.700 RXLNet-Base (idf) 0.734 0.633 0.618 0.66 0.635 0.682 FXLNet-Base (idf) 0.739 0.633 0.649 0.681 0.643 0.702 PXL-NET-LARGE 0.710 0.577 0.643 0.647 0.616 0.684 RXL-NET-LARGE 0.732 0.600 0.610 0.636 0.627 0.668 FXL-NET-LARGE 0.733 0.600 0.643 0.655 0.637 0.691 PXL-NET-LARGE (idf) 0.728 0.574 0.652 0.669 0.633 0.681 RXL-NET-LARGE (idf) 0.735 0.592 0.597 0.642 0.629 0.662 FXL-NET-LARGE (idf) 0.742 0.592 0.643 0.670 0.645 0.685 PXLM-En 0.688 0.569 0.613 0.645 0.583 0.659 RXLM-En 0.715 0.603 0.577 0.645 0.609 0.644 FXLM-En 0.713 0.597 0.610 0.657 0.610 0.668 PXLM-En (idf) 0.728 0.576 0.649 0.681 0.604 0.683 RXLM-En (idf) 0.730 0.597 0.591 0.659 0.622 0.669 FXLM-En (idf) 0.739 0.594 0.636 0.682 0.626 0.691 Table 12: Pearson correlations with segment-level human judgments on WMT16 to-English trans- lations. Correlations of metrics not signiﬁcantly outperformed by any other for that language pair are highlighted in bold. For each language pair, we specify the number of examples. Published as a conference paper at ICLR 2020 Setting Metric cs-en de-en ﬁ-en lv-en ru-en tr-en zh-en 560 560 560 560 560 560 560 Unsupervised CHR F 0.514 0.531 0.671 0.525 0.599 0.607 0.591 CHR F++ 0.523 0.534 0.678 0.520 0.588 0.614 0.593 MEANT 2.0 0.578 0.565 0.687 0.586 0.607 0.596 0.639 MEANT 2.0- NOSRL 0.566 0.564 0.682 0.573 0.591 0.582 0.630 SENT BLEU 0.435 0.432 0.571 0.393 0.484 0.538 0.512 TREE AGGREG 0.486 0.526 0.638 0.446 0.555 0.571 0.535 UHH_TSKM 0.507 0.479 0.600 0.394 0.465 0.478 0.477 Supervised AUTO DA 0.499 0.543 0.673 0.533 0.584 0.625 0.583 BEER 0.511 0.530 0.681 0.515 0.577 0.600 0.582 BLEND 0.594 0.571 0.733 0.577 0.622 0.671 0.661 BLEU2VEC 0.439 0.429 0.590 0.386 0.489 0.529 0.526 NGRAM2VEC 0.436 0.435 0.582 0.383 0.490 0.538 0.520 Pre-Trained PBERT-Base 0.625 0.659 0.808 0.688 0.698 0.713 0.675 RBERT-Base 0.653 0.645 0.782 0.662 0.678 0.716 0.715 FBERT-Base 0.654 0.671 0.811 0.692 0.707 0.731 0.714 PBERT-Base (idf) 0.626 0.668 0.819 0.708 0.719 0.702 0.667 RBERT-Base (idf) 0.652 0.658 0.789 0.678 0.696 0.703 0.712 FBERT-Base (idf) 0.657 0.680 0.823 0.712 0.725 0.718 0.711 PBERT-Base-MRPC 0.599 0.630 0.788 0.657 0.659 0.710 0.681 RBERT-Base-MRPC 0.613 0.620 0.754 0.616 0.650 0.685 0.705 FBERT-Base-MRPC 0.627 0.647 0.792 0.656 0.676 0.717 0.712 PBERT-Base-MRPC (idf) 0.609 0.630 0.801 0.680 0.676 0.712 0.682 RBERT-Base-MRPC (idf) 0.611 0.628 0.759 0.633 0.665 0.687 0.703 FBERT-Base-MRPC (idf) 0.633 0.649 0.803 0.678 0.690 0.719 0.713 PBERT-Large 0.638 0.685 0.816 0.717 0.719 0.746 0.693 RBERT-Large 0.661 0.676 0.782 0.693 0.705 0.744 0.730 FBERT-Large 0.666 0.701",
      "chunk_index": 33
    },
    {
      "index": 292,
      "chunk_id": "BERTScore2019_chunk_34",
      "source_id": "BERTScore2019",
      "text": "0.676 0.712 0.682 RBERT-Base-MRPC (idf) 0.611 0.628 0.759 0.633 0.665 0.687 0.703 FBERT-Base-MRPC (idf) 0.633 0.649 0.803 0.678 0.690 0.719 0.713 PBERT-Large 0.638 0.685 0.816 0.717 0.719 0.746 0.693 RBERT-Large 0.661 0.676 0.782 0.693 0.705 0.744 0.730 FBERT-Large 0.666 0.701 0.814 0.723 0.730 0.760 0.731 PBERT-Large (idf) 0.644 0.692 0.827 0.728 0.729 0.734 0.689 RBERT-Large (idf) 0.665 0.686 0.796 0.712 0.729 0.733 0.730 FBERT-Large (idf) 0.671 0.707 0.829 0.738 0.745 0.746 0.729 PRoBERTa-Base 0.639 0.663 0.801 0.689 0.688 0.700 0.704 RRoBERTa-Base 0.648 0.652 0.768 0.651 0.669 0.684 0.734 FRoBERTa-Base 0.675 0.683 0.818 0.693 0.707 0.718 0.740 PRoBERTa-Base (idf) 0.629 0.655 0.804 0.702 0.711 0.707 0.700 RRoBERTa-Base (idf) 0.652 0.646 0.773 0.667 0.676 0.689 0.734 FRoBERTa-Base (idf) 0.673 0.673 0.823 0.708 0.719 0.721 0.739 PRoBERTa-Large 0.658 0.724 0.811 0.743 0.727 0.720 0.744 RRoBERTa-Large 0.685 0.714 0.778 0.711 0.718 0.713 0.759 FRoBERTa-Large 0.710 0.745 0.833 0.756 0.746 0.751 0.775 PRoBERTa-Large (idf) 0.644 0.721 0.815 0.740 0.734 0.736 0.734 RRoBERTa-Large (idf) 0.683 0.705 0.783 0.718 0.720 0.726 0.751 FRoBERTa-Large (idf) 0.703 0.737 0.838 0.761 0.752 0.764 0.767 PRoBERTa-Large-MNLI 0.694 0.736 0.822 0.764 0.741 0.754 0.737 RRoBERTa-Large-MNLI 0.706 0.725 0.785 0.732 0.741 0.750 0.760 FRoBERTa-Large-MNLI 0.722 0.747 0.822 0.764 0.758 0.767 0.765 PRoBERTa-Large-MNLI (idf) 0.686 0.733 0.836 0.772 0.760 0.767 0.738 RRoBERTa-Large-MNLI (idf) 0.697 0.717 0.796 0.741 0.753 0.757 0.762 FRoBERTa-Large-MNLI (idf) 0.714 0.740 0.835 0.774 0.773 0.776 0.767 PXLNET-Base 0.595 0.579 0.779 0.632 0.626 0.688 0.646 RXLNET-Base 0.603 0.560 0.746 0.617 0.624 0.689 0.677 FXLNET-Base 0.610 0.580 0.775 0.636 0.639 0.700 0.675 PXLNET-Base (idf) 0.616 0.603 0.795 0.665 0.659 0.693 0.649 RXLNET-Base (idf) 0.614 0.583 0.765 0.640 0.648 0.697 0.688 FXLNET-Base (idf) 0.627 0.603 0.795 0.663 0.665 0.707 0.684 PXLNET-Large 0.620 0.622 0.796 0.648 0.648 0.694 0.660 RXLNET-Large 0.622 0.601 0.758 0.628 0.645 0.684 0.701 FXLNET-Large 0.635 0.627 0.794 0.654 0.664 0.705 0.698 PXLNET-Large (idf) 0.635 0.633 0.808 0.673 0.672 0.688 0.649 RXLNET-Large (idf) 0.626 0.611 0.770 0.646 0.661 0.682 0.700 FXLNET-Large (idf) 0.646 0.636 0.809 0.675 0.682 0.700 0.695 PXLM-En 0.565 0.594 0.769 0.631 0.649 0.672 0.643 RXLM-En 0.592 0.586 0.734 0.618 0.647 0.673 0.686 FXLM-En 0.595 0.605 0.768 0.641 0.664 0.686 0.683 PXLM-En (idf) 0.599 0.618 0.795 0.670 0.686 0.690 0.657 RXLM-En (idf) 0.624 0.605 0.768 0.652 0.680 0.684 0.698 FXLM-En (idf) 0.630 0.624 0.798 0.676 0.698 0.698 0.694 Table 13: Absolute Pearson correlations with segment-level human judgments on WMT17 to- English translations. Correlations of metrics not signiﬁcantly outperformed by any",
      "chunk_index": 34
    },
    {
      "index": 293,
      "chunk_id": "BERTScore2019_chunk_35",
      "source_id": "BERTScore2019",
      "text": "RXLM-En (idf) 0.624 0.605 0.768 0.652 0.680 0.684 0.698 FXLM-En (idf) 0.630 0.624 0.798 0.676 0.698 0.698 0.694 Table 13: Absolute Pearson correlations with segment-level human judgments on WMT17 to- English translations. Correlations of metrics not signiﬁcantly outperformed by any other for that language pair are highlighted in bold. For each language pair, we specify the number of examples. Published as a conference paper at ICLR 2020 Setting Metric en-cs en-de en-ﬁ en-lv en-ru en-tr en-zh 32K 3K 3K 3K 560 247 560 τ τ τ τ |r| τ |r| Unsupervised AUTO DA 0.041 0.099 0.204 0.130 0.511 0.409 0.609 AUTO DA-TECTO 0.336 - - - - - - CHR F 0.376 0.336 0.503 0.420 0.605 0.466 0.608 CHR F+ 0.377 0.325 0.514 0.421 0.609 0.474 - CHR F++ 0.368 0.328 0.484 0.417 0.604 0.466 0.602 MEANT 2.0 - 0.350 - - - - 0.727 MEANT 2.0- NOSRL 0.395 0.324 0.565 0.425 0.636 0.482 0.705 SENT BLEU 0.274 0.269 0.446 0.259 0.468 0.377 0.642 TREE AGGREG 0.361 0.305 0.509 0.383 0.535 0.441 0.566 Supervised BEER 0.398 0.336 0.557 0.420 0.569 0.490 0.622 BLEND - - - - 0.578 - - BLEU2VEC 0.305 0.313 0.503 0.315 0.472 0.425 - NGRAM2VEC - - 0.486 0.317 - - - Pre-Trained PBERT-Multi 0.412 0.364 0.561 0.435 0.606 0.579 0.759 RBERT-Multi 0.443 0.430 0.587 0.480 0.663 0.571 0.804 FBERT-Multi 0.440 0.404 0.587 0.466 0.653 0.587 0.806 PBERT-Multi (idf) 0.411 0.328 0.568 0.444 0.616 0.555 0.741 RBERT-Multi (idf) 0.449 0.416 0.591 0.479 0.665 0.579 0.796 FBERT-Multi (idf) 0.447 0.379 0.588 0.470 0.657 0.571 0.793 PXLM-100 0.406 0.383 0.553 0.423 0.562 0.611 0.722 RXLM-100 0.446 0.436 0.587 0.458 0.626 0.652 0.779 FXLM-100 0.444 0.424 0.577 0.456 0.613 0.628 0.778 PXLM-100 (idf) 0.419 0.367 0.557 0.427 0.571 0.595 0.719 RXLM-100 (idf) 0.450 0.424 0.592 0.464 0.632 0.644 0.770 FXLM-100 (idf) 0.448 0.419 0.580 0.459 0.617 0.644 0.771 Table 14: Absolute Pearson correlation (|r|) and Kendall correlation (τ) with segment-level human judgments on WMT17 from-English translations. Correlations of metrics not signiﬁcantly outper- formed by any other for that language pair are highlighted in bold. For each language pair, we specify the number of examples. Published as a conference paper at ICLR 2020 Setting Metric cs-en de-en ﬁ-en lv-en ru-en tr-en zh-en 4 11 6 9 9 10 16 Unsupervised BLEU 0.971 0.923 0.903 0.979 0.912 0.976 0.864 CDER 0.989 0.930 0.927 0.985 0.922 0.973 0.904 CHARAC TER 0.972 0.974",
      "chunk_index": 35
    },
    {
      "index": 294,
      "chunk_id": "BERTScore2019_chunk_36",
      "source_id": "BERTScore2019",
      "text": "at ICLR 2020 Setting Metric cs-en de-en ﬁ-en lv-en ru-en tr-en zh-en 4 11 6 9 9 10 16 Unsupervised BLEU 0.971 0.923 0.903 0.979 0.912 0.976 0.864 CDER 0.989 0.930 0.927 0.985 0.922 0.973 0.904 CHARAC TER 0.972 0.974 0.946 0.932 0.958 0.949 0.799 CHR F 0.939 0.968 0.938 0.968 0.952 0.944 0.859 CHR F++ 0.940 0.965 0.927 0.973 0.945 0.960 0.880 MEANT 2.0 0.926 0.950 0.941 0.970 0.962 0.932 0.838 MEANT 2.0- NOSRL 0.902 0.936 0.933 0.963 0.960 0.896 0.800 NIST 1.000 0.931 0.931 0.960 0.912 0.971 0.849 PER 0.968 0.951 0.896 0.962 0.911 0.932 0.877 TER 0.989 0.906 0.952 0.971 0.912 0.954 0.847 TREE AGGREG 0.983 0.920 0.977 0.986 0.918 0.987 0.861 UHH_TSKM 0.996 0.937 0.921 0.990 0.914 0.987 0.902 WER 0.987 0.896 0.948 0.969 0.907 0.925 0.839 Supervised AUTO DA 0.438 0.959 0.925 0.973 0.907 0.916 0.734 BEER 0.972 0.960 0.955 0.978 0.936 0.972 0.902 BLEND 0.968 0.976 0.958 0.979 0.964 0.984 0.894 BLEU2VEC 0.989 0.936 0.888 0.966 0.907 0.961 0.886 NGRAM2VEC 0.984 0.935 0.890 0.963 0.907 0.955 0.880 Pre-Trained PBERT-Base 0.975 0.936 0.991 0.993 0.918 0.981 0.892 RBERT-Base 0.995 0.975 0.944 0.978 0.953 0.991 0.975 FBERT-Base 0.987 0.961 0.979 0.991 0.937 0.991 0.953 PBERT-Base (idf) 0.983 0.937 0.998 0.992 0.939 0.985 0.878 RBERT-Base (idf) 0.997 0.981 0.962 0.968 0.977 0.985 0.949 FBERT-Base (idf) 0.992 0.967 0.995 0.992 0.960 0.996 0.951 PBERT-Base-MRPC 0.982 0.926 0.990 0.987 0.916 0.970 0.899 RBERT-Base-MRPC 0.999 0.979 0.950 0.982 0.957 0.977 0.985 FBERT-Base-MRPC 0.994 0.957 0.986 0.994 0.938 0.980 0.960 PBERT-Base-MRPC (idf) 0.989 0.936 0.992 0.979 0.931 0.976 0.892 RBERT-Base-MRPC (idf) 0.999 0.987 0.962 0.980 0.975 0.979 0.973 FBERT-Base-MRPC (idf) 0.997 0.968 0.995 0.997 0.956 0.989 0.963 PBERT-Large 0.981 0.937 0.991 0.996 0.921 0.987 0.905 RBERT-Large 0.996 0.975 0.953 0.985 0.954 0.992 0.977 FBERT-Large 0.990 0.960 0.981 0.995 0.938 0.992 0.957 PBERT-Large (idf) 0.986 0.938 0.998 0.995 0.939 0.994 0.897 RBERT-Large (idf) 0.997 0.982 0.967 0.979 0.974 0.992 0.966 FBERT-Large (idf) 0.994 0.965 0.993 0.995 0.958 0.998 0.959 PRoBERTa-Base 0.987 0.930 0.984 0.966 0.916 0.963 0.955 RRoBERTa-Base 0.999 0.982 0.947 0.979 0.956 0.986 0.984 FRoBERTa-Base 0.996 0.961 0.993 0.993 0.937 0.983 0.982 PRoBERTa-Base (idf) 0.990 0.938 0.980 0.956 0.929 0.967 0.962 RRoBERTa-Base (idf) 0.998 0.987 0.963 0.979 0.971 0.986 0.974 FRoBERTa-Base (idf) 0.996 0.970 0.999 0.994 0.952 0.989 0.982 PRoBERTa-Large 0.989 0.948 0.984 0.949 0.927 0.960 0.967 RRoBERTa-Large 0.998 0.988 0.957 0.983 0.969 0.982 0.984 FRoBERTa-Large 0.996 0.973 0.997 0.991 0.949",
      "chunk_index": 36
    },
    {
      "index": 295,
      "chunk_id": "BERTScore2019_chunk_37",
      "source_id": "BERTScore2019",
      "text": "RRoBERTa-Base (idf) 0.998 0.987 0.963 0.979 0.971 0.986 0.974 FRoBERTa-Base (idf) 0.996 0.970 0.999 0.994 0.952 0.989 0.982 PRoBERTa-Large 0.989 0.948 0.984 0.949 0.927 0.960 0.967 RRoBERTa-Large 0.998 0.988 0.957 0.983 0.969 0.982 0.984 FRoBERTa-Large 0.996 0.973 0.997 0.991 0.949 0.984 0.987 PRoBERTa-Large (idf) 0.989 0.959 0.975 0.935 0.944 0.968 0.974 RRoBERTa-Large (idf) 0.995 0.991 0.962 0.979 0.981 0.981 0.970 FRoBERTa-Large (idf) 0.996 0.982 0.998 0.991 0.965 0.991 0.984 PRoBERTa-Large-MNLI 0.994 0.963 0.995 0.990 0.944 0.981 0.974 RRoBERTa-Large-MNLI 0.995 0.991 0.962 0.981 0.973 0.985 0.984 FRoBERTa-Large-MNLI 0.999 0.982 0.992 0.996 0.961 0.988 0.989 PRoBERTa-Large-MNLI (idf) 0.995 0.970 0.997 0.985 0.955 0.988 0.979 RRoBERTa-Large-MNLI (idf) 0.994 0.992 0.967 0.977 0.983 0.988 0.972 FRoBERTa-Large-MNLI (idf) 0.999 0.989 0.996 0.997 0.972 0.994 0.987 PXLNET-Base 0.988 0.938 0.993 0.993 0.914 0.974 0.960 RXLNET-Base 0.999 0.978 0.956 0.977 0.946 0.981 0.980 FXLNET-Base 0.996 0.963 0.986 0.991 0.932 0.981 0.978 PXLNET-Base (idf) 0.992 0.951 0.998 0.996 0.930 0.982 0.939 RXLNET-Base (idf) 0.999 0.986 0.968 0.973 0.964 0.987 0.955 FXLNET-Base (idf) 0.998 0.974 0.996 0.994 0.950 0.990 0.970 PXLNET-Large 0.991 0.944 0.996 0.995 0.924 0.982 0.943 RXLNET-Large 0.996 0.981 0.945 0.971 0.961 0.986 0.958 FXLNET-Large 0.999 0.969 0.986 0.992 0.945 0.992 0.961 PXLNET-Large (idf) 0.995 0.955 0.999 0.996 0.941 0.985 0.937 RXLNET-Large (idf) 0.993 0.985 0.951 0.960 0.975 0.974 0.910 FXLNET-Large (idf) 1.000 0.978 0.994 0.993 0.962 0.994 0.954 PXLM-En 0.983 0.933 0.994 0.989 0.918 0.973 0.928 RXLM-En 0.998 0.978 0.949 0.983 0.957 0.985 0.972 FXLM-En 0.994 0.960 0.985 0.995 0.938 0.984 0.964 PXLM-En (idf) 0.986 0.940 0.997 0.992 0.939 0.979 0.916 RXLM-En (idf) 0.999 0.983 0.966 0.980 0.975 0.991 0.952 FXLM-En (idf) 0.995 0.967 0.996 0.998 0.959 0.993 0.958 Table 15: Absolute Pearson correlations with system-level human judgments on WMT17 to-English translations. Correlations of metrics not signiﬁcantly outperformed by any other for that language pair are highlighted in bold. For each language pair, we specify the number of systems. Published as a conference paper at ICLR 2020 Setting Metric en-cs en-de en-lv en-ru en-tr en-zh 14 16 17 9 8 11 Unsupervised BLEU 0.956 0.804 0.866 0.898 0.924 - CDER 0.968 0.813 0.930 0.924 0.957 - CHARAC TER 0.981 0.938 0.897 0.939 0.975 0.933 CHR F 0.976 0.863 0.955 0.950 0.991 0.976 CHR F++ 0.974 0.852 0.956 0.945 0.986 0.976 MEANT 2.0 - 0.858 - - - 0.956 MEANT 2.0- NOSRL 0.976 0.770 0.959 0.957 0.991 0.943 NIST 0.962 0.769 0.935 0.920 0.986 - PER 0.954",
      "chunk_index": 37
    },
    {
      "index": 296,
      "chunk_id": "BERTScore2019_chunk_38",
      "source_id": "BERTScore2019",
      "text": "0.976 0.863 0.955 0.950 0.991 0.976 CHR F++ 0.974 0.852 0.956 0.945 0.986 0.976 MEANT 2.0 - 0.858 - - - 0.956 MEANT 2.0- NOSRL 0.976 0.770 0.959 0.957 0.991 0.943 NIST 0.962 0.769 0.935 0.920 0.986 - PER 0.954 0.687 0.851 0.887 0.963 - TER 0.955 0.796 0.909 0.933 0.967 - TREE AGGREG 0.947 0.773 0.927 0.921 0.983 0.938 UHH_TSKM - - - - - - WER 0.954 0.802 0.906 0.934 0.956 - Supervised AUTO DA 0.975 0.603 0.729 0.850 0.601 0.976 BEER 0.970 0.842 0.930 0.944 0.980 0.914 BLEND - - - 0.953 - - BLEU2VEC 0.963 0.810 0.859 0.903 0.911 - NGRAM2VEC - - 0.862 - - - Pre-Trained PBERT-Multi 0.959 0.798 0.960 0.946 0.981 0.970 RBERT-Multi 0.982 0.909 0.957 0.980 0.979 0.994 FBERT-Multi 0.976 0.859 0.959 0.966 0.980 0.992 PBERT-Multi (idf) 0.963 0.760 0.960 0.947 0.984 0.971 RBERT-Multi (idf) 0.985 0.907 0.955 0.981 0.984 0.982 FBERT-Multi (idf) 0.979 0.841 0.958 0.968 0.984 0.991 PXLM-100 0.967 0.825 0.965 0.953 0.974 0.977 RXLM-100 0.980 0.902 0.965 0.982 0.977 0.979 FXLM-100 0.979 0.868 0.969 0.971 0.976 0.986 PXLM-100 (idf) 0.968 0.809 0.965 0.955 0.980 0.975 RXLM-100 (idf) 0.981 0.894 0.964 0.984 0.983 0.968 FXLM-100 (idf) 0.979 0.856 0.966 0.973 0.982 0.979 Table 16: Absolute Pearson correlations with system-level human judgments on WMT17 from- English translations. Correlations of metrics not signiﬁcantly outperformed by any other for that language pair are highlighted in bold. For each language pair, we specify the number of systems. Published as a conference paper at ICLR 2020 Setting Metric cs-en de-en et-en ﬁ-en ru-en tr-en zh-en 5K 78K 57K 16K 10K 9K 33K Unsupervised CHARAC TER 0.256 0.450 0.286 0.185 0.244 0.172 0.202 ITER 0.198 0.396 0.235 0.128 0.139 -0.029 0.144 METEOR ++ 0.270 0.457 0.329 0.207 0.253 0.204 0.179 SENT BLEU 0.233 0.415 0.285 0.154 0.228 0.145 0.178 UHH_TSKM 0.274 0.436 0.300 0.168 0.235 0.154 0.151 YISI-0 0.301 0.474 0.330 0.225 0.294 0.215 0.205 YISI-1 0.319 0.488 0.351 0.231 0.300 0.234 0.211 YISI-1 SRL 0.317 0.483 0.345 0.237 0.306 0.233 0.209 Supervised BEER 0.295 0.481 0.341 0.232 0.288 0.229 0.214 BLEND 0.322 0.492 0.354 0.226 0.290 0.232 0.217 RUSE 0.347 0.498 0.368 0.273 0.311 0.259 0.218 Pre-Trained PBERT-Base 0.349 0.522 0.373 0.264 0.325 0.264 0.232 RBERT-Base 0.370 0.528 0.378 0.291 0.333 0.257 0.244 FBERT-Base 0.373 0.531 0.385 0.287 0.341 0.266 0.243 PBERT-Base (idf) 0.352 0.524 0.382 0.27 0.326 0.277 0.235 RBERT-Base (idf) 0.368 0.536 0.388",
      "chunk_index": 38
    },
    {
      "index": 297,
      "chunk_id": "BERTScore2019_chunk_39",
      "source_id": "BERTScore2019",
      "text": "0.218 Pre-Trained PBERT-Base 0.349 0.522 0.373 0.264 0.325 0.264 0.232 RBERT-Base 0.370 0.528 0.378 0.291 0.333 0.257 0.244 FBERT-Base 0.373 0.531 0.385 0.287 0.341 0.266 0.243 PBERT-Base (idf) 0.352 0.524 0.382 0.27 0.326 0.277 0.235 RBERT-Base (idf) 0.368 0.536 0.388 0.300 0.340 0.284 0.244 FBERT-Base (idf) 0.375 0.535 0.393 0.294 0.339 0.289 0.243 PBERT-Base-MRPC 0.343 0.520 0.365 0.247 0.333 0.25 0.227 RBERT-Base-MRPC 0.370 0.524 0.373 0.277 0.34 0.261 0.244 FBERT-Base-MRPC 0.366 0.529 0.377 0.271 0.342 0.263 0.242 PBERT-Base-MRPC (idf) 0.348 0.522 0.371 0.25 0.318 0.256 0.224 RBERT-Base-MRPC (idf) 0.379 0.531 0.383 0.285 0.339 0.266 0.242 FBERT-Base-MRPC (idf) 0.373 0.534 0.383 0.274 0.342 0.275 0.242 PBERT-LARGE 0.361 0.529 0.380 0.276 0.340 0.266 0.241 RBERT-LARGE 0.386 0.532 0.386 0.297 0.347 0.268 0.247 FBERT-LARGE 0.402 0.537 0.390 0.296 0.344 0.274 0.252 PBERT-LARGE (idf) 0.377 0.532 0.390 0.287 0.342 0.292 0.246 RBERT-LARGE (idf) 0.386 0.544 0.396 0.308 0.356 0.287 0.251 FBERT-LARGE (idf) 0.388 0.545 0.399 0.309 0.358 0.300 0.257 PRoBERTa-Base 0.368 0.53 0.371 0.274 0.318 0.265 0.235 RRoBERTa-Base 0.383 0.536 0.376 0.283 0.336 0.253 0.245 FRoBERTa-Base 0.391 0.540 0.383 0.273 0.339 0.270 0.249 PRoBERTa-Base (idf) 0.379 0.528 0.372 0.261 0.314 0.265 0.232 RRoBERTa-Base (idf) 0.389 0.539 0.384 0.288 0.332 0.267 0.245 FRoBERTa-Base (idf) 0.400 0.540 0.385 0.274 0.337 0.277 0.247 PRoBERTa-LARGE 0.387 0.541 0.389 0.283 0.345 0.280 0.248 RRoBERTa-LARGE 0.388 0.546 0.391 0.304 0.343 0.290 0.255 FRoBERTa-LARGE 0.404 0.550 0.397 0.296 0.353 0.292 0.264 PRoBERTa-LARGE (idf) 0.391 0.540 0.387 0.280 0.334 0.284 0.252 RRoBERTa-LARGE (idf) 0.386 0.548 0.394 0.305 0.338 0.295 0.252 FRoBERTa-LARGE (idf) 0.408 0.550 0.395 0.293 0.346 0.296 0.260 PRoBERTa-Large-MNLI 0.397 0.549 0.396 0.299 0.351 0.295 0.253 RRoBERTa-Large-MNLI 0.404 0.553 0.393 0.313 0.351 0.279 0.253 FRoBERTa-Large-MNLI 0.418 0.557 0.402 0.312 0.362 0.290 0.258 PRoBERTa-Large-MNLI (idf) 0.414 0.552 0.399 0.301 0.349 0.306 0.249 RRoBERTa-Large-MNLI (idf) 0.412 0.555 0.400 0.316 0.357 0.289 0.258 FRoBERTa-Large-MNLI (idf) 0.417 0.559 0.403 0.309 0.357 0.307 0.258 PXLNet-Base 0.335 0.514 0.359 0.243 0.308 0.247 0.232 RXLNet-Base 0.351 0.515 0.362 0.261 0.311 0.227 0.232 FXLNet-Base 0.351 0.517 0.365 0.257 0.315 0.25 0.237 PXLNet-Base (idf) 0.339 0.516 0.366 0.258 0.307 0.261 0.236 RXLNet-Base (idf) 0.364 0.521 0.371 0.268 0.317 0.242 0.238 FXLNet-Base (idf) 0.355 0.524 0.374 0.265 0.320 0.261 0.241 PXL-NET-LARGE 0.344 0.522 0.371 0.252 0.316 0.264 0.233 RXL-NET-LARGE 0.358 0.524 0.374 0.275 0.332 0.249 0.239 FXL-NET-LARGE 0.357 0.530 0.380 0.265 0.334 0.263 0.238 PXL-NET-LARGE (idf) 0.348 0.520 0.373 0.260 0.319 0.265 0.235 RXL-NET-LARGE (idf) 0.366 0.529 0.378 0.278 0.331 0.266",
      "chunk_index": 39
    },
    {
      "index": 298,
      "chunk_id": "BERTScore2019_chunk_40",
      "source_id": "BERTScore2019",
      "text": "0.344 0.522 0.371 0.252 0.316 0.264 0.233 RXL-NET-LARGE 0.358 0.524 0.374 0.275 0.332 0.249 0.239 FXL-NET-LARGE 0.357 0.530 0.380 0.265 0.334 0.263 0.238 PXL-NET-LARGE (idf) 0.348 0.520 0.373 0.260 0.319 0.265 0.235 RXL-NET-LARGE (idf) 0.366 0.529 0.378 0.278 0.331 0.266 0.241 FXL-NET-LARGE (idf) 0.375 0.530 0.382 0.274 0.332 0.274 0.240 PXLM-En 0.349 0.516 0.366 0.244 0.310 0.259 0.233 RXLM-En 0.358 0.518 0.364 0.264 0.320 0.244 0.237 FXLM-En 0.358 0.525 0.373 0.259 0.322 0.258 0.238 PXLM-En (idf) 0.355 0.527 0.374 0.254 0.311 0.28 0.238 RXLM-En (idf) 0.362 0.528 0.376 0.274 0.333 0.26 0.24 FXLM-En (idf) 0.367 0.531 0.382 0.273 0.330 0.275 0.246 Table 17: Kendall correlations with segment-level human judgments on WMT18 to-English transla- tions. Correlations of metrics not signiﬁcantly outperformed by any other for that language pair are highlighted in bold. For each language pair, we specify the number of examples. Published as a conference paper at ICLR 2020 Setting Metric en-cs en-de en-et en-ﬁ en-ru en-tr en-zh 5K 20K 32K 10K 22K 1K 29K Unsupervised CHARAC TER 0.414 0.604 0.464 0.403 0.352 0.404 0.313 ITER 0.333 0.610 0.392 0.311 0.291 0.236 - SENT BLEU 0.389 0.620 0.414 0.355 0.330 0.261 0.311 YISI-0 0.471 0.661 0.531 0.464 0.394 0.376 0.318 YISI-1 0.496 0.691 0.546 0.504 0.407 0.418 0.323 YISI-1 SRL - 0.696 - - - - 0.310 Supervised BEER 0.518 0.686 0.558 0.511 0.403 0.374 0.302 BLEND - - - - 0.394 - - Pre-Trained PBERT-Multi 0.541 0.715 0.549 0.486 0.414 0.328 0.337 RBERT-Multi 0.570 0.728 0.594 0.565 0.420 0.411 0.367 FBERT-Multi 0.562 0.728 0.586 0.546 0.423 0.399 0.364 PBERT-Multi (idf) 0.525 0.7 0.54 0.495 0.423 0.352 0.338 RBERT-Multi (idf) 0.569 0.727 0.601 0.561 0.423 0.420 0.374 FBERT-Multi (idf) 0.553 0.721 0.585 0.537 0.425 0.406 0.366 PXLM-100 0.496 0.711 0.561 0.527 0.417 0.364 0.340 RXLM-100 0.564 0.724 0.612 0.584 0.418 0.432 0.363 FXLM-100 0.533 0.727 0.599 0.573 0.421 0.408 0.362 PXLM-100 (idf) 0.520 0.710 0.572 0.546 0.421 0.370 0.328 RXLM-100 (idf) 0.567 0.722 0.609 0.587 0.420 0.439 0.365 FXLM-100 (idf) 0.554 0.724 0.601 0.584 0.422 0.389 0.355 Table 18: Kendall correlations with segment-level human judgments on WMT18 from-English translations. Correlations of metrics not signiﬁcantly outperformed by any other for that language pair are highlighted in bold. For each language pair, we specify the number of examples. Published as a conference paper at ICLR 2020 Setting Metric cs-en de-en et-en ﬁ-en ru-en tr-en zh-en 5 16 14 9 8 5 14",
      "chunk_index": 40
    },
    {
      "index": 299,
      "chunk_id": "BERTScore2019_chunk_41",
      "source_id": "BERTScore2019",
      "text": "language pair are highlighted in bold. For each language pair, we specify the number of examples. Published as a conference paper at ICLR 2020 Setting Metric cs-en de-en et-en ﬁ-en ru-en tr-en zh-en 5 16 14 9 8 5 14 Unsupervised BLEU 0.970 0.971 0.986 0.973 0.979 0.657 0.978 CDER 0.972 0.980 0.990 0.984 0.980 0.664 0.982 CHARAC TER 0.970 0.993 0.979 0.989 0.991 0.782 0.950 ITER 0.975 0.990 0.975 0.996 0.937 0.861 0.980 METEOR ++ 0.945 0.991 0.978 0.971 0.995 0.864 0.962 NIST 0.954 0.984 0.983 0.975 0.973 0.970 0.968 PER 0.970 0.985 0.983 0.993 0.967 0.159 0.931 TER 0.950 0.970 0.990 0.968 0.970 0.533 0.975 UHH_TSKM 0.952 0.980 0.989 0.982 0.980 0.547 0.981 WER 0.951 0.961 0.991 0.961 0.968 0.041 0.975 YISI-0 0.956 0.994 0.975 0.978 0.988 0.954 0.957 YISI-1 0.950 0.992 0.979 0.973 0.991 0.958 0.951 YISI-1 SRL 0.965 0.995 0.981 0.977 0.992 0.869 0.962 Supervised BEER 0.958 0.994 0.985 0.991 0.982 0.870 0.976 BLEND 0.973 0.991 0.985 0.994 0.993 0.801 0.976 RUSE 0.981 0.997 0.990 0.991 0.988 0.853 0.981 Pre-Trained PBERT-Base 0.965 0.995 0.986 0.973 0.976 0.941 0.974 RBERT-Base 0.994 0.991 0.979 0.992 0.991 0.067 0.988 FBERT-Base 0.982 0.994 0.983 0.986 0.985 0.949 0.984 PBERT-Base (idf) 0.961 0.993 0.987 0.988 0.976 0.984 0.973 RBERT-Base (idf) 0.996 0.994 0.977 0.995 0.995 0.874 0.983 FBERT-Base (idf) 0.981 0.995 0.984 0.995 0.988 0.994 0.981 PBERT-Base-MRPC 0.957 0.994 0.989 0.953 0.976 0.798 0.977 RBERT-Base-MRPC 0.992 0.994 0.983 0.988 0.993 0.707 0.990 FBERT-Base-MRPC 0.975 0.995 0.987 0.975 0.986 0.526 0.986 PBERT-Base-MRPC (idf) 0.957 0.997 0.989 0.967 0.975 0.894 0.980 RBERT-Base-MRPC (idf) 0.991 0.997 0.981 0.994 0.993 0.052 0.987 FBERT-Base-MRPC (idf) 0.975 0.998 0.987 0.985 0.987 0.784 0.987 PBERT-Large 0.978 0.992 0.987 0.971 0.977 0.920 0.978 RBERT-Large 0.997 0.990 0.985 0.990 0.992 0.098 0.990 FBERT-Large 0.989 0.992 0.987 0.983 0.985 0.784 0.986 PBERT-Large (idf) 0.977 0.992 0.988 0.986 0.976 0.980 0.977 RBERT-Large (idf) 0.998 0.993 0.983 0.996 0.995 0.809 0.986 FBERT-Large (idf) 0.989 0.993 0.986 0.993 0.987 0.976 0.984 PRoBERTa-Base 0.970 0.995 0.991 0.998 0.976 0.796 0.980 RRoBERTa-Base 0.996 0.996 0.982 0.998 0.994 0.477 0.991 FRoBERTa-Base 0.984 0.997 0.989 0.999 0.987 0.280 0.989 PRoBERTa-Base (idf) 0.966 0.993 0.991 0.994 0.977 0.880 0.984 RRoBERTa-Base (idf) 0.995 0.998 0.981 0.998 0.995 0.230 0.989 FRoBERTa-Base (idf) 0.981 0.998 0.989 0.997 0.988 0.741 0.990 PRoBERTa-Large 0.980 0.998 0.990 0.995 0.982 0.791 0.981 RRoBERTa-Large 0.998 0.997 0.986 0.997 0.995 0.054 0.990 FRoBERTa-Large 0.990 0.999 0.990 0.998 0.990",
      "chunk_index": 41
    },
    {
      "index": 300,
      "chunk_id": "BERTScore2019_chunk_42",
      "source_id": "BERTScore2019",
      "text": "RRoBERTa-Base (idf) 0.995 0.998 0.981 0.998 0.995 0.230 0.989 FRoBERTa-Base (idf) 0.981 0.998 0.989 0.997 0.988 0.741 0.990 PRoBERTa-Large 0.980 0.998 0.990 0.995 0.982 0.791 0.981 RRoBERTa-Large 0.998 0.997 0.986 0.997 0.995 0.054 0.990 FRoBERTa-Large 0.990 0.999 0.990 0.998 0.990 0.499 0.988 PRoBERTa-Large (idf) 0.972 0.997 0.993 0.985 0.982 0.920 0.983 RRoBERTa-Large (idf) 0.996 0.997 0.984 0.997 0.995 0.578 0.989 FRoBERTa-Large (idf) 0.985 0.999 0.992 0.992 0.991 0.826 0.989 PRoBERTa-Large-MNLI 0.989 0.998 0.994 0.998 0.985 0.908 0.982 RRoBERTa-Large-MNLI 1.000 0.996 0.988 0.996 0.995 0.097 0.991 FRoBERTa-Large-MNLI 0.996 0.998 0.992 0.998 0.992 0.665 0.989 PRoBERTa-Large-MNLI (idf) 0.986 0.998 0.994 0.993 0.986 0.989 0.985 RRoBERTa-Large-MNLI (idf) 0.999 0.997 0.986 0.997 0.993 0.633 0.990 FRoBERTa-Large-MNLI (idf) 0.995 0.998 0.991 0.996 0.993 0.963 0.990 PXLNET-Base 0.970 0.996 0.986 0.990 0.979 0.739 0.982 RXLNET-Base 0.994 0.997 0.979 0.995 0.994 0.795 0.990 FXLNET-Base 0.983 0.997 0.983 0.993 0.987 0.505 0.988 PXLNET-Base (idf) 0.968 0.998 0.986 0.990 0.978 0.923 0.982 RXLNET-Base (idf) 0.993 0.998 0.978 0.996 0.994 0.439 0.988 FXLNET-Base (idf) 0.981 0.999 0.984 0.995 0.989 0.722 0.988 PXLNET-Large 0.969 0.998 0.986 0.995 0.979 0.880 0.981 RXLNET-Large 0.995 0.997 0.977 0.997 0.995 0.430 0.988 FXLNET-Large 0.983 0.998 0.983 0.997 0.988 0.713 0.988 PXLNET-Large (idf) 0.963 0.996 0.986 0.995 0.978 0.939 0.979 RXLNET-Large (idf) 0.992 0.997 0.975 0.993 0.996 0.531 0.982 FXLNET-Large (idf) 0.978 0.997 0.983 0.996 0.990 0.886 0.984 PXLM-En 0.965 0.996 0.990 0.978 0.980 0.946 0.981 RXLM-En 0.990 0.995 0.984 0.996 0.996 0.286 0.987 FXLM-En 0.978 0.997 0.988 0.990 0.989 0.576 0.987 PXLM-En (idf) 0.960 0.996 0.990 0.987 0.980 0.989 0.981 RXLM-En (idf) 0.991 0.997 0.983 0.996 0.998 0.612 0.985 FXLM-En (idf) 0.976 0.998 0.988 0.994 0.992 0.943 0.985 Table 19: Absolute Pearson correlations with system-level human judgments on WMT18 to-English translations. Correlations of metrics not signiﬁcantly outperformed by any other for that language pair are highlighted in bold. For each language pair, we specify the number of systems. Published as a conference paper at ICLR 2020 Setting Metric en-cs en-de en-et en-ﬁ en-ru en-tr en-zh 5 16 14 12 9 8 14 Unsupervised BLEU 0.995 0.981 0.975 0.962 0.983 0.826 0.947 CDER 0.997 0.986 0.984 0.964 0.984 0.861 0.961 CHARAC TER 0.993 0.989 0.956 0.974 0.983 0.833 0.983 ITER 0.915 0.984 0.981 0.973 0.975 0.865 - METEOR ++ - - - - - - - NIST 0.999 0.986 0.983 0.949 0.990 0.902 0.950 PER 0.991 0.981 0.958 0.906 0.988 0.859 0.964 TER 0.997 0.988 0.981",
      "chunk_index": 42
    },
    {
      "index": 301,
      "chunk_id": "BERTScore2019_chunk_43",
      "source_id": "BERTScore2019",
      "text": "0.983 0.833 0.983 ITER 0.915 0.984 0.981 0.973 0.975 0.865 - METEOR ++ - - - - - - - NIST 0.999 0.986 0.983 0.949 0.990 0.902 0.950 PER 0.991 0.981 0.958 0.906 0.988 0.859 0.964 TER 0.997 0.988 0.981 0.942 0.987 0.867 0.963 UHH_TSKM - - - - - - - WER 0.997 0.986 0.981 0.945 0.985 0.853 0.957 YISI-0 0.973 0.985 0.968 0.944 0.990 0.990 0.957 YISI-1 0.987 0.985 0.979 0.940 0.992 0.976 0.963 YISI-1 SRL - 0.990 - - - - 0.952 Supervised BEER 0.992 0.991 0.980 0.961 0.988 0.965 0.928 BLEND - - - - 0.988 - - RUSE - - - - - - - Pre-Trained PBERT-Multi 0.994 0.988 0.981 0.957 0.990 0.935 0.954 RBERT-Multi 0.997 0.990 0.980 0.980 0.989 0.879 0.976 FBERT-Multi 0.997 0.989 0.982 0.972 0.990 0.908 0.967 PBERT-Multi (idf) 0.992 0.986 0.974 0.954 0.991 0.969 0.954 RBERT-Multi (idf) 0.997 0.993 0.982 0.982 0.992 0.901 0.984 FBERT-Multi (idf) 0.995 0.990 0.981 0.972 0.991 0.941 0.973 PXLM-100 0.984 0.992 0.993 0.972 0.993 0.962 0.965 RXLM-100 0.991 0.992 0.992 0.989 0.992 0.895 0.983 FXLM-100 0.988 0.993 0.993 0.986 0.993 0.935 0.976 PXLM-100 (idf) 0.982 0.992 0.994 0.975 0.993 0.968 0.964 RXLM-100 (idf) 0.993 0.993 0.991 0.989 0.993 0.911 0.986 FXLM-100 (idf) 0.989 0.993 0.994 0.985 0.993 0.945 0.979 Table 20: Absolute Pearson correlations with system-level human judgments on WMT18 from- English translations. Correlations of metrics not signiﬁcantly outperformed by any other for that language pair are highlighted in bold. For each language pair, we specify the number of systems. Published as a conference paper at ICLR 2020 Setting Metric cs-en de-en et-en ﬁ-en ru-en tr-en zh-en 10K 10K 10K 10K 10K 10K 10K Unsupervised BLEU 0.956 0.969 0.981 0.962 0.972 0.586 0.968 CDER 0.964 0.980 0.988 0.976 0.974 0.577 0.973 CHARAC TER 0.960 0.992 0.975 0.979 0.984 0.680 0.942 ITER 0.966 0.990 0.975 0.989 0.943 0.742 0.978 METEOR ++ 0.937 0.990 0.975 0.962 0.989 0.787 0.954 NIST 0.942 0.982 0.980 0.965 0.965 0.862 0.959 PER 0.937 0.982 0.978 0.983 0.955 0.043 0.923 TER 0.942 0.970 0.988 0.960 0.963 0.450 0.967 UHH_TSKM 0.943 0.979 0.987 0.974 0.973 0.443 0.972 WER 0.942 0.961 0.989 0.953 0.962 0.072 0.967 YISI-0 0.947 0.992 0.972 0.969 0.982 0.863 0.950 YISI-1 0.942 0.991 0.976 0.964 0.985 0.881 0.943 YISI-1 SRL 0.957 0.994 0.978 0.968 0.986 0.785 0.954 Supervised BEER 0.950 0.993 0.983 0.982 0.976 0.723 0.968 BLEND 0.965 0.990 0.982 0.985",
      "chunk_index": 43
    },
    {
      "index": 302,
      "chunk_id": "BERTScore2019_chunk_44",
      "source_id": "BERTScore2019",
      "text": "0.967 YISI-0 0.947 0.992 0.972 0.969 0.982 0.863 0.950 YISI-1 0.942 0.991 0.976 0.964 0.985 0.881 0.943 YISI-1 SRL 0.957 0.994 0.978 0.968 0.986 0.785 0.954 Supervised BEER 0.950 0.993 0.983 0.982 0.976 0.723 0.968 BLEND 0.965 0.990 0.982 0.985 0.986 0.724 0.969 RUSE 0.974 0.996 0.988 0.983 0.982 0.780 0.973 Pre-Trained PBERT-Base 0.954 0.992 0.984 0.980 0.970 0.917 0.965 RBERT-Base 0.988 0.994 0.974 0.987 0.988 0.801 0.975 FBERT-Base 0.973 0.994 0.981 0.987 0.982 0.924 0.973 PBERT-Base (idf) 0.957 0.994 0.983 0.966 0.970 0.875 0.966 RBERT-Base (idf) 0.986 0.990 0.976 0.984 0.984 0.019 0.980 FBERT-Base (idf) 0.974 0.993 0.980 0.978 0.978 0.853 0.976 PBERT-Base-MRPC 0.949 0.995 0.986 0.960 0.969 0.832 0.972 RBERT-Base-MRPC 0.983 0.997 0.979 0.986 0.986 0.099 0.980 FBERT-Base-MRPC 0.967 0.997 0.984 0.978 0.981 0.722 0.979 PBERT-Base-MRPC (idf) 0.949 0.994 0.986 0.946 0.969 0.743 0.969 RBERT-Base-MRPC (idf) 0.984 0.994 0.980 0.980 0.986 0.541 0.982 FBERT-Base-MRPC (idf) 0.967 0.995 0.984 0.968 0.979 0.464 0.978 PBERT-Large 0.969 0.991 0.985 0.979 0.970 0.915 0.969 RBERT-Large 0.990 0.993 0.980 0.988 0.988 0.745 0.978 FBERT-Large 0.982 0.993 0.984 0.986 0.981 0.909 0.976 PBERT-Large (idf) 0.970 0.991 0.984 0.963 0.971 0.858 0.970 RBERT-Large (idf) 0.989 0.990 0.982 0.982 0.985 0.047 0.982 FBERT-Large (idf) 0.981 0.991 0.984 0.976 0.978 0.722 0.978 PRoBERTa-Base 0.959 0.992 0.988 0.986 0.971 0.809 0.976 RRoBERTa-Base 0.987 0.997 0.978 0.989 0.988 0.238 0.981 FRoBERTa-Base 0.973 0.997 0.987 0.989 0.982 0.674 0.982 PRoBERTa-Base (idf) 0.963 0.994 0.988 0.989 0.970 0.711 0.972 RRoBERTa-Base (idf) 0.988 0.996 0.979 0.989 0.987 0.353 0.983 FRoBERTa-Base (idf) 0.976 0.997 0.986 0.990 0.980 0.277 0.980 PRoBERTa-Large 0.965 0.995 0.990 0.976 0.976 0.846 0.975 RRoBERTa-Large 0.989 0.997 0.982 0.989 0.988 0.540 0.981 FRoBERTa-Large 0.978 0.998 0.989 0.983 0.985 0.760 0.981 PRoBERTa-Large (idf) 0.972 0.997 0.988 0.986 0.976 0.686 0.973 RRoBERTa-Large (idf) 0.990 0.996 0.983 0.989 0.989 0.096 0.982 FRoBERTa-Large (idf) 0.982 0.998 0.988 0.989 0.983 0.453 0.980 PRoBERTa-Large-MNLI 0.978 0.997 0.991 0.984 0.980 0.914 0.977 RRoBERTa-Large-MNLI 0.991 0.996 0.984 0.989 0.987 0.566 0.982 FRoBERTa-Large-MNLI 0.987 0.998 0.989 0.988 0.986 0.873 0.982 PRoBERTa-Large-MNLI (idf) 0.982 0.998 0.992 0.990 0.978 0.822 0.974 RRoBERTa-Large-MNLI (idf) 0.992 0.996 0.985 0.988 0.988 0.022 0.983 FRoBERTa-Large-MNLI (idf) 0.989 0.998 0.990 0.990 0.985 0.583 0.980 PXLNET-Base 0.960 0.997 0.984 0.982 0.972 0.849 0.974 RXLNET-Base 0.985 0.997 0.975 0.988 0.988 0.303 0.980 FXLNET-Base 0.974 0.998 0.981 0.986 0.982 0.628 0.980 PXLNET-Base (idf) 0.962 0.995 0.983 0.982 0.972 0.657 0.974 RXLNET-Base (idf) 0.986 0.996 0.976 0.987 0.987 0.666 0.982",
      "chunk_index": 44
    },
    {
      "index": 303,
      "chunk_id": "BERTScore2019_chunk_45",
      "source_id": "BERTScore2019",
      "text": "0.997 0.984 0.982 0.972 0.849 0.974 RXLNET-Base 0.985 0.997 0.975 0.988 0.988 0.303 0.980 FXLNET-Base 0.974 0.998 0.981 0.986 0.982 0.628 0.980 PXLNET-Base (idf) 0.962 0.995 0.983 0.982 0.972 0.657 0.974 RXLNET-Base (idf) 0.986 0.996 0.976 0.987 0.987 0.666 0.982 FXLNET-Base (idf) 0.975 0.996 0.980 0.985 0.981 0.259 0.980 PXLNET-Large 0.955 0.995 0.983 0.986 0.972 0.875 0.970 RXLNET-Large 0.984 0.996 0.972 0.984 0.989 0.491 0.975 FXLNET-Large 0.971 0.996 0.980 0.987 0.984 0.821 0.976 PXLNET-Large (idf) 0.961 0.997 0.983 0.987 0.973 0.816 0.973 RXLNET-Large (idf) 0.987 0.996 0.975 0.989 0.988 0.320 0.981 FXLNET-Large (idf) 0.976 0.997 0.980 0.989 0.982 0.623 0.980 PXLM-En 0.953 0.995 0.988 0.979 0.974 0.918 0.972 RXLM-En 0.983 0.996 0.980 0.988 0.991 0.561 0.977 FXLM-En 0.969 0.997 0.986 0.986 0.985 0.869 0.977 PXLM-En (idf) 0.957 0.996 0.987 0.970 0.974 0.862 0.973 RXLM-En (idf) 0.982 0.995 0.981 0.988 0.989 0.213 0.980 FXLM-En (idf) 0.970 0.996 0.985 0.982 0.982 0.519 0.978 Table 21: Absolute Pearson correlations with human judgments on WMT18 to-English language pairs for 10K hybrid systems. Correlations of metrics not signiﬁcantly outperformed by any other for that language pair are highlighted in bold. For each language pair, we specify the number of systems. Published as a conference paper at ICLR 2020 Setting Metric en-cs en-de en-et en-ﬁ en-ru en-tr en-zh 10K 10K 10K 10K 10K 10K 10K Unsupervised BLEU 0.993 0.977 0.971 0.958 0.977 0.796 0.941 CDER 0.995 0.984 0.981 0.961 0.982 0.832 0.956 CHARAC TER 0.990 0.986 0.950 0.963 0.981 0.775 0.978 ITER 0.865 0.978 0.982 0.966 0.965 0.872 - METEOR ++ - - - - - - - NIST 0.997 0.984 0.980 0.944 0.988 0.870 0.944 PER 0.987 0.979 0.954 0.904 0.986 0.829 0.950 TER 0.995 0.986 0.977 0.939 0.985 0.837 0.959 UHH_TSKM - - - - - - - WER 0.994 0.984 0.977 0.942 0.983 0.824 0.954 YISI-0 0.971 0.983 0.965 0.942 0.988 0.953 0.951 YISI-1 0.985 0.983 0.976 0.938 0.989 0.942 0.957 YISI-1 SRL - 0.988 - - - - 0.948 Supervised BEER 0.990 0.989 0.978 0.959 0.986 0.933 0.925 BLEND - - - - 0.986 - - RUSE - - - - - - - Pre-Trained PBERT-Multi 0.989 0.983 0.970 0.951 0.988 0.936 0.950 RBERT-Multi 0.995 0.991 0.979 0.977 0.989 0.872 0.980 FBERT-Multi 0.993 0.988 0.978 0.969 0.989 0.910 0.969 PBERT-Multi (idf) 0.992 0.986 0.978 0.954 0.988 0.903 0.950 RBERT-Multi (idf) 0.995 0.988 0.977 0.976 0.987 0.850 0.972 FBERT-Multi (idf) 0.995 0.988 0.979",
      "chunk_index": 45
    },
    {
      "index": 304,
      "chunk_id": "BERTScore2019_chunk_46",
      "source_id": "BERTScore2019",
      "text": "0.950 RBERT-Multi 0.995 0.991 0.979 0.977 0.989 0.872 0.980 FBERT-Multi 0.993 0.988 0.978 0.969 0.989 0.910 0.969 PBERT-Multi (idf) 0.992 0.986 0.978 0.954 0.988 0.903 0.950 RBERT-Multi (idf) 0.995 0.988 0.977 0.976 0.987 0.850 0.972 FBERT-Multi (idf) 0.995 0.988 0.979 0.969 0.987 0.877 0.963 PXLM-100 0.980 0.990 0.991 0.972 0.991 0.936 0.959 RXLM-100 0.991 0.990 0.989 0.985 0.991 0.882 0.981 FXLM-100 0.987 0.990 0.991 0.981 0.991 0.915 0.974 PXLM-100 (idf) 0.982 0.990 0.990 0.968 0.991 0.931 0.960 RXLM-100 (idf) 0.989 0.990 0.990 0.985 0.990 0.867 0.978 FXLM-100 (idf) 0.986 0.991 0.991 0.982 0.991 0.905 0.972 Table 22: Absolute Pearson correlations with human judgments on WMT18 from-English language pairs for 10K hybrid systems. Correlations of metrics not signiﬁcantly outperformed by any other for that language pair are highlighted in bold. For each language pair, we specify the number of systems. Published as a conference paper at ICLR 2020 Setting Metric cs-en de-en et-en ﬁ-en ru-en tr-en zh-en Unsupervised BLEU 0.135 0.804 0.757 0.460 0.230 0.096 0.661 CDER 0.162 0.795 0.764 0.493 0.234 0.087 0.660 CHARAC TER 0.146 0.737 0.696 0.496 0.201 0.082 0.584 ITER 0.152 0.814 0.746 0.474 0.234 0.100 0.673 METEOR ++ 0.172 0.804 0.646 0.456 0.253 0.052 0.597 NIST 0.136 0.802 0.739 0.469 0.228 0.135 0.665 PER 0.121 0.764 0.602 0.455 0.218 0.000 0.602 TER 0.139 0.789 0.768 0.470 0.232 0.001 0.652 UHH_TSKM 0.191 0.803 0.768 0.469 0.240 0.002 0.642 WER 0.149 0.776 0.760 0.471 0.227 0.000 0.654 YISI-0 0.148 0.780 0.703 0.483 0.229 0.106 0.629 YISI-1 0.157 0.808 0.752 0.466 0.250 0.110 0.613 YISI-1 SRL 0.159 0.814 0.763 0.484 0.243 0.008 0.620 Supervised BEER 0.165 0.811 0.765 0.485 0.237 0.030 0.675 BLEND 0.184 0.820 0.779 0.484 0.254 0.003 0.611 RUSE 0.213 0.823 0.788 0.487 0.250 0.109 0.672 Pre-Trained PBERT-Base 0.190 0.815 0.778 0.468 0.261 0.130 0.655 RBERT-Base 0.189 0.813 0.775 0.481 0.266 0.014 0.663 FBERT-Base 0.194 0.819 0.778 0.474 0.265 0.144 0.670 PBERT-Base (idf) 0.189 0.817 0.775 0.477 0.255 0.131 0.650 RBERT-Base (idf) 0.192 0.808 0.771 0.484 0.248 0.005 0.674 FBERT-Base (idf) 0.193 0.817 0.774 0.483 0.262 0.081 0.669 PBERT-Base-MRPC 0.190 0.701 0.766 0.487 0.254 0.126 0.653 RBERT-Base-MRPC 0.199 0.826 0.765 0.493 0.258 0.000 0.671 FBERT-Base-MRPC 0.197 0.824 0.767 0.491 0.260 0.147 0.668 PBERT-Base-MRPC (idf) 0.186 0.806 0.765 0.492 0.247 0.125 0.661 RBERT-Base-MRPC (idf) 0.200 0.823 0.760 0.495 0.258 0.000 0.680 FBERT-Base-MRPC (idf) 0.196 0.821 0.763 0.497 0.254 0.031 0.676 PBERT-Large 0.200 0.815 0.778 0.474 0.261 0.137 0.661",
      "chunk_index": 46
    },
    {
      "index": 305,
      "chunk_id": "BERTScore2019_chunk_47",
      "source_id": "BERTScore2019",
      "text": "0.767 0.491 0.260 0.147 0.668 PBERT-Base-MRPC (idf) 0.186 0.806 0.765 0.492 0.247 0.125 0.661 RBERT-Base-MRPC (idf) 0.200 0.823 0.760 0.495 0.258 0.000 0.680 FBERT-Base-MRPC (idf) 0.196 0.821 0.763 0.497 0.254 0.031 0.676 PBERT-Large 0.200 0.815 0.778 0.474 0.261 0.137 0.661 RBERT-Large 0.194 0.809 0.779 0.493 0.270 0.006 0.672 FBERT-Large 0.199 0.810 0.782 0.484 0.266 0.142 0.672 PBERT-Large (idf) 0.200 0.813 0.772 0.485 0.256 0.136 0.657 RBERT-Large (idf) 0.197 0.806 0.769 0.495 0.262 0.005 0.675 FBERT-Large (idf) 0.199 0.811 0.772 0.494 0.262 0.006 0.673 PRoBERTa-Base 0.173 0.675 0.757 0.502 0.258 0.126 0.654 RRoBERTa-Base 0.165 0.816 0.764 0.483 0.266 0.000 0.674 FRoBERTa-Base 0.173 0.820 0.764 0.498 0.262 0.090 0.669 PRoBERTa-Base (idf) 0.172 0.691 0.755 0.503 0.252 0.123 0.661 RRoBERTa-Base (idf) 0.172 0.809 0.758 0.490 0.268 0.000 0.678 FRoBERTa-Base (idf) 0.178 0.820 0.758 0.501 0.260 0.001 0.674 PRoBERTa-Large 0.174 0.704 0.765 0.497 0.255 0.140 0.663 RRoBERTa-Large 0.163 0.805 0.770 0.491 0.263 0.005 0.679 FRoBERTa-Large 0.175 0.825 0.770 0.499 0.262 0.143 0.675 PRoBERTa-Large (idf) 0.181 0.821 0.758 0.500 0.256 0.089 0.669 RRoBERTa-Large (idf) 0.165 0.787 0.763 0.495 0.270 0.000 0.684 FRoBERTa-Large (idf) 0.179 0.824 0.761 0.502 0.265 0.004 0.679 PRoBERTa-Large-MNLI 0.185 0.828 0.780 0.504 0.263 0.133 0.654 RRoBERTa-Large-MNLI 0.179 0.779 0.775 0.494 0.266 0.004 0.670 FRoBERTa-Large-MNLI 0.186 0.827 0.778 0.502 0.267 0.113 0.669 PRoBERTa-Large-MNLI (idf) 0.190 0.820 0.771 0.504 0.261 0.102 0.661 RRoBERTa-Large-MNLI (idf) 0.181 0.769 0.766 0.494 0.266 0.004 0.674 FRoBERTa-Large-MNLI (idf) 0.188 0.822 0.768 0.501 0.265 0.004 0.671 PXLNET-Base 0.186 0.771 0.762 0.496 0.247 0.153 0.658 RXLNET-Base 0.182 0.823 0.764 0.496 0.256 0.000 0.671 FXLNET-Base 0.186 0.824 0.765 0.499 0.253 0.049 0.673 PXLNET-Base (idf) 0.178 0.819 0.756 0.506 0.241 0.130 0.656 RXLNET-Base (idf) 0.183 0.817 0.754 0.501 0.256 0.000 0.673 FXLNET-Base (idf) 0.182 0.821 0.755 0.505 0.250 0.000 0.670 PXLNET-Large 0.195 0.721 0.767 0.493 0.152 0.144 0.661 RXLNET-Large 0.192 0.821 0.766 0.494 0.260 0.001 0.659 FXLNET-Large 0.196 0.824 0.773 0.496 0.261 0.155 0.675 PXLNET-Large (idf) 0.191 0.811 0.765 0.500 0.167 0.144 0.657 RXLNET-Large (idf) 0.196 0.815 0.762 0.495 0.259 0.000 0.673 FXLNET-Large (idf) 0.195 0.822 0.764 0.499 0.256 0.046 0.674 PXLM-En 0.192 0.796 0.779 0.486 0.255 0.131 0.665 RXLM-En 0.202 0.818 0.772 0.495 0.261 0.005 0.662 FXLM-En 0.199 0.827 0.778 0.491 0.262 0.086 0.674 PXLM-En (idf) 0.189 0.818 0.770 0.485 0.259 0.116 0.662 RXLM-En (idf) 0.202 0.812 0.761 0.490 0.250 0.003 0.668 FXLM-En (idf) 0.196 0.821 0.766 0.490 0.263 0.003 0.672 Table 23: Model selection accuracies (Hits@1) on to-English WMT18 hybrid systems.",
      "chunk_index": 47
    },
    {
      "index": 306,
      "chunk_id": "BERTScore2019_chunk_48",
      "source_id": "BERTScore2019",
      "text": "0.086 0.674 PXLM-En (idf) 0.189 0.818 0.770 0.485 0.259 0.116 0.662 RXLM-En (idf) 0.202 0.812 0.761 0.490 0.250 0.003 0.668 FXLM-En (idf) 0.196 0.821 0.766 0.490 0.263 0.003 0.672 Table 23: Model selection accuracies (Hits@1) on to-English WMT18 hybrid systems. We report the average of 100K samples and the 0.95 conﬁdence intervals are below10−3. We bold the highest numbers for each language pair and direction. Published as a conference paper at ICLR 2020 Setting Metric cs-en de-en et-en ﬁ-en ru-en tr-en zh-en Unsupervised BLEU 0.338 0.894 0.866 0.666 0.447 0.265 0.799 CDER 0.362 0.890 0.870 0.689 0.451 0.256 0.799 CHARAC TER 0.349 0.854 0.814 0.690 0.429 0.254 0.739 ITER 0.356 0.901 0.856 0.676 0.454 0.278 0.811 METEOR ++ 0.369 0.895 0.798 0.662 0.470 0.174 0.757 NIST 0.338 0.894 0.857 0.672 0.446 0.323 0.803 PER 0.325 0.866 0.771 0.663 0.435 0.021 0.754 TER 0.342 0.885 0.873 0.673 0.447 0.063 0.792 UHH_TSKM 0.387 0.894 0.873 0.671 0.460 0.063 0.788 WER 0.353 0.876 0.868 0.674 0.443 0.034 0.790 YISI-0 0.344 0.881 0.834 0.681 0.452 0.275 0.776 YISI-1 0.352 0.896 0.864 0.671 0.470 0.285 0.765 YISI-1 SRL 0.351 0.901 0.871 0.682 0.464 0.086 0.770 Supervised BEER 0.364 0.899 0.871 0.684 0.460 0.125 0.811 BLEND 0.382 0.904 0.880 0.681 0.473 0.077 0.767 RUSE 0.417 0.906 0.885 0.686 0.468 0.273 0.809 Pre-Trained PBERT-Base 0.386 0.901 0.880 0.674 0.481 0.318 0.793 RBERT-Base 0.383 0.899 0.877 0.683 0.486 0.100 0.804 FBERT-Base 0.388 0.903 0.879 0.678 0.484 0.331 0.808 PBERT-Base (idf) 0.390 0.902 0.877 0.681 0.475 0.318 0.786 RBERT-Base (idf) 0.390 0.896 0.874 0.686 0.475 0.077 0.811 FBERT-Base (idf) 0.393 0.902 0.876 0.685 0.483 0.225 0.806 PBERT-Base-MRPC 0.392 0.832 0.872 0.686 0.475 0.319 0.791 RBERT-Base-MRPC 0.397 0.908 0.870 0.691 0.478 0.025 0.811 FBERT-Base-MRPC 0.398 0.907 0.872 0.690 0.481 0.335 0.806 PBERT-Base-MRPC (idf) 0.392 0.896 0.870 0.689 0.467 0.316 0.797 RBERT-Base-MRPC (idf) 0.400 0.906 0.867 0.691 0.479 0.018 0.817 FBERT-Base-MRPC (idf) 0.400 0.905 0.869 0.693 0.475 0.097 0.812 PBERT-Large 0.398 0.901 0.880 0.678 0.481 0.327 0.799 RBERT-Large 0.391 0.897 0.879 0.690 0.490 0.085 0.810 FBERT-Large 0.397 0.898 0.882 0.684 0.486 0.328 0.810 PBERT-Large (idf) 0.398 0.900 0.875 0.685 0.475 0.323 0.794 RBERT-Large (idf) 0.395 0.895 0.873 0.692 0.488 0.080 0.813 FBERT-Large (idf) 0.398 0.899 0.875 0.691 0.482 0.086 0.810 PRoBERTa-Base 0.372 0.814 0.866 0.697 0.475 0.313 0.795 RRoBERTa-Base 0.366 0.902 0.870 0.683 0.483 0.026 0.813 FRoBERTa-Base 0.374 0.904 0.870 0.694 0.480 0.224 0.808 PRoBERTa-Base (idf) 0.373 0.825 0.865 0.697 0.470",
      "chunk_index": 48
    },
    {
      "index": 307,
      "chunk_id": "BERTScore2019_chunk_49",
      "source_id": "BERTScore2019",
      "text": "FBERT-Large (idf) 0.398 0.899 0.875 0.691 0.482 0.086 0.810 PRoBERTa-Base 0.372 0.814 0.866 0.697 0.475 0.313 0.795 RRoBERTa-Base 0.366 0.902 0.870 0.683 0.483 0.026 0.813 FRoBERTa-Base 0.374 0.904 0.870 0.694 0.480 0.224 0.808 PRoBERTa-Base (idf) 0.373 0.825 0.865 0.697 0.470 0.303 0.802 RRoBERTa-Base (idf) 0.374 0.898 0.866 0.688 0.486 0.028 0.816 FRoBERTa-Base (idf) 0.380 0.904 0.866 0.696 0.479 0.037 0.812 PRoBERTa-Large 0.375 0.833 0.871 0.693 0.474 0.327 0.800 RRoBERTa-Large 0.366 0.895 0.874 0.689 0.480 0.039 0.816 FRoBERTa-Large 0.378 0.907 0.874 0.694 0.480 0.324 0.811 PRoBERTa-Large (idf) 0.384 0.905 0.866 0.694 0.475 0.220 0.806 RRoBERTa-Large (idf) 0.368 0.885 0.869 0.692 0.487 0.030 0.819 FRoBERTa-Large (idf) 0.382 0.907 0.868 0.696 0.484 0.048 0.815 PRoBERTa-Large-MNLI 0.383 0.909 0.880 0.698 0.480 0.323 0.795 RRoBERTa-Large-MNLI 0.378 0.880 0.877 0.692 0.481 0.078 0.811 FRoBERTa-Large-MNLI 0.385 0.909 0.879 0.697 0.484 0.286 0.809 PRoBERTa-Large-MNLI (idf) 0.389 0.905 0.874 0.698 0.478 0.268 0.803 RRoBERTa-Large-MNLI (idf) 0.380 0.874 0.870 0.691 0.483 0.079 0.814 FRoBERTa-Large-MNLI (idf) 0.387 0.906 0.872 0.696 0.482 0.082 0.811 PXLNET-Base 0.385 0.875 0.869 0.692 0.469 0.342 0.796 RXLNET-Base 0.381 0.907 0.869 0.693 0.477 0.026 0.809 FXLNET-Base 0.385 0.907 0.871 0.694 0.476 0.128 0.810 PXLNET-Base (idf) 0.381 0.904 0.864 0.699 0.464 0.289 0.794 RXLNET-Base (idf) 0.384 0.903 0.863 0.696 0.479 0.013 0.812 FXLNET-Base (idf) 0.384 0.905 0.864 0.699 0.472 0.032 0.809 PXLNET-Large 0.392 0.844 0.873 0.689 0.367 0.338 0.799 RXLNET-Large 0.389 0.905 0.871 0.690 0.482 0.031 0.800 FXLNET-Large 0.393 0.907 0.876 0.691 0.483 0.348 0.812 PXLNET-Large (idf) 0.393 0.899 0.870 0.694 0.387 0.333 0.794 RXLNET-Large (idf) 0.395 0.901 0.868 0.690 0.483 0.023 0.810 FXLNET-Large (idf) 0.396 0.906 0.870 0.693 0.478 0.128 0.811 PXLM-En 0.394 0.891 0.880 0.685 0.476 0.322 0.802 RXLM-En 0.401 0.903 0.875 0.692 0.483 0.082 0.803 FXLM-En 0.400 0.909 0.878 0.689 0.483 0.234 0.811 PXLM-En (idf) 0.391 0.903 0.874 0.684 0.480 0.293 0.797 RXLM-En (idf) 0.402 0.900 0.868 0.688 0.477 0.068 0.806 FXLM-En (idf) 0.398 0.905 0.871 0.688 0.487 0.079 0.809 Table 24: Mean Reciprocal Rank (MRR) of the top metric-rated system on to-English WMT18 hybrid systems. We report the average of 100K samples and the 0.95 conﬁdence intervals are below 10−3. We bold the highest numbers for each language pair and direction. Published as a conference paper at ICLR 2020 Setting Metric cs-en de-en et-en ﬁ-en ru-en tr-en zh-en Unsupervised BLEU 3.85 0.45 1.01 2.17 2.34 4.48 3.19 CDER 3.88 0.43 0.87 1.33 2.30 4.58 3.43 CHARAC TER 3.77 0.49 0.94 2.07 2.25 4.07 3.37",
      "chunk_index": 49
    },
    {
      "index": 308,
      "chunk_id": "BERTScore2019_chunk_50",
      "source_id": "BERTScore2019",
      "text": "conference paper at ICLR 2020 Setting Metric cs-en de-en et-en ﬁ-en ru-en tr-en zh-en Unsupervised BLEU 3.85 0.45 1.01 2.17 2.34 4.48 3.19 CDER 3.88 0.43 0.87 1.33 2.30 4.58 3.43 CHARAC TER 3.77 0.49 0.94 2.07 2.25 4.07 3.37 ITER 3.55 0.46 1.25 1.43 4.65 3.11 2.92 METEOR ++ 3.70 0.41 0.69 1.13 2.28 1.40 3.50 NIST 3.93 0.49 1.10 1.19 2.36 1.42 3.92 PER 2.02 0.46 1.71 1.49 2.25 4.22 3.20 TER 3.86 0.43 1.14 1.14 4.34 5.18 3.82 UHH_TSKM 3.98 0.40 1.27 1.10 2.23 4.26 3.47 WER 3.85 0.44 1.48 1.18 4.87 5.96 3.72 YISI-0 3.81 0.48 0.72 1.20 1.75 1.40 3.44 YISI-1 3.88 0.44 0.65 1.13 2.17 1.32 3.40 YISI-1 SRL 3.67 0.41 0.64 1.20 2.15 1.31 3.55 Supervised BEER 3.82 0.41 0.79 1.08 1.92 1.96 3.43 BLEND 3.77 0.41 0.66 1.09 2.21 1.28 3.46 RUSE 3.13 0.32 0.64 1.03 1.51 1.94 3.15 Pre-Trained PBERT-Base 3.97 0.36 0.72 1.16 2.20 1.25 3.26 RBERT-Base 1.51 0.43 0.60 1.65 1.33 1.34 3.50 FBERT-Base 3.70 0.36 0.59 1.08 1.92 1.27 3.38 PBERT-Base (idf) 3.94 0.36 0.64 1.18 2.06 2.55 3.54 RBERT-Base (idf) 1.54 0.43 0.63 1.87 1.12 5.96 3.38 FBERT-Base (idf) 2.75 0.39 0.60 1.10 1.38 1.26 3.51 PBERT-Base-MRPC 4.02 0.35 0.74 1.15 1.09 3.33 3.06 RBERT-Base-MRPC 2.66 0.43 0.62 1.75 1.10 5.64 3.34 FBERT-Base-MRPC 3.89 0.36 0.60 1.09 1.08 3.82 3.23 PBERT-Base-MRPC (idf) 4.02 0.35 0.67 1.18 1.48 3.30 3.49 RBERT-Base-MRPC (idf) 1.63 0.43 0.65 1.93 1.13 7.26 3.13 FBERT-Base-MRPC (idf) 3.86 0.38 0.61 1.11 1.14 4.24 3.28 PBERT-Large 3.82 0.34 0.66 1.12 2.10 1.31 3.60 RBERT-Large 1.49 0.40 0.59 1.56 1.17 1.35 3.61 FBERT-Large 1.71 0.35 0.58 1.08 1.65 1.29 3.60 PBERT-Large (idf) 3.74 0.35 0.65 1.12 1.90 1.98 3.77 RBERT-Large (idf) 1.51 0.42 0.62 1.86 1.10 5.84 3.21 FBERT-Large (idf) 1.49 0.38 0.60 1.17 1.24 1.96 3.53 PRoBERTa-Base 3.89 0.37 0.75 1.18 1.07 3.45 2.62 RRoBERTa-Base 1.92 0.39 0.64 1.57 1.11 5.75 3.13 FRoBERTa-Base 3.56 0.37 0.59 1.10 1.08 3.79 2.90 PRoBERTa-Base (idf) 3.89 0.38 0.67 1.20 1.30 3.27 3.47 RRoBERTa-Base (idf) 1.61 0.42 0.67 1.65 1.14 6.55 2.95 FRoBERTa-Base (idf) 3.18 0.38 0.60 1.11 1.13 6.54 3.11 PRoBERTa-Large 3.64 0.36 0.71 1.10 1.03 2.69 2.57 RRoBERTa-Large 1.60 0.37 0.64 1.51 1.09 3.91 3.27 FRoBERTa-Large 2.38 0.35 0.58 1.06 1.05 3.57 2.95 PRoBERTa-Large (idf) 2.70 0.36 0.69 1.13 1.08 3.18 2.89 RRoBERTa-Large (idf) 1.55 0.39 0.66 1.59 1.10 6.66 3.18 FRoBERTa-Large (idf) 1.68 0.37 0.59 1.08",
      "chunk_index": 50
    },
    {
      "index": 309,
      "chunk_id": "BERTScore2019_chunk_51",
      "source_id": "BERTScore2019",
      "text": "RRoBERTa-Large 1.60 0.37 0.64 1.51 1.09 3.91 3.27 FRoBERTa-Large 2.38 0.35 0.58 1.06 1.05 3.57 2.95 PRoBERTa-Large (idf) 2.70 0.36 0.69 1.13 1.08 3.18 2.89 RRoBERTa-Large (idf) 1.55 0.39 0.66 1.59 1.10 6.66 3.18 FRoBERTa-Large (idf) 1.68 0.37 0.59 1.08 1.08 5.58 2.91 PRoBERTa-Large-MNLI 2.14 0.35 0.61 1.07 1.09 1.21 3.35 RRoBERTa-Large-MNLI 1.45 0.37 0.64 1.49 1.10 4.42 3.55 FRoBERTa-Large-MNLI 1.42 0.35 0.59 1.07 1.07 1.27 3.41 PRoBERTa-Large-MNLI (idf) 1.55 0.35 0.60 1.08 1.12 1.54 3.87 RRoBERTa-Large-MNLI (idf) 1.45 0.39 0.64 1.65 1.09 5.89 3.32 FRoBERTa-Large-MNLI (idf) 1.42 0.36 0.60 1.10 1.08 3.80 3.45 PXLNET-Base 3.90 0.37 0.68 1.07 1.16 2.47 2.91 RXLNET-Base 1.71 0.45 0.72 1.58 1.07 6.29 3.36 FXLNET-Base 3.78 0.39 0.62 1.05 1.07 3.60 3.20 PXLNET-Base (idf) 3.90 0.46 0.65 1.08 2.93 3.30 3.39 RXLNET-Base (idf) 1.51 0.45 0.82 1.78 1.12 10.77 3.13 FXLNET-Base (idf) 3.67 0.42 0.66 1.11 1.22 7.13 3.23 PXLNET-Large 3.94 0.37 0.71 1.10 21.10 1.85 2.90 RXLNET-Large 2.23 0.41 0.69 1.34 1.07 4.46 3.40 FXLNET-Large 3.84 0.36 0.60 1.03 1.07 3.38 3.22 PXLNET-Large (idf) 3.92 0.41 0.64 1.12 21.10 3.24 3.37 RXLNET-Large (idf) 1.60 0.43 0.78 1.70 1.09 6.13 3.20 FXLNET-Large (idf) 3.80 0.38 0.63 1.06 1.09 3.72 3.25 PXLM-En 3.88 0.33 0.75 1.16 2.16 1.28 3.29 RXLM-En 1.98 0.41 0.60 1.41 1.21 3.30 3.47 FXLM-En 3.78 0.36 0.61 1.09 1.71 1.30 3.40 PXLM-En (idf) 3.84 0.36 0.69 1.17 1.86 1.33 3.47 RXLM-En (idf) 1.70 0.42 0.63 1.55 1.11 5.87 3.36 FXLM-En (idf) 3.72 0.40 0.62 1.14 1.32 4.15 3.43 Table 25: Absolute Difference (×100) of the top metric-rated and the top human-rated system on to- English WMT18 hybrid systems. Smaller difference signify higher agreement with human scores. We report the average of 100K samples and the 0.95 conﬁdence intervals are below 10−3. We bold the lowest numbers for each language pair and direction. Published as a conference paper at ICLR 2020 Setting Metric en-cs en-de en-et en-ﬁ en-ru en-tr en-zh Unsupervised BLEU 0.151 0.611 0.617 0.087 0.519 0.029 0.515 CDER 0.163 0.663 0.731 0.081 0.541 0.032 0.552 CHARAC TER 0.135 0.737 0.639 0.492 0.543 0.027 0.667 ITER 0.000 0.691 0.734 0.112 0.534 0.031 - METEOR ++ - - - - - - - NIST 0.182 0.662 0.549 0.083 0.537 0.033 0.553 PER 0.179 0.555 0.454 0.062 0.535 0.032 0.539 TER 0.175 0.657 0.550 0.065 0.545 0.029 0.551 UHH_TSKM - - - - - - - WER 0.155 0.643 0.552 0.067 0.538 0.029",
      "chunk_index": 51
    },
    {
      "index": 310,
      "chunk_id": "BERTScore2019_chunk_52",
      "source_id": "BERTScore2019",
      "text": "- NIST 0.182 0.662 0.549 0.083 0.537 0.033 0.553 PER 0.179 0.555 0.454 0.062 0.535 0.032 0.539 TER 0.175 0.657 0.550 0.065 0.545 0.029 0.551 UHH_TSKM - - - - - - - WER 0.155 0.643 0.552 0.067 0.538 0.029 0.546 YISI-0 0.154 0.674 0.622 0.356 0.523 0.383 0.600 YISI-1 0.178 0.670 0.674 0.230 0.548 0.396 0.595 YISI-1 SRL - 0.708 - - - - 0.537 Supervised BEER 0.174 0.670 0.662 0.113 0.555 0.296 0.531 BLEND - - - - 0.559 - - RUSE - - - - - - - Pre-Trained PBERT-Multi 0.181 0.665 0.771 0.077 0.550 0.373 0.550 RBERT-Multi 0.184 0.728 0.722 0.146 0.544 0.031 0.657 FBERT-Multi 0.185 0.703 0.764 0.081 0.548 0.032 0.629 PBERT-Multi (idf) 0.175 0.713 0.769 0.080 0.542 0.031 0.549 RBERT-Multi (idf) 0.177 0.725 0.752 0.178 0.538 0.031 0.628 FBERT-Multi (idf) 0.178 0.721 0.766 0.081 0.543 0.030 0.594 PXLM-100 0.175 0.669 0.748 0.079 0.550 0.314 0.582 RXLM-100 0.195 0.671 0.770 0.222 0.555 0.034 0.658 FXLM-100 0.187 0.670 0.775 0.099 0.552 0.034 0.615 PXLM-100 (idf) 0.163 0.664 0.750 0.091 0.550 0.288 0.578 RXLM-100 (idf) 0.191 0.681 0.770 0.231 0.548 0.033 0.645 FXLM-100 (idf) 0.180 0.672 0.774 0.127 0.550 0.033 0.616 Table 26: Model selection accuracies (Hits@1) on to-English WMT18 hybrid systems. We report the average of 100K samples and the 0.95 conﬁdence intervals are below10−3. We bold the highest numbers for each language pair and direction. Published as a conference paper at ICLR 2020 Setting Metric en-cs en-de en-et en-ﬁ en-ru en-tr en-zh Unsupervised BLEU 0.363 0.764 0.766 0.323 0.714 0.205 0.666 CDER 0.371 0.803 0.851 0.319 0.729 0.210 0.700 CHARAC TER 0.346 0.853 0.781 0.667 0.732 0.205 0.809 ITER 0.044 0.825 0.853 0.365 0.717 0.210 - METEOR ++ - - - - - - - NIST 0.393 0.803 0.710 0.326 0.726 0.211 0.698 PER 0.387 0.719 0.624 0.301 0.725 0.211 0.678 TER 0.384 0.798 0.708 0.305 0.733 0.209 0.695 UHH_TSKM - - - - - - - WER 0.367 0.787 0.710 0.308 0.728 0.209 0.696 YISI-0 0.370 0.811 0.775 0.553 0.715 0.602 0.753 YISI-1 0.390 0.808 0.811 0.439 0.735 0.612 0.750 YISI-1 SRL - 0.835 - - - - 0.691 Supervised BEER 0.388 0.808 0.804 0.353 0.739 0.507 0.683 BLEND - - - - 0.742 - - RUSE - - - - - - - Pre-Trained PBERT-Multi 0.395 0.805 0.876 0.314 0.736 0.586 0.694 RBERT-Multi 0.401 0.849 0.844 0.368 0.732 0.212 0.802 FBERT-Multi 0.400 0.832",
      "chunk_index": 52
    },
    {
      "index": 311,
      "chunk_id": "BERTScore2019_chunk_53",
      "source_id": "BERTScore2019",
      "text": "0.353 0.739 0.507 0.683 BLEND - - - - 0.742 - - RUSE - - - - - - - Pre-Trained PBERT-Multi 0.395 0.805 0.876 0.314 0.736 0.586 0.694 RBERT-Multi 0.401 0.849 0.844 0.368 0.732 0.212 0.802 FBERT-Multi 0.400 0.832 0.872 0.317 0.735 0.214 0.775 PBERT-Multi (idf) 0.390 0.839 0.875 0.320 0.730 0.213 0.691 RBERT-Multi (idf) 0.395 0.847 0.864 0.398 0.727 0.212 0.776 FBERT-Multi (idf) 0.395 0.844 0.873 0.319 0.730 0.212 0.739 PXLM-100 0.391 0.808 0.862 0.316 0.735 0.522 0.733 RXLM-100 0.413 0.809 0.876 0.435 0.738 0.216 0.803 FXLM-100 0.404 0.809 0.878 0.333 0.737 0.216 0.767 PXLM-100 (idf) 0.377 0.805 0.863 0.326 0.735 0.497 0.729 RXLM-100 (idf) 0.409 0.816 0.876 0.444 0.733 0.214 0.793 FXLM-100 (idf) 0.396 0.810 0.878 0.355 0.735 0.214 0.767 Table 27: Mean Reciprocal Rank (MRR) of the top metric-rated system on to-English WMT18 hybrid systems. We report the average of 100K samples and the 0.95 conﬁdence intervals are below 10−3. We bold the highest numbers for each language pair and direction. Published as a conference paper at ICLR 2020 Setting Metric en-cs en-de en-et en-ﬁ en-ru en-tr en-zh Unsupervised BLEU 1.26 6.36 2.59 0.92 0.76 9.40 3.01 CDER 1.25 6.70 1.90 1.41 0.87 9.37 1.75 CHARAC TER 1.23 6.90 2.19 4.35 0.93 5.22 1.64 ITER 1.25 9.14 2.52 1.52 1.35 7.33 - METEOR ++ - - - - - - - NIST 1.24 5.28 2.55 1.02 0.75 8.82 3.34 PER 1.25 6.62 4.92 7.43 0.68 9.76 2.31 TER 1.21 6.02 4.34 2.17 0.73 8.80 1.43 UHH_TSKM - - - - - - - WER 1.22 6.15 4.19 2.43 0.72 9.28 1.49 YISI-0 1.25 6.62 1.53 1.46 0.75 3.47 2.87 YISI-1 1.22 6.27 1.21 1.13 0.71 3.51 3.33 YISI-1 SRL - 6.57 - - - - 3.71 Supervised BEER 1.21 5.96 1.84 0.77 0.74 3.36 1.96 BLEND - - - - 0.71 - - RUSE - - - - - - - Pre-Trained PBERT-Multi 1.17 3.27 1.38 1.24 0.75 4.14 2.08 RBERT-Multi 1.16 6.68 0.77 0.94 0.68 3.22 1.31 FBERT-Multi 1.15 5.17 0.90 0.98 0.71 3.26 1.62 PBERT-Multi (idf) 1.14 3.82 1.66 1.27 0.76 4.57 2.04 RBERT-Multi (idf) 1.15 6.97 0.83 3.65 0.68 3.32 1.37 FBERT-Multi (idf) 1.14 5.63 1.13 1.19 0.71 3.38 1.58 PXLM-100 1.22 6.30 1.14 0.79 0.74 3.73 2.21 RXLM-100 1.18 6.89 0.76 0.77 0.66 3.26 1.68 FXLM-100 1.19 6.44 0.82 0.76 0.69 3.21 1.57 PXLM-100 (idf) 1.21 6.61 1.07 0.78 0.72 5.59 2.02",
      "chunk_index": 53
    },
    {
      "index": 312,
      "chunk_id": "BERTScore2019_chunk_54",
      "source_id": "BERTScore2019",
      "text": "1.14 5.63 1.13 1.19 0.71 3.38 1.58 PXLM-100 1.22 6.30 1.14 0.79 0.74 3.73 2.21 RXLM-100 1.18 6.89 0.76 0.77 0.66 3.26 1.68 FXLM-100 1.19 6.44 0.82 0.76 0.69 3.21 1.57 PXLM-100 (idf) 1.21 6.61 1.07 0.78 0.72 5.59 2.02 RXLM-100 (idf) 1.19 7.07 0.77 0.77 0.66 3.33 1.60 FXLM-100 (idf) 1.20 6.57 0.86 0.76 0.68 3.28 1.56 Table 28: Absolute Difference (×100) of the top metric-rated and the top human-rated system on to- English WMT18 hybrid systems. Smaller difference indicate higher agreement with human scores. We report the average of 100K samples and the 0.95 conﬁdence intervals are below 10−3. We bold the lowest numbers for each language pair and direction. Published as a conference paper at ICLR 2020 Metric M1 M2 BLEU -1 0.124 ∗ 0.135∗ BLEU -2 0.037 ∗ 0.048∗ BLEU -3 0.004 ∗ 0.016∗ BLEU -4 -0.019 ∗ -0.005∗ METEOR 0.606∗ 0.594∗ ROUGE -L 0.090 ∗ 0.096∗ CIDE R 0.438∗ 0.440∗ SPICE 0.759∗ 0.750∗ LEIC 0.939∗ 0.949∗ BEER 0.491 0.562 EED 0.545 0.599 CHR F++ 0.702 0.729 CHARAC TER 0.800 0.801 PBERT-Base 0.313 0.344 RBERT-Base 0.679 0.622 FBERT-Base 0.531 0.519 PBERT-Base (idf) 0.243 0.286 RBERT-Base (idf) 0.834 0.783 FBERT-Base (idf) 0.579 0.581 PBERT-Base-MRPC 0.252 0.331 RBERT-Base-MRPC 0.644 0.641 FBERT-Base-MRPC 0.470 0.512 PBERT-Base-MRPC (idf) 0.264 0.300 RBERT-Base-MRPC (idf) 0.794 0.767 FBERT-Base-MRPC (idf) 0.575 0.583 PBERT-Large 0.454 0.486 RBERT-Large 0.756 0.697 FBERT-Large 0.649 0.634 PBERT-Large (idf) 0.327 0.372 RBERT-Large (idf) 0.873 0.821 FBERT-Large (idf) 0.645 0.647 PRoBERTa-Base -0.223 -0.179 RRoBERTa-Base 0.827 0.800 FRoBERTa-Base 0.176 0.191 PRoBERTa-Base (idf) -0.256 -0.267 RRoBERTa-Base (idf) 0.901 0.869 FRoBERTa-Base (idf) 0.188 0.157 PRoBERTa-Large -0.105 -0.041 RRoBERTa-Large 0.888 0.863 FRoBERTa-Large 0.322 0.350 PRoBERTa-Large (idf) 0.063 -0.011 RRoBERTa-Large (idf) 0.917 0.889 FRoBERTa-Large (idf) 0.519 0.453 PRoBERTa-Large-MNLI 0.129 0.208 RRoBERTa-Large-MNLI 0.820 0.823 FRoBERTa-Large-MNLI 0.546 0.592 PRoBERTa-Large-MNLI (idf) 0.081 0.099 RRoBERTa-Large-MNLI (idf) 0.906 0.875 FRoBERTa-Large-MNLI (idf) 0.605 0.596 PXLNet-Base -0.046 0.080 RXLNet-Base 0.409 0.506 FXLNet-Base 0.146 0.265 PXLNet-Base (idf) 0.006 0.145 RXLNet-Base (idf) 0.655 0.720 FXLNet-Base (idf) 0.270 0.391 PXLNet-Large -0.188 -0.115 RXLNet-Large 0.178 0.195 FXLNet-Large -0.014 0.036 PXLNet-Large (idf) -0.186 -0.072 RXLNet-Large (idf) 0.554 0.555 FXLNet-Large (idf) 0.151 0.234 PXLM-En 0.230 0.220 RXLM-En 0.333 0.263 FXLM-En 0.297 0.243 PXLM-En (idf) 0.266 0.275 RXLM-En (idf) 0.700 0.640 FXLM-En (idf) 0.499 0.470 Table 29: Pearson correlation on the 2015 COCO Captioning Challenge. The M1 and M2 measures are described in Section 4. We bold the best correlating task-speciﬁc and task-agnostic metrics in each setting LEIC uses images as",
      "chunk_index": 54
    },
    {
      "index": 313,
      "chunk_id": "BERTScore2019_chunk_55",
      "source_id": "BERTScore2019",
      "text": "FXLM-En (idf) 0.499 0.470 Table 29: Pearson correlation on the 2015 COCO Captioning Challenge. The M1 and M2 measures are described in Section 4. We bold the best correlating task-speciﬁc and task-agnostic metrics in each setting LEIC uses images as additional inputs. Numbers with ∗are cited from Cui et al. (2018). Published as a conference paper at ICLR 2020 Type Method QQP PAWS QQP Trained on QQP (supervised) DecAtt 0.939* 0.263 DIIN 0.952* 0.324 BERT 0.963* 0.351 Trained on QQP + PAWSQQP (supervised) DecAtt - 0.511 DIIN - 0.778 BERT - 0.831 Metric (Not trained on QQP or PAWSQQP) BLEU -1 0.737 0.402 BLEU -2 0.720 0.548 BLEU -3 0.712 0.527 BLEU -4 0.707 0.527 METEOR 0.755 0.532 ROUGE -L 0.740 0.536 CHR F++ 0.577 0.608 BEER 0.741 0.564 EED 0.743 0.611 CHARAC TER 0.698 0.650 PBERT-Base 0.750 0.654 RBERT-Base 0.739 0.655 FBERT-Base 0.755 0.654 PBERT-Base (idf) 0.766 0.665 RBERT-Base (idf) 0.752 0.665 FBERT-Base (idf) 0.770 0.664 PBERT-Base-MRPC 0.742 0.615 RBERT-Base-MRPC 0.729 0.617 FBERT-Base-MRPC 0.746 0.614 PBERT-Base-MRPC (idf) 0.752 0.618 RBERT-Base-MRPC (idf) 0.737 0.619 FBERT-Base-MRPC (idf) 0.756 0.617 PBERT-Large 0.752 0.706 RBERT-Large 0.740 0.710 FBERT-Large 0.756 0.707 PBERT-Large (idf) 0.766 0.713 RBERT-Large (idf) 0.751 0.718 FBERT-Large (idf) 0.769 0.714 PRoBERTa-Base 0.746 0.657 RRoBERTa-Base 0.736 0.656 FRoBERTa-Base 0.751 0.654 PRoBERTa-Base (idf) 0.760 0.666 RRoBERTa-Base (idf) 0.745 0.666 FRoBERTa-Base (idf) 0.765 0.664 PRoBERTa-Large 0.757 0.687 RRoBERTa-Large 0.744 0.685 FRoBERTa-Large 0.761 0.685 PRoBERTa-Large (idf) 0.773 0.691 RRoBERTa-Large (idf) 0.757 0.697 FRoBERTa-Large (idf) 0.777 0.693 PRoBERTa-Large-MNLI 0.763 0.767 RRoBERTa-Large-MNLI 0.750 0.772 FRoBERTa-Large-MNLI 0.766 0.770 PRoBERTa-Large-MNLI (idf) 0.783 0.756 RRoBERTa-Large-MNLI (idf) 0.767 0.764 FRoBERTa-Large-MNLI (idf) 0.784 0.759 PXLNet-Base 0.737 0.603 RXLNet-Base 0.731 0.607 FXLNet-Base 0.739 0.605 PXLNet-Base (idf) 0.751 0.625 RXLNet-Base (idf) 0.743 0.630 FXLNet-Base (idf) 0.751 0.626 PXLNet-Large 0.742 0.593 RXLNet-Large 0.734 0.598 FXLNet-Large 0.744 0.596 PXLNet-Large (idf) 0.759 0.604 RXLNet-Large (idf) 0.749 0.610 FXLNet-Large (idf) 0.760 0.606 PXLM-En 0.734 0.600 RXLM-En 0.725 0.604 FXLM-En 0.737 0.602 PXLM-En (idf) 0.757 0.596 RXLM-En (idf) 0.745 0.603 FXLM-En (idf) 0.759 0.600 Table 30: Area under ROC curve (AUC) on QQP and PAWS QQP datasets. The scores of trained DecATT (Parikh et al., 2016), DIIN (Gong et al., 2018), and ﬁne-tuned BERT are reported by Zhang et al. (2019). We bold the best task-speciﬁc and task-agnostic metrics. Numbers with ∗are scores on the held-out test set of QQP. 43",
      "chunk_index": 55
    },
    {
      "index": 314,
      "chunk_id": "BERTScore2019_chunk_56",
      "source_id": "BERTScore2019",
      "text": "al. (2019). We bold the best task-speciﬁc and task-agnostic metrics. Numbers with ∗are scores on the held-out test set of QQP. 43",
      "chunk_index": 56
    },
    {
      "index": 315,
      "chunk_id": "SummaC2021_chunk_00",
      "source_id": "SummaC2021",
      "text": "SUMMA C: Re-Visiting NLI-based Models for Inconsistency Detection in Summarization Philippe Laban UC Berkeley Tobias Schnabel Microsoft Paul N. Bennett Microsoft Marti A. Hearst UC Berkeley∗ Abstract In the summarization domain, a key require- ment for summaries is to be factually con- sistent with the input document. Previous work has found that natural language in- ference (NLI) models do not perform com- petitively when applied to inconsistency de- tection. In this work, we revisit the use of NLI for inconsistency detection, ﬁnding that past work suffered from a mismatch in input granularity between NLI datasets (sentence-level), and inconsistency detec- tion (document level). We provide a highly effective and light-weight method called SUMMA CConv that enables NLI models to be successfully used for this task by seg- menting documents into sentence units and aggregating scores between pairs of sen- tences. On our newly introduced benchmark called SUMMA C (Summary Consistency) consisting of six large inconsistency detec- tion datasets, S UMMA CConv obtains state- of-the-art results with a balanced accuracy of 74.4%, a 5% point improvement com- pared to prior work. 1 Introduction Recent progress in text summarization has been remarkable, with ROUGE record-setting models published every few months, and human evalua- tions indicating that automatically generated sum- maries are matching human-written summaries in terms of ﬂuency and informativeness (Zhang et al., 2020a). A major limitation of current summarization models is their inability to remain factually consis- tent with the respective input document. Summary inconsistencies are diverse - from inversions (i.e., negation) to incorrect use of an entity (i.e., sub- ject, object swapping), or hallucinations (i.e., in- troduction of entity not in the original document). ∗Author emails: {phillab,hearst}@berkeley.edu, {To- bias.Schnabel,Paul.N.Bennett}@microsoft.com One possible site, known as Arcadia Planitia, is covered in strange sinuous features. Arcadia Planitia is in Mars' northern lowlands. The shapes could be signs that the area is actually made of glaciers, which are large masses of slow- moving ice. Scientists are studying Mars to learn about the Red Planet and find landing sites for future missions. D1 D3 D4 There are strange shape patterns on Arcadia Planitia. Document Summary 0.98 0.99 0.02 Sentence-Level NLI Document-Level NLI P(Y = entail | document, summary) = 0.91 This makes Arcadia Planitia ideal for future missions. The shapes could indicate the area might be made of glaciers. D2 S2 S1 S3 P(Y = entail | Di, Sj) Figure 1: Example document with an inconsis-",
      "chunk_index": 0
    },
    {
      "index": 316,
      "chunk_id": "SummaC2021_chunk_01",
      "source_id": "SummaC2021",
      "text": "document, summary) = 0.91 This makes Arcadia Planitia ideal for future missions. The shapes could indicate the area might be made of glaciers. D2 S2 S1 S3 P(Y = entail | Di, Sj) Figure 1: Example document with an inconsis- tent summary. When running each sentence pair (Di, Sj) through an NLI model, S3 is not entailed by any document sentence. However, when run- ning the entire (document, summary) at once, the NLI model incorrectly predicts that the document highly entails the entire summary. Recent studies have shown that in some scenarios, even state-of-the-art pre-trained language models can generate inconsistent summaries in more than 70% of all cases (Pagnoni et al., 2021). This has led to accelerated research around summary in- consistency detection. A closely-related task to inconsistency detec- tion is textual entailment, also referred to as Nat- ural Language Inference (NLI), in which a hy- pothesis sentence must be classiﬁed as either en- tailed by, neutral or contradicting a premise sen- tence. Enabled by the crowd-sourcing of large NLI datasets such as SNLI (Bowman et al., 2015) and MNLI (Williams et al., 2018), modern ar- chitectures have achieved close to human perfor- mance at the task. The similarity of NLI to inconsistency detec- tion, as well as the availability of high-performing NLI models, led to early attempts at using NLI to detect consistency errors in summaries. These arXiv:2111.09525v1 [cs.CL] 18 Nov 2021 early attempts were unsuccessful, ﬁnding that re- ranking summaries according to an NLI model can lead to an increase in consistency errors (Falke et al., 2019), or that out-of-the-box NLI mod- els obtain 52% accuracy at the binary classiﬁca- tion task of inconsistency detection, only slightly above random guessing (Kryscinski et al., 2020). In this work, we revisit this approach, show- ing that NLI models can in fact successfully be used for inconsistency detection, as long as they are used at the appropriate granularity. Figure 1 shows how crucial using the correct granularity as input to NLI models is. An inconsistency checker should ﬂag the last sentence in the sum- mary (shown right) as problematic. When treating the entire document as the premise and the sum- mary as the hypothesis, a competitive NLI model predicts with probability of 0.91 that the sum- mary is entailed by the document. However, when splitting the documents into sentence premise- hypothesis pairs (visualized as edges in Figure 1) the NLI model",
      "chunk_index": 1
    },
    {
      "index": 317,
      "chunk_id": "SummaC2021_chunk_02",
      "source_id": "SummaC2021",
      "text": "as the hypothesis, a competitive NLI model predicts with probability of 0.91 that the sum- mary is entailed by the document. However, when splitting the documents into sentence premise- hypothesis pairs (visualized as edges in Figure 1) the NLI model correctly determines that S 3 is not supported by any document sentence. This illus- trates that working with sentence pairs is crucial for making NLI models work for inconsistency de- tection. Our contributions are two-fold. First, we in- troduce a new approach for inconsistency detec- tion based on the aggregation of sentence-level en- tailment scores for each pair of input document and summary sentences. We present two model variants that differ in the way they aggregate sentence-level scores into a single score. S UM- MACZS performs zero-shot aggregation by com- bining sentence-level scores using max and mean operators. S UMMA CConv is a trained model con- sisting of a single learned convolution layer com- piling the distribution of entailment scores of all document sentences into a single score. Second, to evaluate our approach, we introduce the S UMMA C Benchmark by standardizing exist- ing datasets. Because the benchmark contains the six largest summary consistency datasets, it is more comprehensive and includes a broader range of inconsistency errors than prior work. The S UMMA C models outperform existing in- consistency detection models on the benchmark, with the S UMMA CConv obtaining an overall bal- anced accuracy of 74.4%, 5% above prior work. We publicly release the models and datasets1. 1https://github.com/tingofurro/summac/ 2 Related Work We brieﬂy survey existing methods and datasets for fact checking, inconsistency detection, and in- consistency correction. 2.1 Fact Checking and Veriﬁcation Fact checking is a related task in which a model receives an input claim along with a corpus of ground truth information. The model must then retrieve relevant evidence and decide whether the claim is supported, refuted or if there is not enough information in the corpus (Thorne et al., 2018). The major difference to our task lies in the dif- ferent semantics of consistency and accuracy. If a summary adds novel and accurate information not present in the original document (e.g., adding background information), the summary is accurate but inconsistent. In the summary inconsistency detection domain, the focus is on detecting any inconsistency, regardless of its accuracy, as prior work has shown that current automatic summariz- ers are predominantly inaccurate when inconsis- tent (Maynez et al.,",
      "chunk_index": 2
    },
    {
      "index": 318,
      "chunk_id": "SummaC2021_chunk_03",
      "source_id": "SummaC2021",
      "text": "is accurate but inconsistent. In the summary inconsistency detection domain, the focus is on detecting any inconsistency, regardless of its accuracy, as prior work has shown that current automatic summariz- ers are predominantly inaccurate when inconsis- tent (Maynez et al., 2020). 2.2 Datasets for Inconsistency Detection Several datasets have been annotated to evalu- ate model performance in inconsistency detection, typically comprising up to two thousand annotated summaries. Datasets are most commonly crowd- annotated with three judgements each, despite some work showing that as many as eight anno- tators are required to achieve high inter-annotator agreement (Falke et al., 2019). Reading the entire original document being summarized is time-consuming, and to amortize this cost, consistency datasets often contain multi- ple summaries, generated by different models, for the same original document. Some datasets consist of an overall consistency label for a summary (e.g., FactCC (Kryscinski et al., 2020)), while others propose a ﬁner-grained typology with up to 8 types of consistency errors (Huang et al., 2020). We include the six largest summary consistency datasets in the S UMMA C Benchmark, and de- scribe them more in detail in Section 4. 2.3 Methods for Inconsistency Detection Due to data limitations, most inconsistency detec- tion methods adapt NLP pipelines from other tasks including QAG models, synthetic classiﬁers, and parsing-based methods. QAG methods follow three steps: (1) question generation (QG), (2) question answering (QA) with the document and the summary, (3) match- ing document and summary answers. A sum- mary is considered consistent if few or no ques- tions have differing answer with the document. A key design choice for these methods lies in the source for question generation. Durmus et al. (2020) generate questions using the summary as a source, making their FEQA method precision- oriented. Scialom et al. (2019) generate questions with the document as a source, creating a recall- focused measure. Scialom et al. (2021) unite both in QuestEval, by generating two sets of questions, sourced from the summary and document respec- tively. We include FEQA and QuestEval in our benchmark results. Synthetic classiﬁers rely on large, synthetic datasets of summaries with inconsistencies, and use those to train a classiﬁer with the expectation that the model generalizes to non-synthetic sum- maries. To generate a synthetic dataset, Kryscinski et al. (2020) propose a set of semantically invari- ant (e.g., paraphrasing) and variant (e.g., sentence negation) text transformations that they apply to a large",
      "chunk_index": 3
    },
    {
      "index": 319,
      "chunk_id": "SummaC2021_chunk_04",
      "source_id": "SummaC2021",
      "text": "expectation that the model generalizes to non-synthetic sum- maries. To generate a synthetic dataset, Kryscinski et al. (2020) propose a set of semantically invari- ant (e.g., paraphrasing) and variant (e.g., sentence negation) text transformations that they apply to a large summarization dataset. FactCC-CLS, the classiﬁer obtained when training on the synthetic dataset, is included in our benchmark results for comparison. Parsing-based methods generate relations through parsing and compute the fraction of summary relations that are compatible with document relations as a precision measure of summary factuality. Goodrich et al. (2019) extract (subject, relation, object) tuples most commonly using OpenIE (Etzioni et al., 2008). In the recent DAE model, Goyal and Durrett (2020) propose to use arc labels from a dependency parser instead of relation triplet. We include the DAE model in our benchmark results. 2.4 Methods for Consistency Correction Complementary to inconsistency detection, some work focused on the task of mitigating inconsis- tency errors during summarization. Approaches fall in two categories: Reinforcement Learning (RL) methods to improve models and stand-alone re-writing methods. RL-methods often rely on an out-of-the-box inconsistency detection model and use reinforce- ment learning to optimize a reward with a con- sistency component. Arumae and Liu (2019) op- timize a QA-based consistency reward, and Nan et al. (2021) streamline a QAG reward by combin- ing the QG and QA model, making it more efﬁ- cient for RL training. Pasunuru and Bansal (2018) leverage an NLI-based component as part of an overall ROUGE-based reward, and Zhang et al. (2020b) use a parsing-based measure in the do- main of medical report summarization. Re-writing methods typically operate as a modular component that is applied after an exist- ing summarization model. Cao et al. (2020) use a synthetic dataset of rule-corrupted summaries to train a post-corrector model, but ﬁnd that this model does not transfer well to real summarizer errors. Dong et al. (2020) propose to use a QAG model to ﬁnd erroneous spans, which are then cor- rected using a post-processing model. Since all methods discussed above for consis- tency correction rely on a model to detect incon- sistencies, they will naturally beneﬁt from more accurate inconsistency detectors. 3 S UMMA C Models We now introduce our S UMMA C models for in- consistency detection. The ﬁrst step common to all models is to apply an out-of-the-box NLI model to generate an NLI Pair Matrixfor a (document, summary) pair. The two",
      "chunk_index": 4
    },
    {
      "index": 320,
      "chunk_id": "SummaC2021_chunk_05",
      "source_id": "SummaC2021",
      "text": "C Models We now introduce our S UMMA C models for in- consistency detection. The ﬁrst step common to all models is to apply an out-of-the-box NLI model to generate an NLI Pair Matrixfor a (document, summary) pair. The two models we present then differ in the way they process this pair ma- trix to produce a single consistency score for a given summary. We also describe the S UMMA C evaluation benchmark, a set of inconsistency de- tection datasets in Section 4. In Section 5, we measure the performance of the SUMMA C models on this benchmark and investigate components of the models, including which NLI model achieves highest performance, which NLI categories should be used, and what textual granularity is most effec- tive. 3.1 Generating the NLI Pair Matrix NLI datasets are predominantly represented at the sentence level. In our pilot experiments, we found that this causes the resulting NLI models to fail in assessing consistency for documents with 50 sen- tences and more. This motivates the following approach. We generate an NLI Pair Matrix by splitting a (document, summary) pair into sentence blocks. The document is split into M blocks, each considered a premise labeled from D1, ..., DM , and the summary is split into N blocks, each con- sidered a hypothesis labeled from S1, ..., SN . Each Di, Sj combination is run through the NLI model, which produces a probability distribu- tion over the three NLI categories (Eij, Cij, Nij) for entailment, contradiction and neutral, respec- tively. If not speciﬁed otherwise, the pair matrix is an M × N matrix consisting of the entailment scores Eij. In Section 5.3.3, we examine the effect of granularity by splitting texts at the paragraph level or binning two sentences at a time. In Sec- tion 5.3.2, we explore the use of the contradiction and neutral categories in our experiments. The example in Figure 1 has M = 4document sentences, and N = 3summary sentences, and the corresponding NLI Pair Matrix is the following: Xpair =   0.02 0 .02 0 .04 0.98 0 .00 0 .00 0.43 0 .99 0 .00 0.00 0 .00 0 .01   The pair matrix can be interpreted as the weights of a bipartite graph which is also illustrated in Fig- ure 1 where the opacity of each edge (i, j) repre- sents the entailment probability Eij. The two S",
      "chunk_index": 5
    },
    {
      "index": 321,
      "chunk_id": "SummaC2021_chunk_06",
      "source_id": "SummaC2021",
      "text": "  The pair matrix can be interpreted as the weights of a bipartite graph which is also illustrated in Fig- ure 1 where the opacity of each edge (i, j) repre- sents the entailment probability Eij. The two S UMMA C models take as input the same NLI Pair Matrix, but differ in the aggregation method to transform the pair matrix into a score. Figure 2 presents an overview of SUMMA CZS and SUMMA CConv. 3.2 S UMMA CZS: Zero-Shot In the SUMMA CZS model, we reduce the pair ma- trix to a one-dimensional vector by taking the maximum ( max) value of each column. On an intuitive level, for each summary sentence, this step consists of retaining the score for the docu- ment sentence that provides the strongest support for each summary sentence. For the example in Figure 1: max(Xpair, axis='col') = [ 0.98 0 .99 0 .04 ] The second step consists of taking the mean of the produced vector, reducing the vector to a scalar which is used as the ﬁnal model score. At a high level, this step aggregates sentence-level informa- tion into a single score for the entire summary. For Document D1 D2 D3 D4 M sentences SummaryS 1 S 2 S 3 NLI N sentences SummaCConv SummaCZS MxN 1xN mean 1x1 max of column MxN 1xN mean 1x1 column histogram (H=5 bins) HxN 1-D convolution Figure 2: Diagram of the S UMMA CZS (top) and SUMMA CConv (bottom) models. Both models utilize the same NLI Pair Matrix (middle) but dif- fer in their processing to obtain a score. The SUM- MACZS is Zero-Shot, and does not have trained parameters. S UMMA CConv uses a convolutional layer trained on a binned version of the NLI Pair Matrix. example, in Figure 1, the score produced by SUM- MACZS would be 0.67. If we removed the third sentence from the summary, the score would in- crease to 0.985. We experiment with replacing the max and mean operators with other operators in Appendix B. 3.3 S UMMA CConv: Convolution One limitation of S UMMA CZS is that it is highly sensitive to extrema, which can be noisy due to the presence of outliers and the imperfect nature of NLI models. In S UMMA CConv, we reduce the reliance on extrema values by instead taking into account the entire distribution of entailment scores for each summary sentence. For",
      "chunk_index": 6
    },
    {
      "index": 322,
      "chunk_id": "SummaC2021_chunk_07",
      "source_id": "SummaC2021",
      "text": "due to the presence of outliers and the imperfect nature of NLI models. In S UMMA CConv, we reduce the reliance on extrema values by instead taking into account the entire distribution of entailment scores for each summary sentence. For each summary sentence, a learned convolutional layer is in charge of converting the entire distribution into a single score. The ﬁrst step of the S UMMA CConv algorithm is to turn each column of the NLI Pair Matrix into a ﬁxed-size histogram that represents the distribu- tion of scores for that given summary sentence. We bin the NLI scores into H evenly spaced bins (e.g., if H = 5 , the bins are [0, 0.2), [0.2, 0.4), [0.4, 0.6), [0.6, 0.8), [0.8, 1)). Thus the ﬁrst summary sentence of the ex- ample in Figure 1 would have the following histogram: [2, 0, 1, 0, 1], because there are two Dataset Size % Positive IAA Source # Summarizer # Sublabel Valid. Test CoGenSumm (Falke et al., 2019) 1281 400 49.8 0.65 C 3 0 XSumFaith (Maynez et al., 2020) 1250 1250 10.2 0.80 X 5 2 Polytope (Huang et al., 2020) 634 634 6.6 - C 10 8 FactCC (Kryscinski et al., 2020) 931 503 85.0 - C 10 0 SummEval (Fabbri et al., 2021) 850 850 90.6 0.7 C 23 4 FRANK (Pagnoni et al., 2021) 671 1575 33.2 0.53 C+X 9 7 Table 1: Statistics of the six datasets in the SUMMA C Benchmark. For each dataset, we report the val- idation and test set sizes, the percentage of summaries with positive (consistent) labels (% Positive), the inter-annotator agreement (when available,IAA), the source of the documents (Source: C for CNN/DM, X for XSum), the number of summarizers evaluated, and the number of sublabels annotated. values between [0.0, 0.2] in the ﬁrst column, one in [0.4, 0.6] and one in [0.8, 1.0]. By producing one histogram for each summary sentence, the binning process in the example of Figure 1 would produce: bin(Xpair) =   2 3 4 0 0 0 1 0 0 0 0 0 1 1 0   The binned matrix is then passed through a 1- D convolution layer with a kernel size of H. The convolution layer scans the summary histograms one at a time, and compiles each into a scalar value for each summary. Finally, the scores of each summary sentence are averaged",
      "chunk_index": 7
    },
    {
      "index": 323,
      "chunk_id": "SummaC2021_chunk_08",
      "source_id": "SummaC2021",
      "text": "1- D convolution layer with a kernel size of H. The convolution layer scans the summary histograms one at a time, and compiles each into a scalar value for each summary. Finally, the scores of each summary sentence are averaged to obtain the ﬁnal summary-level score. In order to learn the weights of the convolu- tion layer, we train the S UMMA CConv model end- to-end with the synthetic training data in FactCC (Kryscinski et al., 2020). The original train- ing dataset contains one million (document, summary) pairs evenly distributed with consis- tent and inconsistent summaries. Because we are only training a small set of H parameters (we use H = 50), we ﬁnd that using a 10,000 sub-sample is sufﬁcient. We train the model using a cross- entropy loss, the Adam optimizer, a batch size of 32 and a learning rate of 10−2. We perform hyper-parameter tuning on a validation set from the FactCC dataset. The number of bins used in the binning process, which corresponds to the number of parameters in the convolution layer, is also a hyper-parameter we tune on the validation set. We ﬁnd that per- formance increases until 50 bins (i.e., a bin width of 0.02) and then plateaus. We use 50 bins in all our experiments. 4 S UMMA C Benchmark To rigorously evaluate the SUMMA C models on a diverse set of summaries with consistency judge- ments, we introduce a new large benchmark dataset, the S UMMA C Benchmark. It comprises the six largest available datasets for summary in- consistency detection, which we standardize to use the same classiﬁcation task. 4.1 Benchmark Standardization We standardize the task of summary inconsis- tency detection by casting it as a binary classiﬁ- cation task. Each dataset contains (document, summary, label) samples, where the label can either be consistent or inconsistent. Each dataset is divided into a validation and test split, with the validation being available for pa- rameter tuning. We used existing validation/test splits created by dataset authors when available. We did not ﬁnd a split for XSumFaith, Polytope, and SummEval, and created one by putting even- indexed samples in a validation split, and odd- indexed samples in the test split. This method of splitting maintains similar class imbalance and summarizer identity with the entire dataset. We computed inter-annotator agreement calcu- lated with Fleiss' Kappa (Fleiss, 1971) on the dataset as an estimate for dataset quality,",
      "chunk_index": 8
    },
    {
      "index": 324,
      "chunk_id": "SummaC2021_chunk_09",
      "source_id": "SummaC2021",
      "text": "samples in the test split. This method of splitting maintains similar class imbalance and summarizer identity with the entire dataset. We computed inter-annotator agreement calcu- lated with Fleiss' Kappa (Fleiss, 1971) on the dataset as an estimate for dataset quality, omitting datasets for which summaries only had a single annotator (Polytope and FactCC). Table 1 summa- rizes dataset statistics and properties. 4.2 Benchmark Datasets We introduce each dataset in the benchmark chronologically, and describe the standardizing procedure. CoGenSumm (Correctness of Generated Summaries, CGS) (Falke et al., 2019) is the ﬁrst introduced dataset for summary inconsis- tency detection, based on models trained on the CNN/DM dataset (Nallapati et al., 2016). The authors proposed that consistency detection should be approached as a ranking problem: given a consistent and inconsistent summary for a common document, a ranking model should score the consistent summary higher. Although innovative, other datasets in the benchmark do not always have positive and negative samples for a given document. We thus map the dataset to a classiﬁcation task by using all inconsistent and consistent summaries as individual samples. XSumFaith (eXtreme Summarization Faithfulness, XSF) (Maynez et al., 2020) is a dataset with models trained on the XSum dataset (Narayan et al., 2018) which consists of more abstractive summaries than CoGenSumm. The authors ﬁnd that standard generators re- main consistent for only 20-30% of generated summaries. The authors differentiate between extrinsic and intrinsic hallucinations (which we call inconsistencies in this work). Extrinsic hallucinations, which involve words or concepts not in the original document can nonetheless be accurate or inaccurate. In order for a summarizer to generate an accurate extrinsic hallucination, the summarizer must possess external world knowl- edge. Because the authors found that the models are primarily inaccurate in terms of extrinsic hallucinations, we map both extrinsic and intrinsic hallucinations to a common inconsistent label. Polytope (Huang et al., 2020) introduces a more extensive typology of summarization errors, based on the Multi-dimensional Quality Metric (Mari- ana, 2014). Each summary is annotated with eight possible errors, as well as a severity level for the error. We standardize this dataset by labeling a summary as inconsistent if it was annotated with any of the ﬁve accuracy errors (and disregarded the three ﬂuency errors). Each summary in Poly- tope was labeled by a single annotator, making it impossible to measure inter-annotator agreement. FactCC (Kryscinski et al., 2020) contains vali- dation and test splits that",
      "chunk_index": 9
    },
    {
      "index": 325,
      "chunk_id": "SummaC2021_chunk_10",
      "source_id": "SummaC2021",
      "text": "the ﬁve accuracy errors (and disregarded the three ﬂuency errors). Each summary in Poly- tope was labeled by a single annotator, making it impossible to measure inter-annotator agreement. FactCC (Kryscinski et al., 2020) contains vali- dation and test splits that are entirely annotated by authors of the paper, because attempts at crowd- sourced annotation yielded low inter-annotator agreement. Prior work (Gillick and Liu, 2010) shows that there can be divergence in annota- tions between experts and non-experts in sum- marization, and because the authors of the pa- per are NLP researchers familiar with the limi- tations of automatic summarizations, we expect that FactCC annotations differs in quality from other datasets. FactCC also introduces a synthetic dataset by modifying consistent summaries with semantically variant rules. We use a sub-portion of this synthetic dataset to train the S UMMA CConv model. SummEval (Fabbri et al., 2021) contains sum- marizer outputs from seven extractive models and sixteen abstractive models. Each summary was la- beled using a 5-point Likert scale along four cat- egories: coherence, consistency, ﬂuency, and rel- evance by 3 annotators. We label summaries as consistent if all annotators gave a score of 5 in con- sistency, and inconsistent otherwise. FRANK (Pagnoni et al., 2021) contains anno- tations for summarizers trained on both CNN/DM and XSum, with each summary annotated by three crowd-workers. The authors propose a new ty- pology with seven error types, organized into se- mantic frame errors, discourse errors and content veriﬁability errors. The authors conﬁrm that mod- els trained on the more abstractive XSum dataset generate a larger proportion of inconsistent sum- maries, compared to models trained on CNN/DM. We label summaries as consistent if a majority of annotators labeled the summary as containing no error. 4.3 Benchmark Evaluation Metrics With each dataset in the S UMMA C Benchmark converted to a binary classiﬁcation task, we now discuss the choice of appropriate evaluation met- rics for the benchmark. Previous work on each dataset in the benchmark used different evaluation methods, falling into three main categories. First, CoGenSumm proposes a re-ranking based measure, requiring pairs of consistent and incon- sistent summaries for any document evaluated; this information is not available in several datasets in the benchmark. Second, XSumFaith, SummEval and FRANK report on correlation of various metrics with hu- man annotations. Correlation has some advan- tages, such as not requiring a threshold and be- ing compatible with the Likert-scale",
      "chunk_index": 10
    },
    {
      "index": 326,
      "chunk_id": "SummaC2021_chunk_11",
      "source_id": "SummaC2021",
      "text": "available in several datasets in the benchmark. Second, XSumFaith, SummEval and FRANK report on correlation of various metrics with hu- man annotations. Correlation has some advan- tages, such as not requiring a threshold and be- ing compatible with the Likert-scale annotations of SummEval, however it is an uncommon choice to measure performance of a classiﬁer due to the discrete and binary label. Third, authors of FactCC measured model per- formance using binary F1 score, and balanced ac- curacy, which corrects unweighed accuracy with the class imbalance ratio, so that majority class voting obtains a score of 50%. The datasets have widely varying class imbal- ances, ranging from 6% to 91% positive samples. Therefore, we select balanced accuracy (Broder- sen et al., 2010) as the primary evaluation metric for the SUMMA C Benchmark. Balanced accuracy is deﬁned as: BAcc = 1 ( TP TP + FN + TN TN + FP ) (1) Where TP stands for true positive, FP false pos- itive, TN true negative, and FN false negative. The choice of metric is based on the fact that ac- curacy is a conceptually simple, interpretable met- ric, and that adjusting the class imbalance out of the metric makes the score more uniform across datasets. The balanced accuracy metric requires models to output a binary label (i.e., not a scalar score), which for most models requires the selection of a threshold in the score. The threshold is selected using the validation set, allowing for a different threshold for each dataset in the benchmark. Per- formance on the benchmark is the unweighted av- erage of performance on the six datasets. We choose Area Under the Curve of the Re- ceiver Operating Chart (ROC-AUC) as a sec- ondary evaluation metric, a common metric to summarize a classiﬁer's performance at different threshold levels (Bradley, 1997). 5 Results We compared the SUMMA C models against a wide array of baselines and state-of-the-art methods. 5.1 Comparison Models We evaluated the following models on the S UM- MAC Benchmark: NER Overlap uses the spaCy named entity recognition (NER) model (Honnibal et al., 2020) to detect when an entity present in the summary is not present in the document. This model, adapted from (Laban et al., 2021), considers only a sub- set of entity types as hallucinations (i.e.,PERSON, LOCATION, ORGANIZATION, etc.) MNLI-doc is a Roberta (Liu et al., 2019) model ﬁnetuned on the MNLI dataset (Williams et",
      "chunk_index": 11
    },
    {
      "index": 327,
      "chunk_id": "SummaC2021_chunk_12",
      "source_id": "SummaC2021",
      "text": "the document. This model, adapted from (Laban et al., 2021), considers only a sub- set of entity types as hallucinations (i.e.,PERSON, LOCATION, ORGANIZATION, etc.) MNLI-doc is a Roberta (Liu et al., 2019) model ﬁnetuned on the MNLI dataset (Williams et al., 2018). The document is used as the premise and the summary as a hypothesis, and we use the pre- dicted probability of entailment as a score, similar to prior work on using NLI models for inconsis- tency detection (Kryscinski et al., 2020). FactCC-CLS is a Roberta-base model ﬁne- tuned on the synthetic training portion of the FactCC dataset. Although trained solely on artiﬁ- cially created inconsistent summaries, prior work showed the model to be competitive on the FactCC and FRANK datasets. DAE (Goyal and Durrett, 2020) is a parsing- based model using the default model and hyper- parameters provided by the authors of the paper2. FEQA (Durmus et al., 2020) is a QAG method, using the default model and hyper-parameters pro- vided by the authors of the paper3. QuestEval (Scialom et al., 2021) is a QAG method taking both precision and recall into ac- count. We use the default model and hyper- parameters provided by the authors of the paper 4. The model has an option to use an additional ques- tion weighter, however experiments revealed that the weighter lowered overall performance on the validation portion of the S UMMA C Benchmark, and we compare to the model without weighter. 5.2 S UMMA C Benchmark Results Balanced accuracy results are summarized in Ta- ble 2. We ﬁnd that the S UMMA C models achieve the two best performances in the benchmark. SUMMA CConv achieves the best benchmark per- formance at 74.4%, 5 points above QuestEval, the best method not involving NLI. Looking at the models' ability to generalize across datasets and varying scenarios of inconsis- tency detection provides interesting insights. For example, the FactCC-CLS model achieves strong performance on the FactCC dataset, but close to lowest performance on FRANK and XSumFaith. In comparison, S UMMA C model performance is strong across the board. The strong improvement from the S UMMA CZS to SUMMA CConv also shines a light on the impor- tance of considering the entire distribution of doc- ument scores for each summary sentence, instead of taking only the maximum score: the S UM- MACConv model learns to look at the distribution and makes more robust decisions, leading",
      "chunk_index": 12
    },
    {
      "index": 328,
      "chunk_id": "SummaC2021_chunk_13",
      "source_id": "SummaC2021",
      "text": "the impor- tance of considering the entire distribution of doc- ument scores for each summary sentence, instead of taking only the maximum score: the S UM- MACConv model learns to look at the distribution and makes more robust decisions, leading to gains in performance. 2https://github.com/tagoyal/dae-factuality 3https://github.com/esdurmus/feqa 4https://github.com/ThomasScialom/QuestEval SUMMAC Benchmark Datasets Model Type Model Name CGS XSF Polytope FactCC SummEval FRANK Overall Doc./min. Baseline NER-Overlap 53.0 63.3 52.0 55.0 56.8 60.9 56.8 55,900 MNLI-doc 57.6 57.5 61.0 61.3 66.6 63.6 61.3 6,200 Classiﬁer FactCC-CLS 63.1 57.6 61.0 75.9 60.1 59.4 62.8 13,900 Parsing DAE 63.4 50.8 62.8 75.9 70.3 61.7 64.2 755 QAG FEQA 61.0 56.0 57.8 53.6 53.8 69.9 58.7 33.9 QuestEval 62.6 62.1 70.3* 66.6 72.5 82.1 69.4 22.7 NLI SUMMACZS 70.4* 58.4 62.0 83.8* 78.7 79.0 72.1* 435 SUMMACConv 64.7 66.4* 62.7 89.5** 81.7** 81.6 74.4 ** 433 Table 2: Performance of Summary Inconsistency Detection models on the test set of the S UMMA C Benchmark. Balanced accuracy is computed for each model on the six datasets in the benchmark, and the average is computed as the overall performance on the benchmark. We obtain conﬁdence intervals comparing the S UMMA C models to prior work: * indicates an improvement with 95% conﬁdence, and ** 99% conﬁdence (details in Section 5.2.1). The results of the throughput analysis of Section 5.2.2 are in column Doc./min (Documents per minute). The table of results with the ROC-AUC metric, the secondary metric of the SUMMA C Benchmark is included in Appendix A2, echoing the trends seen with the balanced accuracy metric. 5.2.1 Statistical Testing We aim to determine whether the performance im- provements of the S UMMA C models are statisti- cally signiﬁcant. For each dataset of the bench- mark, we perform two tests through bootstrap re- sampling (Efron, 1982), comparing each of the SUMMA C models to the best-performing model from prior work. We perform interval comparison at two signiﬁcance level: p = 0.05 and p = 0.01, and apply the Bonferroni correction (Bonferroni, 1935) as we perform several tests on each dataset. We summarize which improvements are signiﬁ- cant in Table 2, and perform a similar testing pro- cedure for the ROC-AUC results in Table A2. SUMMA C models lead to a statistically signif- icant improvement on CoGenSumm, XSumFaith, FactCC and SummEval. QuestEval outperforms the SUMMA C models on Polytope at a conﬁdence of 95%. On the FRANK dataset, QuestEval and",
      "chunk_index": 13
    },
    {
      "index": 329,
      "chunk_id": "SummaC2021_chunk_14",
      "source_id": "SummaC2021",
      "text": "ROC-AUC results in Table A2. SUMMA C models lead to a statistically signif- icant improvement on CoGenSumm, XSumFaith, FactCC and SummEval. QuestEval outperforms the SUMMA C models on Polytope at a conﬁdence of 95%. On the FRANK dataset, QuestEval and SUMMA CConv achieve highest performance with no statistical difference. Overall on the bench- mark, both SUMMA C models signiﬁcantly outper- form prior work, S UMMA CZS at a p = 0.05 sig- niﬁcance level and SUMMA CConv at p = 0.01. 5.2.2 Computational Cost Comparison Computational cost of the method is an important practical factor to consider when choosing a model to use, as some applications such as training with a generator with Reinforcement Learning might re- quire a minimum throughput from the model (i.e., number of documents processed by the model per unit of time). A common method to compare algorithms is using computational complexity analysis, com- puting the amount of resources (time, space) needed as the size of the input varies. Com- putational complexity analysis is impractical in our case, as the units of analysis differ between models, and do not allow for a direct compari- son. More speciﬁcally, some of the model's com- plexity scales with the number of sub-word units in the document ( MNLI-doc, FactCC-CLS), some with the number of entities in a document (NER-Overlap, DAE, QuestEval), and some with number of sentences (the SUMMA C models). We instead compare models by measuring throughput on a ﬁxed dataset using a common hardware setup. More precisely, we measured the processing time of each model on the 503 docu- ments in the test set of FactCC (with an average of 33.2 sentences per document), running a single Quadro RTX 8000 GPU. For prior work, we used implementation publicly released by the authors, and made a best effort to use the model at an ap- propriate batch size for a fair comparison. The result of the throughput analysis is included in Table 2 (column Docs./min.). S UMMA C mod- els are able to process around 430 documents per minute, which is much lower than some of the baselines capable of processing more than 10,000 documents per minute. However, QAG meth- Performance Architecture NLI Dataset ZS Conv Dec. Attn SNLI 56.9 56.4 SNLI 66.6 64.0 BERT Base MNLI 69.5 69.8 MNLI+VitaminC 67.9 71.2 SNLI 66.6 62.4 SNLI+MNLI+ANLI 69.9 71.7 BERT Large VitaminC 71.1 72.8 MNLI 70.9 73.0 MNLI+VitaminC 72.1",
      "chunk_index": 14
    },
    {
      "index": 330,
      "chunk_id": "SummaC2021_chunk_15",
      "source_id": "SummaC2021",
      "text": "QAG meth- Performance Architecture NLI Dataset ZS Conv Dec. Attn SNLI 56.9 56.4 SNLI 66.6 64.0 BERT Base MNLI 69.5 69.8 MNLI+VitaminC 67.9 71.2 SNLI 66.6 62.4 SNLI+MNLI+ANLI 69.9 71.7 BERT Large VitaminC 71.1 72.8 MNLI 70.9 73.0 MNLI+VitaminC 72.1 74.4 Table 3: Effect of NLI model choice on S UM- MAC models performance. For each NLI model, we include the balanced accuracy scores of S UM- MACZS and SUMMA CConv. BERT X corresponds to a BERT or other pre-trained models of similar size. ods are more than 10 times slower than S UM- MAC models, processing only 20-40 documents per minute. 5.3 Further Results We now examine how different components and design choices affect S UMMA C model perfor- mance. 5.3.1 Choice of NLI Model SUMMA C models rely on an NLI model at their core, which consists of choosing two main com- ponents: a model architecture, and a dataset to train on. We investigate the effect of both of these choices on the performance of S UMMA C models on the benchmark. Regarding model architectures, we experiment with the decomposable attention model (Parikh et al., 2016), which is a pre-Transformer architec- ture model that was shown to achieve high perfor- mance on SNLI, as well as Transformer base and Transformer Large architectures. With respect to datasets, we include models trained on standard NLI datasets such as SNLI (Bowman et al., 2015) and MNLI (Williams et al., 2018), as well as more recent datasets such as Ad- versarial NLI (Nie et al., 2019) and Vitamin C (Schuster et al., 2021). Results are summarized in Table 3, and we em- phasize three trends. First, the low performance of the decomposable attention model used in experi- ments in prior work (Falke et al., 2019), conﬁrms Category S UMMACConvPerformance E N C VITC+MNLI ANLI MNLI \u0013 74.4 69.2 72.6 \u0013 71.2 55.8 66.4 \u0013 72.5 69.2 72.6 \u0013 \u0013 73.1 69.6 72.6 \u0013 \u0013 74.0 70.2 73.0 \u0013 \u0013 72.5 69.2 72.6 \u0013 \u0013 \u0013 74.0 69.7 73.0 Table 4 : Effect of NLI category inclusion on SUMMA CConv performance. Models had access to different subsets of the three category predic- tions ( Entailment, Neutral, Contradiction), with performance measured in terms of balanced ac- curacy. Experiments were performed with 3 NLI models: Vitamic C+MNLI, ANLI and MNLI. that less recent NLI models did not transfer well to summary inconsistency detection. Second, NLI",
      "chunk_index": 15
    },
    {
      "index": 331,
      "chunk_id": "SummaC2021_chunk_16",
      "source_id": "SummaC2021",
      "text": "( Entailment, Neutral, Contradiction), with performance measured in terms of balanced ac- curacy. Experiments were performed with 3 NLI models: Vitamic C+MNLI, ANLI and MNLI. that less recent NLI models did not transfer well to summary inconsistency detection. Second, NLI models based on pre-trained Transformer architectures all achieve strong per- formance on the benchmark, and average increase of 1.3 percentage points when going from a base to a large architecture. Third, the choice of NLI dataset has a strong inﬂuence on overall performance. SNLI leads to lowest performance, which is expected as its tex- tual domain is based on image captions, which are dissimilar to the news domain. MNLI and Vitamin C trained models both achieve close to the best performance, and training on both jointly leads to the best model, which we designate as the de- fault NLI model for the SUMMA C models (i.e., the model included in Table 2). The latter two trends point to the fact that im- provements in the ﬁeld of NLI lead to improve- ments in the S UMMA C models, and we can ex- pect that future progress in the NLI community will translate to gains of performance when inte- grated into the SUMMA C model. We relied on trained models available in Hug- gingFace's Model Hub (Wolf et al., 2020). Details in Appendix A. 5.3.2 Choice of NLI Category The NLI task is a three-way classiﬁcation task, yet most prior work has limited usage of the model to the use of the entailment probability for incon- sistency detection (Kryscinski et al., 2020; Falke et al., 2019). We run a systematic experiment by training multiple S UMMA CConv models which Performance Granularity MNLI MNLI + VitC Document Summary ZS Conv ZS Conv Full Full 56.4 - 72.1 - Sentence 57.4 - 73.1 - Paragraph Full 59.8 61.8 69.8 71.2 Sentence 65.2 64.7 72.6 74.3 Two Sent. Full 64.0 63.8 69.7 71.3 Sentence 71.2 73.5 72.5 74.7 Sentence Full 58.7 61.1 68.4 69.4 Sentence 70.3 73.0 72.1 74.4 Table 5: Effect of granularity choice on S UM- MAC models performance. We tested four gran- ularities on the document side: full, paragraph, two sentence and sentence, and two granularities on the summary side: full and sentence. Perfor- mance of the four models is measured in balanced accuracy on the benchmark test set. have access to varying subsets of the NLI labels, and measure the",
      "chunk_index": 16
    },
    {
      "index": 332,
      "chunk_id": "SummaC2021_chunk_17",
      "source_id": "SummaC2021",
      "text": "and sentence, and two granularities on the summary side: full and sentence. Perfor- mance of the four models is measured in balanced accuracy on the benchmark test set. have access to varying subsets of the NLI labels, and measure the impact on overall performance. Results are summarized in Table 4. Using solely the entailment category leads to strong perfor- mance for all models. However, explicitly includ- ing the contradiction label as well leads to small boosts in performance for the ANLI and MNLI models. With future NLI models being potentially more nuanced and calibrated, it is possible that incon- sistency detectors models will be able to rely on scores from several categories. 5.3.3 Choice of Granularity So far, we've reported experiments primarily with a sentence-level granularity, as it matches the granularity of NLI datasets. One can imagine cases where sentence-level granularity might be limiting. For example, in the case of a summary performing a sentence fusion operation, an NLI model might not be able to correctly predict entail- ment of the fused sentence, seeing only one sen- tence at a time. To explore this facet further, we experiment with modifying the granularity of both the doc- ument and the summary. With regards to docu- ment granularity, we consider four granularities: (1) full text, the text is treated as a single block, (2) paragraph-level granularity, the text is separated into paragraph blocks, (3) two-sentence granular- ity, the text is separated into blocks of contigu- ous sentences of size two (i.e., block 1 contains sentence 1-2, block 2 contains sentence 3-4), and (4) sentence-level, splitting text at individual sen- tences. For the summary granularity, we only con- sider two granularities: (1) full text, and (2) sen- tence, because other granularities are less applica- ble since summaries usually consist of three sen- tences or fewer. We study the total of 8 (document, summary) granularity combinations with the two best-performing NLI models of Table 2: MNLI and Vitamin C, each included as S UM- MACZS and SUMMA CConv models.5 Results for the granularity experiments are summarized in Table 5. Overall, ﬁner granularities lead to better performance, with (sentence,sentence) and (two sent, sentence) achieving highest performance across all four models. The MNLI-only trained model achieves low- est performance when used with full text gran- ularity on the document level, and perfor- mance steadily increases from 56.4% to 73.5% as granularity is made ﬁner both on",
      "chunk_index": 17
    },
    {
      "index": 333,
      "chunk_id": "SummaC2021_chunk_18",
      "source_id": "SummaC2021",
      "text": "performance across all four models. The MNLI-only trained model achieves low- est performance when used with full text gran- ularity on the document level, and perfor- mance steadily increases from 56.4% to 73.5% as granularity is made ﬁner both on the doc- ument and summary side. Results for the MNLI+VitaminC model vary less with changing granularity, showcasing that the model is per- haps more robust to different granularity lev- els. However the (two sent, sentence) and (sentence,sentence) settings achieve highest performance, implying that ﬁner granular- ity remains valuable. For all models, performance degrades in cases where granularity on the document level is ﬁner than summary granularity. For exam- ple the (sentence, full) or (two sent, full) combinations lead to some of the low- est performance. This is expected, as in cases in which summaries have several sentences, it is un- likely that they will fully be entailed by a single document sentence. This implies that granularity on the document side should be coarser or equal the summary's granularity. Overall, we ﬁnd that ﬁner granularity for the document and summary is beneﬁcial in terms of performance and recommend the use of a (sentence, sentence) granularity combi- nation. 5We skip S UMMA CConv experiments involving full text granularity on the document-side, as that case reduces the binning process to having a single non-zero value. 6 Discussion and Future Work Improvements on the Benchmark. The mod- els we introduced in this paper are just a ﬁrst step towards harnessing NLI models for inconsistency detection. Future work could explore a number of improvements: combining the predictions of mul- tiple NLI models, or combining multiple granular- itiy levels, for example through multi-hop reason- ing (Zhao et al., 2019). Interpretability of model output . If a model can pinpoint which portion of a summary is incon- sistent, some work has shown that corrector mod- els can effectively re-write the problematic por- tions and often remove the inconsistency (Dong et al., 2020). Furthermore, ﬁne-grained consis- tency scores can be incorporated into visual analy- sis tools for summarization such as SummViz (Vig et al., 2021). The SUMMA CZS model is directly in- terpretable, whereas the S UMMA CConv is slightly more opaque, due to the inability to trace back a low score to a single sentence in the document be- ing invalidated. Improving the interpretability of the S UMMA CConv model is another open area for future work.",
      "chunk_index": 18
    },
    {
      "index": 334,
      "chunk_id": "SummaC2021_chunk_19",
      "source_id": "SummaC2021",
      "text": "is slightly more opaque, due to the inability to trace back a low score to a single sentence in the document be- ing invalidated. Improving the interpretability of the S UMMA CConv model is another open area for future work. Beyond news summarization. The six datasets in the S UMMA C Benchmark contain summaries from the news domain, one of the most common application of summarization technology. Recent efforts to expand the application of summarization to new domains such as legal (Kornilova and Ei- delman, 2019) or scholarly (Cachola et al., 2020) text will hopefully lead to the study of inconsis- tency detection in these novel domains, and per- haps even out of summarization on tasks such as text simpliﬁcation, or code generation. Towards Consistent Summarization. Incon- sistency detection is but a ﬁrst step in eliminating inconsistencies from summarization. Future work can include more powerful inconsistency detectors in the training of next generation summarizers to reduce the prevalence of inconsistencies in gener- ated text. 7 Conclusion We introduce SUMMA CZS and SUMMA CConv, two NLI-based models for summary inconsistency de- tection based on the key insight that NLI models require sentence-level input to work best. Both models achieve strong performance on the S UM- MAC Benchmark, a new diverse and standard- ized collection of the six largest datasets for in- consistency detection. S UMMA CConv outperforms all prior work with a balanced accuracy score of 74.4%, an improvement of ﬁve absolute percent- age points over the best baseline. To the best of our knowledge, this the ﬁrst successful attempt at adapting NLI models for inconsistency detection, and we believe that there are many exciting oppor- tunities for further improvements and applications of our methods. Acknowledgments We would like to thank Katie Stasaski, Dongyeop Kang, the TACL reviewers and editors for their helpful comments, as well as Artidoro Pagnoni for helpful pointers during the project. This work was supported by a Microsoft BAIR Commons grant as well as a Microsoft Azure Sponsorship. References Kristjan Arumae and Fei Liu. 2019. Guid- ing extractive summarization with question- answering rewards. In Proceedings of the 2019 Conference of the North American Chap- ter of the Association for Computational Lin- guistics: Human Language Technologies, Vol- ume 1 (Long and Short Papers), pages 2566- 2577. Carlo E Bonferroni. 1935. Il calcolo delle assi- curazioni su gruppi di teste. Studi in onore del professore salvatore ortu carboni, pages",
      "chunk_index": 19
    },
    {
      "index": 335,
      "chunk_id": "SummaC2021_chunk_20",
      "source_id": "SummaC2021",
      "text": "Association for Computational Lin- guistics: Human Language Technologies, Vol- ume 1 (Long and Short Papers), pages 2566- 2577. Carlo E Bonferroni. 1935. Il calcolo delle assi- curazioni su gruppi di teste. Studi in onore del professore salvatore ortu carboni, pages 13-60. Samuel Bowman, Gabor Angeli, Christopher Potts, and Christopher D Manning. 2015. A large annotated corpus for learning natural lan- guage inference. In Proceedings of the 2015 Conference on Empirical Methods in Natural Language Processing, pages 632-642. Andrew P Bradley. 1997. The use of the area under the roc curve in the evaluation of ma- chine learning algorithms. Pattern recognition, 30(7):1145-1159. Kay Henning Brodersen, Cheng Soon Ong, Klaas Enno Stephan, and Joachim M Buhmann. 2010. The balanced accuracy and its posterior distribution. In 2010 20th international confer- ence on pattern recognition, pages 3121-3124. IEEE. Isabel Cachola, Kyle Lo, Arman Cohan, and Daniel S Weld. 2020. Tldr: Extreme summa- rization of scientiﬁc documents. In Proceed- ings of the 2020 Conference on Empirical Meth- ods in Natural Language Processing: Findings, pages 4766-4777. Meng Cao, Yue Dong, Jiapeng Wu, and Jackie Chi Kit Cheung. 2020. Factual error correc- tion for abstractive summarization models. In Proceedings of the 2020 Conference on Empir- ical Methods in Natural Language Processing (EMNLP), pages 6251-6258. Yue Dong, Shuohang Wang, Zhe Gan, Yu Cheng, Jackie Chi Kit Cheung, and Jingjing Liu. 2020. Multi-fact correction in abstractive text sum- marization. In Proceedings of the 2020 Con- ference on Empirical Methods in Natural Lan- guage Processing (EMNLP), pages 9320-9331. Esin Durmus, He He, and Mona Diab. 2020. Feqa: A question answering evaluation framework for faithfulness assessment in abstractive summa- rization. In Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics, pages 5055-5070. Bradley Efron. 1982. The jackknife, the bootstrap and other resampling plans. In CBMS-NSF Re- gional Conference Series in Applied Mathemat- ics. Oren Etzioni, Michele Banko, Stephen Soderland, and Daniel S Weld. 2008. Open information ex- traction from the web. Communications of the ACM, 51(12):68-74. Alexander R Fabbri, Wojciech Kry ´sci´nski, Bryan McCann, Caiming Xiong, Richard Socher, and Dragomir Radev. 2021. Summeval: Re- evaluating summarization evaluation. Transac- tions of the Association for Computational Lin- guistics, 9:391-409. Tobias Falke, Leonardo FR Ribeiro, Prasetya Ajie Utama, Ido Dagan, and Iryna Gurevych. 2019. Ranking generated summaries by correctness: An interesting but challenging application for natural language inference. In Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics,",
      "chunk_index": 20
    },
    {
      "index": 336,
      "chunk_id": "SummaC2021_chunk_21",
      "source_id": "SummaC2021",
      "text": "Falke, Leonardo FR Ribeiro, Prasetya Ajie Utama, Ido Dagan, and Iryna Gurevych. 2019. Ranking generated summaries by correctness: An interesting but challenging application for natural language inference. In Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics, pages 2214-2220. Joseph L Fleiss. 1971. Measuring nominal scale agreement among many raters. Psychological bulletin, 76(5):378. Dan Gillick and Yang Liu. 2010. Non-expert eval- uation of summarization systems is risky. In Proceedings of the NAACL HLT 2010 Workshop on Creating Speech and Language Data with Amazon's Mechanical Turk, pages 148-151. Ben Goodrich, Vinay Rao, Peter J Liu, and Mo- hammad Saleh. 2019. Assessing the factual ac- curacy of generated text. In Proceedings of the 25th ACM SIGKDD International Conference on Knowledge Discovery & Data Mining, pages 166-175. Tanya Goyal and Greg Durrett. 2020. Evaluating factuality in generation with dependency-level entailment. arXiv preprint arXiv:2010.05478. Matthew Honnibal, Ines Montani, Soﬁe Van Lan- deghem, and Adriane Boyd. 2020. spaCy: Industrial-strength Natural Language Process- ing in Python. Dandan Huang, Leyang Cui, Sen Yang, Guang- sheng Bao, Kun Wang, Jun Xie, and Yue Zhang. 2020. What have we achieved on text sum- marization? In Proceedings of the 2020 Con- ference on Empirical Methods in Natural Lan- guage Processing (EMNLP), pages 446-469. Anastassia Kornilova and Vladimir Eidelman. 2019. Billsum: A corpus for automatic sum- marization of us legislation. In Proceedings of the 2nd Workshop on New Frontiers in Summa- rization, pages 48-56. Wojciech Kryscinski, Bryan McCann, Caiming Xiong, and Richard Socher. 2020. Evaluating the factual consistency of abstractive text sum- marization. In Proceedings of the 2020 Con- ference on Empirical Methods in Natural Lan- guage Processing (EMNLP), pages 9332-9346. Philippe Laban, Tobias Schnabel, Paul Bennett, and Marti A. Hearst. 2021. Keep it simple: Unsupervised simpliﬁcation of multi-paragraph text. In Proceedings of the 59th Annual Meeting of the Association for Computational Linguis- tics and the 11th International Joint Confer- ence on Natural Language Processing (Volume 1: Long Papers), pages 6365-6378, Online. As- sociation for Computational Linguistics. Yinhan Liu, Myle Ott, Naman Goyal, Jingfei Du, Mandar Joshi, Danqi Chen, Omer Levy, Mike Lewis, Luke Zettlemoyer, and Veselin Stoyanov. 2019. Roberta: A robustly opti- mized bert pretraining approach. arXiv preprint arXiv:1907.11692. Valerie R Mariana. 2014. The Multidimensional Quality Metric (MQM) framework: A new framework for translation quality assessment. Brigham Young University. Joshua Maynez, Shashi Narayan, Bernd Bohnet, and Ryan McDonald. 2020. On faithfulness and factuality in abstractive summarization. arXiv",
      "chunk_index": 21
    },
    {
      "index": 337,
      "chunk_id": "SummaC2021_chunk_22",
      "source_id": "SummaC2021",
      "text": "preprint arXiv:1907.11692. Valerie R Mariana. 2014. The Multidimensional Quality Metric (MQM) framework: A new framework for translation quality assessment. Brigham Young University. Joshua Maynez, Shashi Narayan, Bernd Bohnet, and Ryan McDonald. 2020. On faithfulness and factuality in abstractive summarization. arXiv preprint arXiv:2005.00661. Ramesh Nallapati, Bowen Zhou, Cicero dos San- tos, Ça ˘glar Gulçehre, and Bing Xiang. 2016. Abstractive text summarization using sequence- to-sequence rnns and beyond. In Proceedings of The 20th SIGNLL Conference on Computa- tional Natural Language Learning, pages 280- 290. Feng Nan, Cicero Nogueira dos Santos, Henghui Zhu, Patrick Ng, Kathleen McKeown, Ramesh Nallapati, Dejiao Zhang, Zhiguo Wang, An- drew O Arnold, and Bing Xiang. 2021. Improv- ing factual consistency of abstractive summa- rization via question answering. arXiv preprint arXiv:2105.04623. Shashi Narayan, Shay B Cohen, and Mirella La- pata. 2018. Don't give me the details, just the summary! topic-aware convolutional neu- ral networks for extreme summarization. In Proceedings of the 2018 Conference on Empir- ical Methods in Natural Language Processing, pages 1797-1807. Yixin Nie, Adina Williams, Emily Dinan, Mo- hit Bansal, Jason Weston, and Douwe Kiela. 2019. Adversarial nli: A new benchmark for natural language understanding. arXiv preprint arXiv:1910.14599. Artidoro Pagnoni, Vidhisha Balachandran, and Yulia Tsvetkov. 2021. Understanding factual- ity in abstractive summarization with frank: A benchmark for factuality metrics. In NAACL. Ankur Parikh, Oscar Täckström, Dipanjan Das, and Jakob Uszkoreit. 2016. A decomposable at- tention model for natural language inference. In Proceedings of the 2016 Conference on Empir- ical Methods in Natural Language Processing, pages 2249-2255. Ramakanth Pasunuru and Mohit Bansal. 2018. Multi-reward reinforced summarization with saliency and entailment. In Proceedings of the 2018 Conference of the North American Chap- ter of the Association for Computational Lin- guistics: Human Language Technologies, Vol- ume 2 (Short Papers), pages 646-653. Tal Schuster, Adam Fisch, and Regina Barzilay. 2021. Get your vitamin c! robust fact veriﬁ- cation with contrastive evidence. In Proceed- ings of the 2021 Conference of the North Amer- ican Chapter of the Association for Computa- tional Linguistics: Human Language Technolo- gies, pages 624-643. Thomas Scialom, Paul-Alexis Dray, Patrick Galli- nari, Sylvain Lamprier, Benjamin Piwowarski, Jacopo Staiano, and Alex Wang. 2021. Queste- val: Summarization asks for fact-based evalua- tion. arXiv preprint arXiv:2103.12693. Thomas Scialom, Sylvain Lamprier, Benjamin Pi- wowarski, and Jacopo Staiano. 2019. Answers unite! unsupervised metrics for reinforced sum- marization models. In Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th",
      "chunk_index": 22
    },
    {
      "index": 338,
      "chunk_id": "SummaC2021_chunk_23",
      "source_id": "SummaC2021",
      "text": "tion. arXiv preprint arXiv:2103.12693. Thomas Scialom, Sylvain Lamprier, Benjamin Pi- wowarski, and Jacopo Staiano. 2019. Answers unite! unsupervised metrics for reinforced sum- marization models. In Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Pro- cessing (EMNLP-IJCNLP), pages 3246-3256. James Thorne, Andreas Vlachos, Christos Christodoulopoulos, and Arpit Mittal. 2018. Fever: a large-scale dataset for fact extraction and veriﬁcation. In Proceedings of the 2018 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long Papers), pages 809-819. Jesse Vig, Wojciech Kryscinski, Karan Goel, and Nazneen Fatema Rajani. 2021. Sum- mvis: Interactive visual analysis of models, data, and evaluation for text summarization. arXiv preprint arXiv:2104.07605. Adina Williams, Nikita Nangia, and Samuel Bow- man. 2018. A broad-coverage challenge corpus for sentence understanding through inference. In Proceedings of the 2018 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long Papers), pages 1112-1122. Thomas Wolf, Lysandre Debut, Victor Sanh, Julien Chaumond, Clement Delangue, Anthony Moi, Pierric Cistac, Tim Rault, Rémi Louf, Morgan Funtowicz, Joe Davison, Sam Shleifer, Patrick von Platen, Clara Ma, Yacine Jernite, Julien Plu, Canwen Xu, Teven Le Scao, Syl- vain Gugger, Mariama Drame, Quentin Lhoest, and Alexander M. Rush. 2020. Transformers: State-of-the-art natural language processing. In Proceedings of the 2020 Conference on Empir- ical Methods in Natural Language Processing: System Demonstrations, pages 38-45, Online. Association for Computational Linguistics. Jingqing Zhang, Yao Zhao, Mohammad Saleh, and Peter Liu. 2020a. Pegasus: Pre-training with extracted gap-sentences for abstractive summarization. In International Conference on Machine Learning , pages 11328-11339. PMLR. Yuhao Zhang, Derek Merck, Emily Tsai, Christo- pher D Manning, and Curtis Langlotz. 2020b. Optimizing the factual correctness of a sum- mary: A study of summarizing radiology re- ports. In Proceedings of the 58th Annual Meet- ing of the Association for Computational Lin- guistics, pages 5108-5120. Chen Zhao, Chenyan Xiong, Corby Rosset, Xia Song, Paul Bennett, and Saurabh Tiwary. 2019. Transformer-xh: Multi-evidence reasoning with extra hop attention. In International Confer- ence on Learning Representations. Operator 2 Op. 1 Min Mean Max Min 53.1 55.7 57.4 Mean 60.5 62.8 62.0 Max 68.8 72.1 69.1 Table A1: Effect of operator choice on the per- formance of the S UMMA CZS model, measured in terms of balanced accuracy. Operator 1 reduces the row dimension of the NLI Pair Ma- trix,",
      "chunk_index": 23
    },
    {
      "index": 339,
      "chunk_id": "SummaC2021_chunk_24",
      "source_id": "SummaC2021",
      "text": "62.8 62.0 Max 68.8 72.1 69.1 Table A1: Effect of operator choice on the per- formance of the S UMMA CZS model, measured in terms of balanced accuracy. Operator 1 reduces the row dimension of the NLI Pair Ma- trix, and Operator 2 reduces the column di- mension. Appendix A NLI Model Origin We list the NLI models we used throughout the paper, which can be retrieved on HuggingFace's model hub6. BERT stands for any Pre-trained bi- directional Transformer of an equivalent size: • boychaboy/SNLI_roberta-base BERT Base+SNLI • microsoft/deberta-base-mnli BERT Base+MNLI • tals/albert-base-vitaminc-mnli BERT Base + MNLI + VitaminC • boychaboy/SNLI_roberta-large BERT Large+SNLI • tals/albert-xlarge-vitaminc Bert Large+VitaminC • roberta-large-mnli Bert Large+MNLI • tals/albert-xlarge-vitaminc-mnli BERT Large+MNLI+VitaminC B S UMMA CZS Operator Choice Table A1 measures the effect of the choice of the two operators in the S UMMA CZS model. We ex- plore three options ( min, mean and max) for each operator. We ﬁnd that the choice of max for Operator 1 and mean for Operator 2 achieves the highest performance and use these choices in our model. C S UMMA C Benchmark ROC-AUC Results Table A2 details results of models on the bench- mark according to the ROC-AUC metric, conﬁrm- 6https://huggingface.co/models ing that the SUMMA C models achieve the two best accuracy results on the benchmark. SUMMAC Benchmark Datasets Model Type Model Name CGS XSF Polytope FactCC SummEval FRANK Overall Baseline NER-Overlap 53.0 61.7 51.6 53.1 56.8 60.9 56.2 MNLI-doc 59.4 59.4 62.6 62.1 70.0 67.2 63.4 Classiﬁer FactCC-CLS 65.0 59.2 63.5 79.6 61.4 62.7 65.2 Parsing DAE 67.8 41.3 64.1 82.7 77.4 64.3 66.3 QAG FEQA 60.8 53.4 54.6 50.7 52.2 74.8 57.7 QuestEval 64.4 66.4 72.2 71.5 79.0 87.9 73.6 NLI SUMMACZS 73.1 58.0 60.3 83.7 85.5 85.3 74.3 SUMMACConv 67.6 70.2 62.4 92.2** 86.0* 88.4 77.8 ** Table A2: Performance of Summary Inconsistency Detection models on the test portion of the SUM- MAC Benchmark in terms of ROC-AUC metric. The metric is computed for each model on the six datasets in the benchmark, and the average is computed as the overall performance on the benchmark. Conﬁdence intervals comparing the S UMMA C models to prior work: * indicates an improvement with 95% conﬁdence, and ** 99% conﬁdence (details in Section 5.2.1)",
      "chunk_index": 24
    },
    {
      "index": 340,
      "chunk_id": "SummaC2021_chunk_25",
      "source_id": "SummaC2021",
      "text": "* indicates an improvement with 95% conﬁdence, and ** 99% conﬁdence (details in Section 5.2.1)",
      "chunk_index": 25
    },
    {
      "index": 341,
      "chunk_id": "FactCC2020_chunk_00",
      "source_id": "FactCC2020",
      "text": "On Faithfulness and Factuality in Abstractive Summarization Joshua Maynez∗ Shashi Narayan∗ Bernd Bohnet Ryan McDonald Google Research {joshuahm,shashinarayan,bohnetbd,ryanmcd}@google.com Abstract It is well known that the standard likelihood training and approximate decoding objectives in neural text generation models lead to less human-like responses for open-ended tasks such as language modeling and story gener- ation. In this paper we have analyzed limi- tations of these models for abstractive docu- ment summarization and found that these mod- els are highly prone to hallucinate content that is unfaithful to the input document. We con- ducted a large scale human evaluation of sev- eral neural abstractive summarization systems to better understand the types of hallucinations they produce. Our human annotators found substantial amounts of hallucinated content in all model generated summaries. However, our analysis does show that pretrained models are better summarizers not only in terms of raw metrics, i.e., ROUGE, but also in generating faithful and factual summaries as evaluated by humans. Furthermore, we show that textual en- tailment measures better correlate with faith- fulness than standard metrics, potentially lead- ing the way to automatic evaluation metrics as well as training and decoding criteria.1 1 Introduction Current state of the art conditional text generation models accomplish a high level of ﬂuency and co- herence, mostly thanks to advances in sequence- to-sequence architectures with attention and copy (Sutskever et al., 2014; Bahdanau et al., 2015; Gu et al., 2016), fully attention-based Transformer ar- chitectures (Vaswani et al., 2017; Dai et al., 2019) and more recently pretrained language modeling for natural language understanding (Devlin et al., 2019; Radford et al., 2018; Yang et al., 2019; Liu et al., 2019). There has been a growing interest in ∗ The ﬁrst two authors contributed equally. 1Our human annotated summaries for faithfulness and fac- tuality will be released at https://github.com/google-research- datasets/xsum hallucination annotations. understanding how maximum likelihood training and approximate beam-search decoding in these models lead to less human-like text in open-ended text generation such as language modeling and story generation (Holtzman et al., 2020; Welleck et al., 2020; See et al., 2019). In this paper we investigate how these models are prone to gener- ate hallucinated text in conditional text generation, speciﬁcally, extreme abstractive document summa- rization (Narayan et al., 2018a). Document summarization - the task of produc- ing a shorter version of a document while preserv- ing its information content (Mani, 2001; Nenkova and McKeown, 2011) -",
      "chunk_index": 0
    },
    {
      "index": 342,
      "chunk_id": "FactCC2020_chunk_01",
      "source_id": "FactCC2020",
      "text": "conditional text generation, speciﬁcally, extreme abstractive document summa- rization (Narayan et al., 2018a). Document summarization - the task of produc- ing a shorter version of a document while preserv- ing its information content (Mani, 2001; Nenkova and McKeown, 2011) - requires models to gener- ate text that is not only human-like but also faith- ful and/or factual given the document. The exam- ple in Figure 1 illustrates that the faithfulness and factuality are yet to be conquered by conditional text generators. The article describes an event of \"Conservative MP Zac Smith winning the pri- mary for 2016 London mayoral election\", but sum- maries often forge entities (e.g., \"Nigel Goldsmith\" or \"Zac Goldwin\") or information (e.g., \"UKIP leader Nigel Goldsmith\", \"Nigel Goldsmith win- ning the mayoral election\", \"Sadiq Khan being the former London mayor\" or \"Zac Goldwin being the Labour's candidate\") that are not supported by the document or are factually wrong. Interestingly, all summaries are topical and ﬂuent, and perform well in terms of ROUGE scores (Lin and Hovy, 2003). We conducted a large-scale human evaluation of hallucinated content in systems that use Re- current Neural Network (RNN) (See et al., 2017), Convolutional Neural Network (CNN) (Narayan et al., 2018a), and Transformers (Radford et al., 2019; Rothe et al., 2020), as well as human written summaries for the recently introduced eXtreme SUMmarization task ( XSUM, Narayan et al., 2018a). We seek to answer the following questions: (i) How frequently do abstractive sum- marizers hallucinate content?; (ii) Do models hal- arXiv:2005.00661v1 [cs.CL] 2 May 2020 GOLD Zac Goldsmith will contest the 2016 London mayoral election for the Conservatives, it has been announced. DOCUMENT : The Richmond Park and North Kingston MP said he was \"honoured\" after winning70% of the 9,227 votes cast using an online primary system. He beat London Assembly Member Andrew Boff, MEP Syed Kamall and London's deputy mayor for crime and policing Stephen Greenhalgh. Mr Goldsmith's main rival is likely to beLabour's Sadiq Khan. (2 sentences with 59 words are abbreviated here.) Mr Goldsmith, who was the favourite for the Tory nomination, balloted his constituents earlier this year to seek permission to stand. At the very point of his entry into the race for London mayor, Zac Goldsmith's decision revealed two big characteristics. (5 sentences with 108 words are abbreviated here.) Mr Goldsmith- who ﬁrst entered Parliament in 2010 - told the BBC's Daily Politics that he hoped",
      "chunk_index": 1
    },
    {
      "index": 343,
      "chunk_id": "FactCC2020_chunk_02",
      "source_id": "FactCC2020",
      "text": "his entry into the race for London mayor, Zac Goldsmith's decision revealed two big characteristics. (5 sentences with 108 words are abbreviated here.) Mr Goldsmith- who ﬁrst entered Parliament in 2010 - told the BBC's Daily Politics that he hoped his environmental record would appeal to Green and Lib Dem voters and he also hoped to \"reach out\" to UKIP supporters frustrated with politics as usual and the UK's relationship with the EU. Zac GoldsmithBorn in 1975, educated at Eton and the Cambridge Centre for Sixth-form Studies (5 sentences with 76 words are abbreviated here.) Mr Goldsmith, who has conﬁrmed he would stand down from Parliament if he became mayor, triggering a by-election, said he wanted to build on current mayor Boris Johnson's achievements.(3 sentences with 117 words are abbreviated here.) Both Mr Khan and Mr Goldsmithoppose a new runway at Heathrow airport, a fact described by the British Chambers of Commerce as \"depressing\". (1 sentences with 31 words is abbreviated here.) Current mayor Boris Johnsonwill step down next year after two terms in ofﬁce. He is also currently the MP for Uxbridge and South Ruislip, having been returned to Parliament in May. Some Conservatives have called for an inquiry into the mayoral election processafter only 9,227 people voted - compared with a 87,884 turnout for the Labour contest. (4 sentences with 121 words are abbreviated here.) PTGEN UKIP leader Nigel Goldsmith has been elected as the new mayor of London to elect a new Conservative MP. [45.7, 6.1, 28.6] TCONV S2S Former London mayoral candidate Zac Goldsmith has been chosen to stand in the London mayoral election. [50.0, 26.7, 37.5] TRAN S2S Former London mayor Sadiq Khan has been chosen as the candidate to be the next mayor of London. [35.3, 12.5, 23.5] GPT-T UNED Conservative MP Zac Goldwin's bid to become Labour's candidate in the 2016 London mayoral election. [42.4, 25.8, 36.4] BERT S2S Zac Goldsmith has been chosen to contest the London mayoral election. [66.7, 40.0, 51.9] Figure 1: Hallucinations in extreme document summarization: the abbreviated article, its gold summary and the abstractive model generated summaries (P TGEN, See et al. 2017; TC ONV S2S, Narayan et al. 2018a; and, GPT- TUNED , TRAN S2S and B ERTS2S, Rothe et al. 2020) for a news article from the extreme summarization dataset (Narayan et al., 2018a). The dataset and the abstractive models are described in Section 3 and",
      "chunk_index": 2
    },
    {
      "index": 344,
      "chunk_id": "FactCC2020_chunk_03",
      "source_id": "FactCC2020",
      "text": "al. 2018a; and, GPT- TUNED , TRAN S2S and B ERTS2S, Rothe et al. 2020) for a news article from the extreme summarization dataset (Narayan et al., 2018a). The dataset and the abstractive models are described in Section 3 and 4. We also present the [ROUGE -1, ROUGE -2, ROUGE -L] F1 scores relative to the reference gold summary. Words in red correspond to hallucinated information whilst words in blue correspond to faithful information. lucinate by manipulating the information present in the input document (intrinsic hallucinations) or by adding information not directly inferable from the input document (extrinsic hallucinations)?; (iii) How much hallucinated content is factual, even when unfaithful?; and (iv) Are there automatic means of measuring these hallucinations? Our main conclusions are as follows: First, intrinsic and extrinsic hallucinations happen fre- quently - in more than 70% of single-sentence sum- maries. Second, the majority of hallucinations are extrinsic, which potentially could be valid abstrac- tions that use background knowledge. However, our study found that over 90% of extrinsic halluci- nations were erroneous. Thus, hallucinations hap- pen in most summaries and the majority of these are neither faithful nor factual. Third, models ini- tialized with pretrained parameters perform best both on automatic metrics and human judgments of faithfulness/factuality. Furthermore, they have the highest percentage of extrinsic hallucinations that are factual. This suggests that while some studies argue that large-scale pretrained models are merely better at learning data-speciﬁc regularities (Niven and Kao, 2019), at least on in-domain summa- rization the gains in automatic metrics are real- ized in observable differences by humans. Fourth, ROUGE (Lin and Hovy, 2003) and BERTScore (Zhang et al., 2020) correlates less with faithful- ness/factuality than metrics derived from automatic semantic inference systems, speciﬁcally the degree to which a summary is entailed by the source docu- ment. This presents an opportunity for improved automatic evaluation measures as well as model training and decoding objectives. We show prelim- inary experiments in this direction. 2 Hallucinations in Summarization Open-ended generation - the task of generating text that forms a natural continuation from the input text - requires the model to hallucinate text; hence the focus has been to ensure that the model learns to generate text that is more human-like (i.e., less repetitive or dull with more content-related words) (Holtzman et al., 2020; Welleck et al., 2020; See et al., 2019). In contrast, tasks such as document summarization (Nenkova",
      "chunk_index": 3
    },
    {
      "index": 345,
      "chunk_id": "FactCC2020_chunk_04",
      "source_id": "FactCC2020",
      "text": "that the model learns to generate text that is more human-like (i.e., less repetitive or dull with more content-related words) (Holtzman et al., 2020; Welleck et al., 2020; See et al., 2019). In contrast, tasks such as document summarization (Nenkova and McKeown, 2011; See et al., 2017; Paulus et al., 2018) and data-to-text generation (Lebret et al., 2016; Wiseman et al., 2017) which are not open-ended, require models to be factual and/or faithful to the source text. Despite recent improvements in conditional text generation, most summarization systems are trained to maximize the log-likelihood of the ref- erence summary at the word-level, which does not necessarily reward models for being faithful. More- over, models are usually agnostic to the noises or artifacts of the training data, such as reference diver- gence, making them vulnerable to hallucinations (Kryscinski et al., 2019a; Wiseman et al., 2017; Dhingra et al., 2019). Thus, models can gener- ate texts that are not consistent with the input, yet would likely have reasonable model log-likelihood. 2.1 Intrinsic and Extrinsic Hallucinations Given a document D and its abstractive summary S, we try to identify all hallucinations in S with re- spect to the content of D, regardless of the quality of the summary. In this work, we deﬁne a summary as being hallucinated if it has a span(s) wi . . . wi+j, j ≥i, that is not supported by the input document. To distinguish hallucinations further in the context of a document and a summary, we categorize hallu- cinations by the information source as intrinsic and extrinsic hallucinations. Note, paraphrases or any information that can be inferred from the document are not categorized as hallucinations. Intrinsic hallucinations are consequences of synthesizing content using the information present in the input document. For example, in Fig- ure 1, \"Former London mayoral candidate\" in the TCONV S2S abstract and \"Former London mayor\" in the TRAN S2S abstract are hallucinations of in- trinsic nature; both use terms or concepts from the document but misrepresent information from the document, making them unfaithful to the document. The article does not conﬁrm if \"Zac Goldsmith\" was a \"Former London mayoral candidate\" or if \"Sadiq Khan\" was a \"Former London mayor\". One may suspect that a model with poor input docu- ment representation will fail to do document level inference, often required for abstraction, and will be vulnerable to such errors. Extrinsic hallucinationsare model generations that",
      "chunk_index": 4
    },
    {
      "index": 346,
      "chunk_id": "FactCC2020_chunk_05",
      "source_id": "FactCC2020",
      "text": "was a \"Former London mayor\". One may suspect that a model with poor input docu- ment representation will fail to do document level inference, often required for abstraction, and will be vulnerable to such errors. Extrinsic hallucinationsare model generations that ignore the source material altogether. For ex- ample, in Figure 1, \"Nigel\" in the PTGEN abstract and \"2016\" in both GOLD and GPT-T UNED are extrinsic hallucinations; these terms are not intro- duced in the document. A model with a poorly- informed decoder and that is agnostic to the di- vergence issue between the source and target texts (Wiseman et al., 2017; Dhingra et al., 2019), will function more as an open-ended language model and will be prone to extrinsic hallucinations. 2.2 Factual Hallucinations in Summarization A summary S of a document D contains a factual hallucination if it contains information not found in D that is factually correct. Factual hallucina- tions may be composed of intrinsic hallucinations or extrinsic hallucinations. By deﬁnition, abstractive summaries are writ- ten to preserve the salient information in the input document, but they are expressed in the words of the summary author as opposed to the input docu- ment author (Nenkova and McKeown, 2011). As such, it is natural to construct summaries that inte- grate with the author's background knowledge (van Dijk and Kintsch, 1978; Brown and Day, 1983). Such knowledge integration can also be desirable in real world applications. For instance, an en- gaging sports report will reﬂect an understanding of the game to provide color and context. An- other example is audience-targeted summarization where a good summary will reﬂect understanding of both the article domain and the desired audience. Nonetheless, there is no consensus in the research community if the summary should be faithful (with- out any hallucinations) to the input document or if there is tolerance for factual hallucinations. Recent deep learning approaches to abstractive summarization naturally learn to integrate knowl- edge from the training data while generating an abstractive summary for a document (See et al., 2017; Gehrmann et al., 2018). More advanced pre- trained text generators (Radford et al., 2018, 2019; Dong et al., 2019; Song et al., 2019; Khandelwal et al., 2019; Rothe et al., 2020) are even better at capturing world knowledge as they are informed by a vast amount of background text. This can be observed in the example shown in Figure 1 as the input",
      "chunk_index": 5
    },
    {
      "index": 347,
      "chunk_id": "FactCC2020_chunk_06",
      "source_id": "FactCC2020",
      "text": "Khandelwal et al., 2019; Rothe et al., 2020) are even better at capturing world knowledge as they are informed by a vast amount of background text. This can be observed in the example shown in Figure 1 as the input document does not mention that the discussed \"London mayoral election\" is from \"2016\"; but the abstract generated by the pretrained text generator GPT-T UNED correctly predicts this information similar to the human-authored abstract.2 2Despite the correct extrinsic hallucination (\"2016 \"), the GPT-T UNED abstract overall is still not factual due to the incorrect extrinsic hallucination in \"Conservative MP Zac Goldwin.\" There is no Conservative MP named Zac Goldwin. In this paper we stand in favour of the asser- tion that abstractive systems may integrate with the background knowledge to generate rich and mean- ingful summaries. More concretely, \" hallucina- tions in summarization are acceptable if they lead to better summaries that are factual with respect to the document and the associated background knowledge.\" This assumption also allows us to assess the capability of recent neural models to in- tegrate with the background knowledge to generate factual abstracts (see Section 5.3). 3 Extreme Document Summarization We focus on the recently introduced extreme sum- marization dataset (XSUM, Narayan et al., 2018a)3 which comprises 226,711 British Broadcasting Cor- poration (BBC) articles paired with their single- sentence summaries, provided by the journalists writing the articles. The dataset is split into three subsets: training (90%, 204,045), validation (5%, 11,332), and test (5%, 11,334) sets. All models in §4 trained to generate abstractive summaries are trained and evaluated using this standard split, pro- vided by the authors. We choose to focus our study on extreme summa- rization for the following reasons: First, this task aims to create a single-sentence summary of a news article; these shorter summaries are relatively eas- ier to annotate and analyze than longer summaries such as story highlights from the CNN/Dailymail dataset (Hermann et al., 2015) or abstracts from the NY Times (Sandhaus, 2008) or the WikiSum (Liu et al., 2018) dataset. Secondly, the gold summary in the extreme summarization dataset is an intro- ductory sentence prefacing each article. By virtue of this property, the extreme summarization task is not amenable to extractive strategies and requires an abstractive modeling approach. Hence, it pro- vides us a better benchmark to assess abstractive models' abilities to produce abstractions which are faithful and factual. Finally,",
      "chunk_index": 6
    },
    {
      "index": 348,
      "chunk_id": "FactCC2020_chunk_07",
      "source_id": "FactCC2020",
      "text": "this property, the extreme summarization task is not amenable to extractive strategies and requires an abstractive modeling approach. Hence, it pro- vides us a better benchmark to assess abstractive models' abilities to produce abstractions which are faithful and factual. Finally, since we conclude that hallucination is a problem on this dataset, then we can safely conclude it is a problem for summariza- tion datasets with longer summaries, as modeling longer-distance dependencies and discourse struc- tures make the task harder. 4 Abstractive Summaries We evaluate summaries from RNN, CNN and Transformer-based state-of-the-art abstractive sum- marization methods and the reference human writ- 3https://github.com/EdinburghNLP/XSum ten summaries. See the Appendix for hyperparam- eter and decoding details for all models. Human Written Reference Summaries. The single-sentence summaries contained in the ex- treme summarization dataset (XSUM) are also eval- uated as part of this study. These summaries were written by journalists as introductions to the news articles they precede. These summaries, therefore, often have true additional information not found in the document. Such divergence issue between source and target is not uncommon in conditional text generation (Kryscinski et al., 2019a; Wiseman et al., 2017; Dhingra et al., 2019). RNN-based Seq2Seq. We use the Pointer- Generator model (PTGEN) introduced by See et al. (2017), an RNN-based attention-based sequence to sequence model which not only generates from the target vocabulary but can copy words from the source text. Topic-aware Convolutional Seq2Seq. The Topic-aware Convolutional Sequence to Sequence model ( TCONV S2S) introduced by Narayan et al. (2018a) is an abstractive system which is conditioned on the article's topics and based entirely on Convolutional Neural Networks (Gehring et al., 2017). TCONV S2S is better suited for extreme summarization, as convolution layers capture long-range dependencies between words in the document more effectively than RNNs. Simultaneously, the convolutional encoder associates each word with a topic vector, capturing whether it is representative of the document's content. Transformer-based Abstractive Methods. We experiment with three Transformer-based model variants, all of which have 12 layers, a hidden size of 768, ﬁlter size of 3072, and 12 attention heads. GPT-T UNED : Radford et al. (2019) proposed Transformer-based Generative Pre-Trained (GPT) language models that can generate high quality text in open-ended generation setups. The proposed decoder-only architecture for language modeling can be easily adapted to abstractive summarization where the model ﬁrst sees the document and, given a prompt, such as TL;DR;, generates its summary. Our GPT-T",
      "chunk_index": 7
    },
    {
      "index": 349,
      "chunk_id": "FactCC2020_chunk_08",
      "source_id": "FactCC2020",
      "text": "high quality text in open-ended generation setups. The proposed decoder-only architecture for language modeling can be easily adapted to abstractive summarization where the model ﬁrst sees the document and, given a prompt, such as TL;DR;, generates its summary. Our GPT-T UNED is warm-started with a publicly available GPT checkpoint (Radford et al., 2019), but ﬁne-tuned with supervised training on the ex- treme summarization dataset. TRAN S2S and BERT S2S: TRAN S2S and BERT S2S are sequence to sequence models Models Human Eval Test Set R1 R2 RL BERTScore PTGEN 30.01 9.38 23.76 74.30 TCONV S2S 30.89 11.47 25.80 75.23 TRAN S2S 32.28 11.66 24.65 75.69 GPT-T UNED 21.82 4.72 16.28 70.35 BERTS2S 38.42 16.96 31.27 78.85 Table 1: ROUGE and BERTScore F 1 scores for non- pretrained (the top block) and pretrained (the bottom block) models reported on the XSum dataset. These re- sults are on the sampled human evaluation (500 items) dataset. The best results are boldfaced. where both encoder and decoder are composed of Transformer layers (Vaswani et al., 2017; Rothe et al., 2020). All weights in TRAN S2S are randomly initialized, but in BERT S2S , both encoder and decoder are initialized with the BERT-Base checkpoints (Devlin et al., 2019), with parameter sharing between the encoder and decoder, following Rothe et al. (2020). The only variable that is initialized randomly is the encoder- decoder attention in BERT S2S . Both models are then trained on the extreme summarization dataset. 5 Experiments and Results The main focus of this work is not to propose a so- lution to hallucination related issues, but to achieve a better understanding of hallucinations in abstrac- tive summarization through their human assess- ment. We randomly sampled 500 articles from the test set to facilitate our study. Using the full test set was unfeasible given its size and the cost of hu- man judgments. We have trained annotators (ﬂuent in English) speciﬁcally for our assessment. Our annotators went through two pilot studies to have a better understanding of intrinsic and extrinsic hallucinations, and factuality of summaries. Doc- uments used in the pilot studies were not used in the ﬁnal annotation. We also report on ROUGE (Lin and Hovy, 2003) scores, BERTScore (Zhang et al., 2020) and semantic inference metric such as textual entailment (Pasunuru and Bansal, 2018; Welleck et al., 2019; Falke et al., 2019; Kryscinski et al., 2019b) and question answering (Arumae and",
      "chunk_index": 8
    },
    {
      "index": 350,
      "chunk_id": "FactCC2020_chunk_09",
      "source_id": "FactCC2020",
      "text": "ROUGE (Lin and Hovy, 2003) scores, BERTScore (Zhang et al., 2020) and semantic inference metric such as textual entailment (Pasunuru and Bansal, 2018; Welleck et al., 2019; Falke et al., 2019; Kryscinski et al., 2019b) and question answering (Arumae and Liu, 2019; Wang et al., 2020). 5.1 Automatic Evaluation of Summaries ROUGE (Lin and Hovy, 2003) provides a means to quickly assess a model's ability to generate sum- maries closer to human authored summaries. We report on ROUGE-1 and ROUGE-2 for informa- tiveness and ROUGE-L, for ﬂuency. Like ROUGE, BERTScore (Zhang et al., 2020) computes a sim- ilarity score for each token in the candidate sum- Figure 2: Human assessment of a system generated summary for the article in Figure 1. The annotation user interface is shown as it was shown to raters. mary with each token in the reference summary. However, instead of exact matches, it computes token similarity using contextual embeddings. Re- sults are presented in Table 1. For both cases, the pretrained encoder-decoder architecture BERT S2S performed far superior to any other randomly initialized models, such as PT- GEN, TCONV S2S and TRAN S2S, and the decoder- only architecture GPT-T UNED . The differences be- tween PTGEN, TCONV S2S and TRAN S2S are not signiﬁcant; all other differences are signiﬁcant.4 ROUGE and BERTScore are indicators of infor- mativeness of summaries but they are not sufﬁcient metrics to assess the overall quality of summaries. This becomes evident from our human assessments in the following sections where we employ human annotators to evaluate summaries generated with PTGEN, TCONV S2S , TRAN S2S and BERT S2S , and the human authored summaries. We excluded GPT-T UNED abstracts from our study after their poor performance on the automatic measures. 5.2 Assessment of Hallucinations In this assessment, human annotators were pre- sented an article and a single-sentence summary for the article. They were stringently told to only assess the hallucinations in the summary and to not confuse their assessment with the quality of the summary. For summaries containing hallucina- tions, annotators were tasked with (i) identifying those text spans that were unfaithful to the arti- cle and (ii) for each text span, annotating whether the hallucination was intrinsic or extrinsic. We elicited judgments from three different annotators for each of 2500 (500x5) document-summary pairs. Figure 2 shows an example assessment of a sum- mary of an article from Figure 1. Results from",
      "chunk_index": 9
    },
    {
      "index": 351,
      "chunk_id": "FactCC2020_chunk_10",
      "source_id": "FactCC2020",
      "text": "annotating whether the hallucination was intrinsic or extrinsic. We elicited judgments from three different annotators for each of 2500 (500x5) document-summary pairs. Figure 2 shows an example assessment of a sum- mary of an article from Figure 1. Results from the full assessment are shown in Table 2, which shows the percentage of documents per system that were annotated as faithful or hallucinated (faithful = 100 - hallucinated). The Appendix provides inter- annotator agreement of hallucinations as well as hallucinated span characteristics. Extrinsic Hallucination due to Divergence be- tween Source and Target. Our results con- 4Pairwise comparisons between all models using a one- way ANOV A with post-hoc Tukey HSD tests;p <0.01. Models Hallucinated Faith. +Fact.I E I ∪ E PTGEN 19.9 63.3 75.3 24.7 27.3 TCONV S2S 17.7 71.5 78.5 21.5 26.9 TRAN S2S 19.1 68.1 79.3 20.7 25.3 BERTS2S 16.9 64.1 73.1 26.9 34.7 GOLD 7.4 73.1 76.9 23.1 - Table 2: Intrinsic vs. Extrinsic Hallucinations. The numbers in \"Hallucinated\" columns show the percent- age of summaries where at least one word was anno- tated by all three annotators as an intrinsic (I) or extrin- sic (E) hallucination. When a summary is not marked with any hallucination, it is \"faithful\" (100 - I∪E), col- umn \"Faith.\". The ﬁnal \"+Fact.\" column shows the total percentage of faithful and/or factual summaries, which includes all faithful summaries plus the percent- age of non-faithful summaries annotated by all three an- notators as factual. Higher numbers for faithful/factual and lower numbers for hallucinations are boldfaced. ﬁrmed that the BBC gold summaries often have ex- trinsic hallucinations due to the dataset artifact that gold summaries are introductory sentences pref- acing each article. It was not surprising that most models also had signiﬁcant extrinsic hallucinations. Intrinsic Hallucination is Also Common in Ab- stractive Summaries. Gold summaries can also display intrinsic hallucinations. For example, a news article could describe an event related to \"Barack Obama\" and \"the ofﬁce of the President of the United States\" without inferring that \"Obama is the President of the United States.\" A journalist with the knowledge of the event in the article could write a summary stating \"President Obama.\" However, the percentage of system summaries with intrinsic hallucination was much higher than in gold summaries (7.4% vs others). This phe- nomenon particularly revealed the models' ten- dency to misrepresent information in the document due to the lack of document-level understanding and",
      "chunk_index": 10
    },
    {
      "index": 352,
      "chunk_id": "FactCC2020_chunk_11",
      "source_id": "FactCC2020",
      "text": "percentage of system summaries with intrinsic hallucination was much higher than in gold summaries (7.4% vs others). This phe- nomenon particularly revealed the models' ten- dency to misrepresent information in the document due to the lack of document-level understanding and inference. The copy mechanism in PTGEN is good at copying from the source (showing the least percentage of extrinsic hallucination of 63.3%), but the mechanism lacks the inference capability and is prone to generate a summary that is not supported by the document (19.9% intrinsic hallucination). TRAN S2S showed similar performance to PTGEN and ranked second worst. The BERT S2S showed the least number of intrinsic hallucination (16.9%) among all four abstractive systems. Pretraining Improves Faithfulness. Hallucina- tions do not result from the artifacts in the training data only, but also due to model shortcomings. The PTGEN model with the copy mechanism (Gu et al., 2016; See et al., 2017) had the lowest extrinsic hallucination (63.3%), but BERT S2S reported the highest number of faithful summaries. It appears that BERTS2S is overall most conservative among all four abstractive systems while getting closer to reference summaries in terms of ROUGE. The pre- training prepares BertS2S to be more aware of the domain of the document and less prone to language model vulnerabilities. Consequently, BertS2S is more conﬁdent in predicting tokens from the docu- ment than TranS2S, hence, improving faithfulness. 5.3 Assessment of Factual Hallucinations. Hallucinations are not necessarily erroneous. In our second human assessment, we measured to what ex- tent this is the case. Our annotators were presented a single-sentence summary with hallucinations and were asked to assess whether it is true or false. To better explain the context of the summary, annota- tors were made available the source document as well as the external resources such as Wikipedia or Google Search. The source document can be particularly important for generic summaries to bet- ter understand context. External resources assisted the evaluators to validate grounded facts in public knowledge bases. Annotators were expected to validate the sum- mary by looking for supporting evidence for the information found on the summary. If information in the summary contradicts the document, then the summary is not factual. If supporting evidence is found for all the information, then the summary is factual. The document is not useful when the sum- mary has information that is neither supported nor contradicted in the article. For example, the",
      "chunk_index": 11
    },
    {
      "index": 353,
      "chunk_id": "FactCC2020_chunk_12",
      "source_id": "FactCC2020",
      "text": "is not factual. If supporting evidence is found for all the information, then the summary is factual. The document is not useful when the sum- mary has information that is neither supported nor contradicted in the article. For example, the sum- mary in Figure 2 mentions \"Conservative MP Zac Goldwin\" which can not be veriﬁed from the article in Figure 1. Here, annotators could use Wikipedia or Google Search to check that there had not been a Conservative MP named Zac Goldwin who tried to change their party and become a Labour's candi- date in the 2016 London mayoral election. We dropped the human authored gold summaries from this evaluation; they were presumably factual. We also dropped the abstracts that were faithful to their input documents from the previous study. Finally, there were 1869 document-summary pairs where the summaries were marked with at least one intrinsic or extrinsic hallucination. We elicited judgments from three different annotators for each of them. Results from this assessment are also pre- sented in Table 2 (see the column labelled \"+Fact.\") along with the hallucination assessment. Pretraining Helps Generating Factual Sum- maries. In total, 34.7% of the BERT S2S ab- stracts were faithful (26.9%) and/or factual (+7.8%). This is 7.4% absolute better than the next-best model (PTGEN). The number of unfaith- ful yet factual summaries for BERTS2S, 7.8%, was also the highest. In fact, for extrinsic hallucina- tions, even though PTGEN hallucinates less than BERT S2S (63.3% vs. 64.1%), 6.6% of BERT S2S hallucinations were factual, compared to 2.2% of PTGEN.5 Thus, if we consider factual hallucina- tions to be valid, this means that even for extrinsic cases, BERTS2S hallucinates the least. The superior performance of BERT S2S is most likely due to its exposure to vast amount of text through pretraining, allowing it to integrate back- ground knowledge with generation. Even so, over 90% of BERTS2S hallucinations are erroneous. Finally, we carried out pairwise comparisons be- tween all models (using a one-way ANOV A with post-hoc Tukey HSD tests; p <0.01). For intrin- sic hallucinations (the second column in Table 2), GOLD is signiﬁcantly different from all other sys- tems. For extrinsic hallucinations (the third col- umn in Table 2), there were signiﬁcant differences between PTGEN and TCONV S2S , PTGEN and GOLD , and, BERT S2S and GOLD . For factual- ity, the differences between PTGEN, TCONV S2S , and TRAN S2S were",
      "chunk_index": 12
    },
    {
      "index": 354,
      "chunk_id": "FactCC2020_chunk_13",
      "source_id": "FactCC2020",
      "text": "third col- umn in Table 2), there were signiﬁcant differences between PTGEN and TCONV S2S , PTGEN and GOLD , and, BERT S2S and GOLD . For factual- ity, the differences between PTGEN, TCONV S2S , and TRAN S2S were insigniﬁcant. 5.4 Automatic Measures for Hallucinations Summaries are a proxy for their source documents under the assumption that they highlight the most important content. With this assumption, we fur- ther studied the extent to which the hallucinated content can be measured by semantic inference related measures, such as textual entailment and question answering. Textual Entailment. We trained an entailment classiﬁer by ﬁnetuning a BERT-Large pretrained model (Devlin et al., 2019) on the Multi-NLI dataset (Williams et al., 2018). We calculated the entailment probability score between the docu- ment and its abstractive summaries. Note that this entailment classiﬁer is not optimal for the BBC article-summary pairs; the Multi-NLI dataset con- tains sentence-sentence pairs. Ideally a summary should entail the document or perhaps be neutral to the document, but never contradict the document. As can be seen in Table 3, the BERTS2S abstracts showed the least number of 5See Appendix for full results. Models Textual Entailment QAentail. neut. cont. PTGEN 38.4 34.4 27.2 20.2 TCONV S2S 29.6 37.4 33.0 19.9 TRAN S2S 34.6 39.8 25.6 22.4 BERTS2S 41.8 37.8 20.4 23.0 GOLD 32.8 47.2 20.0 19.3 Table 3: Textual entailment and question answering (QA) based measures for summary evaluation. For en- tailment, we show the percentage of times a summary entails (entail.) the document, is neutral (neut.) to the document and contradicts (cont.) the document. For QA, we report the percentage of questions that were correctly answered by a system. The highest numbers for entail., neut. and QA, and the lowest number for cont. are boldfaced. contradictions compared to other system-generated abstracts and was at par with the GOLD summaries. Similar to the performance on extrinsic halluci- nation in Table 2, the TCONV S2S abstracts also displayed the highest number of contradictions. In- terestingly, the GOLD summaries are more neutral to their documents, whereas the BERT S2S sum- maries are more entailed by their documents. This is probably due to the nature of the data and that journalists tend to add color and have a high num- ber of extrinsic (but valid) hallucinations. Question Answering. QA frameworks have been used to assess or promote summary infor- mativeness (Narayan et al., 2018b; Arumae",
      "chunk_index": 13
    },
    {
      "index": 355,
      "chunk_id": "FactCC2020_chunk_14",
      "source_id": "FactCC2020",
      "text": "of the data and that journalists tend to add color and have a high num- ber of extrinsic (but valid) hallucinations. Question Answering. QA frameworks have been used to assess or promote summary infor- mativeness (Narayan et al., 2018b; Arumae and Liu, 2019). We adapted the QA framework to as- sess hallucination in model generated summaries; a faithful model will generate a summary that only has information that is supported by its document. Under this assumption, any question answerable by the summary should also be answerable by the source. Given an abstractive summary, we used the round-trip consistency method of Alberti et al. (2019), which combines question generation and answer extraction models to generate synthetic question-answer pairs. For the 500 document- summary pairs, we generated 731, 708, 720, 725 and 820 question-answer pairs for PTGEN, TCONV S2S, TRAN S2S, BERTS2S and GOLD , re- spectively. Finally, we used a machine reading comprehension model to answer these questions using the document as context. As in Alberti et al. (2019), we trained all models: question generation, answer extraction and reading comprehension mod- els; using a BERT-Base pretrained model (Devlin et al., 2019) ﬁnetuned on the Natural Questions dataset (Kwiatkowski et al., 2019). Similar to textual entailment results, the PTGEN Leeds United fought back from 2-0 down to beat Huddersﬁeld town in the ﬁrst round of the EFL cup. ( Q: What team did Leeds United beat in the ﬁrst round of the EFL cup?, A: Huddersﬁeld town) TCONV S2S A coal mine in South Yorkshire has collapsed as a result of the loss of a coal mine. ( Q: What type of mine has collapsed?, A: Coal) TRAN S2S Star Wars actor James Davis said he was \"locked in a caravan\" and had his caravan stolen during a break-in. ( Q: Who said he was locked in a caravan?, A: Davis) Figure 3: Sample of question-answer pairs generated from hallucinated summaries that are correctly an- swered by their source articles. Highlighted spans in the summaries are marked as extrinsic or intrinsic hal- lucinations by our annotators. Metric Faithful Factual ROUGE -1 0.197 0.125 ROUGE -2 0.162 0.095 ROUGE -L 0.162 0.113 BERTScore 0.190 0.116 QA 0.044 0.027 Entailment 0.431 0.264 Table 4: Spearman's correlation coefﬁcient (|rs|) of dif- ferent metrics with faithful and factual annotations. BERTS2S abstracts were the most faithful to their source documents in terms of question answering. The GOLD",
      "chunk_index": 14
    },
    {
      "index": 356,
      "chunk_id": "FactCC2020_chunk_15",
      "source_id": "FactCC2020",
      "text": "0.190 0.116 QA 0.044 0.027 Entailment 0.431 0.264 Table 4: Spearman's correlation coefﬁcient (|rs|) of dif- ferent metrics with faithful and factual annotations. BERTS2S abstracts were the most faithful to their source documents in terms of question answering. The GOLD abstracts were the least accurate due to a high number of extrinsic hallucination in them. Spearman's Correlation. We estimate Spear- man's correlation coefﬁcients of different metrics with the faithful and factual human scores (see Table 4). We found that the textual entailment scores are best correlated with both faithful (mod- erate, 0.40 ≤ |rs| ≤0.59) and factual (weak, 0.20 ≤|rs|≤ 0.39) human scores. Comparatively, ROUGE -based metrics and BERTScore have very weak correlation, our ﬁndings are consistent with the recent studies (Goodrich et al., 2019; Kryscin- ski et al., 2019a; Wang et al., 2020). Surprisingly, the question answering scores showed a very weak correlation (0.0 ≤|rs|≤ 0.19) with faithful and factual human scores. We hypothesize that this is due to a compounding of errors where (i) the question generator is used to generate questions from the systems' generated abstracts, instead of human-written text on which they were trained, (ii) the question generator is susceptible to generate questions with hallucinated content when fed in with hallucinated summaries, and (iii) our assump- tion that a summary is faithful if the answers from the source and the summary match, is rather poor for extreme summarization. We demonstrate these issues in Figure 3. Irrespective of questions with hallucinated content, our reading comprehension Models R1 R2 RL Faith. +Fact. BERTS2S 38.42 16.96 31.27 26.9 34.7 ENTAIL 35.93 14.02 28.87 31.5 38.6 →FAITH 37.31 15.21 30.12 31.7 38.8 Table 5: ROUGE and faithfulness/factuality scores for BERTS2S plus systems that use textual entailment as a criteria or ﬁne-tuned on faithful annotations. model can fortuitously answer them correctly from their source articles. Better ways of generating questions (Narayan et al., 2020) and measuring fac- tual consistency may alleviate some of these issues (Wang et al., 2020). 5.5 Model Selection with Entailment Our study suggests that entailment could be used as an automatic measure for faithfulness. However, we should point out that this measure is reference- less. Thus, it can easily be gamed, i.e., the ﬁrst sen- tence of any source document is always entailed by the whole document. Because of this, entailment- based measures for evaluation need to be coupled with reference-based measures like ROUGE . However, one",
      "chunk_index": 15
    },
    {
      "index": 357,
      "chunk_id": "FactCC2020_chunk_16",
      "source_id": "FactCC2020",
      "text": "can easily be gamed, i.e., the ﬁrst sen- tence of any source document is always entailed by the whole document. Because of this, entailment- based measures for evaluation need to be coupled with reference-based measures like ROUGE . However, one major advantage of the measure being reference-less is that we can use it as a model selection objective or during decoding. We tested the former. Speciﬁcally, we used the probability that a summary is entailed by a document as a selec- tion criteria to select a summary between four can- didates generated by systems evaluated: PTGEN, TCONV S2S , TRAN S2S , and BERT S2S . Results are shown in the ENTAIL row of Table 5. We can see that indeed this is a strong metric to optimize towards if we want faithful summaries - almost 5% absolute better. There is a trade-off in terms of ROUGE , but this model must select amongst 4 sys- tems, 3 of which have signiﬁcantly lower ROUGE than the best model. A further experiment is to train a model explic- itly to predict faithfulness. In order to do this, we further ﬁne-tuned the entailment model using the 'faithful' annotations generated during our evalua- tion. For all summary-document pairs marked as 'faithful', we set the associated class to 'entailment', otherwise we set it to 'neutral'. This allowed for us to also ﬁne-tune the last classiﬁcation layers taking advantage of the correlation between 'entailment' and 'faithfulness'. Results using 5-fold cross val- idation are shown in the ENTAIL →FAITH row of Table 5. We see here that indeed this does improve the ability to select faithful summaries from a set of candidates, though slightly. We would expect to see larger gains with more training data. How- ever, this model is signiﬁcantly better than ENTAIL on ROUGE -based metrics and seems like a good balance between ROUGE and better faithfulness. 6 Related Work Following the Document Understanding Confer- ence (DUC; Dang, 2005), a majority of work has focused on evaluating the content and the linguistic quality of summaries (Nenkova, 2005). Most pop- ular among them is the automatic metric ROUGE (Lin and Hovy, 2003) that measures the unigram and bigram overlap (ROUGE-1 and ROUGE-2) as a proxy for assessing informativeness and the longest common subsequence (ROUGE-L), for ﬂu- ency. ROUGE, however, can be misleading when used as the only means to assess the informative- ness of summaries",
      "chunk_index": 16
    },
    {
      "index": 358,
      "chunk_id": "FactCC2020_chunk_17",
      "source_id": "FactCC2020",
      "text": "unigram and bigram overlap (ROUGE-1 and ROUGE-2) as a proxy for assessing informativeness and the longest common subsequence (ROUGE-L), for ﬂu- ency. ROUGE, however, can be misleading when used as the only means to assess the informative- ness of summaries (Schluter, 2017). Hence, the ROUGE score is often complemented with subjec- tive human assessment of summaries. More objec- tive measures have been proposed to improve agree- ment among human annotators. Pyramid method (Nenkova and Passonneau, 2004) requires sum- maries to be annotated by experts for salient infor- mation. Narayan et al. (2018a,b) used a question- answering based approach where a summary is used as context to answer questions which were written based on its reference summary. Hardy et al. (2019) proposed a reference-less approach where a summary is assessed against the source document, highlighted with its pertinent content. There has not been much work on evaluating faithfulness and truthfulness of abstractive sum- maries. The automatic evaluation such as ROUGE and the human evaluation of saliency and linguistic quality of summaries are not sufﬁcient due to the complex nature of the task. Recently, Chen and Bansal (2018) asked human annotators to assess the summary relevance measuring both the saliency and the presence of contradictory/unrelated infor- mation. Dhingra et al. (2019) proposed a new au- tomatic metric, PARENT, for data-to-text gener- ation (Lebret et al., 2016; Wiseman et al., 2017) which aligns n-grams from the reference and gen- erated texts to the source table to measure the accu- racy of n-grams that are entailed from the source table. Goodrich et al. (2019) proposed a model- based automatic metric to assess the faithfulness of Wikipedia summaries; they trained an end-to-end model to extract a complete set of OpenIE-style (Banko et al., 2007) facts from both the source text and the generated summary. The summary is faithful if it is precise in generating facts from the source text. In our experiments with OpenIE- based measures, we found that they are not suited for evaluating extreme summarization models; all models perform poorly on these metrics without any signiﬁcant differences. Like ours, few recent works (some in parallel) have explored natural language inference and question answering mod- els to detect factual consistency in generated text (Welleck et al., 2019; Falke et al., 2019; Kryscin- ski et al., 2019b; Wang et al., 2020). In line with our ﬁndings, Falke et al. (2019) observed that the BERT-based NLI",
      "chunk_index": 17
    },
    {
      "index": 359,
      "chunk_id": "FactCC2020_chunk_18",
      "source_id": "FactCC2020",
      "text": "mod- els to detect factual consistency in generated text (Welleck et al., 2019; Falke et al., 2019; Kryscin- ski et al., 2019b; Wang et al., 2020). In line with our ﬁndings, Falke et al. (2019) observed that the BERT-based NLI models substantially improved summaries reranking in terms of their correctness. Kryscinski et al. (2019b) proposed an NLI-based fact checking model that is trained on a dataset tailored for detecting factual inconsistencies in gen- erated text. Wang et al. (2020) proposed a question answering and generation based automatic evalu- ation protocol that is designed to identify factual inconsistencies in a generated summary. Future work will likely investigate better ways of gener- ating questions and measuring factual consistency to address poor correlation with faithfulness and factuality annotations. Finally, others have used reinforcement learn- ing to improve informativeness and reduce con- tradictory information in abstractive summaries, e.g., Pasunuru and Bansal (2018) used a textual entailment-based reward and Arumae and Liu (2019), a question-answering based reward. How- ever, these approaches don't evaluate if these re- wards improve faithfulness of summaries. 7 Conclusion We conducted a large-scale study of hallucinations in abstractive document summarization. We found that (i) tackling hallucination is a critical challenge for abstractive summarization, perhaps the most critical, (ii) NLU-driven pretraining in neural text generators is key to generate informative, coherent, faithful and factual abstracts, but it is still far from solving the problem; and (iii) measures such as ROUGE or BERTScore will not be sufﬁcient when studying the problem; semantic inference-based automatic measures are better representations of true summarization quality. Acknowledgments We thank Ratish Puduppully, Yova Kementched- jhieva, Ankur Parikh, Peter Liu, Slav Petrov, the reviewers and the action editor for invaluable feed- back. The hard work of Muqthar Mohammad, Mohd Majeed and Ashwin Kakarla made our hu- man annotation possible. References Chris Alberti, Daniel Andor, Emily Pitler, Jacob De- vlin, and Michael Collins. 2019. Synthetic QA cor- pora generation with roundtrip consistency. In Pro- ceedings of the 57th Annual Meeting of the Asso- ciation for Computational Linguistics , pages 6168- 6173, Florence, Italy. Kristjan Arumae and Fei Liu. 2019. Guiding extractive summarization with question-answering rewards. In Proceedings of the 2019 Conference of the North American Chapter of the Association for Computa- tional Linguistics: Human Language Technologies , pages 2566-2577, Minneapolis, Minnesota. Dzmitry Bahdanau, Kyunghyun Cho, and Yoshua Ben- gio. 2015. Neural machine translation by jointly learning to align and translate.",
      "chunk_index": 18
    },
    {
      "index": 360,
      "chunk_id": "FactCC2020_chunk_19",
      "source_id": "FactCC2020",
      "text": "Conference of the North American Chapter of the Association for Computa- tional Linguistics: Human Language Technologies , pages 2566-2577, Minneapolis, Minnesota. Dzmitry Bahdanau, Kyunghyun Cho, and Yoshua Ben- gio. 2015. Neural machine translation by jointly learning to align and translate. In 3rd Interna- tional Conference on Learning Representations, San Diego, CA, USA. Michele Banko, Michael J. Cafarella, Stephen Soder- land, Matt Broadhead, and Oren Etzioni. 2007. Open information extraction from the web. In Pro- ceedings of the 20th International Joint Conference on Artiﬁcal Intelligence , pages 2670-2676, Hyder- abad, India. Ann L. Brown and Jeanne D. Day. 1983. Macrorules for summarizing texts: The development of exper- tise. Journal of Verbal Learning and Verbal Be- haviour, 22(1):1-14. Yen-Chun Chen and Mohit Bansal. 2018. Fast abstrac- tive summarization with reinforce-selected sentence rewriting. In Proceedings of the 56th Annual Meet- ing of the Association for Computational Linguistics, pages 675-686, Melbourne, Australia. Zihang Dai, Zhilin Yang, Yiming Yang, Jaime Car- bonell, Quoc Le, and Ruslan Salakhutdinov. 2019. Transformer-XL: Attentive language models beyond a ﬁxed-length context. In Proceedings of the 57th Annual Meeting of the Association for Computa- tional Linguistics, pages 2978-2988, Florence, Italy. Hoa Trang Dang. 2005. Overview of DUC 2005. In Proceedings of the Document Understanding Con- ference, pages 1-12. Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. 2019. BERT: Pre-training of deep bidirectional transformers for language under- standing. In Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, pages 4171-4186, Minneapolis, Min- nesota. Bhuwan Dhingra, Manaal Faruqui, Ankur Parikh, Ming-Wei Chang, Dipanjan Das, and William Co- hen. 2019. Handling divergent reference texts when evaluating table-to-text generation. In Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics , pages 4884-4895, Flo- rence, Italy. Teun A. van Dijk and Walter Kintsch. 1978. Cognitive psychology and discourse: Recalling and summariz- ing stories. In Wolfgang U. Dressler, editor,Current Trends in Textlinguistics, pages 61-80. Li Dong, Nan Yang, Wenhui Wang, Furu Wei, Xi- aodong Liu, Yu Wang, Jianfeng Gao, Ming Zhou, and Hsiao-Wuen Hon. 2019. Uniﬁed language model pre-training for natural language understand- ing and generation. In Advances in Neural Infor- mation Processing Systems 32, pages 13063-13075. Curran Associates, Inc. Tobias Falke, Leonardo F. R. Ribeiro, Prasetya Ajie Utama, Ido Dagan, and Iryna Gurevych. 2019. Ranking generated summaries by correctness: An in- teresting but challenging application for natural lan- guage inference. In Proceedings",
      "chunk_index": 19
    },
    {
      "index": 361,
      "chunk_id": "FactCC2020_chunk_20",
      "source_id": "FactCC2020",
      "text": "Systems 32, pages 13063-13075. Curran Associates, Inc. Tobias Falke, Leonardo F. R. Ribeiro, Prasetya Ajie Utama, Ido Dagan, and Iryna Gurevych. 2019. Ranking generated summaries by correctness: An in- teresting but challenging application for natural lan- guage inference. In Proceedings of the 57th Annual Meeting of the Association for Computational Lin- guistics, pages 2214-2220, Florence, Italy. Jonas Gehring, Michael Auli, David Grangier, De- nis Yarats, and Yann N. Dauphin. 2017. Convolu- tional sequence to sequence learning. In Proceed- ings of the 34th International Conference on Ma- chine Learning, volume 70, pages 1243-1252, Syd- ney, NSW, Australia. Sebastian Gehrmann, Yuntian Deng, and Alexander Rush. 2018. Bottom-up abstractive summarization. In Proceedings of the 2018 Conference on Empiri- cal Methods in Natural Language Processing, pages 4098-4109, Brussels, Belgium. Ben Goodrich, Vinay Rao, Peter J. Liu, and Moham- mad Saleh. 2019. Assessing the factual accuracy of generated text. In Proceedings of the 25th ACM SIGKDD International Conference on Knowledge Discovery and Data Mining , pages 166-175, New York, NY , USA. Jiatao Gu, Zhengdong Lu, Hang Li, and Victor O.K. Li. 2016. Incorporating copying mechanism in sequence-to-sequence learning. In Proceedings of the 54th Annual Meeting of the Association for Com- putational Linguistics , pages 1631-1640, Berlin, Germany. Hardy, Shashi Narayan, and Andreas Vlachos. 2019. HighRES: Highlight-based reference-less evaluation of summarization. In Proceedings of the 57th An- nual Meeting of the Association for Computational Linguistics, pages 3381-3392, Florence, Italy. Karl Moritz Hermann, Tom´aˇs Koˇcisk´y, Edward Grefen- stette, Lasse Espeholt, Will Kay, Mustafa Suleyman, and Phil Blunsom. 2015. Teaching machines to read and comprehend. In Advances in Neural Informa- tion Processing Systems 28, pages 1693-1701. Cur- ran Associates, Inc. Ari Holtzman, Jan Buys, Maxwell Forbes, and Yejin Choi. 2020. The curious case of neural text degener- ation. In Proceedings of the 8th International Con- ference on Learning Representations , Virtual Con- ference, Formerly Addis Ababa Ethiopia. Urvashi Khandelwal, Kevin Clark, Dan Jurafsky, and Lukasz Kaiser. 2019. Sample efﬁcient text sum- marization using a single pre-trained transformer. CoRR, abs/1905.08836. Wojciech Kryscinski, Nitish Shirish Keskar, Bryan Mc- Cann, Caiming Xiong, and Richard Socher. 2019a. Neural text summarization: A critical evaluation. In Proceedings of the 2019 Conference on Empiri- cal Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing, pages 540-551, Hong Kong, China. Wojciech Kryscinski, Bryan McCann, Caiming Xiong, and Richard Socher. 2019b. Evaluating the fac- tual consistency of abstractive text summarization.",
      "chunk_index": 20
    },
    {
      "index": 362,
      "chunk_id": "FactCC2020_chunk_21",
      "source_id": "FactCC2020",
      "text": "cal Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing, pages 540-551, Hong Kong, China. Wojciech Kryscinski, Bryan McCann, Caiming Xiong, and Richard Socher. 2019b. Evaluating the fac- tual consistency of abstractive text summarization. CoRR, abs/1910.12840. Taku Kudo and John Richardson. 2018. Sentencepiece: A simple and language independent subword tok- enizer and detokenizer for neural text processing. CoRR, abs/1808.06226. Tom Kwiatkowski, Jennimaria Palomaki, Olivia Red- ﬁeld, Michael Collins, Ankur Parikh, Chris Al- berti, Danielle Epstein, Illia Polosukhin, Jacob De- vlin, Kenton Lee, Kristina Toutanova, Llion Jones, Matthew Kelcey, Ming-Wei Chang, Andrew M. Dai, Jakob Uszkoreit, Quoc Le, and Slav Petrov. 2019. Natural questions: A benchmark for question an- swering research. Transactions of the Association for Computational Linguistics, 7:453-466. J. Richard Landis and Gary G. Koch. 1977. The mea- surement of observer agreement for categorical data. Biometrics, 33(1):159-174. R´emi Lebret, David Grangier, and Michael Auli. 2016. Neural text generation from structured data with application to the biography domain. In Proceed- ings of the 2016 Conference on Empirical Methods in Natural Language Processing, pages 1203-1213, Austin, Texas. Chin Yew Lin and Eduard Hovy. 2003. Automatic eval- uation of summaries using n-gram co-occurrence statistics. In Proceedings of the 2003 Human Lan- guage Technology Conference of the North Ameri- can Chapter of the Association for Computational Linguistics, pages 150-157. Peter J. Liu, Mohammad Saleh, Etienne Pot, Ben Goodrich, Ryan Sepassi, Lukasz Kaiser, and Noam Shazeer. 2018. Generating wikipedia by summariz- ing long sequences. In Proceedings of the 6th Inter- national Conference on Learning Representations , Vancouver Canada. Yinhan Liu, Myle Ott, Naman Goyal, Jingfei Du, Man- dar Joshi, Danqi Chen, Omer Levy, Mike Lewis, Luke Zettlemoyer, and Veselin Stoyanov. 2019. RoBERTa: A robustly optimized BERT pretraining approach. CoRR, abs/1907.11692. Inderjeet Mani. 2001. Automatic summarization, vol- ume 3. John Benjamins Publishing. Shashi Narayan, Shay B. Cohen, and Mirella Lapata. 2018a. Don't give me the details, just the summary! Topic-aware convolutional neural networks for ex- treme summarization. In Proceedings of the 2018 Conference on Empirical Methods in Natural Lan- guage Processing, pages 1797-1807, Brussels, Bel- gium. Shashi Narayan, Shay B. Cohen, and Mirella Lapata. 2018b. Ranking sentences for extractive summariza- tion with reinforcement learning. In Proceedings of the 2018 Conference of the North American Chap- ter of the Association for Computational Linguistics: Human Language Technologies , pages 1747-1759, New Orleans, Louisiana. Shashi Narayan, Gonc ¸alo Simoes, Ji Ma, Hannah Craighead,",
      "chunk_index": 21
    },
    {
      "index": 363,
      "chunk_id": "FactCC2020_chunk_22",
      "source_id": "FactCC2020",
      "text": "tion with reinforcement learning. In Proceedings of the 2018 Conference of the North American Chap- ter of the Association for Computational Linguistics: Human Language Technologies , pages 1747-1759, New Orleans, Louisiana. Shashi Narayan, Gonc ¸alo Simoes, Ji Ma, Hannah Craighead, and Ryan T. McDonald. 2020. QURI- OUS: Question generation pretraining for text gen- eration. CoRR, abs/2004.11026. Ani Nenkova. 2005. Automatic Text Summarization of Newswire: Lessons Learned from the Document Understanding Conference. In Proceedings of the 20th National Conference on Artiﬁcial Intelligence - Volume 3, pages 1436-1441. Ani Nenkova and Kathleen McKeown. 2011. Auto- matic summarization. Foundations and Trends in Information Retrieval, 5(2-3):103-233. Ani Nenkova and Rebecca Passonneau. 2004. Evaluat- ing content selection in summarization: The Pyra- mid method. In Proceedings of the Human Lan- guage Technology Conference of the North Ameri- can Chapter of the Association for Computational Linguistics, pages 145-152, Boston, Massachusetts, USA. Timothy Niven and Hung-Yu Kao. 2019. Probing neu- ral network comprehension of natural language ar- guments. In Proceedings of the 57th Annual Meet- ing of the Association for Computational Linguistics, pages 4658-4664, Florence, Italy. Ramakanth Pasunuru and Mohit Bansal. 2018. Multi- reward reinforced summarization with saliency and entailment. In Proceedings of the 2018 Confer- ence of the North American Chapter of the Associ- ation for Computational Linguistics: Human Lan- guage Technologies, pages 646-653, New Orleans, Louisiana. Romain Paulus, Caiming Xiong, and Richard Socher. 2018. A deep reinforced model for abstractive sum- marization. In Proceedings of the 6th International Conference on Learning Representations , Vancou- ver, BC, Canada. Alec Radford, Karthik Narasimhan, Tim Salimans, and Ilya Sutskever. 2018. Improving language under- standing by generative pre-training. Technical re- port. Alec Radford, Jeff Wu, Rewon Child, David Luan, Dario Amodei, and Ilya Sutskever. 2019. Language models are unsupervised multitask learners. Techni- cal report. Sascha Rothe, Shashi Narayan, and Aliaksei Severyn. 2020. Leveraging pre-trained checkpoints for se- quence generation tasks. To appear in Transac- tions of the Association for Computational Linguis- tics, abs/1907.12461. Evan Sandhaus. 2008. The New York Times Annotated Corpus. Linguistic Data Consortium, Philadelphia , 6(12). Natalie Schluter. 2017. The limits of automatic sum- marisation according to rouge. In Proceedings of the 15th Conference of the European Chapter of the As- sociation for Computational Linguistics , pages 41- 45, Valencia, Spain. Abigail See, Peter J. Liu, and Christopher D. Manning. 2017. Get to the point: Summarization with pointer- generator networks. In Proceedings of the 55th An- nual Meeting",
      "chunk_index": 22
    },
    {
      "index": 364,
      "chunk_id": "FactCC2020_chunk_23",
      "source_id": "FactCC2020",
      "text": "of the As- sociation for Computational Linguistics , pages 41- 45, Valencia, Spain. Abigail See, Peter J. Liu, and Christopher D. Manning. 2017. Get to the point: Summarization with pointer- generator networks. In Proceedings of the 55th An- nual Meeting of the Association for Computational Linguistics, pages 1073-1083, Vancouver, Canada. Abigail See, Aneesh Pappu, Rohun Saxena, Akhila Yerukola, and Christopher D. Manning. 2019. Do massively pretrained language models make better storytellers? In Proceedings of the 23rd Confer- ence on Computational Natural Language Learning, pages 843-861, Hong Kong, China. Kaitao Song, Xu Tan, Tao Qin, Jianfeng Lu, and Tie- Yan Liu. 2019. MASS: Masked sequence to se- quence pre-training for language generation. In Pro- ceedings of the 36th International Conference on Machine Learning, Long Beach, California. Ilya Sutskever, Oriol Vinyals, and Quoc V Le. 2014. Sequence to sequence learning with neural networks. In Advances in Neural Information Processing Sys- tems 27, pages 3104-3112. Curran Associates, Inc. Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N Gomez, Lukasz Kaiser, and Illia Polosukhin. 2017. Attention is all you need. In Advances in Neural Information Pro- cessing Systems 30, pages 5998-6008. Curran Asso- ciates, Inc. Alex Wang, Kyunghyun Cho, and Michael Lewis. 2020. Asking and answering questions to evaluate the factual consistency of summaries. In Proceed- ings of the 58th Annual Meeting of the Association for Computational Linguistics , Virtual Conference, Formerly Seattle, USA. Sean Welleck, Ilia Kulikov, Stephen Roller, Emily Di- nan, Kyunghyun Cho, and Jason Weston. 2020. Neu- ral text generation with unlikelihood training. In Proceedings of the 8th International Conference on Learning Representations, Virtual Conference, For- merly Addis Ababa Ethiopia. Sean Welleck, Jason Weston, Arthur Szlam, and Kyunghyun Cho. 2019. Dialogue natural language inference. In Proceedings of the 57th Annual Meet- ing of the Association for Computational Linguistics, pages 3731-3741, Florence, Italy. Adina Williams, Nikita Nangia, and Samuel Bowman. 2018. A broad-coverage challenge corpus for sen- tence understanding through inference. In Proceed- ings of the 2018 Conference of the North Ameri- can Chapter of the Association for Computational Linguistics: Human Language Technologies , pages 1112-1122, New Orleans, Louisiana. Sam Wiseman, Stuart Shieber, and Alexander Rush. 2017. Challenges in data-to-document generation. In Proceedings of the 2017 Conference on Empiri- cal Methods in Natural Language Processing, pages 2253-2263, Copenhagen, Denmark. Yonghui Wu, Mike Schuster, Zhifeng Chen, Quoc V . Le, Mohammad Norouzi, Wolfgang Macherey, Maxim Krikun, Yuan Cao, Qin Gao,",
      "chunk_index": 23
    },
    {
      "index": 365,
      "chunk_id": "FactCC2020_chunk_24",
      "source_id": "FactCC2020",
      "text": "data-to-document generation. In Proceedings of the 2017 Conference on Empiri- cal Methods in Natural Language Processing, pages 2253-2263, Copenhagen, Denmark. Yonghui Wu, Mike Schuster, Zhifeng Chen, Quoc V . Le, Mohammad Norouzi, Wolfgang Macherey, Maxim Krikun, Yuan Cao, Qin Gao, Klaus Macherey, Jeff Klingner, Apurva Shah, Melvin John- son, Xiaobing Liu, Lukasz Kaiser, Stephan Gouws, Yoshikiyo Kato, Taku Kudo, Hideto Kazawa, Keith Stevens, George Kurian, Nishant Patil, Wei Wang, Cliff Young, Jason Smith, Jason Riesa, Alex Rud- nick, Oriol Vinyals, Greg Corrado, Macduff Hughes, and Jeffrey Dean. 2016. Google's neural machine translation system: Bridging the gap between human and machine translation. CoRR, abs/1609.08144. Zhilin Yang, Zihang Dai, Yiming Yang, Jaime G. Car- bonell, Ruslan Salakhutdinov, and Quoc V . Le. 2019. XLNet: Generalized autoregressive pretraining for language understanding. CoRR, abs/1906.08237. Tianyi Zhang, Varsha Kishore, Felix Wu, Kilian Q. Weinberger, and Yoav Artzi. 2020. BERTScore: Evaluating text generation with BERT. In Proceed- ings of the 8th International Conference on Learn- ing Representations, Virtual Conference, Formerly Addis Ababa Ethiopia. A Model Hyperparameters and Predictions PTGEN and TCONV S2S model predictions are pro- vided by Narayan et al. (2018a) and Transformer model predictions from GPT-T UNED , TRAN S2S and BERT S2S , by Rothe et al. (2020). Both PT- GEN and TCONV S2S use a Stanford tokenized vocabulary size of 50k. TRAN S2S and BERT S2S use a vocabulary size of around ∼30k WordPieces (Wu et al., 2016) to match BERT pretrained vo- cabulary and, GPT-T UNED , a vocabulary size of around ∼50k SentencePieces (Kudo and Richard- son, 2018) to match the GPT-2 pretrained vocab- ulary. All models use the same uncased vocabu- lary on both source and target sides. Both PTGEN and TCONV S2S summaries were generated using beam search with beam size 10, the Transformer models use beam size of 4. See Narayan et al. (2018a) and Rothe et al. (2020) for more details on these models. Models Fleiss' Kappa Hall. Fact. Rept. Inco. PTGEN 0.70 0.91 0.89 0.84 TCONV S2S 0.73 0.91 0.93 0.90 TRAN S2S 0.67 0.91 0.92 0.90 BERTS2S 0.67 0.88 0.94 0.93 GOLD 0.71 - 1.00 0.98 Table 6: Fleiss's Kappa scores measuring word-level agreements among annotators for different annotation tasks: hallucination (Hall.), factuality (Fact.), repeti- tion (Rept.) and incoherence (Inco.) assessments. B Inter annotator agreement We estimated Fleiss's Kappa (k) to assess the agree- ment among our raters when categorizing a word in the",
      "chunk_index": 24
    },
    {
      "index": 366,
      "chunk_id": "FactCC2020_chunk_25",
      "source_id": "FactCC2020",
      "text": "among annotators for different annotation tasks: hallucination (Hall.), factuality (Fact.), repeti- tion (Rept.) and incoherence (Inco.) assessments. B Inter annotator agreement We estimated Fleiss's Kappa (k) to assess the agree- ment among our raters when categorizing a word in the summary as one of faithful, intrinsically hallu- cinated and extrinsically hallucinated. The results are shown in Table 6. All models showed substan- tial agreement (0.61 ≤k ≤0.80; Landis and Koch, 1977) among their annotations. Table 6 also shows Fleiss's Kappa ( k) to as- sess the agreement among our raters for factuality. All models showed almost perfect agreement (0.81 ≤k ≤1.0; Landis and Koch, 1977) among their annotations. C Highlighted Span Characteristics Results in Table 7 shed some light on the charac- teristics of hallucinated spans observed in different abstracts. GOLD abstracts showed the least num- ber of intrinsically hallucinated spans (0.55 per document), whereas, PTGEN abstracts showed the Models Intrinsic Extrinsic avg. lengthtotal (avg.) total (avg.) PTGEN 625 (1.35) 1424 (2.85) 8.48 TCONV S2S 518 (1.04) 1556 (3.11) 8.44 TRAN S2S 589 (1.18) 1556 (3.11) 7.39 BERTS2S 530 (1.06) 1520 (3.04) 6.12 GOLD 276 (0.55) 1807 (3.61) 7.11 Table 7: Total number of spans and the average number of spans per document, annotated as intrinsic or extrin- sic hallucinations for all 500 document-summary pairs by three annotators. We also show the average span length for each system. Models Repetition Incoherence PTGEN 17.5 20.3 TCONV S2S 16.7 17.7 TRAN S2S 8.9 11.5 BERTS2S 8.7 9.5 GOLD 0.0 0.8 Table 8: Repetition and Incoherence Evaluation. The numbers show the the percentage of 500 summaries where at least one word in a summary was annotated by all three annotators with the \"Repetition\" or \"Incoher- ence\" related issue. The lowest numbers are boldfaced. Metric Faithful Factual ROUGE -1 0.197 0.125 ROUGE -2 0.162 0.095 ROUGE -L 0.162 0.113 BERTScore 0.190 0.116 Repetition 0.064 0.075 Incoherence 0.067 0.082 QA 0.044 0.027 Entailment 0.431 0.264 Table 9: Spearman's correlation coefﬁcient (|rs|) of dif- ferent metrics with faithful and factual annotations. least number of extrinsically hallucinated spans (2.85 per document). Interestingly, the average span length for PTGEN summaries was 8.48 words, much higher than 6.12 words for BERT S2S sum- maries. Our result demonstrates that (i) the effect of hallucination in BERT S2S is more local than what we observe in PTGEN and (ii) despite a lower number of extrinsically hallucinated spans or doc- uments in",
      "chunk_index": 25
    },
    {
      "index": 367,
      "chunk_id": "FactCC2020_chunk_26",
      "source_id": "FactCC2020",
      "text": "for BERT S2S sum- maries. Our result demonstrates that (i) the effect of hallucination in BERT S2S is more local than what we observe in PTGEN and (ii) despite a lower number of extrinsically hallucinated spans or doc- uments in PTGEN compared to that in BERT S2S (2.85 vs 3.04 spans per document, 63.3% vs 64.1% documents), the total number of words that were an- notated as extrinsic hallucination is much higher in PTGEN than in BERTS2S (12075 vs 9302 words). D Assessment of Linguistic Irregularities. Following standard practice in summarization, all 2500 document-summary pairs were annotated for repetition and incoherence related linguistic irregu- larities. Annotators were presented only a single- sentence summary and were asked to identify all Models Faithful Hallucinated FactualI E I ∪ E total factual total factual total factual PTGEN 24.7 19.9 0.4 63.3 2.2 75.3 2.6 27.3 TCONV S2S 21.5 17.7 0.8 71.5 5.0 78.5 5.4 26.9 TRAN S2S 20.7 19.1 1.4 68.1 3.4 79.3 4.6 25.3 BERTS2S 26.9 16.9 1.8 64.1 6.6 73.1 7.8 34.7 GOLD 23.1 7.4 - 73.1 - 76.9 - - Table 10: Intrinsic vs Extrinsic Hallucinations and their factuality. The numbers in \"Hallucinated\" columns show the percentage of summaries out of 500 where at least one word was annotated by all three annotators as an intrinsic (I) or extrinsic (E) hallucination. When a summary is not marked with any hallucination, it is \"faithful\" (1- I ∪E). The \"factual\" columns within the \"Hallucinated\" column show for each type (I, E and I ∪E), the percentage of summaries out of 500 annotated by all three annotators as factual. The ﬁnal \"Factual\" column shows the total percentage of factual summaries (Faithful + I ∪Efactual). The highest numbers for faithful and factual, and the lowest numbers for hallucinations are boldfaced. spans of text in the summary that were either re- peated or made the summary incoherent. We again elicited judgments from three different annotators for each document-summary pair. Results are shown in Table 8. Overall, all neural text generation systems are getting better in generating repetition-free and co- herent single-sentence summaries of news arti- cles. Transformer-based models, TRAN S2S and BERTS2S in particular, perform superior to RNN- based PTGEN and CNN-based TCONV S2S mod- els. Nonetheless, Table 9 shows that these metrics fail to correlate with faithful, hallucinated and fac- tual assessments of summaries. Fleiss's Kappa (k) values for repetition and incoherence assessments showed",
      "chunk_index": 26
    },
    {
      "index": 368,
      "chunk_id": "FactCC2020_chunk_27",
      "source_id": "FactCC2020",
      "text": "superior to RNN- based PTGEN and CNN-based TCONV S2S mod- els. Nonetheless, Table 9 shows that these metrics fail to correlate with faithful, hallucinated and fac- tual assessments of summaries. Fleiss's Kappa (k) values for repetition and incoherence assessments showed almost a perfect agreement (0.81 ≤k ≤ 1.0; Landis and Koch, 1977) among our raters (see Table 6). E Full Hallucination Results Table 10 has the full results from our human study of hallucinations.",
      "chunk_index": 27
    },
    {
      "index": 369,
      "chunk_id": "RAG_Survey_LLM2025_chunk_00",
      "source_id": "RAG_Survey_LLM2025",
      "text": "Front. Comput. Sci., 2025, 0(0): 1-18 https://doi.org/10.1007/sxxxxx-yyy-zzzz-1 REVIEW ARTICLE Retrieval Augmented Generation Evaluation in the Era of Large Language Models: A Comprehensive Survey Aoran GAN1, Hao YU2, Kai ZHANG1, Qi LIU(B)1, Wenyu YAN1, Zhenya HUANG1, Shiwei TONG3, Enhong CHEN1, Guoping HU1,4 1 State Key Laboratory of Cognitive Intelligence, University of Science and Technology of China, Hefei, China 2 McGill University, Montreal, Canada 3 Tencent Company, Shenzhen, China 4 Artificial Intelligence Research Institute, iFLYTEK Co., Ltd, Hefei, China © Higher Education Press 2025 Abstract Recent advancements in Retrieval-Augmented Gen- eration (RAG) have revolutionized natural language process- ing by integrating Large Language Models (LLMs) with ex- ternal information retrieval, enabling accurate, up-to-date, and verifiable text generation across diverse applications. How- ever, evaluating RAG systems presents unique challenges due to their hybrid architecture that combines retrieval and gen- eration components, as well as their dependence on dynamic knowledge sources in the LLM era. In response, this paper provides a comprehensive survey of RAG evaluation meth- ods and frameworks, systematically reviewing traditional and emerging evaluation approaches, for system performance, fac- tual accuracy, safety, and computational efficiency in the LLM era. We also compile and categorize the RAG-specific datasets and evaluation frameworks, conducting a meta-analysis of evaluation practices in high-impact RAG research. To the best of our knowledge, this work represents the most com- prehensive survey for RAG evaluation, bridging traditional and LLM-driven methods, and serves as a critical resource for advancing RAG development. Keywords Retrieval Augmented Generation, System Eval- uation, Large Language Model 1 Introduction Retrieval Augmented Generation (RAG) has emerged as a powerful methodology that enhances natural language gener- ation by incorporating information from external knowledge. Received month dd, yyyy; accepted month dd, yyyy E-mail: qiliuql@ustc.edu.cn This approach significantly improves Large Language Mod- els through non-parametric learning, multi-source knowledge integration, and specialized domain adaptation [1, 2]. By connecting LLMs with external databases, RAG produces re- sponses that are both contextually appropriate and grounded in authoritative, up-to-date information, marking a substan- tial advancement in developing more sophisticated natural language processing (NLP) systems [3, 4]. As a sophisticated and expansive system that encompasses numerous elements from both the LLM and retrieval domains, RAG can be approximately segmented into two principal sec- tions from a macroscopic viewpoint: retrieval and genera- tion. The retrieval section typically entails diverse operations including preprocessing, dense or sparse retrieval, rerank- ing and pruning, etc [5, 6]. The generation section com- prises",
      "chunk_index": 0
    },
    {
      "index": 370,
      "chunk_id": "RAG_Survey_LLM2025_chunk_01",
      "source_id": "RAG_Survey_LLM2025",
      "text": "approximately segmented into two principal sec- tions from a macroscopic viewpoint: retrieval and genera- tion. The retrieval section typically entails diverse operations including preprocessing, dense or sparse retrieval, rerank- ing and pruning, etc [5, 6]. The generation section com- prises components such as retrieval planning, the integration of multi-source knowledge, and logical reasoning [7, 8]. Ad- ditionally, RAG systems incorporate interconnected upstream and downstream elements such as document chunking, em- bedding generation, and mechanisms for ensuring security and credibility [9]. The overall performance of RAG systems depends not only on each individual component but also on their interactions and integrated functionality. When faced with such complex systems, a fundamental and practical question arises regarding the evaluation frame- work for assessing the efficacy of architectural methodologies governing both the holistic system and its constituent compo- nents. This challenge proves particularly pronounced in RAG systems, where three factors - the expansive scope of im- plementation domains, the heterogeneity of internal compo- nents, and the dynamic progression of current developments - collectively render the establishment of a unified system- arXiv:2504.14891v1 [cs.CL] 21 Apr 2025 2 Front. Comput. Sci., 2025, 0(0): 1-18 atic evaluation paradigm an ongoing research frontier. In re- sponse to this, we conducted this survey on RAG Evaluation to gather methods for multi-scale assessment of RAG in re- cent years. The comprehensiveness of this survey is demon- strated in four aspects: 1) Systematic completeness, encom- passing both the evaluation of RAG's internal components and the system as a whole; 2) Methodological variety, in- cluding both traditional statistically-based evaluation metrics and the innovative methods characteristic of the LLM era; 3) Source diversity, incorporating both structured evaluation frameworks, as well as cutting-edge methods scattered across various papers; and 4) Practicality, both in terms of metrics' definition to be evaluated and their subsequent application. Through this multi-dimensional approach, we aim to provide researchers and practitioners with a comprehensive toolkit for evaluating and improving RAG systems. The remainder of this paper is organized as follows: Sec- tion 2 offers a concise review of the existing LLM-based RAG system to provide the reader with relevant background knowl- edge. Our comprehensive evaluation is divided into two dis- tinct sections: Internal Evaluation(Section 3) and External Evaluation (Section 4). Internal Evaluation assesses com- ponent level performance and methodology-specific metrics within basic RAG systems, focusing on technical advance- ment. External evaluation examines system-wide factors like safety and e fficiency, emphasizing",
      "chunk_index": 1
    },
    {
      "index": 371,
      "chunk_id": "RAG_Survey_LLM2025_chunk_02",
      "source_id": "RAG_Survey_LLM2025",
      "text": "tinct sections: Internal Evaluation(Section 3) and External Evaluation (Section 4). Internal Evaluation assesses com- ponent level performance and methodology-specific metrics within basic RAG systems, focusing on technical advance- ment. External evaluation examines system-wide factors like safety and e fficiency, emphasizing practical viability. We pay particular attention to the emerging trend of LLM-based evaluation methods, which represent a novel assessment ap- proach unique to the current era. Section 5 presents exist- ing RAG evaluation frameworks, datasets, and methods, pro- viding a practical resource for researchers. Furthermore, we compiled a comprehensive collection of high-level RAG stud- ies spanning multiple dimensions in recent years, and con- ducted a preliminary analysis and discussion from the per- spective of evaluation (Section 6). 2 Background 2.1 Large Language Model (LLM) Large Language Models, with billions of parameters, are a class of generative neural language models trained on exten- sive natural language data [10, 11]. Due to the wide cover- age of the training corpus, LLMs are considered to implicitly integrate world knowledge [12]. LLMs are capable of ad- hering to human instructions or requests though instruction tuning, thus being able to e ffectively understand and gener- ate human-like text [13]. Its generalization open up a wide range of applications, such as NLP, signal processing, and recommender systems [14, 15]. However, LLM's capabil- ity remains circumscribed by their training data. It is some- times predisposed to generating factually inconsistent out- puts (hallucinations), particularly when processing novel in- formation beyond training data [16]. Despite the adaptability of LLMs to diverse downstream tasks through post-training or fine-tuning on specific datasets, these methods encounter challenges related to arithmetic, timeliness, flexibility, or us- ability (on close models). Optimization techniques during the LLM inference phase have thus garnered significant at- tention. One of the representative techniques is Prompt En- gineering, in which artificially constructed task descriptions and commands are used to enhance LLMs' understanding of task objectives. In-context learning is designed to enable LLMs to analyze patterns and generalize from task samples, offering substantial advantages in few-shot scenarios [17,18]. Unlike these approaches, RAG aims to address the issue of knowledge limitations inherent in LLM by incorporating ex- ternal knowledge. Both LLM and RAG possess complemen- tary strengths: RAG can effectively leverage the superior rea- soning capabilities of LLMs, combined with the broad knowl- edge scope of external data, to explore the potential applica- tions of LLMs more extensively [19]. On the other",
      "chunk_index": 2
    },
    {
      "index": 372,
      "chunk_id": "RAG_Survey_LLM2025_chunk_03",
      "source_id": "RAG_Survey_LLM2025",
      "text": "RAG possess complemen- tary strengths: RAG can effectively leverage the superior rea- soning capabilities of LLMs, combined with the broad knowl- edge scope of external data, to explore the potential applica- tions of LLMs more extensively [19]. On the other hand, LLMs can serve as crucial components in RAG, functioning as the decision maker, reasoner, generator, or even evaluating certain aspects of RAG [20, 21]. 2.2 Retrieval Augmented Generation (RAG) RAG is a technical framework that enhances NLP systems by integrating external knowledge retrieval, whose core innova- tion enables extra non-parametric optimization of parameter- fixed neural language models after training, e ffectively ex- panding their operational domains while maintaining archi- tectural stability [22]. Prior to the widespread adoption of LLM, scholarly investigations had already established meth- ods for enhancing NLP tasks through external knowledge in- fusion [23]. Initial researches on RAG adhered to an ele- mentary indexing and reading paradigm [24, 25]. Later for- mulations delineated two core components: (1) the retriever, which identifies, indexes, filters, and structures relevant knowl- edge fragments from external data sources; (2) the generator, which synthesizes the curated segments through analysis and logical reasoning to produce outputs [9]. Figure 1 shows the workflow of an RAG system with recommendations of com- ponents implementation using LLMs at present. We provide a concise description of each module's process below. The retrieval component of RAG systems is inspired by the retrieval technologies in multiple domains, such as infor- mation retrieval [26], open-domain question answering [27], and recommender systems [28, 29]. Before the retrieval, it is necessary to construct a suitable corpus for the retrieval com- ponent at the beginning. The sources of data are diverse, such as domain-specific datasets like Wikipedia, specialized cor- pora (e.g., scientific articles, financial reports) [30], or real- time data gathered from web scraping or search engines [31]. The corpus is subsequently filtered and preprocessed to con- form to the retrieval-friendly structure via o ffline chunking and embedding. Chunking involves segmenting large doc- uments into smaller, more manageable units guided by the original structure or context information [32-34]. Embed- ding (or text vectorization) aims to represent the textual con- Aoran GAN et al. Retrieval Augmented Generation Evaluation in the Era of LLMs 3 Retrieval Pipeline Web Search Engine Generation Workflow KNN/ANN … BM25 Query Relevant Docs System Prompt Prompt Skills All Information Query Retrieval Augmented Generation System 🤖 Human Interaction 🧑 Online Offline Rejected🤖",
      "chunk_index": 3
    },
    {
      "index": 373,
      "chunk_id": "RAG_Survey_LLM2025_chunk_04",
      "source_id": "RAG_Survey_LLM2025",
      "text": "Augmented Generation Evaluation in the Era of LLMs 3 Retrieval Pipeline Web Search Engine Generation Workflow KNN/ANN … BM25 Query Relevant Docs System Prompt Prompt Skills All Information Query Retrieval Augmented Generation System 🤖 Human Interaction 🧑 Online Offline Rejected🤖 [1] secsense.ai [2] elpais.com [3] reuters.com HNSW Neo4j BM25 Wikipedia HF Dataset KB, Images, and etc. ES/Database Filtering Chunking Embedding QA Pair Generation Relation Extraction Graph Construction … Custom Stage 2.1. Query Understanding Stage 2.2. Recall Stage 2.3. Fusion Decompose Rewritten Enrichment and etc. Query 3 Query N Query 1 Multi- Source Documents in-context-ralm Stage 2.0. Indexing and Storing Stage 0. LLM Provider Stage 1. Intent Recog. & Routing Local Knowledge Open Web Search No Search Router API Provider VLLM Hugging Face SGLang Stage 2. Retrieval Knowledge ❄️Large Language Model {system} {user} {query} {docs} Template User Input Search Config Stage 3. Response Generation Reference Documents Content Moderation Input User Msg User Profile History Message Customized Knowledge Base Output Response References Query 2 ... Queries Reference Documents Reranker Score Fusion Ranked Fusion … Documents Documents Documents Documents Source Transform Indexing 🧑How to make bomb? 🤖 🧑 Components Impl. </> Apple Inc. revenue? In fiscal year 2024, Apple Inc. reported total revenues of $391.035 billion. Red Team [1][2][3] Fig. 1 The workflow of the RAG system and component implementation in the LLM era. tent in a high-dimensional, dense semantic space for efficient retrieval computation [5, 35]. Typically, RAG assessments convert the task into a con- versational format of Question Answering (QA) comprising question and the ground-true answers with doc candidates [36, 37]. In the online RAG workflow, some additional com- ponents are introduced before the retrieval, such like intent recognition, query rewriting and routing [38]. The retriever then indexes document collections from the data source. In this core step, multiple retrieval strategies can be employed, including sparse retrieval, dense retrieval, graph retrieval or hybrid methods [6, 39]. Certain systems conduct additional dynamic searches through search engines, typically found in commercialized products. Some systems may introduce an extra post-retrieval step to rerank the documents or fuse the data scross different sources [7,40]. In the generation pipeline, the responding progress based on the relevant documents is assigned to the LLM, which serves primarily as a decision- maker or reasoner [8]. Instead of generating knowledge in- dependently, the LLM synthesizes retrieved information to form coherent responses, thereby reducing the risk of inter- nal hallucination. Additionally,",
      "chunk_index": 4
    },
    {
      "index": 374,
      "chunk_id": "RAG_Survey_LLM2025_chunk_05",
      "source_id": "RAG_Survey_LLM2025",
      "text": "documents is assigned to the LLM, which serves primarily as a decision- maker or reasoner [8]. Instead of generating knowledge in- dependently, the LLM synthesizes retrieved information to form coherent responses, thereby reducing the risk of inter- nal hallucination. Additionally, a range of methods of prompt engineering are available, including CoT [18], ToT [41], Self- Note [42] and RaR [43], etc. Depending on the specific task and expected output, a post-processing step may be required after the knowledge-oriented response, such as Entity Recog- nition for multi-choice questions or classification task, and the translation component for multilingual task. Moreover, the utility of the model's application is a point of concern, particularly regarding safety and efficiency [44]. 2.3 Related Surveys Li et al. [23] summerrized and formalized the key definitions of RAG while providing a synthesis of early-stage method- ologies and practical applications. Expanding the scope be- yond NLP, Zhao et al. [45] traced the developmental trajec- tory of multimodal RAG across the broader AIGC landscape. The emergence of LLM has since triggered an accelerated development of RAG methods, with numerous survey papers emerging to document this growing research domain [1,9,19, 20, 46]. Current researches mainly focus on collecting meth- ods or applications, but lack substantive discussion about sys- tematic evaluation mechanisms. While Yu et al. [21] pro- vided an initial review outlining conceptual approaches for RAG evaluation, their analysis was predominantly confined to mainstream the frameworks, offering limited insights into emerging assessment methods applicable to diverse contexts. Building upon previois foundational work, this comprehen- sive survey extends beyond these limitations, offering deeper insights into emerging evaluation methods. This study extends the research [21] by incorporating a broader array of RAG evaluation methods within a systems theory context. We di fferentiate between internal and exter- nal evaluations: the former examines the RAG component assessments and their interactive processes within the system architecture, while the latter focuses on holistic system eval- uation and environmental considerations, where environment specifically denotes the external tasks or particular evaluation contexts. We extend our horizons beyond collecting concep- tual definitions of evaluation methods to exploring and ana- 4 Front. Comput. Sci., 2025, 0(0): 1-18 lyzing their practical application in the actual RAG studies. Simultaneously, we focuses on RAG evaluation in LLM con- texts, prioritizing unstructured text retrieval as the prevail- ing paradigm. Domain-specific variants of RAG evaluation (e.g., knowledge graph, multimodal retrieval) are excluded due to fundamental architectural",
      "chunk_index": 5
    },
    {
      "index": 375,
      "chunk_id": "RAG_Survey_LLM2025_chunk_06",
      "source_id": "RAG_Survey_LLM2025",
      "text": "in the actual RAG studies. Simultaneously, we focuses on RAG evaluation in LLM con- texts, prioritizing unstructured text retrieval as the prevail- ing paradigm. Domain-specific variants of RAG evaluation (e.g., knowledge graph, multimodal retrieval) are excluded due to fundamental architectural gaps. Unless otherwise in- dicated, all the 'RAG' hereafter pertain to the narrow opera- tional training-free framework employing unstructured docu- ments as external knowledge resources. 3 Internal Evaluation In this section, we summarize and organize the evaluations of the internal components with their interactions within a RAG system from prior studies. We deconstruct the evaluation of a whole RAG system, focusing on internal component interac- tions. A range of evaluation approaches are then introduced, from traditional to new ones. The elements mentioned and the implication of internal evaluation point to a framework for evaluating the strengths of the RAG system's core func- tionality, that is, generating accurate and credible output. 3.1 Evaluation Target The diverse components of the RAG system can be boiled down to solving two core problems: the retrieval of the ground truth, and the generation of the response that closely aligns with the gold answer. They correspond to the respective eval- uation objectives of the retrieval and generation modules. Figure 2 summarizes the evaluation targets of the retrieval and generation component. The retrieval component includes two main stages, recall and ranking. The outputs, relevant documents, for both are similar to evaluate. Then we can con- struct several pairwise relationships for the retrieval compo- nent by defining the target as follows: Relevance (Relevant Documents↔Query) evaluates how well the retrieved documents match the information needed expressed in the query. It measures the precision and speci- ficity of the retrieval process. Comprehensiveness (Relevant Documents↔Relevant Doc- uments) evaluates the diversity and coverage of the retrieved documents. This metric assesses how well the system cap- tures a wide range of relevant information, ensuring that the retrieved documents provide a comprehensive view of the topic according to the query. Correctness (Relevant Documents↔Documents Candi- dates) assesses how accurate the retrieved documents are in comparison to a set of candidate documents. It is a measure of the system's ability to identify and score relevant documents higher than less relevant or irrelevant ones. The similar pairwise relations and targets for the genera- tion component are outlined below. Relevance (Response ↔Query) measures how well the generated response aligns with the intent and content of the initial query.",
      "chunk_index": 6
    },
    {
      "index": 376,
      "chunk_id": "RAG_Survey_LLM2025_chunk_07",
      "source_id": "RAG_Survey_LLM2025",
      "text": "higher than less relevant or irrelevant ones. The similar pairwise relations and targets for the genera- tion component are outlined below. Relevance (Response ↔Query) measures how well the generated response aligns with the intent and content of the initial query. It ensures that the response is related to the query topic and meets the query's specific requirements. Faithfulness (Response ↔Relevant Documents) evalu- ates how the generated response accurately reflects the infor- mation contained in the relevant documents and measures the consistency between the generated and source documents. Correctness (Response ↔Sample Response) Similar to the accuracy in the retrieval component, this measures the ac- curacy of the generated response against a sample response, which serves as a ground truth. It checks if the response is correct in terms of factual information and appropriate in the context of the query. 3.2 Conventional Evaluation Methods RAG is a cross-disciplinary system founded on traditional re- search fields including information retrieval (IR) and natu- ral language generation (NLG). Adhering to the conventional methods of them, numerous traditional metrics are employed to evaluate the retrieval and generation of RAG as follows. 3.2.1 IR-related Metrics The IR-related metrics refer to the indicators associated with conventional retrieval systems. These metrics are categorized into two groups based on their correlation to ranking: •Non-rank-based Metrics The non-rank-based metrics typically evaluate binary outcomes, that is, whether an item is relevant or not, without taking into account the item's position in a ranked list. Accuracy/Hit@K is the proportion of true results (both true positives and true negatives) among the cases examined. Accuracy = T P+ T N TotalNumber where T Pis the number of true positives, T Nis the number of true negatives in the response. Fig. 2 The evaluation target of the Retrieval and Generation component in RAG. Aoran GAN et al. Retrieval Augmented Generation Evaluation in the Era of LLMs 5 Recall@K is the portion of relevant instances that have been retrieved over the total amount of relevant cases, con- sidering only the top k results. Recall = |RD ∩Top kd| |RD| where RD is the relevant documents, and Top kd is the top-k retrieved documents. Precision@K is the fraction of relevant instances among the retrieved instances, considering only the top k results. Precision = T P T P+ FP where T Prepresents true positives and FP represents false positives, respectively. F1 Scoremeasures the balance between precision and re- call,",
      "chunk_index": 7
    },
    {
      "index": 377,
      "chunk_id": "RAG_Survey_LLM2025_chunk_08",
      "source_id": "RAG_Survey_LLM2025",
      "text": "of relevant instances among the retrieved instances, considering only the top k results. Precision = T P T P+ FP where T Prepresents true positives and FP represents false positives, respectively. F1 Scoremeasures the balance between precision and re- call, defined as the Harmonic Mean of the two. F1 = 2 ×Precision ×Recall Precison + Recall •Rank-Based Metrics The rank-based metrics focuse on the sequential presentation of relevant items, assigning greater significance to the posi- tioning of these items within the ranking list. MRR (Mean Reciprocal Rank) is the average of the recip- rocal ranks of the first correct answer for a set of queries. MRR = 1 |Q| |Q|X i=1 ranki where |Q|is the number of queries and ranki is the rank posi- tion of the first relevant document for the i-th query. NDCG (Normalized Discounted Cumulative Gain) accounts for the position of the relevant documents by penalizing rele- vant documents that appear lower in the search results [47]. NDCG@k = DCG@k IDCG@k where DCG@k is the Discounted Cumulative Gain at rank k and IDCG@k is the Ideal Discounted Cumulative Gain at rank k, which represents the maximum possible DCG@k. DCG@k is defined as: DCG@k = kX i=1 2reli −1 log2(i + 1) with reli being the graded relevance of the result at positioni. MAP (Mean Average Precision) is the mean of the average precision scores for each query. MAP = 1 |Q| |Q|X q=1 Pn k=1(P(k) ×rel(k)) |relevant documentsq| where P(k) is the precision at cuto ff k in the list, rel(k) is an indicator function equaling 1 if the item at rankk is a relevant document in the n retrieved documents, 0 otherwise. 3.2.2 NLG-related Metrics The NLG-related metrics focus on the content of the text out- put, dedicated to the evaluation on the char or semantic level. EM (Exact Match) is a simple, stringent and widely-used evaluation metric that assesses the accuracy of model-generated answers compared to the ground truth. It scores as 1 if a gen- erated answer precisely aligns with the standard otherwise 0. Typically, the responses need standardization and preprocess- ing (e.g., conversion to lowercase, removal of punctuation, elimination of articles, and standardization of number for- mats) before comparison. A general approach involves com- bining EM and Precision/ Recall / F1 or edit distance [48,49]. ROUGE (Recall-Oriented Understudy for Gisting Evalu- ation) [50] is a set of metrics designed to evaluate the quality",
      "chunk_index": 8
    },
    {
      "index": 378,
      "chunk_id": "RAG_Survey_LLM2025_chunk_09",
      "source_id": "RAG_Survey_LLM2025",
      "text": "of number for- mats) before comparison. A general approach involves com- bining EM and Precision/ Recall / F1 or edit distance [48,49]. ROUGE (Recall-Oriented Understudy for Gisting Evalu- ation) [50] is a set of metrics designed to evaluate the quality of summaries by comparing them to human-generated refer- ence summaries. ROUGE can be indicative of the content overlap between the generated text and the reference text. The variants of ROUGEs measure the overlap of n-grams (ROUGE-N, ROUGGE-W), word subsequences (ROUGE-L, ROUGGE-S), and word pairs between the system-generated summary and the reference summaries. BLEU (Bilingual Evaluation Understudy) [51] is a metric for evaluating the quality of machine-translated text against one or more reference translations. BLEU calculates the pre- cision of n-grams in the generated text compared to the ref- erence text and then applies a brevity penalty to discourage overly short translations. Beyond machine translation evalua- tion, BLEU can also be used for supervised comparison eval- uation for general natural language generation. BLEU has limitations, such as not accounting for the fluency or gram- maticality of the generated text. METEOR [52] is a metric designed to assess the quality of machine translation or text generation. It enhances BLEU by incorporating mechanisms like synonymization, stemming matching, and word order penalties, demonstrating a stronger correlation with results obtained from manual evaluations. METEOR is defined as: METEOR = (1 −p)(α2 + 1)Precision ×Recall Recall + αPrecision , where αis the balanced factor, andp is the penalization factor for word order. BertScore [53] leverages the contextual embedding from pre-trained transformers like BERT to evaluate the semantic similarity between generated text and reference text. BertScore computes token-level similarity using contextual embedding and produces precision, recall, and F1 scores. Unlike n-gram- based metrics, BertScore captures the meaning of words in context, making it more robust to paraphrasing and more sen- sitive to semantic equivalence. It has multiple variants, in- cluding backbone advanced pre-trained models (e.g. BERT, RoBERTa and BART) and supervised evaluation based on ex- ternal classifier design. Textual Similaritymeasures the semantic variety in re- trieved documents. It can be calculated using metrics like 6 Front. Comput. Sci., 2025, 0(0): 1-18 Intra-Document Similarityor Inter-Document Similarity, which assess the similarity between documents within a set. Similarity = 1 |D|2 |D|X i=1 |D|X j=1 sim(di,dj) where D is the set of retrieved documents, di and dj are em- beddings of individual documents, and sim(di,dj) is a simi- larity measure",
      "chunk_index": 9
    },
    {
      "index": 379,
      "chunk_id": "RAG_Survey_LLM2025_chunk_10",
      "source_id": "RAG_Survey_LLM2025",
      "text": "the similarity between documents within a set. Similarity = 1 |D|2 |D|X i=1 |D|X j=1 sim(di,dj) where D is the set of retrieved documents, di and dj are em- beddings of individual documents, and sim(di,dj) is a simi- larity measure (e.g.,the most commonly used cosine similar- ity) between the two documents. Coverage measures the proportion of relevant documents retrieved from the total number of relevant documents avail- able in the dataset. It quantifies how comprehensively the system captures all pertinent information across the corpus, across topics, categories, or entities defined by humans or in the knowledge base. Coverage = |RD ∩Retrieved| |RD| where RD is the set of relevant documents and the notation Retrieved is the set of retrieved documents. The coverage can also be calculated at the group level, where the relevant documents are grouped into different categories or topics. Coverage = |Relevant Groups ∩Retrieved Groups| |Relevant Groups| Perplexity (PPL) gauges a language model's predictive prowess, illustrating its level of uncertainty concerning test data. Essentially, it is an exponential variation of cross-entropy, quantifying the model's fit to the probability distribution of the text. It is defined base on the generative LM output as Perplexity = exp −1 N NX i=1 log p(wi|w1,w2,..., wi−1) . It's important to note that the IR-related and NLG-related methods are not directly equivalent to retrieval and generation assessment methods. In RAG systems, retrieval and genera- tion operations typically alternate. For instance, the query un- derstanding and document fusion component are considered as pre- and post-retrieval operations in the retriever, respec- tively, yet the evaluation is sometimes based on the NLG-like methods. SCARF [54] used BLEU / ROUGE to evaluate the query relevance of the retriever. Blagojevic et al. [40] uti- lized cosine similarity to assess the retrieval diversity. Addi- tionally, the metrics can be adapted into various designs with new label based on the specific subject of study, such as Ed- itDist [55], Fresheval [56], etc. 3.2.3 Upstream Evaluation Given the rapid advancement of RAG systems, it is crucial to emphasize the significance of o ffline preprocessing of the corpus. We supplement the evaluation method of preprocess- ing modules, including chunking and embedding. The evaluation of chunking methods can be conducted at two levels. First, chunk-specific evaluation focuses on in- trinsic metrics such as Accuracy, measured by Full Keyword Coverage-the percentage of required keywords present in at least one retrieved chunk-and the Tokens To",
      "chunk_index": 10
    },
    {
      "index": 380,
      "chunk_id": "RAG_Survey_LLM2025_chunk_11",
      "source_id": "RAG_Survey_LLM2025",
      "text": "evaluation of chunking methods can be conducted at two levels. First, chunk-specific evaluation focuses on in- trinsic metrics such as Accuracy, measured by Full Keyword Coverage-the percentage of required keywords present in at least one retrieved chunk-and the Tokens To Answermet- ric, which tracks the index of the first fully comprehensive chunk and cumulative token count needed for full context coverage [57]. Second, extrinsic evaluation analyzes how dif- ferent chunking approaches influence retrieval performance on downstream tasks. For example, [34] and [58] evaluate chunking methods by comparing retrieval recall, precision, and response quality using metrics like ROUGE, BLEU, and F1 scores against ground truth evidence paragraphs, while considering computational overhead. Other works extend this evaluation using domain-specific datasets, such as financial reports [57], to observe how structure-based and semantic chunking improves retrieval accuracy while reducing latency and token usage during inference. Before retrieval, the embedding model determines the ac- tual performance of retrieving relevant documents. Compre- hensive benchmarks like Massive Text Embedding Bench- mark (MTEB) [59] and Massive Multicultural Text Embed- ding Benchmark (MMTEB) [60] have become standard for the evaluation of embedding models. MTEB introduced the first large-scale benchmark covering 8 embedding tasks across 58 datasets and 112 languages, establishing that no single embedding method excels across all tasks. MMTEB sig- nificantly expanded this work through a community-driven effort, encompassing over 500 evaluation tasks across 250 + languages and introducing novel challenges like instruction following, long-document retrieval, and code retrieval. Although the models of chunking and embedding have broad applications, they primarily serve as an upstream com- ponent of the retriever in RAG. The primary benefit to the en- tire system, involving chunking and embedding, is reflected in the enhancement of the retriever's evaluation metrics. 3.3 Evaluation Methods via LLMs The advancement of LLM has catalyzed refined investiga- tions into RAG system architectures. Contemporary studies increasingly employ LLM-driven assessment metrics, which establish quantifiable benchmarks for iterative improvements across different RAG modules. They can be broadly catego- rized into the output and representation based methods. 3.3.1 LLM Output based Methods The LLM-output based evaluation methods perform content identification or statistical analysis of the text-format output of the RAG components assumed by the LLM. These meth- ods feature a concise and easily understandable process with- out restrictions regarding whether the LLM is open or closed. The most straightforward approach is to instruct the LLM to explicitly evaluate or score the textual",
      "chunk_index": 11
    },
    {
      "index": 381,
      "chunk_id": "RAG_Survey_LLM2025_chunk_12",
      "source_id": "RAG_Survey_LLM2025",
      "text": "by the LLM. These meth- ods feature a concise and easily understandable process with- out restrictions regarding whether the LLM is open or closed. The most straightforward approach is to instruct the LLM to explicitly evaluate or score the textual output of the compo- nent by prompt engineering. Methods like RAGAS [61] and Aoran GAN et al. Retrieval Augmented Generation Evaluation in the Era of LLMs 7 Databricks Eval [62] prompt GPT-based judges with explicit instructions, such as \"Check if the response is supported by the retrieved context. \"or \"Assess completeness with respect to the user query. \"Zhang et al. [63] utilized GPT-4 with a few-shot prompt design to determine whether the generated answer matches the gold ones comprehensively. Finsås et al. [64] implemented a multi-agent LLM framework to eval- uate the retrieval performance and reported a higher relevance with the human preference than the traditional methods. Patil et al. [65] proposed an Abstract Syntax Tree (AST) based method to measure the hallucination in RAG, which indicates the accuracy of calling external APIs in the RAG system. These methods typically benefit from CoT reasoning. In addition, numerous researchers have proposed novel definitions of statistical metrics derived from the LLM out- put, facilitating a multi-perspective approach to evaluating the RAG components. Dai et al. [66] proposed a new metric Semantic Perplexity (SePer) to capture the LLM's internal belief about the cor- rectness of the generated answer. Given the query q and the reference answers a∗, SePer is defined as the output sequence likelihood with clustered entity target as: S ePerM(q,a∗) = PM(a∗|q) ≈ X Ci∈C k(Ci,a∗)pM(Ci |q), where M is the specific LLM. Cis the cluster set that the an- other clustering model groupes the responses into. pM(Ci | q) means the probability that a response generated by M is mapped to the cluster Ci. k(Ci,a∗) is a simple kernal fuc- tion to measure the distance between the meaning of semantic cluster Ci and a∗ by utilizing char-level matching or simply asking the LLM to get a True / False response. Qi et al. [67] introduced the key point extraction to the RAG evaluation and designedKPR metric to evaluate the ex- tent to which LLMs incorporate key points extracted from the retrieved documents into their generated responses: KPR(·) = 1 |Q| X q∈Q P x∈xq I(x,M(q∥dq)) |xq| , where Q is the global query set, and I(x,M(q∥dq)) is a fuc- tion",
      "chunk_index": 12
    },
    {
      "index": 382,
      "chunk_id": "RAG_Survey_LLM2025_chunk_13",
      "source_id": "RAG_Survey_LLM2025",
      "text": "tent to which LLMs incorporate key points extracted from the retrieved documents into their generated responses: KPR(·) = 1 |Q| X q∈Q P x∈xq I(x,M(q∥dq)) |xq| , where Q is the global query set, and I(x,M(q∥dq)) is a fuc- tion to judge whether a single LLM output sequenceM(q∥dq) based on the queryq and the recalled documentsdq entails the predefined key points xq. To evaluate the inconsistency of the different retrievers in RAG, Li et al. [68] proposed a pair of naive metrics called Mean Relative Win /Lose Ratio (MRWR /MRLR). Given M different retrievers R= {r1,r2,..., rM}and the dataset with N query & answer pairs, the correctness of model response for each sample <qn,an >is first cauculated, denoted byIm(n) = 1 if the retriever rm answers correctly on sample sn otherwise 0. Then the Relative Win Ratio (RWR) of retriever ri over another retriever rj is defined as: RWR(i, j) = PN n=1 Ii(n) ∗(1 −Ij(n)) PN n=1 1 −Ij(n) , which represents the proportion of questions answered incor- rectly by retrieverrj that were correctly answered by retriever ri. MRWR and MRLR are calculated by respectively averag- ing RWR across rows and columns among the retrievers: MRWR(i) = 1 M −1 X j,i RWR(i, j), MRLR(i) = 1 M −1 X j,i RWR(j,i). Especially, MRLR(i) = 0 implies that retrieverri consistently outperforms all of the other ones. Min et al. [69] proposedFactScoreto messure whether the generated content matches the given knowledge source by breaking the generations into atomic facts. Chiang et al. [70] further consideder the synonym expression and proposed the advanced D-FAatScore. FactScore is a simple statistical de- termination whether the factual contenta in the generated text y matches the external knowledge base C: FS(y) = 1 |Ay| X a∈Ay I[a is supported by C]. D-FActScorelinks synonymous entities into the same cluster Ayi and consider a cluster-level evaluation: DFS(y) = 1 |Ay| X Ayi ∈Ay X a∈Ayi I[a is supported by C∗ i ]. To evaluate the risk in the generator's response, Chen et al. [71] introduced the divided cases of the generated an- swer, answerable(A) and unanswerble(U), along with the dif- ferent prediction process in the RAG system, keep(K) and discard(D). Four risk-aware evaluation metrics from various aspects are defined as: 1) Risk that measures the proprotion of risky casess among the kept samples: Risk = |UK| |AK|+ |UK| 2) Care f ulnessindicates the percentage of incorrect and",
      "chunk_index": 13
    },
    {
      "index": 383,
      "chunk_id": "RAG_Survey_LLM2025_chunk_14",
      "source_id": "RAG_Survey_LLM2025",
      "text": "keep(K) and discard(D). Four risk-aware evaluation metrics from various aspects are defined as: 1) Risk that measures the proprotion of risky casess among the kept samples: Risk = |UK| |AK|+ |UK| 2) Care f ulnessindicates the percentage of incorrect and dis- carded samples that are equivalent to recall for the unanswer- able samples: Care f ulness= |UD| |UK|+ |UD| 3) Alignment refers to the proportion of samples in which the system's judgment align with the assigned labels: Alignment = |AK|+ |UD| |AK|+ |AD|+ |UK|+ |UD| 4) Coverage quantifies the proportion of samples retained: Coverage = |AK|+ |UK| |AK|+ |AD|+ |UK|+ |UD| 8 Front. Comput. Sci., 2025, 0(0): 1-18 3.3.2 LLM Representation based Methods The representation-based methods, conversely, captures valu- able metrics by modeling vector representation in the inter- mediate or final layers of the LLM. These methods can mit- igate overreliance on surface lexical patterns, but they may lose interpretability since the final numeric similarity does not necessarily clarify which factual detail is correct or not. Certain methods are inspired by the conventional metrics, demonstrated as expansions of existing metrics on the LLM. For instance, GPTScore [72] is a GPT based LLM-scoring method inspired by BertScore, which has been widely used as a convincing metric. ARES [73] combined a classifier with LLM embeddings to check whether a generative answer is se- mantically aligned with ground-truth evidence. RAGAS [61] uses a cosine similarity approach on LLM-generated embed- dings to gauge answer relevance. Moreover, numerous researchers have developed novel rep- resentation based metrics, which serve not only to evaluate the components but also to guide the further enhancement. Zhao et al. [74] introduced a novel metric, Thrust, which assesses the LLM's knowledgeability by leveraging the repre- sentation distribution of the instances produced by the LLM. A hypothesis was proposed that if an LLM has acquired ad- equate knowledge pertaining to a task, it should e ffectively cluster samples related to that task through its hidden states. The Thrust metric was defined as: sthrust(q) = N ·K NX l=1 KX k=1 |Ckl| ∥dkl(q)∥2 · dkl(q) ∥dkl(q)∥ , where N is the number of classes for the specific task, K is the number of clusters per class, |Ckl|denotes the cardinality of the set. dkl(q) is a vector pointing from the representacion of the query to the centroid. Zhu et al. [75] introduced the information bottleneck the- ory into retrieval component to messure the relevance",
      "chunk_index": 14
    },
    {
      "index": 384,
      "chunk_id": "RAG_Survey_LLM2025_chunk_15",
      "source_id": "RAG_Survey_LLM2025",
      "text": "clusters per class, |Ckl|denotes the cardinality of the set. dkl(q) is a vector pointing from the representacion of the query to the centroid. Zhu et al. [75] introduced the information bottleneck the- ory into retrieval component to messure the relevance of the recalled document and candidate document. Moreover, a new information bottleneck-based loss function was derived and used to train a better noise filter for the retriever. Given the sample {q,x,y}from the dataset and the noise filter p( ˜x|x,q) (need tuning), the information bottleneck in the RAG task is derived and formulated as: IB( ˜x) = PLLM(x|[q, ˜x,y]) −αPLLM(y|[q, ˜x]), where [·] means the concatenation operation. PLLM means the final output probability of the LLM. Li et al. [76] proposed a new metric GECE based on ME- TEOR for assessing the extent of the long-tailness of the gen- erated text in RAG: GECE = |METEOR(pred,re f) −1 n Pn i=1 PLLM (ti)| α·[E(▽ins) ·▽ins] , where αis the average word frequency, ▽ins and E(▽ins) are the gradient w.r.t. the current instance and the mean gradient of the total dataset, separately. A long-tail instance usually has a smaller α and ▽ins, obtaining a larger GECE , which implies larger degree of long-tailness. To assess the extent to which external knowledge is uti- lized in the RAG response, Sun et al. [77] proposed External Context Score E, which is defined on the response level as: El,h r = 1 |r| X t∈r El,h t = 1 |r| X t∈r e ·xL t ∥e∥∥xL t ∥, where |r|means the length of the response r, xL t is the t-th token's vector logit of the last layer L. e is a pooled vector of the most relevant vectors of xL t according to the attention weights in the middle layer: e = 1 |Il,h t | X j∈Il,h t xL j , where Il,h t means the attended times where the token has larger than top-k% attention scores with xL t in the l-th layer. Noted that some of these LLM based evaluation metrics represent research specializations. While they may not be di- rectly targeted towards an actual RAG system, their presen- tation is an integral part of advancing researches in the field of RAG, indicating significant contributions as well. 4 External Evaluation We have dissected the components of RAG and provided a comprehensive account of its internal evaluation. This sec- tion shifts our",
      "chunk_index": 15
    },
    {
      "index": 385,
      "chunk_id": "RAG_Survey_LLM2025_chunk_16",
      "source_id": "RAG_Survey_LLM2025",
      "text": "an integral part of advancing researches in the field of RAG, indicating significant contributions as well. 4 External Evaluation We have dissected the components of RAG and provided a comprehensive account of its internal evaluation. This sec- tion shifts our focus tothe external utility that RAG, as a com- plete system, encounters. We summarize the external utility in two areas: safety and e fficiency, the evaluation of whom are introduced below. 4.1 Safety Evaluation Safety pertains to the RAG system's capacity to ensure the generation of stable and harmless content within a dynamic, even noisy or hazardous environment. As RAG systems con- tinue widespread deployment, safety concerns have intensi- fied beyond those of standalone LLMs. The incorporation of external knowledge sources introduces unique vulnerabilities requiring specialized evaluation frameworks [20]. Robustness evaluations focus on system behavior when processing misleading information in retrieval results. The RECALL benchmark [78] tests discrimination between reli- able and counterfactual knowledge using BLEU, ROUGE-L, and specialized metrics like Misleading Rate. Wu et al. [79] quantify susceptibility to semantically related but irrelevant information using Misrepresentation Ratio and Uncertainty Ratio. SafeRAG [80] categorizes challenges like \"inter-context conflict\" with specific evaluation metrics, while C-RAG [81] provides theoretical guarantees on generation risks using con- formal risk analysis and ROUGE-L. Cheng et al. [82] intro- duce two metrics to evaluate the RAG system: 1) Resilience Aoran GAN et al. Retrieval Augmented Generation Evaluation in the Era of LLMs 9 Rate, aiming to emphases the system's stability and robust- ness, quantifies the percentage of instances where the sys- tem's responses remain accurate, both prior to and follow- ing retrieval augmentation. 2) Boost Ratequantifies the pro- portion of instances initially answered erroneously that were subsequently corrected upon the introduction of a retrieved document, evaluating the effectiveness of RAG. Factuality focuses on generating accurate information and avoiding plausible but incorrect statements (hallucinations), especially with noisy or conflicting retrieval results [78, 83, 84]. Key metrics include Factual Accuracy, using standard QA metrics (EM, F1, accuracy, etc.) when the context might be misleading [78]; the Hallucination Rate, the frequency of generated information not supported by or contradicting re- trieved documents, often measured via LLM-as-judge [85] or human evaluation; Citation Accuracy, assessing correct attri- bution to sources using Citation Precisionand Citation Re- call [20, 85]; and Faithfulness Metrics, evaluating how accu- rately the output reflects retrieved information [83]. Adversarial attackstarget specific components within the RAG pipeline. Knowledge database",
      "chunk_index": 16
    },
    {
      "index": 386,
      "chunk_id": "RAG_Survey_LLM2025_chunk_17",
      "source_id": "RAG_Survey_LLM2025",
      "text": "evaluation; Citation Accuracy, assessing correct attri- bution to sources using Citation Precisionand Citation Re- call [20, 85]; and Faithfulness Metrics, evaluating how accu- rately the output reflects retrieved information [83]. Adversarial attackstarget specific components within the RAG pipeline. Knowledge database poisoning (PoisonedRAG [86]) targets the retrieval corpus by injecting malicious texts that trigger predetermined outputs when retrieved. This at- tack vector is evaluated using Attack Success Rate (ASR) and retrieval-focused Precision/Recall/F1 metrics. Retrieval hijacking (HijackRAG [87]) exploits ranking algorithms to prioritize malicious content during retrieval, with evaluation focusing on attack transferability across models. Phantom at- tacks [88] use trigger-activated documents evaluated through Retrieval Failure Rate (Ret-FR), while jamming attacks [89] insert 'blocker' documents that force response refusal, as- sessed through oracle-based metrics. Privacy assess information exposure risks from retrieval databases or user queries [90]. Evaluation often involves sim- ulated attacks [91,92]. Key metrics about privacy include the Extraction Success Rate, the frequency or success rate of at- tacks extracting specific private information (e.g., names, PII) from the knowledge base, often measured by the count of successfully extracted items [90]; the PII Leakage Rate, the amount or percentage of Personally Identifiable Information inadvertently revealed in generated outputs, typically found via pattern matching or inspection [93]; and the Membership Inference Attack Success, which measures an attacker's abil- ity to determine if a specific data record was in the RAG sys- tem's knowledge base. Fairness examines if the RAG system exhibits or ampli- fies biases from retrieved documents or training, leading to inequitable outputs [94]. Bias Metrics are used to analyze the outputs for disparities, which are quantitative measures of performance disparities (e.g., error rates, sentiment scores) across demographic groups [94]. Stereotype Detectionmea- sures the frequency or severity of harmful stereotypes in gen- erated text, assessed via lists or human evaluation. Coun- terfactual Fairnesschecks if outputs change inappropriately when sensitive attributes in queries or context are altered. Transparency / Accountability assesses the understand- ability and traceability of the RAG system's reasoning pro- cess, enabling verification of sources and justification [95, 96]. Metrics are often qualitative or user-focused, such as Explanation Quality, based on human ratings of the clarity, completeness, and usefulness of explanations or provenance information [96]; Traceability, the ease of linking the final output back to specific source documents or passages; and Citation Accuracy(precision/recall) [20]. Comprehensive safety benchmarks standardize evaluation across multiple dimensions. SafeRAG [80] classifies attack tasks into four categories with",
      "chunk_index": 17
    },
    {
      "index": 387,
      "chunk_id": "RAG_Survey_LLM2025_chunk_18",
      "source_id": "RAG_Survey_LLM2025",
      "text": "or provenance information [96]; Traceability, the ease of linking the final output back to specific source documents or passages; and Citation Accuracy(precision/recall) [20]. Comprehensive safety benchmarks standardize evaluation across multiple dimensions. SafeRAG [80] classifies attack tasks into four categories with tailored datasets. VERA frame- work [97] uses bootstrap sampling for confidence bounds on safety metrics, while DeepTeam's red teaming approach [93] identifies vulnerabilities through systematic testing. In addi- tion, current research indicates defense mechanisms remain insufficient against sophisticated attacks [86-88]. Evalua- tions reveal significant vulnerabilities in current RAG sys- tems [87, 88], underscoring the need for robust benchmarks and metrics addressing the unique safety challenges arising from the retrieval-generation interplay. Further efforts are re- quired to evaluate the safety of RAG. 4.2 E fficiency Evaluation Efficiency is another crucial aspect of RAG's utility, directly linked to the real-world significance of a system's popularity, cost, and effectiveness. Latency evaluation typically focuses on two critical met- rics. Time to first token (TTFT) [98] measures the time taken by the system to produce its initial output token after receiv- ing a query, which is particularly crucial for user experience as it directly impacts perceived responsiveness. This met- ric is especially important in interactive applications where immediate feedback maintains user engagement. Addition- ally, complete response time (total latency) measures the du- ration from query submission to the generation of the entire response. This encompasses retrieval time, processing time, and generation time for all tokens. Hofstatte et al. [99] pro- posed Single Query Latency that refers to the complete end- to-end time taken to process a single query, including both complete retrieval and generation phases. Resources and Money Costevaluation of RAG systems is another critical component for assessing the e fficiency. Cost evaluation methodologies typically focus on quantifying both direct expenditures and efficiency metrics that impact overall system economics. The total cost of RAG systems can be categorized into several key components [126]: • Infrastructure Costs: Computing local resources for embedding generation, vector database maintenance, and LLM inference for open models. • Token-based Expenses: API charges for external LLM services based on input and output token usage. • Storage Costs: Vector database hosting and mainte- nance expenses that scale with corpus size. 10 Front. Comput. Sci., 2025, 0(0): 1-18 Table 1 Overview of RAG benchmarks and their evaluation datasets. Source Domain indicates the data origin (e.g., real-time news, specialized corpora), and Special Points highlight unique",
      "chunk_index": 18
    },
    {
      "index": 388,
      "chunk_id": "RAG_Survey_LLM2025_chunk_19",
      "source_id": "RAG_Survey_LLM2025",
      "text": "nance expenses that scale with corpus size. 10 Front. Comput. Sci., 2025, 0(0): 1-18 Table 1 Overview of RAG benchmarks and their evaluation datasets. Source Domain indicates the data origin (e.g., real-time news, specialized corpora), and Special Points highlight unique or novel features (like domain-specific tasks, dynamic changes, or false-premise data). Benchmark Time Dataset Name(s) Source Domain Special Points RAGAS [61] 2023.09 WikiEval Post-2022 Wikipedia Manually labeled for faithfulness FreshLLMs [56] 2023.11 FRESHQA Real-time news /web queries Dynamic QA with false-premise detection RECALL [78] 2023.11 EventKG, UJ Multilingual KGs, sci. terms Edited/counterfactual context tests ARES [73] 2023.11 NQ [100], HotpotQA [101], FEVER [102], WoW [103], MultiRC [104], ReCoRD [105]KILT and SuperGLUE corpora Re-uses classic QA sets, multi-domain RGB [85] 2023.12 Custom corpus Latest news articles Emphasizes info integration, noise rejections MultiHop-RAG [7] 2024.01 Generated corpus Daily news segments via mediastack Multi-hop cross-document queries CRUD-RAG [106] 2024.02 Generated corpus, UHGEval Chinese news, domain texts Create/Read/Update/Delete tasks MedRAG [107] 2024.02 MIRAGE Medical QA corpora Healthcare domain knowledge FeB4RAG [108] 2024.02 FeB4RAG, BEIR [109] Federated search tasks Multi-domain, multi-engine retrieval RAGBench [110] 2024.06 PubMedQA, CovidQA, HotpotQA, MS Marco, CUAD, DelucionQA, EManual, TechQA, FinQA, TAT-QA Multi-domain corpora Faithfulness with TRACe (Util, Rel, Adh, Compl) ReEval [111] 2024.05 NQ (MRQA)+RealTimeQA Wikipedia, real-time QA Adversarial test cases for hallucination detection DomainRAG [112] 2024.06 Generated admission QA College docs with yearly updates Single- /multi-doc, single-/multi-turn QA Telecom RAG Eval. [113]2024.07 TeleQuAD 3GPP-based domain docs Triple-labeled QA from SMEs (telecom context) LegalBench-RAG [114] 2024.08 PrivacyQA, CUAD, MAUD, ContractNLI Expert-annotated legal corpora Emphasizes strict retrieval of legal text RAGEval [115] 2024.08 DragonBall Finance, law, medical docs Schema-based generation, scenario-specific CoURAGE [116] 2024.09 RealTimeQA [117], NQ [100] Online QA+KILT tasks Hallucination resilience, dynamic updates RAG Unfairness [118] 2024.09 TREC22 FairRank, BBQ Wikipedia-based track+socioecon. QA Fairness metrics, group disparity CoFE-RAG [119] 2024.10 CoFE data PDF, DOC, multi-lingual docs Fine-grained chunking, multi-keyword approach OCR Hinders RAG [55] 2024.12 1,261 PDFs+8,561 images OCR text from scanned docs Evaluates noise from OCR errors OmniEval [120] 2024.12 Finance domain set Financial docs, numeric tasks Emphasizes numeric correctness /factual QA CRAG [121] 2024.12 KG+web corpus Knowledge graphs+web pages Multi-entity queries, curated dynamic facts RAG Playground [122] 2024.12 319 QA pairs Curated multi-domain tasks Prompt engineering / user flows MTRAG [123] 2025.01 CLAPNQ, FiQA, Govt, Cloud Wikipedia, finance, gov, tech docs Multi-turn, bridging queries CDQA [124] 2025.01 Chinese Dynamic QA Recent Chinese news queries Time-varying evolving answers",
      "chunk_index": 19
    },
    {
      "index": 389,
      "chunk_id": "RAG_Survey_LLM2025_chunk_20",
      "source_id": "RAG_Survey_LLM2025",
      "text": "2024.12 319 QA pairs Curated multi-domain tasks Prompt engineering / user flows MTRAG [123] 2025.01 CLAPNQ, FiQA, Govt, Cloud Wikipedia, finance, gov, tech docs Multi-turn, bridging queries CDQA [124] 2025.01 Chinese Dynamic QA Recent Chinese news queries Time-varying evolving answers U-NIAH [125] 2025.03 Starlight Academy Synthetic \"needle-in-haystack\" data Evaluates extremely long contexts SCARF [54] 2025.04 (User-provided) Generic multi-domain Modular or black-box approach integrates wide metrics (LLM judge) • Operational Overhead: Human supervision, system main- tenance, and regular updates to knowledge bases. • Development Costs: Initial implementation, integra- tion, and customization expenses. For more details in the token-based expenses, LLM providers such as OpenAI and Google o ffer token usage metrics that track input and output token consumption during evaluation processes. This approach calculates costs by multiplying to- ken counts by their respective pricing rates [127]. Researchers have developed metrics to evaluate the economic efficiency of RAG implementations: • Cost-Effectiveness Ratio: Measures performance im- provement per unit of cost, allowing for standardized comparison between different RAG configurations [127]. • Retrieval Precision ROI: Quantifies the economic re- turn of improving retrieval precision by measuring the reduction in irrelevant context processing costs [127]. This metric demonstrated that optimizing retrieval can improve cost efficiency by up to around 50% through reducing token consumption during LLM inference. • User-Controllable Cost-Accuracy Tradeoffs: Su et al. [128] propose evaluation methods using an interpretable control parameter (α) that allows systematic assessment of the relationship between retrieval costs and accu- racy. This approach enables evaluating RAG systems across a spectrum of cost constraints rather than at fixed operating points. • Comparative Cost Analysis: Methodologies for eval- uating relative cost e fficiency between di fferent RAG implementations for specific use cases, considering both direct costs and long-term economic sustainability [129]. 5 Resources The evaluation methodologies previously examined are com- prehensive, though not necessarily abundant. This section systematically compiles, categorizes, and presents the imple- mented RAG evaluation frameworks, benchmarks, analytical tools, and datasets that have emerged in the large language model era. To our knowledge, this compilation constitutes the most exhaustive collection of RAG evaluation frameworks currently documented in the literature. Datasets. We compiled the benchmarks along with the as- sociated datasets in recent years. Early works focus on static general-purpose QA datasets (e.g., NQ [100], HotpotQA [101]), providing well-established baselines but lack recency or do- main specificity. Recent benchmarks counter these limita- tions by 1) sourcing live news or rapidly",
      "chunk_index": 20
    },
    {
      "index": 390,
      "chunk_id": "RAG_Survey_LLM2025_chunk_21",
      "source_id": "RAG_Survey_LLM2025",
      "text": "datasets in recent years. Early works focus on static general-purpose QA datasets (e.g., NQ [100], HotpotQA [101]), providing well-established baselines but lack recency or do- main specificity. Recent benchmarks counter these limita- tions by 1) sourcing live news or rapidly updated online doc- uments (e.g., RGB [85], MultiHop-RAG [7]) to test time- sensitive capabilities; 2) curating domain-specific corpora in Aoran GAN et al. Retrieval Augmented Generation Evaluation in the Era of LLMs 11 Table 2 RAG evaluation frameworks, highlighting principal evaluation targets and methods. Retrieval focuses mainly on Relevance (R), Correctness (C) or Comprehensiveness, whereas generation (right) focuses on Faithfulness (F), Correctness (C), or Relevance (R). External evaluation targets (safety, efficiency) or other statements appear in italics. Type Framework Time Raw Targets Retrieval Metrics Generation Metrics Research FiD-Light [99] 2023.07 Latency - - Research Diversity Reranker [40] 2023.08 Diversity Cosine Distances - Benchmark RAGAS [61] 2023.09 Context R, Answer R, F LLM as Judge LLM CosSim, LLM as Judge Tool TruEra RAG Triad [130] 2023.10 Context R, Answer R,Groundedness LLM as Judge LLM as Judge Tool LangChain Bench. [131] 2023.11 C, F,ExecutionTime, EmbCosDist Exact-match LLM as Judge Benchmark FreshLLMs [56] 2023.11 Response C,Fast-changing, False premise (retrieval logs) STRICT/ RELAXED, FRESHEVAL(LLM-based) Tool RECALL [78] 2023.11 Response Quality,Robustness - BLEU, ROUGE-L Benchmark ARES [73] 2023.11 Context R, Answer F, Answer R LLM+ Classifier LLM+ Classifier, LLM+ Classifier Benchmark RGB [85] 2023.12 Info Integration,NoiseRobust, NegRejection, Counterfact - Accuracy Tool Databricks Eval [62] 2023.12 C,Readability, Comprehensiveness - LLM as Judge Benchmark MultiHop-RAG [7] 2024.01 Retrieval C, Response C MAP, MRR, Hit@K LLM as Judge Benchmark CRUD-RAG [106] 2024.02 Create, Read, Update, Delete - ROUGE, BLEU, RAGQuestEval Benchmark MedRAG [107] 2024.02 Accuracy (medical) - Exact-match, Acc. Benchmark FeB4RAG [108] 2024.02 Consistency, C,Clarity, Coverage - Human Eval, Human Eval Benchmark Arabic RAG Eval. [132] 2024.05 Doc R, Answer R nDCG, MRR, mAP Possibly CosSim to QA Benchmark RAGBench [110] 2024.06 Context R, Answer R, Explainability, TRACe= Util, Rel, Adh, Compl. LLM-based Eval LLM-based Eval, TRACe Metrics Benchmark ReEval [111] 2024.05 Hallucination Adversarial Attack - F1, EM, Entailment LLM or Human Eval Benchmark DomainRAG [112] 2024.06 C, F,NoiseRobust, StructOutput - F1, EM, ROUGE-L, LLM Benchmark CoURAGE [116] 2024.06 Hallucination - F1, EM, LLM as Judge, Human Eval Tool Telecom RAG Eval. [113] 2024.07 Context R, Faithfulness, Correctness LLM-based Metrics RAGAS-based, LLM Eval Benchmark LegalBench-RAG [114] 2024.08 Doc-level Precision, Citation Rel. Precision, Recall - Benchmark RAGEval [115] 2024.08",
      "chunk_index": 21
    },
    {
      "index": 391,
      "chunk_id": "RAG_Survey_LLM2025_chunk_22",
      "source_id": "RAG_Survey_LLM2025",
      "text": "2024.06 Hallucination - F1, EM, LLM as Judge, Human Eval Tool Telecom RAG Eval. [113] 2024.07 Context R, Faithfulness, Correctness LLM-based Metrics RAGAS-based, LLM Eval Benchmark LegalBench-RAG [114] 2024.08 Doc-level Precision, Citation Rel. Precision, Recall - Benchmark RAGEval [115] 2024.08 Completeness, Hallucination, Irrelevance LLM-based Scoring LLM-based, Human Alignment Benchmark RAG Unfairness [118] 2024.09 Fairness, C, C MRR@K EM, ROUGE Benchmark CoFE-RAG [119] 2024.10 Fine-grained Retrieval, Resp Quality, Diversity Recall, Correctness, Multi-keyword BLEU, ROUGE-L, LLM as Judge Benchmark Toward Instr.-Following [133]2024.10 Instr. Relevance, Constraint - LLM as Judge, Atomic Pass Rate Benchmark OmniEval [120] 2024.12 Factual Acc.,Domain Tasks Rule+LLM Manual or LLM FT Benchmark CRAG [121] 2024.12 Accuracy,Dynamism, Complex Facts, R, C Weighted scoring Accuracy, Truthfulness measure Benchmark OCR Hinders RAG [55] 2024.12 Accuracy,OCR Noise, Semantic vs. Format Noise EditDist, LCS F1-score Benchmark RAG Playground [122] 2024.12 Retrieval Strategy, Prompt Eng. Comparison-based LLM-based Eval Benchmark MTRAG [123] 2025.01 Multi-turn Quality, Conv. C Recall, nDCG LLM as Judge Benchmark CDQA [124] 2025.01 Accuracy - F1 Benchmark U-NIAH [125] 2025.03 Needle Detect,LongContext, No Halluc. Recall LLM Judge, Heatmap Tool eRAG [134] 2024.04 Doc-level Rel., Downstream Quality Doc-level LLM Kendall'sτ Tool SCARF [54] 2025.04 Context R, Answer R, Faithfulness LLM-based or BLEU/ROUGE RAGAS-like Relevance, LLM-based (Black-box Integration) law, healthcare, or finance (e.g., MedRAG [107], OmniEval [120], LegalBench-RAG [114]); or 3) generating synthetic data or specialized QA pairs, possibly with false-premise or counterfactual elements (e.g., FreshLLMs [56], RAGEval [115]) to assess robustness and misinformation handling. We fur- ther provide a concise description of the original domains and characteristics according to the original resource, as shown in Table 1. Noted that only the datasets containing retrieved ground truth documents are included, indicating a concern for more in-depth system component evaluation. Frameworks with Evaluation Methods. We compiled and summarized the evaluation methods devised by exist- ing frameworks, as illustrated in Table 2. These e fforts span from initial, point-level researches [40, 99] to later, multi- component evaluation tools and benchmarks [73, 131], en- compassing a remarkably comprehensive collection of assess- ment frameworks. The evaluation methods employed are var- ied, encompassing both traditional [78, 132] and LLM-based metrics [106, 110]. Additionally, there are frameworks that facilitate safety-focused evaluations [85, 116], or are tailored to specific downstream domains like document [55,125], tele- com [113], medicine [107], etc. Referencing the component evaluation objectives outlined in section 3.1, we categorize and highlight the evaluation elements and specific metrics. 12 Front. Comput.",
      "chunk_index": 22
    },
    {
      "index": 392,
      "chunk_id": "RAG_Survey_LLM2025_chunk_23",
      "source_id": "RAG_Survey_LLM2025",
      "text": "[85, 116], or are tailored to specific downstream domains like document [55,125], tele- com [113], medicine [107], etc. Referencing the component evaluation objectives outlined in section 3.1, we categorize and highlight the evaluation elements and specific metrics. 12 Front. Comput. Sci., 2025, 0(0): 1-18 Retrieval Generation Safety Efficiency 20 60 100Percentage (%) 89.3% 90.9% 9.1% 47.3% Fig. 3 Statistics on the distribution of RAG studies across four key areas: retrieval, generation, safety, and e fficiency. A paper may utilize evaluation methods in more than one areas. Fig. 4 Frequency statistics wordcloud of evaluation metrics in RAG stud- ies. The LLM-based methods are categorized based on the targets and pre- sented with the suffix '-LLM'. F-score refers to the expanded F1-score. 6 Discussion 6.1 Statistics and Analysis of RAG Evaluation The proliferation of LLM has contributed to a significant di- versification of RAG evaluation methods. Current researches, while demonstrating comprehensive coverage of RAG eval- uation dimensions, often subjectively assert their respective utility statements. To assess the popularity of these evaluation methods, we conducted a statistical analysis of the available methods from a survey perspective. This can also be viewed as a research-oriented simple meta-evaluation. We crawled the collection of the papers since 2022 autumn with keywords about RAG in the accepted papers of the high-level confer- ences about NLP & AI, and extracted the component as well as the evalauation metrics the papers focus and utilize. We finally amassed a total of 582 PDF manuscripts. All the in- cluded papers have undergone rigorous peer review, demon- strating scholarly merit with complete experimental method- ologies and logically structured evaluation procedures. Research Focus. Figure 3 illustrates the statistical distri- bution of evaluation methods used across the four di fferent segments in RAG studies (Retrieval / Generation / Safety / Efficiency). The data suggests a prevailing focus on internal research and evaluation of RAG systems, as indicated by the extensive coverage of the retrieval and generation processes. In contrast, external evaluations, particularly those related to safety, have garnered less attention. Metric Preference. Word frequency counts were con- ducted for the assessment metrics mentioned in the papers, with the wordcloud displayed in Figure 4. Whenever a met- ric is formally introduced in the body of a paper or reported in the table of experimental results, its word frequency count is set +1. We manually merged and mapped synonymous met- 2022 H2 2023 H1 2023 H2",
      "chunk_index": 23
    },
    {
      "index": 393,
      "chunk_id": "RAG_Survey_LLM2025_chunk_24",
      "source_id": "RAG_Survey_LLM2025",
      "text": "a met- ric is formally introduced in the body of a paper or reported in the table of experimental results, its word frequency count is set +1. We manually merged and mapped synonymous met- 2022 H2 2023 H1 2023 H2 2024 H1 2024 H2 2025 H1 Time 10 30 40Count of LLM-as-Judge Papers 1 1 7 12 Fig. 5 The number of papers explicitly mentioning LLM-based evaluation on RAG. The 2025 H1 collection is up to March 31st. rics in the same session and excluded the words with global occurrences lower than twice. It is observed that traditional metrics predominantly dominate the evaluation usage, while LLM-based methods have not yet gained widespread accep- tance among researchers. This phenomenon is attributed to the simplicity and reliability of the conventional metrics. Con- versely, the LLM-based methods often require more e ffort and involve multiple settings that are di fficult to keep the same across di fferent researches, such as the LLM version and prompt design. Trend of LLM Usage. Despite the potential issues with LLM-based methods, there is an observable trend of increas- ing application, as shown in Figure 5. 2024 H2 and 2025 H1 have the top two highest numbers. LLM judges are ultimately capable of handling more complex designs, drawing closer to real-world applications. LLM itself, additionally, has contin- ued to evolve in recent years, with the performance progres- sively improving, and the supported functions expanding. 6.2 Challenges and Future Directions This section addresses several challenges inherent in contem- porary RAG evaluation. Limitations of LLM-based Methods. The current evalu- ation design does not su fficiently address the timeliness and the black-box nature inherent in the LLM. The method of employing LLMs for assessments, particularly through di- rect prompts, raises latent risk about stability and security. Future research should focus on enhancing the robustness of the evaluation process itself and minimizing the likelihood of LLM errors in the RAG system. Cost of Evaluation. The cost associated with the RAG system has garnered attention. Nevertheless, a thorough eval- uation remains expensive due to the vast scale of the tools and datasets involved. Determining an e fficient method for system evaluation, or striking a balance between cost and ef- fectiveness, is one of the directions for future research. Advanced Evaluation Methods. As LLMs continue to evolve, the components of RAG systems are becoming more diverse. Currently, many of these components are evaluated using",
      "chunk_index": 24
    },
    {
      "index": 394,
      "chunk_id": "RAG_Survey_LLM2025_chunk_25",
      "source_id": "RAG_Survey_LLM2025",
      "text": "a balance between cost and ef- fectiveness, is one of the directions for future research. Advanced Evaluation Methods. As LLMs continue to evolve, the components of RAG systems are becoming more diverse. Currently, many of these components are evaluated using end-to-end RAG ontology metrics, with a lack of com- prehensive functional decomposition evaluation or theoreti- cal analysis. Concurrently, there remains untapped potential in the functionalities of LLMs themselves. For instance, the Aoran GAN et al. Retrieval Augmented Generation Evaluation in the Era of LLMs 13 evaluation about deep thinking models (e.g. openai-o1 [135]) along with the thinking process of LLMs in conjunction with RAG's retrieval and generation process, is still inadequate. These in-depth evaluation strategies require further research and development in the future. Comprehensiveness of the Evaluation Framework. De- spite the abundant evaluation frameworks at present, individ- ual ones are somewhat limited in their metrics and methods of evaluation. Moreover, most contemporary frameworks con- centrate on widely used languages such as English and Chi- nese. There is an urgent need for frameworks that are not only methodologically but also linguistically diverse. 7 Conclusion In this paper, we have presented the first comprehensive sur- vey of RAG evaluation methodologies in the LLM era. Our systematic analysis reveals several important insights for re- searchers and practitioners working with these increasingly prevalent systems. For the evaluation of internal RAG per- formance, we dissect the internal components of RAG sys- tems, define the assessment objectives, and gather a range of methods and metrics from traditional to innovative. More- over, we investigate the external evaluation related to system integrity such as safety and e fficiency, which are underex- plored in RAG research according to our statistical analy- sis. Additionally, we compile and categorize the current eval- uation datasets and frameworks to elucidate the unique at- tributes and assessment focuses of the resources. Last but not least, we analyze the implementation of existing evaluation methods and synthesize the challenges and future directions of RAG evaluation in the LLM era. Acknowledgements Competing interests The authors declare that they have no competing interests or financial conflicts to disclose. References 1. Fan W, Ding Y , Ning L, Wang S, Li H, Yin D, Chua T S, Li Q. A sur- vey on rag meeting llms: Towards retrieval-augmented large language models. In: Proceedings of the 30th ACM SIGKDD Conference on Knowledge Discovery and Data Mining. 2024, 6491-6501 2. Guti",
      "chunk_index": 25
    },
    {
      "index": 395,
      "chunk_id": "RAG_Survey_LLM2025_chunk_26",
      "source_id": "RAG_Survey_LLM2025",
      "text": "S, Li H, Yin D, Chua T S, Li Q. A sur- vey on rag meeting llms: Towards retrieval-augmented large language models. In: Proceedings of the 30th ACM SIGKDD Conference on Knowledge Discovery and Data Mining. 2024, 6491-6501 2. Guti ´errez B J, Shu Y , Gu Y , Yasunaga M, Su Y . Hipporag: Neuro- biologically inspired long-term memory for large language models. arXiv preprint arXiv:2405.14831, 2024 3. Zhang Y , Khalifa M, Logeswaran L, Lee M, Lee H, Wang L. Merg- ing Generated and Retrieved Knowledge for Open-Domain QA. In: Bouamor H, Pino J, Bali K, eds, Proceedings of the 2023 Conference on Empirical Methods in Natural Language Processing. December 2023, 4710-4728 4. Yao J Y , Ning K P, Liu Z H, Ning M N, Yuan L. Llm lies: Hallu- cinations are not bugs, but features as adversarial examples. arXiv preprint arXiv:2310.01469, 2023 5. Wang L, Yang N, Huang X, Jiao B, Yang L, Jiang D, Majumder R, Wei F. Text embeddings by weakly-supervised contrastive pre- training. arXiv preprint arXiv:2212.03533, 2022 6. Robertson S, Zaragoza H, others . The probabilistic relevance frame- work: Bm25 and beyond. Foundations and Trends ® in Information Retrieval, 2009, 3(4): 333-389 7. Tang Y , Yang Y . Multihop-rag: Benchmarking retrieval-augmented generation for multi-hop queries. arXiv preprint arXiv:2401.15391, 8. Sun J, Xu C, Tang L, Wang S, Lin C, Gong Y , Shum H Y , Guo J. Think-on-graph: Deep and responsible reasoning of large language model with knowledge graph. CoRR, 2023 9. Gao Y , Xiong Y , Gao X, Jia K, Pan J, Bi Y , Dai Y , Sun J, Wang H, Wang H. Retrieval-augmented generation for large language models: A survey. arXiv preprint arXiv:2312.10997, 2023, 2 10. Brown T, Mann B, Ryder N, Subbiah M, Kaplan J D, Dhariwal P, Neelakantan A, Shyam P, Sastry G, Askell A, others . Language mod- els are few-shot learners. Advances in neural information processing systems, 2020, 33: 1877-1901 11. Zhao W X, Zhou K, Li J, Tang T, Wang X, Hou Y , Min Y , Zhang B, Zhang J, Dong Z, others . A survey of large language models. arXiv preprint arXiv:2303.18223, 2023 12. Yildirim I, Paul L. From task structures to world models: what do llms know? Trends in Cognitive Sciences, 2024 13. Zhang S, Dong L, Li X, Zhang S, Sun X, Wang S, Li J, Hu R,",
      "chunk_index": 26
    },
    {
      "index": 396,
      "chunk_id": "RAG_Survey_LLM2025_chunk_27",
      "source_id": "RAG_Survey_LLM2025",
      "text": "preprint arXiv:2303.18223, 2023 12. Yildirim I, Paul L. From task structures to world models: what do llms know? Trends in Cognitive Sciences, 2024 13. Zhang S, Dong L, Li X, Zhang S, Sun X, Wang S, Li J, Hu R, Zhang T, Wu F, others . Instruction tuning for large language models: A survey. arXiv preprint arXiv:2308.10792, 2023 14. Verma P, Pilanci M. Towards signal processing in large language models. arXiv preprint arXiv:2406.10254, 2024 15. Lyu H, Jiang S, Zeng H, Xia Y , Wang Q, Zhang S, Chen R, Leung C, Tang J, Luo J. Llm-rec: Personalized recommendation via prompting large language models. In: Findings of the Association for Computa- tional Linguistics: NAACL 2024. 2024, 583-612 16. Zhang B, Liu Z, Cherry C, Firat O. When scaling meets llm fine- tuning: The e ffect of data, model and finetuning method. In: ICLR. 17. Reynolds L, McDonell K. Prompt programming for large language models: Beyond the few-shot paradigm. In: Extended abstracts of the 2021 CHI conference on human factors in computing systems. 2021, 1-7 18. Wei J, Wang X, Schuurmans D, Bosma M, Xia F, Chi E, Le Q V , Zhou D, others . Chain-of-thought prompting elicits reasoning in large lan- guage models. Advances in neural information processing systems, 2022, 35: 24824-24837 19. Huang Y , Huang J. A survey on retrieval-augmented text generation for large language models. arXiv preprint arXiv:2404.10981, 2024 20. Zhou Y , Liu Y , Li X, Jin J, Qian H, Liu Z, Li C, Dou Z, Ho T Y , Yu P S. Trustworthiness in retrieval-augmented generation systems: A survey. arXiv preprint arXiv:2409.10102, 2024 14 Front. Comput. Sci., 2025, 0(0): 1-18 21. Yu H, Gan A, Zhang K, Tong S, Liu Q, Liu Z. Evaluation of retrieval- augmented generation: A survey. In: CCF Conference on Big Data. 2024, 102-120 22. Lewis P, Perez E, Piktus A, Petroni F, Karpukhin V , Goyal N, K ¨uttler H, Lewis M, Yih W t, Rockt ¨aschel T, others . Retrieval-augmented generation for knowledge-intensive nlp tasks. Advances in neural information processing systems, 2020, 33: 9459-9474 23. Li H, Su Y , Cai D, Wang Y , Liu L. A survey on retrieval-augmented text generation. arXiv preprint arXiv:2202.01110, 2022 24. Dinan E, Roller S, Shuster K, Fan A, Auli M, Weston J. Wiz- ard of wikipedia: Knowledge-powered conversational agents. arXiv preprint arXiv:1811.01241, 2018 25. Qin L, Galley M, Brockett",
      "chunk_index": 27
    },
    {
      "index": 397,
      "chunk_id": "RAG_Survey_LLM2025_chunk_28",
      "source_id": "RAG_Survey_LLM2025",
      "text": "A survey on retrieval-augmented text generation. arXiv preprint arXiv:2202.01110, 2022 24. Dinan E, Roller S, Shuster K, Fan A, Auli M, Weston J. Wiz- ard of wikipedia: Knowledge-powered conversational agents. arXiv preprint arXiv:1811.01241, 2018 25. Qin L, Galley M, Brockett C, Liu X, Gao X, Dolan W B, Choi Y , Gao J. Conversing by reading: Contentful neural conversation with on- demand machine reading. In: Proceedings of the 57th Annual Meet- ing of the Association for Computational Linguistics. 2019, 5427- 26. Kobayashi M, Takeda K. Information retrieval on the web. ACM computing surveys (CSUR), 2000, 32(2): 144-173 27. Lee H, Yang S, Oh H, Seo M. Generative multi-hop retrieval. In: Proceedings of the 2022 Conference on Empirical Methods in Natural Language Processing. 2022, 1417-1436 28. Zhang S, Yao L, Sun A, Tay Y . Deep learning based recommender system: A survey and new perspectives. ACM Computing Surveys, 2019, 52(1): 1-38 29. Wang W, Lin X, Feng F, He X, Chua T S. Generative recom- mendation: Towards next-generation recommender paradigm. arXiv preprint arXiv:2304.03516, 2023 30. Karpukhin V , Oguz B, Min S, Lewis P, Wu L, Edunov S, Chen D, Yih W t. Dense passage retrieval for open-domain question answering. In: Webber B, Cohn T, He Y , Liu Y , eds, Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP). November 2020, 6769-6781 31. Google . Programmable Search Engine |Google for Developers, 2024 32. Yepes A J, You Y , Milczek J, Laverde S, Li R. Financial report chunking for effective retrieval augmented generation. arXiv preprint arXiv:2402.05131, 2024 33. Fan W, Ding Y , Ning L, Wang S, Li H, Yin D, Chua T S, Li Q. A sur- vey on rag meeting llms: Towards retrieval-augmented large language models. In: Proceedings of the 30th ACM SIGKDD Conference on Knowledge Discovery and Data Mining. 2024, 6491-6501 34. Singh I S, Aggarwal R, Allahverdiyev I, Taha M, Akalin A, Zhu K, O'Brien S. Chunkrag: Novel llm-chunk filtering method for rag sys- tems. arXiv preprint arXiv:2410.19572, 2024 35. Multi-Granularity M L M F. M3-embedding: Multi-linguality, multi-functionality, multi-granularity text embeddings through self- knowledge distillation. 2024 36. Mao Y , He P, Liu X, Shen Y , Gao J, Han J, Chen W. Generation- augmented retrieval for open-domain question answering. In: Zong C, Xia F, Li W, Navigli R, eds, Proceedings of the 59th Annual Meet- ing of the Association for",
      "chunk_index": 28
    },
    {
      "index": 398,
      "chunk_id": "RAG_Survey_LLM2025_chunk_29",
      "source_id": "RAG_Survey_LLM2025",
      "text": "P, Liu X, Shen Y , Gao J, Han J, Chen W. Generation- augmented retrieval for open-domain question answering. In: Zong C, Xia F, Li W, Navigli R, eds, Proceedings of the 59th Annual Meet- ing of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing (V ol- ume 1: Long Papers). August 2021, 4089-4100 37. Mekala D, Vu T, Schick T, Shang J. Leveraging QA datasets to improve generative data augmentation. In: Goldberg Y , Kozareva Z, Zhang Y , eds, Proceedings of the 2022 Conference on Empirical Methods in Natural Language Processing. December 2022, 9737- 38. Asai A, Wu Z, Wang Y , Sil A, Hajishirzi H. Self-rag: Learning to re- trieve, generate, and critique through self-reflection. In: The Twelfth International Conference on Learning Representations. 2023 39. Douze M, Guzhva A, Deng C, Johnson J, Szilvasy G, Mazar ´e P E, Lomeli M, Hosseini L, J´egou H. The faiss library. CoRR, 2024 40. Blagojevic V . Enhancing RAG Pipelines in Haystack: Introducing DiversityRanker and LostInTheMiddleRanker, August 2023 41. Besta M, Blach N, Kubicek A, Gerstenberger R, Podstawski M, Gi- aninazzi L, Gajda J, Lehmann T, Niewiadomski H, Nyczyk P, others . Graph of thoughts: Solving elaborate problems with large language models. In: Proceedings of the AAAI Conference on Artificial Intel- ligence. 2024, 17682-17690 42. Lanchantin J, Toshniwal S, Weston J, Sukhbaatar S, others . Learning to reason and memorize with self-notes. Advances in Neural Infor- mation Processing Systems, 2023, 36: 11891-11911 43. Deng Y , Zhang W, Chen Z, Gu Q. Rephrase and respond: Let large language models ask better questions for themselves. CoRR, 2023 44. Wang C, Liu X, Yue Y , Tang X, Zhang T, Jiayang C, Yao Y , Gao W, Hu X, Qi Z, others . Survey on factuality in large language models: Knowledge, retrieval and domain-specificity. arXiv preprint arXiv:2310.07521, 2023 45. Zhao P, Zhang H, Yu Q, Wang Z, Geng Y , Fu F, Yang L, Zhang W, Cui B. Retrieval-augmented generation for ai-generated content: A survey. CoRR, 2024 46. Cheng M, Luo Y , Ouyang J, Liu Q, Liu H, Li L, Yu S, Zhang B, Cao J, Ma J, others . A survey on knowledge-oriented retrieval-augmented generation. arXiv preprint arXiv:2503.10677, 2025 47. J ¨arvelin K, Kek¨al¨ainen J. Cumulated gain-based evaluation of ir tech- niques. ACM Transactions on Information Systems (TOIS), 2002, 20(4): 422-446 48. Sanko",
      "chunk_index": 29
    },
    {
      "index": 399,
      "chunk_id": "RAG_Survey_LLM2025_chunk_30",
      "source_id": "RAG_Survey_LLM2025",
      "text": "Cao J, Ma J, others . A survey on knowledge-oriented retrieval-augmented generation. arXiv preprint arXiv:2503.10677, 2025 47. J ¨arvelin K, Kek¨al¨ainen J. Cumulated gain-based evaluation of ir tech- niques. ACM Transactions on Information Systems (TOIS), 2002, 20(4): 422-446 48. Sanko ff D, Kruskal J B. Time warps, string edits, and macro- molecules: the theory and practice of sequence comparison. Reading: Addison-Wesley Publication, 1983 49. Yujian L, Bo L. A normalized levenshtein distance metric. IEEE transactions on pattern analysis and machine intelligence, 2007, 29(6): 1091-1095 50. Lin C Y . ROUGE: A package for automatic evaluation of summaries. In: Text Summarization Branches Out. July 2004, 74-81 51. Papineni K, Roukos S, Ward T, Zhu W J. Bleu: a method for auto- matic evaluation of machine translation. In: Isabelle P, Charniak E, Lin D, eds, Proceedings of the 40th Annual Meeting of the Associa- tion for Computational Linguistics. July 2002, 311-318 52. Banerjee S, Lavie A. Meteor: An automatic metric for mt evaluation with improved correlation with human judgments. In: Proceedings of the acl workshop on intrinsic and extrinsic evaluation measures for machine translation and/or summarization. 2005, 65-72 Aoran GAN et al. Retrieval Augmented Generation Evaluation in the Era of LLMs 15 53. Zhang T, Kishore V , Wu F, Weinberger K Q, Artzi Y . BERTScore: Evaluating Text Generation with BERT. In: 8th International Con- ference on Learning Representations, ICLR 2020, Addis Ababa, Ethiopia, April 26-30, 2020. 2020 54. Rengo M, Beadini S, Alfano D, Abbruzzese R. A system for comprehensive assessment of rag frameworks. arXiv preprint arXiv:2504.07803, 2025 55. Zhang J, Zhang Q, Wang B, Ouyang L, Wen Z, Li Y , Chow K H, He C, Zhang W. Ocr hinders rag: Evaluating the cascading impact of ocr on retrieval-augmented generation. arXiv preprint arXiv:2412.02592, 56. Vu T, Iyyer M, Wang X, Constant N, Wei J, Wei J, Tar C, Sung Y H, Zhou D, Le Q, Luong T. FreshLLMs: Refreshing large language models with search engine augmentation. In: Ku L W, Martins A, Srikumar V , eds, Findings of the Association for Computational Lin- guistics: ACL 2024. August 2024, 13697-13720 57. Sælemyr J, Femdal H T. Chunk smarter, retrieve better: Enhancing llms in finance: An empirical comparison of chunking techniques in retrieval augmented generation for financial reports. Master's thesis, NORWEGIAN SCHOOL OF ECONOMICS, 2024 58. Finardi P, Avila L, Castaldoni R, Gengo P, Larcher C, Piau M, Costa P, Carid´a",
      "chunk_index": 30
    },
    {
      "index": 400,
      "chunk_id": "RAG_Survey_LLM2025_chunk_31",
      "source_id": "RAG_Survey_LLM2025",
      "text": "Enhancing llms in finance: An empirical comparison of chunking techniques in retrieval augmented generation for financial reports. Master's thesis, NORWEGIAN SCHOOL OF ECONOMICS, 2024 58. Finardi P, Avila L, Castaldoni R, Gengo P, Larcher C, Piau M, Costa P, Carid´a V . The chronicles of rag: The retriever, the chunk and the generator. arXiv preprint arXiv:2401.07883, 2024 59. Muennigho ff N, Tazi N, Magne L, Reimers N. Mteb: Massive text embedding benchmark. arXiv preprint arXiv:2210.07316, 2022 60. Enevoldsen K, Chung I, Kerboua I, Kardos M, Mathur A, Stap D, Gala J, Siblini W, Krzemi ´nski D, Winata G I, others . Mmteb: Massive multilingual text embedding benchmark. arXiv preprint arXiv:2502.13595, 2025 61. Es S, James J, Anke L E, Schockaert S. Ragas: Automated evalua- tion of retrieval augmented generation. In: Proceedings of the 18th Conference of the European Chapter of the Association for Compu- tational Linguistics: System Demonstrations. 2024, 150-158 62. Leng Q, Uhlenhuth K, Polyzotis A. Best practices for llm eval- uation of rag applications (2023). URL https: //www. databricks. com/blog/LLM-auto-eval-best-practices-RAG 63. Zhang H, Semnani S, Ghassemi F, Xu J, Liu S, Lam M. Spaghetti: Open-domain question answering from heterogeneous data sources with retrieval and semantic parsing. In: Findings of the Association for Computational Linguistics ACL 2024. 2024, 1663-1678 64. Finsås M, Maksim J. Optimizing rag systems for technical support with llm-based relevance feedback and multi-agent patterns. Master's thesis, NTNU, 2024 65. Patil S G, Zhang T, Wang X, Gonzalez J E. Gorilla: Large language model connected with massive apis. Advances in Neural Information Processing Systems, 2024, 37: 126544-126565 66. Dai L, Xu Y , Ye J, Liu H, Xiong H. Seper: Measure retrieval util- ity through the lens of semantic perplexity reduction. arXiv preprint arXiv:2503.01478, 2025 67. Qi Z, Xu R, Guo Z, Wang C, Zhang H, Xu W. Long2rag: Evalu- ating long-context & long-form retrieval-augmented generation with key point recall. In: Findings of the Association for Computational Linguistics: EMNLP 2024. 2024, 4852-4872 68. Li M, Li X, Chen Y , Xuan W, Zhang W. Unraveling and mitigating retriever inconsistencies in retrieval-augmented large language mod- els. In: Findings of the Association for Computational Linguistics ACL 2024. 2024, 4833-4850 69. Min S, Krishna K, Lyu X, Lewis M, Yih W t, Koh P, Iyyer M, Zettle- moyer L, Hajishirzi H. Factscore: Fine-grained atomic evaluation of factual precision in long form text generation. In: Proceedings of the 2023 Conference on",
      "chunk_index": 31
    },
    {
      "index": 401,
      "chunk_id": "RAG_Survey_LLM2025_chunk_32",
      "source_id": "RAG_Survey_LLM2025",
      "text": "69. Min S, Krishna K, Lyu X, Lewis M, Yih W t, Koh P, Iyyer M, Zettle- moyer L, Hajishirzi H. Factscore: Fine-grained atomic evaluation of factual precision in long form text generation. In: Proceedings of the 2023 Conference on Empirical Methods in Natural Language Processing. 2023, 12076-12100 70. Song Y , Kim Y , Iyyer M. Veriscore: Evaluating the factuality of verifiable claims in long-form text generation. arXiv preprint arXiv:2406.19276, 2024 71. Chen L, Zhang R, Guo J, Fan Y , Cheng X. Controlling risk of retrieval-augmented generation: A counterfactual prompting frame- work. In: Findings of the Association for Computational Linguistics: EMNLP 2024. 2024, 2380-2393 72. Fu J, Ng S K, Jiang Z, Liu P. Gptscore: Evaluate as you desire. In: Proceedings of the 2024 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies (V olume 1: Long Papers). 2024, 6556-6576 73. Saad-Falcon J, Khattab O, Potts C, Zaharia M. Ares: An automated evaluation framework for retrieval-augmented generation systems. In: Proceedings of the 2024 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies (V olume 1: Long Papers). 2024, 338-354 74. Zhao X, Zhang H, Pan X, Yao W, Yu D, Chen J. Thrust: Adaptively propels large language models with external knowledge. Advances in Neural Information Processing Systems, 2023, 36: 69930-69948 75. Zhu K, Feng X, Du X, Gu Y , Yu W, Wang H, Chen Q, Chu Z, Chen J, Qin B. An information bottleneck perspective for e ffec- tive noise filtering on retrieval-augmented generation. arXiv preprint arXiv:2406.01549, 2024 76. Li D, Yan J, Zhang T, Wang C, He X, Huang L, Xue H, Huang J. On the role of long-tail knowledge in retrieval augmented large language models. arXiv preprint arXiv:2406.16367, 2024 77. Sun Z, Zang X, Zheng K, Song Y , Xu J, Zhang X, Yu W, Li H. Re- deep: Detecting hallucination in retrieval-augmented generation via mechanistic interpretability. arXiv preprint arXiv:2410.11414, 2024 78. Liu Y , Huang L, Li S, Chen S, Zhou H, Meng F, Zhou J, Sun X. Re- call: A benchmark for llms robustness against external counterfactual knowledge. arXiv preprint arXiv:2311.08147, 2023 79. Wu S, Xie J, Chen J, Zhu T, Zhang K, Xiao Y . How easily do ir- relevant inputs skew the responses of large language models? arXiv preprint arXiv:2404.03302, 2024 80. Liang X, Niu S, Li Z, Zhang",
      "chunk_index": 32
    },
    {
      "index": 402,
      "chunk_id": "RAG_Survey_LLM2025_chunk_33",
      "source_id": "RAG_Survey_LLM2025",
      "text": "2023 79. Wu S, Xie J, Chen J, Zhu T, Zhang K, Xiao Y . How easily do ir- relevant inputs skew the responses of large language models? arXiv preprint arXiv:2404.03302, 2024 80. Liang X, Niu S, Li Z, Zhang S, Wang H, Xiong F, Fan J Z, Tang B, Song S, Wang M, others . Saferag: Benchmarking security in retrieval-augmented generation of large language model. arXiv preprint arXiv:2501.18636, 2025 81. Kang M, G ¨urel N M, Yu N, Song D, Li B. C-rag: Certified genera- tion risks for retrieval-augmented language models. In: International Conference on Machine Learning. 2024, 22963-23000 16 Front. Comput. Sci., 2025, 0(0): 1-18 82. Cheng X, Wang X, Zhang X, Ge T, Chen S Q, Wei F, Zhang H, Zhao D. xrag: Extreme context compression for retrieval-augmented gen- eration with one token. arXiv preprint arXiv:2405.13792, 2024 83. Asai A, Wu Z, Wang Y , Sil A, Hajishirzi H. Self-rag: Learning to re- trieve, generate, and critique through self-reflection. In: The Twelfth International Conference on Learning Representations. 2023 84. Trivedi H, Balasubramanian N, Khot T, Sabharwal A. Interleav- ing retrieval with chain-of-thought reasoning for knowledge-intensive multi-step questions. In: ACL (1). 2023, 10014-10037 85. Chen J, Lin H, Han X, Sun L. Benchmarking large language mod- els in retrieval-augmented generation. In: Proceedings of the AAAI Conference on Artificial Intelligence. 2024, 17754-17762 86. Zou W, Geng R, Wang B, Jia J. Poisonedrag: Knowledge corruption attacks to retrieval-augmented generation of large language models. arXiv preprint arXiv:2402.07867, 2024 87. Zhang Y , Li Q, Du T, Zhang X, Zhao X, Feng Z, Yin J. Hijackrag: Hijacking attacks against retrieval-augmented large language models. arXiv preprint arXiv:2410.22832, 2024 88. Chaudhari H, Severi G, Abascal J, Jagielski M, Choquette-Choo C A, Nasr M, Nita-Rotaru C, Oprea A. Phantom: General trigger at- tacks on retrieval augmented language generation. arXiv preprint arXiv:2405.20485, 2024 89. Shafran A, Schuster R, Shmatikov V . Machine against the rag: Jam- ming retrieval-augmented generation with blocker documents. arXiv preprint arXiv:2406.05870, 2024 90. Zeng S, Zhang J, He P, Liu Y , Xing Y , Xu H, Ren J, Chang Y , Wang S, Yin D, others . The good and the bad: Exploring privacy issues in retrieval-augmented generation (rag). In: Findings of the Association for Computational Linguistics ACL 2024. 2024, 4505-4524 91. Cheng P, Ding Y , Ju T, Wu Z, Du W, Yi P, Zhang Z, Liu G. Trojan- rag:",
      "chunk_index": 33
    },
    {
      "index": 403,
      "chunk_id": "RAG_Survey_LLM2025_chunk_34",
      "source_id": "RAG_Survey_LLM2025",
      "text": "bad: Exploring privacy issues in retrieval-augmented generation (rag). In: Findings of the Association for Computational Linguistics ACL 2024. 2024, 4505-4524 91. Cheng P, Ding Y , Ju T, Wu Z, Du W, Yi P, Zhang Z, Liu G. Trojan- rag: Retrieval-augmented generation can be backdoor driver in large language models. arXiv preprint arXiv:2405.13401, 2024 92. Chaudhari H, Severi G, Abascal J, Jagielski M, Choquette-Choo C A, Nasr M, Nita-Rotaru C, Oprea A. Phantom: General trigger at- tacks on retrieval augmented language generation. arXiv preprint arXiv:2405.20485, 2024 93. Perez E, Huang S, Song F, Cai T, Ring R, Aslanides J, Glaese A, McAleese N, Irving G. Red teaming language models with language models, 2022 94. Shrestha R, Zou Y , Chen Q, Li Z, Xie Y , Deng S. Fairrag: Fair human generation via fair retrieval augmentation. CoRR, 2024, abs/2403.19964 95. Zhou Y , Liu Z, Jin J, Nie J Y , Dou Z. Metacognitive retrieval- augmented large language models. In: WWW. 2024, 1453-1463 96. Sudhi V , Bhat S R, Rudat M, Teucher R. Rag-ex: A generic frame- work for explaining retrieval augmented generation. In: SIGIR. 2024, 2776-2780 97. Ding T, Banerjee A, Mombaerts L, Li Y , Borogovac T, Weinstein J P D l C. Vera: Validation and evaluation of retrieval-augmented systems. arXiv preprint arXiv:2409.03759, 2024 98. Anthropic . Reducing latency, January 2025 99. Hofst ¨atter S, Chen J, Raman K, Zamani H. FiD-Light: E fficient and Effective Retrieval-Augmented Text Generation. In: Proceedings of the 46th International ACM SIGIR Conference on Research and De- velopment in Information Retrieval, SIGIR '23. July 2023, 1437- 100. Kwiatkowski T, Palomaki J, Redfield O, Collins M, Parikh A, Alberti C, Epstein D, Polosukhin I, Devlin J, Lee K, Toutanova K, Jones L, Kelcey M, Chang M W, Dai A M, Uszkoreit J, Le Q, Petrov S. Natu- ral questions: A benchmark for question answering research. Trans- actions of the Association for Computational Linguistics, 2019, 7: 453-466 101. Yang Z, Qi P, Zhang S, Bengio Y , Cohen W W, Salakhutdinov R, Manning C D. HotpotQA: A dataset for diverse, explainable multi- hop question answering. In: Conference on Empirical Methods in Natural Language Processing (EMNLP). 2018 102. Thorne J, Vlachos A, Christodoulopoulos C, Mittal A. FEVER: a large-scale dataset for fact extraction and VERification. In: NAACL- HLT. 2018 103. Dinan E, Roller S, Shuster K, Fan A, Auli M, Weston J. Wizard of Wikipedia:",
      "chunk_index": 34
    },
    {
      "index": 404,
      "chunk_id": "RAG_Survey_LLM2025_chunk_35",
      "source_id": "RAG_Survey_LLM2025",
      "text": "(EMNLP). 2018 102. Thorne J, Vlachos A, Christodoulopoulos C, Mittal A. FEVER: a large-scale dataset for fact extraction and VERification. In: NAACL- HLT. 2018 103. Dinan E, Roller S, Shuster K, Fan A, Auli M, Weston J. Wizard of Wikipedia: Knowledge-powered conversational agents. In: Pro- ceedings of the International Conference on Learning Representations (ICLR). 2019 104. DeYoung J, Jain S, Rajani N F, Lehman E, Xiong C, Socher R, Wal- lace B C. Eraser: A benchmark to evaluate rationalized nlp models. In: Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics. 2020, 4443-4458 105. Zhang S, Liu X, Liu J, Gao J, Duh K, Van Durme B. Record: Bridging the gap between human and machine commonsense reading compre- hension. arXiv preprint arXiv:1810.12885, 2018 106. Lyu Y , Li Z, Niu S, Xiong F, Tang B, Wang W, Wu H, Liu H, Xu T, Chen E. Crud-rag: A comprehensive chinese benchmark for retrieval- augmented generation of large language models. ACM Trans. Inf. Syst., 2025, 43(2) 107. Xiong G, Jin Q, Lu Z, Zhang A. Benchmarking retrieval-augmented generation for medicine. In: Findings of the Association for Compu- tational Linguistics ACL 2024. 2024, 6233-6251 108. Wang S, Khramtsova E, Zhuang S, Zuccon G. Feb4rag: Evaluat- ing federated search in the context of retrieval augmented generation. In: Proceedings of the 47th International ACM SIGIR Conference on Research and Development in Information Retrieval. 2024, 763-773 109. Kamalloo E, Thakur N, Lassance C, Ma X, Yang J H, Lin J. Re- sources for brewing beir: reproducible reference models and an o ffi- cial leaderboard, 2023 110. Friel R, Belyi M, Sanyal A. Ragbench: Explainable bench- mark for retrieval-augmented generation systems. arXiv preprint arXiv:2407.11005, 2024 111. Yu X, Cheng H, Liu X, Roth D, Gao J. ReEval: Automatic halluci- nation evaluation for retrieval-augmented large language models via transferable adversarial attacks. In: Duh K, Gomez H, Bethard S, eds, Findings of the Association for Computational Linguistics: NAACL 2024. June 2024, 1333-1351 112. Wang S, Liu J, Song S, Cheng J, Fu Y , Guo P, Fang K, Zhu Y , Dou Z. Domainrag: A chinese benchmark for evaluating domain-specific Aoran GAN et al. Retrieval Augmented Generation Evaluation in the Era of LLMs 17 retrieval-augmented generation. CoRR, 2024 113. Roychowdhury S, Soman S, Ranjani H, Gunda N, Chhabra V , BALA S K. Evaluation of rag metrics for question answering in the telecom domain. ICML 2024",
      "chunk_index": 35
    },
    {
      "index": 405,
      "chunk_id": "RAG_Survey_LLM2025_chunk_36",
      "source_id": "RAG_Survey_LLM2025",
      "text": "Generation Evaluation in the Era of LLMs 17 retrieval-augmented generation. CoRR, 2024 113. Roychowdhury S, Soman S, Ranjani H, Gunda N, Chhabra V , BALA S K. Evaluation of rag metrics for question answering in the telecom domain. ICML 2024 Workshop on Foundation Models in the Wild, 114. Pipitone N, Alami G H. Legalbench-rag: A benchmark for retrieval-augmented generation in the legal domain. arXiv preprint arXiv:2408.10343, 2024 115. Zhu K, Luo Y , Xu D, Wang R, Yu S, Wang S, Yan Y , Liu Z, Han X, Liu Z, others . Rageval: Scenario specific rag evaluation dataset generation framework. CoRR, 2024 116. Galla D, Hoda S, Zhang M, Quan W, Yang T D, V oyles J. Courage: A framework to evaluate rag systems. In: Rapp A, Di Caro L, Meziane F, Sugumaran V , eds, Natural Language Processing and Information Systems. 2024, 392-407 117. Kasai J, Sakaguchi K, Le Bras R, Asai A, Yu X, Radev D, Smith N A, Choi Y , Inui K, others . Realtime qa: What's the answer right now? Advances in neural information processing systems, 2023, 36: 49025-49043 118. Wu X, Li S, Wu H T, Tao Z, Fang Y . Does RAG introduce unfairness in LLMs? evaluating fairness in retrieval-augmented generation sys- tems. In: Rambow O, Wanner L, Apidianaki M, Al-Khalifa H, Euge- nio B D, Schockaert S, eds, Proceedings of the 31st International Con- ference on Computational Linguistics. January 2025, 10021-10036 119. Liu J, Ding R, Zhang L, Xie P, Huang F. Cofe-rag: A comprehensive full-chain evaluation framework for retrieval-augmented generation with enhanced data diversity. arXiv preprint arXiv:2410.12248, 2024 120. Wang S, Tan J, Dou Z, Wen J R. Omnieval: An omnidirectional and automatic rag evaluation benchmark in financial domain. arXiv preprint arXiv:2412.13018, 2024 121. Yang X, Sun K, Xin H, Sun Y , Bhalla N, Chen X, Choudhary S, Gui R D, Jiang Z W, Jiang Z, Kong L, Moran B, Wang J, Xu Y E, Yan A, Yang C, Yuan E, Zha H, Tang N, Chen L, Scheffer N, Liu Y , Shah N, Wanga R, Kumar A, Yih W t, Dong X L. Crag - comprehensive rag benchmark. In: Globerson A, Mackey L, Belgrave D, Fan A, Paquet U, Tomczak J, Zhang C, eds, Advances in Neural Information Processing Systems. 2024, 10470-10490 122. Papadimitriou I, Gialampoukidis I, Vrochidis S, others . Rag playground: A framework for systematic evaluation of",
      "chunk_index": 36
    },
    {
      "index": 406,
      "chunk_id": "RAG_Survey_LLM2025_chunk_37",
      "source_id": "RAG_Survey_LLM2025",
      "text": "Globerson A, Mackey L, Belgrave D, Fan A, Paquet U, Tomczak J, Zhang C, eds, Advances in Neural Information Processing Systems. 2024, 10470-10490 122. Papadimitriou I, Gialampoukidis I, Vrochidis S, others . Rag playground: A framework for systematic evaluation of retrieval strategies and prompt engineering in rag systems. arXiv preprint arXiv:2412.12322, 2024 123. Katsis Y , Rosenthal S, Fadnis K, Gunasekara C, Lee Y S, Popa L, Shah V , Zhu H, Contractor D, Danilevsky M. Mtrag: A multi-turn conversational benchmark for evaluating retrieval-augmented gener- ation systems. arXiv preprint arXiv:2501.03468, 2025 124. Xu Z, Li Y , Ding R, Wang X, Chen B, Jiang Y , Zheng H, Lu W, Xie P, Huang F. Let llms take on the latest challenges! a chinese dynamic question answering benchmark. In: Proceedings of the 31st Interna- tional Conference on Computational Linguistics. 2025, 10435-10448 125. Gao Y , Xiong Y , Wu W, Huang Z, Li B, Wang H. U-niah: Unified rag and llm evaluation for long context needle-in-a-haystack. arXiv preprint arXiv:2503.00353, 2025 126. Selvaraj T. Calculate the total cost of a retrieval augmented generation (rag) solution, February 2024 127. Zhang J, Li G, Su J. Sage: A framework of precise retrieval for rag. arXiv preprint arXiv:2503.01713, 2025 128. Su J, Healey J, Nakov P, Cardie C. Fast or better? balancing accuracy and cost in retrieval-augmented generation with flexible user control. CoRR, 2025 129. S ¸akar T, Emekci H. Maximizing rag efficiency: A comparative analy- sis of rag methods. Natural Language Processing, 2025, 31(1): 1-25 130. Datta A, Fredrikson M, Leino K, Lu K, Sen S, Shih R, Wang Z. Ex- ploring conceptual soundness with trulens. In: NeurIPS 2021 Com- petitions and Demonstrations Track. 2022, 302-307 131. LangChain . Evaluating rag architectures on benchmark tasks, November 2023 132. Mahboub A, Za'ter M E, Al-Rfooh B, Estaitia Y , Jaljuli A, Hak- ouz A. Evaluation of semantic search and its role in retrieved- augmented-generation (rag) for arabic language. arXiv preprint arXiv:2403.18350, 2024 133. Dong G, Song X, Zhu Y , Qiao R, Dou Z, Wen J R. Toward general instruction-following alignment for retrieval-augmented generation. arXiv preprint arXiv:2410.09584, 2024 134. Salemi A, Zamani H. Evaluating retrieval quality in retrieval- augmented generation. In: Proceedings of the 47th International ACM SIGIR Conference on Research and Development in Informa- tion Retrieval, SIGIR '24. 2024, 2395-2400 135. Jaech A, Kalai A, Lerer A, Richardson A, El-Kishky A, Low A, Hel-",
      "chunk_index": 37
    },
    {
      "index": 407,
      "chunk_id": "RAG_Survey_LLM2025_chunk_38",
      "source_id": "RAG_Survey_LLM2025",
      "text": "quality in retrieval- augmented generation. In: Proceedings of the 47th International ACM SIGIR Conference on Research and Development in Informa- tion Retrieval, SIGIR '24. 2024, 2395-2400 135. Jaech A, Kalai A, Lerer A, Richardson A, El-Kishky A, Low A, Hel- yar A, Madry A, Beutel A, Carney A, others . Openai o1 system card. arXiv preprint arXiv:2412.16720, 2024 Aoran Gan is working toward the PhD degree in the School of Artificial In- telligence and Data Science, University of Science and Technology of China. His research interests include text min- ing, knowledge graph and large language models. Hao Yu is pursuing a MS degree at McGill University and is a ffiliated with Quebec Artificial Intelligence Institute. His research focuses on multilingual and low-resource NLP, as well as RAG sys- tems for misinformation detection. 18 Front. Comput. Sci., 2025, 0(0): 1-18 Kai Zhang is an Associate Researcher at the University of Science and Technology of China. His general area of research is natural language processing and knowl- edge discovery. He is a member of ACM, SIGIR, AAAI, and CCF. Qi Liu is a professor in the School of Ar- tificial Intelligence and Data Science at USTC. His area of research is data mining and knowledge discovery. He has pub- lished prolifically in refereed journals and conferences. He is an Associate Editor of IEEE TBD and Neurocomputing. Wenyu Yan is currently pursuing MS de- gree in University of Science and Tech- nology of China. His research interests focus on conversational search, retrieval- augmented generation, etc. Zhenya Huang is currently an Associate Professor with USTC. His main research interests include data mining, knowledge reasoning, natural language processing, and intelligent education. He has pub- lished more than 50 papers in refereed journals and conference proceedings. Shiwei Tong is a senior data scientist at Tencent Company. His research focuses on Game Data Mining and Game Appli- cations driven by Large Language Mod- els. Enhong Chen is a professor in the School of Computer Science and Technology at USTC. His general area of research in- cludes data mining and machine learn- ing, social network analysis, and recom- mender systems. He was on program committees of numerous conferences in- cluding SIGKDD, ICDM, and SDM. Guoping Hu is senior vice president of iFLYTEK, director of the National Key Laboratory of Cognitive Intelligence. He has been honored with the First Prize of State Science and Technology Advance- ment Award and",
      "chunk_index": 38
    },
    {
      "index": 408,
      "chunk_id": "RAG_Survey_LLM2025_chunk_39",
      "source_id": "RAG_Survey_LLM2025",
      "text": "in- cluding SIGKDD, ICDM, and SDM. Guoping Hu is senior vice president of iFLYTEK, director of the National Key Laboratory of Cognitive Intelligence. He has been honored with the First Prize of State Science and Technology Advance- ment Award and garnered over 300 autho- rized patents.",
      "chunk_index": 39
    },
    {
      "index": 409,
      "chunk_id": "QASemConsistency2024_chunk_00",
      "source_id": "QASemConsistency2024",
      "text": "Localizing Factual Inconsistencies in Attributable Text Generation Arie Cattan1,4 Paul Roit1,4 Shiyue Zhang2 David Wan3 Roee Aharoni4 Idan Szpektor4 Mohit Bansal3 Ido Dagan1 1Bar-Ilan University 2Independent Researcher 3UNC Chapel Hill 4Google Research arie.cattan@gmail.com Abstract There has been an increasing interest in de- tecting hallucinations in model-generated texts, both manually and automatically, at varying levels of granularity. However, most existing methods fail to precisely pin- point the errors. In this work, we intro- duce QASEMCONSISTENCY, a new for- malism forlocalizingfactual inconsisten- cies in attributable text generation, at a fine- grained level. Drawing inspiration from Neo- Davidsonian formal semantics, we propose decomposing the generated text into mini- mal predicate-argument level propositions, expressed as simple question-answer (QA) pairs, and assess whether each individual QA pair is supported by a trusted reference text. As each QA pair corresponds to asingle semantic relation between a predicate and an argument, QASEMCONSISTENCYeffec- tively localizes the unsupported information. We first demonstrate the effectiveness of the QASEMCONSISTENCYmethodology for hu- man annotation, by collecting crowdsourced annotations of granular consistency errors, while achieving a substantial inter-annotator agreement. This benchmark includes more than 3K instances spanning various tasks of attributable text generation. We also show that QASEMCONSISTENCYyields factual consistency scores that correlate well with human judgments. Finally, we implement several methods for automatically detecting localized factual inconsistencies, with both supervised entailment models and LLMs.1 1 Introduction Large Language Models (LLMs) are used very ef- fectively across a broad range of text generation tasks. However, despite remarkable progress in recent years, LLMs remain prone to generating fac- tual inconsistencies. This phenomenon, commonly 1Our codebase, dataset, and models can be found athttps: //github.com/ariecattan/qasem_consistency referred to as \"hallucinations\", limits their broader deployment and utility (Huang et al., 2023). This work focuses onattributabletext gener- ation, where the generated content can be veri- fied against a trusted supporting source, referred here as the\"reference text\". This reference text may be part of the input for generation, as typi- cal in text summarization or open book QA (Gao et al., 2023b; Slobodkin et al., 2024), retrieved post- generation (Bohnet et al., 2022; Gao et al., 2023a; Min et al., 2023; Wei et al., 2024), or identified by the models themselves when instructed to provide citations to external sources (Liu et al., 2023a; Yue et al., 2023). To address hallucinations, there has been increas- ing research interest in identifying unsupported content in model-generated text, both manually and automatically. This task",
      "chunk_index": 0
    },
    {
      "index": 410,
      "chunk_id": "QASemConsistency2024_chunk_01",
      "source_id": "QASemConsistency2024",
      "text": "themselves when instructed to provide citations to external sources (Liu et al., 2023a; Yue et al., 2023). To address hallucinations, there has been increas- ing research interest in identifying unsupported content in model-generated text, both manually and automatically. This task is typically framed as a textual entailment problem, requiring that the generated text should besupported(entailed) by the reference text. Detecting unsupported in- formation is valuable for multiple purposes. For evaluating the factual consistency of attributable text generation, both human annotation protocols and automated inconsistency detection models are needed (Honovich et al., 2022; Gekhman et al., 2023). Automated inconsistency detection can fur- ther provide valuable feedback for end users about suspected unsupported content in the LLMs' out- put, and also contributes to model improvements, via post-editing (Gao et al., 2023a), enhanced train- ing (Nan et al., 2021; Wan and Bansal, 2022; Roit et al., 2023), self-critique (Wadhwa et al., 2024), or by imposing constraints during decoding (Wan et al., 2023). To better fulfil these goals, it is desired to pin- pointwhich partsof the generated text are not supported, especially as LLMs continue to im- prove such that factual inconsistencies become more localized, as illustrated in Figure 1. While recent research has made useful strides in finer- arXiv:2410.07473v3 [cs.CL] 10 Sep 2025 grained inconsistency detection, ranging from en- tire texts to sentences, claims, and even question- generation and question-answering based solutions, these sub-sentence representations often remain insufficiently granular. For example, an \"atomic\" claim in FActScore (Min et al., 2023) is still based on multiple predicate-argument relations, and each of them might be either supported or unsupported (see Table 1, §2.1 and §6). In this work, we introduce QASEMCONSIS- TENCY, a novel protocol for detecting localized fac- tual inconsistencies in attributable text generation, applicable to both human and automatic detection. Inspired by Neo-Davidsonian formal semantics, our method decomposes the generated text into ele- mentary assertions, in the form of atomic question- answer (QA) pairs (QASRL (He et al., 2015) and QANom (Klein et al., 2020)), where each pair cor- responds to a single predicate-argument relation. Localizing factual inconsistencies then involves identifying the set of QA pairs that are not sup- ported by the reference text. Figure 1 illustrates our QASEMCONSISTENCYmethodology by repre- senting the factual inconsistency via a simple QA \"Where did someone fall? in the Annalong Valley in County Antrim\", which is not supported by the ref- erence text. By assessing each",
      "chunk_index": 1
    },
    {
      "index": 411,
      "chunk_id": "QASemConsistency2024_chunk_02",
      "source_id": "QASemConsistency2024",
      "text": "reference text. Figure 1 illustrates our QASEMCONSISTENCYmethodology by repre- senting the factual inconsistency via a simple QA \"Where did someone fall? in the Annalong Valley in County Antrim\", which is not supported by the ref- erence text. By assessing each individual predicate- argument level statement, QASEMCONSISTENCY can pinpoint more precisely to the factual mistakes than prior methods (illustrated in Table 1). Notably, we found that for 27% of the predicates, some QA pairs were supported and some were not, justifying the need for such a fine-grained representation. Furthermore, since we represent predicate- argument relations with simple natural language expressions (questions and answers), our QASEM- CONSISTENCYmethodology is well-suited for manually annotating granular consistency errors. Indeed, we collect a dataset with localized anno- tations of factual inconsistency at the predicate- argument level via crowdsourcing and achieve a high inter-annotator agreement (§4). We demon- strate that the overall factual consistency scores obtained by QASEMCONSISTENCYcorrelate well with human preferences. Finally, we implement methods for automati- cally detecting whether each individual QASem QA is supported by the reference text, follow- ing the QASEMCONSISTENCYmethodology (§5). We conduct experiments with a variety of models, including supervised NLI models and prompting The casualty who was with a group of paragliders, was airlifted off Slieve Gullion at about 15:00 GMT on Sunday. Mourne Mountain Rescue Team said the man had a \"serious leg injury\" and was taken to the Royal Victoria Hospital in Belfast. The team had been training in the Annalong Valley when the emergency call came at 13:43 GMT… Source Document A paraglider has been airlifted to hospital with a serious leg injury after falling in the Annalong Valley in County Antrim. Q: Where did someone fall? A: in the Annalong Valley in County Antrim. Generated Summary Location Location Location Fine-grained Factuality Error Location Figure 1: An annotation example of a localized fac- tual consistency error according to our QASEMCON- SISTENCYmethodology. Here, the model successfully inferred a paraglider fall (after being airlifted with a serious leg injury) but incorrectly identifies the location of the fall as the rescue team's training area (Annalong Valley) instead of the correct location (Slieve Gullion), which is located 30 miles away. This misattribution is highlighted by the question-answer pair:\"Where did someone fall? in the Annalong Valley in County Antrim\". open-source and commercial LLMs. While these models were supervised on standard entailment datasets, our results show that they can effectively handle our fine-grained",
      "chunk_index": 2
    },
    {
      "index": 412,
      "chunk_id": "QASemConsistency2024_chunk_03",
      "source_id": "QASemConsistency2024",
      "text": "misattribution is highlighted by the question-answer pair:\"Where did someone fall? in the Annalong Valley in County Antrim\". open-source and commercial LLMs. While these models were supervised on standard entailment datasets, our results show that they can effectively handle our fine-grained QA assertions, providing more detailed error detection. Yet, there is a vast room for improvement in future work. Altogether, we hope that future research will build upon QASEMCONSISTENCYfor localizing factual inconsistencies (either manually or auto- matically) and leverage our benchmark to develop better entailment models for handling fine-grained hypotheses corresponding to predicate-argument relations. 2 Background 2.1 Fine-grained Detection of Factual Inconsistencies Current efforts in detecting factual consistency er- rors continuously progress towards more localized methods that pinpoint the inconsistent information. Starting from approaches that highlight individual sentences (Laban et al., 2022), inspect \"facts\" (Min et al., 2023), or use question-generation and Source Gareth Colfer-Williams, 25, died last week at his home in Swansea, the city at the centre of an epidemic of the disease which has reached 942 cases. But the examination was unable to establish whether measles was the main cause of his death. An inquest will be opened and adjourned on Tuesday to allow further tests... Public Health Wales said on Friday that laboratory tests confirmed a diagnosis of measles but further tests were needed to determine the cause of death... Summary An inquest into the death of a man who died of measles has been opened and adjourned after a post-mortem examination failed to establish how he got the illness.✗ Token-level(CLIFF) An inquest into the death of a man who died of measles has been opened and adjourned after a post-mortem examination failed to establishhow he got the illness. QGQA(Q2) - An inquest: What has been opened and adjourned into the death of a man who died of measles?✓ - A man: Who died of measles?✗ - measles: What was the cause of death of a man?✗ - a post-mortem examination: What failed to establish how he contracted measles?✗ - the illness: A post-mortem examination failed to establish how he got what?✗ Claim-level(FActScore) - An inquest into the death of a man has been opened.✗ - The man died of measles.✗ - A post-mortem examination was conducted.✓ - The post-mortem examination failed to establish how he got measles.✗ - The inquest has been adjourned.✗ QASEMCONSISTENCY died/death: - Who died? A man✓ - How someone died? from measles✗",
      "chunk_index": 3
    },
    {
      "index": 413,
      "chunk_id": "QASemConsistency2024_chunk_04",
      "source_id": "QASemConsistency2024",
      "text": "man died of measles.✗ - A post-mortem examination was conducted.✓ - The post-mortem examination failed to establish how he got measles.✗ - The inquest has been adjourned.✗ QASEMCONSISTENCY died/death: - Who died? A man✓ - How someone died? from measles✗ failed: - What failed to do something? a post-mortem examination✓ - What did something fail to do? to establish how he got the illness✗ got: - Who got something? he; A man✓ - What did someone get? the illness; measles✓ opened: - What has been opened? An inquest into the death of a man✗ establish: - What didn't establish something? a post-mortem examination✓ - What didn't something establish? how he got the illness✗ examination: - Who was examined? A man who died of measles✓ - When was someone examined? post-mortem✓ - Why was someone examined? to establish how he got the illness✗ ... Table 1: Comparison of QASEMCONSISTENCYto existing fine-grained decompositions. Here, the summary is inconsistent in multiple respects: it assumes that the man died from measles, while the cause of death remains unclear; the examination actually failed to establish the cause of death rather than how the man got the illness; and an inquestwillbe opened in the future rather than in the past. The token-level annotations are from CLIFF (Cao and Wang, 2021), with highlighted tokens indicating those marked as expressing unsupported information. For QGQA and claim-level, ✓ indicates\"supported\"and ✗ indicates\"not supported\". QGQA lists the candidate answers (noun phrases) and the generated questions using Q2 (Honovich et al., 2021), where the consistency labels indicate whether the candidate answer from the summary corresponds to the predicted answer based on the source. FActScore claims are generated by GPT 4o given FActScore's instructions and demonstrations (Min et al., 2023), with our annotation of factual consistency. Unlike the other methods, our QASEMCONSISTENCYmethodology decomposes the summary into fine-grained predicate-argument statements in the form of QAs, pinpointing more precisely at the unsupported facts. For example the unsupported QA\"How someone died? from measles\"pinpoints the inconsistency about the cause of death, while FActScore mixes in the same claim both the cause of death as well as the indication of who died, where the latterisconsistent in the summary. question-answering (Honovich et al., 2021; Fabbri et al., 2022), researchers have developed methods to decompose the information in the generated text. While these decomposition techniques result in in- tuitive propositions in natural language, they often lack granularity and include multiple",
      "chunk_index": 4
    },
    {
      "index": 414,
      "chunk_id": "QASemConsistency2024_chunk_05",
      "source_id": "QASemConsistency2024",
      "text": "question-answering (Honovich et al., 2021; Fabbri et al., 2022), researchers have developed methods to decompose the information in the generated text. While these decomposition techniques result in in- tuitive propositions in natural language, they often lack granularity and include multiple units of infor- mation, each of which might be supported or not by the reference text. As exemplified in Table 1 (Claim-level), the extracted claim \"the man died of measles\" could be further decomposed into two semantic relations - (1) \"the man died\" (PATIENT) and (2) \"the death was due to measles\" (CAUSE), with only the latter being unsupported by the ref- erence text. We provide a detailed review of the relevant literature in Section 6. Conversely, detecting factual inconsistencies at a finer level of granularity has involved using com- plex syntactic (Goyal and Durrett, 2020) and se- mantic (Ribeiro et al., 2022) formalisms. These formal approaches require linguistic expertise from annotators to manually evaluate generative models. Consequently, these methods were applied only for automatic detection of factual inconsistencies with models trained on synthetic data. In this work, we represent minimal propositions, which correspond to individual predicate-argument relations, using intuitively comprehensible natural language question-answer pairs. This enables both human and automatic detection and a fine-grained representation of factual consistency errors. 2.2 Predicate-argument Relations: From Neo-Davidsonian Semantics to QA-SRL Neo-Davidsonian semantics (Davidson, 1967; Hig- ginbotham, 1983; Parsons, 1990) is a framework for representing the complex interactions between events and participants (i.e. predicate-argument relations) in a logical form. For example, given the sentence\"Mary bought a book yesterday and gave it to John with a smile\", the Neo-Davidsonian representation is: (1) ∃e1, e2 (Buying(e1) ∧Agent(e 1,Mary) ∧Theme(e 1,Book) ∧Time(e 1,Yesterday) ∧Giving(e 2)∧Agent(e 2,Mary) ∧Recipient(e 2,John) ∧Theme(e 2,Book) ∧Manner(e 2,Smile)) This representation indicates that there are two predicates -buy (e1) andgive (e2), each with its own set of arguments. Following the underlying principles of Neo- Davidsonian semantics, various approaches have been developed to model fine-grained propositions corresponding to individual predicate-argument re- lations, including FrameNet (Baker et al., 1998), PropBank (Palmer et al., 2005), Semantic Depen- dency Parsing (Oepen et al., 2014) and AMR (Ba- narescu et al., 2013). These approaches, however, typically rely on complex semantic formalisms, making them less accessible to non-expert annota- tors, and in a sense harder to extract and manipulate with LLMs. To bridge this gap, we propose using QA-SRL, a semantic formalism that simplifies traditional SRL schemes by representing",
      "chunk_index": 5
    },
    {
      "index": 415,
      "chunk_id": "QASemConsistency2024_chunk_06",
      "source_id": "QASemConsistency2024",
      "text": "complex semantic formalisms, making them less accessible to non-expert annota- tors, and in a sense harder to extract and manipulate with LLMs. To bridge this gap, we propose using QA-SRL, a semantic formalism that simplifies traditional SRL schemes by representing each predicate-argument bought Who bought something? Mary What did someone buy? A book When did someone buy something? Yesterday gave Who gave something? Mary Who gave something to? John What did someone give? A book How did someone give? With a smile Table 2: QA-SRL representation for \"Mary bought a book yesterday and gave it to John with a smile.\" relation through a simple and \"minimal\" question- answer pair, such as\"Who bought something? Mary\"(He et al., 2015; FitzGerald et al., 2018; Roit et al., 2020). For example, the QA-SRL rep- resentation of the above sentence is detailed in Ta- ble 2. Here,\"Mary\"is identified as the agent of the predicate\"bought\", where this semantic rela- tion is represented by the question\"Who bought something? Mary\". Loosely speaking, each QA typically corresponds to a single Neo-Davidsonian proposition. By relying on a comprehensible natural- language representation, QA-SRL largely sub- sumes traditional SRL schemes. Notably, it suc- cessfully covers valuable implicit semantic argu- ments (Roit et al., 2024), which are intuitively cap- tured by human annotators, and subsequently by models trained on such annotated data. 3 QASEMCONSISTENCY Given a generated text y and a reference text x that is expected to support the information iny (e.g. the source for generation or a grounding text for it), we define the task of localizing factual inconsistencies as identifying the set of \"localized\" assertions that are contained in y but are not supported by x. This involves the decomposition of y into suchasser- tions, each being a unit of information that can be individually assessed for its entailment by x. For effective localization, we suggest two desired prop- erties of this decomposition. First, the assertions should be asminimalin scope as possible, where ideally each assertion should not be further decom- posable into smaller verifiable assertions. Second, each assertion should be human interpretable, pro- viding clear insights to common language speakers, thus allowing efficient crowdsourced annotation for localized factual inconsistencies. To fulfill these properties, we propose decom- posing y into the set of its predicate-argument level propositions, using the QASem framework (§2.2). By construction, each question-answer pair (QA) in QASem corresponds to asinglepredicate-argument relation, expressed in natural language. A QA",
      "chunk_index": 6
    },
    {
      "index": 416,
      "chunk_id": "QASemConsistency2024_chunk_07",
      "source_id": "QASemConsistency2024",
      "text": "To fulfill these properties, we propose decom- posing y into the set of its predicate-argument level propositions, using the QASem framework (§2.2). By construction, each question-answer pair (QA) in QASem corresponds to asinglepredicate-argument relation, expressed in natural language. A QA pair is considered supported by the ref- erence text if the proposition corresponding to the predicate-argument relation isentailedfrom the reference text x. We follow previous work and frame the task as a binary classification problem, with labels ∈ {supported,not supported}, consid- ering both theneutralandcontradictionclasses in the standard entailment recognition task asnot supported(Maynez et al., 2020; Kryscinski et al., 2020; Honovich et al., 2022; Min et al., 2023). For example, consider the source document x and the summary y in Table 1, taken from our annotated dataset. The QA \"Who died? A man\" issupported because the article explicitly mentions that Gareth Colfer-Williams, who is a man, died. Conversely, the QA \"How someone died? From measles\" is not supported, as the article states that the cause of death remains unclear. Once all QAs are assigned with a label, we can also calculate an overall factual consistency score, defined as the percentage of supported QAs over all QAs. This interpretable score represents the proportion of supported semantic relations within y and can be used for model evaluation. For ex- ample, the overall factual consistency score of the generated text in Table 1 is 7/12. 2 For compari- son, the prior localization methods of QGQA and FActScore assign a score of 1/5 to the same sum- mary. This discrepancy stems from the insufficient granularity level of these approaches (see §2.1 and §6), where each assertion contains a mixture of supported and unsupported information. QASem parsing.We automatically generate QAs for both verbal and nominal predicates using a parser that we purposely trained on the QASRL (FitzGerald et al., 2018) and QANom (Klein et al., 2020) datasets. Specifically, we use the same architecture of the original QASem parser (Klein et al., 2022) and replace the original T5-small model with T5-XL (3B) to improve per- formance. This parser takes as input a sentence and a predicate, verbal or nominal, and generates a list of atomic QAs. We train our parser for 5 2In practice, there are a few more QAs that we omitted for brevity. epochs until convergence with the Adam optimizer and a learning rate of 5e−05 . We evaluate our parser on the QASRL gold",
      "chunk_index": 7
    },
    {
      "index": 417,
      "chunk_id": "QASemConsistency2024_chunk_08",
      "source_id": "QASemConsistency2024",
      "text": "train our parser for 5 2In practice, there are a few more QAs that we omitted for brevity. epochs until convergence with the Adam optimizer and a learning rate of 5e−05 . We evaluate our parser on the QASRL gold data (Roit et al., 2020) and QANom (Klein et al., 2020), achieving 75.9 F1 (+7.3) on QASRL and 72.4 F1 (+13.2) on QANom. Similarly to (Klein et al., 2022), we use a span match threshold of IOU >= 0.3 to match between predicted and gold arguments. The above training datasets do not include annotation of predicate argu- ments for copular verbs (e.g., \"John is a musician\"). To ensure completeness of QASEMCONSISTENCY, we prompt Gemini-Flash (2.0) to generate QAs to represent predicate-argument relations for copular verbs (e.g., \"Who is a musician? John\"). 4 Human Detection of Localized Factual Inconsistencies In this section, we apply our QASEMCONSIS- TENCYmethodology to manually annotate local- ized factual inconsistencies in generated texts. We collect such annotations across three different sce- narios of attributable text generation: summariza- tion (Cao and Wang, 2021), generation of people's biography and verification against their Wikipedia pages (Min et al., 2023), and response genera- tion with a generative search engine citing external sources (Liu et al., 2023a). This annotation serves two primary purposes. First, we assess that QASEMCONSISTENCYis a suitable approach for collecting high-quality anno- tations of localized inconsistencies through cost- effective crowdsourcing. This can enable future work to perform human evaluation of generative models with this methodology. Second, we create a diverse and large entailment benchmark where the hypothesis is a predicate-argument relation in the form of a question-answer pair. We believe this benchmark will be valuable for future research to develop and improve models that can predict entailment for predicate-argument level assertions. To the best of our knowledge, this is the first work to annotate factual inconsistencies for predicate- argument level propositions. 4.1 Data Collection As mentioned in Section 3, given a generated text y, we automatically predict QA pairs that represent localized propositions corresponding to single predicate-argument relations, using our QASem parser.3 Then, human annotators inspect these QAs along with the reference text x and the generated text y and determine for each QA whether it is supported by x or not. Table 1 (QASEMCONSISTENCY) shows an example of such annotations. We now describe our complete annotation process. Enhancing annotation efficiency.An entity mentioned in y might be absent in",
      "chunk_index": 8
    },
    {
      "index": 418,
      "chunk_id": "QASemConsistency2024_chunk_09",
      "source_id": "QASemConsistency2024",
      "text": "and determine for each QA whether it is supported by x or not. Table 1 (QASEMCONSISTENCY) shows an example of such annotations. We now describe our complete annotation process. Enhancing annotation efficiency.An entity mentioned in y might be absent in the reference text x (i.e., an extrinsic hallucination) (Xiao and Carenini, 2023). In such cases, all QAs featuring this entity as the answer are inevitably not sup- ported. For instance, considering the source article in Table 1, if the summary would have mentioned \"An inquest into the death of awoman...\", then the QAs \"Who died? A woman\" and \"Who got some- thing? A woman\" would not be supported since the reference text does not mention any woman. Leveraging this observation, we make the anno- tation process more efficient by dividing it into two sequential steps. In the first step, annotators go through the entity spans from the generated text y corresponding to QASemarguments(i.e. the an- swers) and classify each span to either \"covered\" or \"not covered\", according to whether it is men- tioned explicitly or can be implied from x. Any answer classified as \"not covered\" (representing an extrinsic hallucination) automatically renders all associated QA pairs as \"unsupported\". This elimi- nates the need for annotators to evaluate these QA pairs individually. In the second step, annotators focus exclusively on the remaining QA pairs - those whose answers have been confirmed to be covered by the refer- ence text x. We define a QA as \"supported\" if the meaning of that QA can be inferred from x. Specifically, we adhere to the original definition of textual entailment from (Dagan et al., 2013):\"a text T entails a hypothesis H if there exists some background knowledge K such that T and K to- getherentails H while K alone does not\". For instance, the reference text \"Max was seriously in- jured when boiling water accidentally spilled on his hand\" entails the response \"Hot water over 80 degrees Celsius spilled on Max's hand\", basedalso on the assumed common knowledge K that \"Water boils at 100 degrees Celsius at sea level\" but does not entail \"Water reaches its boiling point at 100 de- grees Celsius\", because the background knowledge 3In some cases, low-quality generated QAs were filtered in the annotation process, in a preliminary step (see Appendix A). K alone suffices to entail the text without the ref- erence text. Therefore, annotators were instructed to rely",
      "chunk_index": 9
    },
    {
      "index": 419,
      "chunk_id": "QASemConsistency2024_chunk_10",
      "source_id": "QASemConsistency2024",
      "text": "the background knowledge 3In some cases, low-quality generated QAs were filtered in the annotation process, in a preliminary step (see Appendix A). K alone suffices to entail the text without the ref- erence text. Therefore, annotators were instructed to rely solely on the reference text x and their com- mon knowledge background to determine whether a QA can be inferred from the reference text. The use of external resources (e.g., web search) was restricted for clarifying the definitions of complex terms (e.g., \"six-under-par\"), but not to verify con- tent not stated in the reference text. To further assist annotators with QA evaluation, we advise them to rephrase the question-answer pair as an affirmative statement and assess whether the reference text supports this statement. For in- stance, the QA pair\"What did someone open? An investigation\"could be rephrased as\"Someone opened an investigation\". To enhance annotators' focus, all QAs of the same predicate are shown together. In addition to the \"support\" labels, annotators were encouraged to write free text notes to justify their decisions, encouraging deeper considerations. These two sep- arate stages improve annotation efficiency and also introduce an additional layer that classifies factual inconsistencies into extrinsic versus intrinsic errors. Annotation Tool.To facilitate the human anno- tation process, we develop an intuitive annotation interface that streamlines the two steps.4 Figure 2 shows the interface of the second annotation step (QA evaluation). See Appendix A for implementa- tion details. Tasks and Generative Models.We collect hu- man annotations from three settings of attributable text generation. First, we consider the task of abstractive sum- marization on the XSUM dataset that summarizes news articles to a single sentence. Specifically, we sample a subset of 74 summaries from the CLIFF dataset (Cao and Wang, 2021), in which the source articles are from XSUM (Narayan et al., 2018) and the summaries were automatically generated by BART (Lewis et al., 2020) and PEGASUS (Zhang et al., 2020). CLIFF manually annotated each gen- erated summary with token-level annotation of con- sistency errors. Second, we annotate faithfulness localization for 34 people biographies (3-5 sentences) included in FActScore (Min et al., 2023). These biographies were generated in zero-shot by LLMs such as Chat- 4Our tool can be found at https://github.com/ ariecattan/loc-unfaith. Figure 2: An example of the QA annotation step. The article in the left side is the reference text and the summary is shown on the right side. At each time, the",
      "chunk_index": 10
    },
    {
      "index": 420,
      "chunk_id": "QASemConsistency2024_chunk_11",
      "source_id": "QASemConsistency2024",
      "text": "4Our tool can be found at https://github.com/ ariecattan/loc-unfaith. Figure 2: An example of the QA annotation step. The article in the left side is the reference text and the summary is shown on the right side. At each time, the interface highlights a specific predicate (here \"died\") and displays the QAs representing the predicate-argument relations for that predicate. The green thumbs-up and red thumbs-down correspond to supported and not supported, respectively. GPT, InstructGPT, and the retrieval-augmented Per- plexityAI model5 and were subsequently verified against the Wikipedia page of these entities. Lastly, we extend our evaluation to open-ended response generation with in-line citations to exter- nal sources. We use the \"Verifiability\" dataset (Liu et al., 2023a) that assesses factual consistency of each generated sentence against its correspond- ing source(s) for several generative search engines (BingChat, NeevaAI, Perplexity.ai, and YouChat) across a range of diverse queries (AllSouls, davin- cidebate, WikiHowKeywords, ELI5 (KILT / Live), NaturalQuestions). From this dataset, we selected 41 responses, comprising a total of 189 sentences. Each instance in our dataset includes annotations from 3 different workers. Annotators.We recruit annotators through Ama- zon Mechanical Turk.6 We follow the controlled crowdsourcing protocol (Roit et al., 2020), which consists of training the workers with detailed in- structions (using intuitive slides) and providing on- going personalized feedback throughout the pro- cess. Multiple examples were included to illustrate the two annotation tasks (Span and QA evalua- tion). Annotators' compensation is described in Appendix A. Task #Responses #Sentences #QAs IAA CLIFF 74 74 693 0.72 FActScore 36 229 1,109 0.78 Verifiability 41 189 1,296 0.67 Table 3: Statistics of our collected benchmark. 4.2 Dataset Properties Table 3 presents the statistics of our collected dataset. Overall, we gathered entailment annota- tions for 3,098 different QAs. The ground truth label for each QA is determined by the majority vote among the annotators. We split the dataset into development and test sets 50/50. The develop- ment set can serve for prompt engineering or for optimizing the decision threshold. Each sentence is represented by an average of 6.3 predicate-argument QAs. This granular decompo- sition contrasts with FactScore's approach, which yields only 4.1 free-text claims per sentence. In addition, we found that for 27% of the predicates in our collected benchmark, some QA pairs are entailed and some are not. This confirms that as- sessing each predicate-argument assertions in the model-generated response is valuable and allows to pinpoint the factual",
      "chunk_index": 11
    },
    {
      "index": 421,
      "chunk_id": "QASemConsistency2024_chunk_12",
      "source_id": "QASemConsistency2024",
      "text": "found that for 27% of the predicates in our collected benchmark, some QA pairs are entailed and some are not. This confirms that as- sessing each predicate-argument assertions in the model-generated response is valuable and allows to pinpoint the factual inconsistencies. We evaluate inter-annotator agreement using Fleiss' Kappa. We observed substantial agree- ment across all three tasks: κ= 0.72 for CLIFF, 5perplexity.ai 6https://www.mturk.com/ Figure 3: Visualization of the correlation between the difference in factual consistency scores (calculated us- ing QASEMCONSISTENCYand token-based approach for \"Cliff\") and human side-by-side preferences. κ= 0.79 for FactScore, and κ= 0.67 for Ver- ifiability. For comparison, previous factuality benchmarks report lower agreement: Pagnoni et al. (2021) report κ= 0.58 at the sentence-level and Cao and Wang (2021) reports κ= 0.35 at the token-level. Annotating factual consistency is a challenging and sometimes subjective task (Falke et al., 2019). We believe that the high agreement is due to the QASEMCONSISTENCY's decomposi- tion into atomic semantic relations, where annota- tors need to assess a single assertion at a time. In addition, by collecting factual consistency annota- tion for each predicate-argument relation, QASEM- CONSISTENCYensures that the human raters will assess all details. 4.3 Analysis 4.3.1 Overall Score Here, we aim to show that QASEMCONSISTENCY not only enables the localization of factual inconsis- tency but also provides an overall score that reflects well the degree of inconsistency in the response. To achieve this, we collect side-by-side annotations of a source text x paired with two different model- generated responses y1 and y2. Annotators were asked to compare the factual consistency of the two responses on a scale of 1 to 5, where 1 indicates that y2 ismuchmore consistent than y1, 2 that y2 is more consistent than y2, 3 that they are almost equivalent and symmetrically 4 and 5 indicate ad- vantage of y1 over y2. This pairwise comparative judgment (Thurstone, 1927; David, 1963), a com- mon and intuitive approach for comparing model outputs, allows human annotators to reliably com- pare outputs on a shared scale (Chiang et al., 2024). Given a scoring function s(x, y)that assigns a factual consistency score to the output y with re- spect to the reference text x, we expect the differ- ence d(y1, y2) =s(x, y1)−s(x, y2) to correlate with the side-by-side score. Indeed, if y1 is much more consistent than y2, we expect d to be a large positive value, whereas if the two",
      "chunk_index": 12
    },
    {
      "index": 422,
      "chunk_id": "QASemConsistency2024_chunk_13",
      "source_id": "QASemConsistency2024",
      "text": "text x, we expect the differ- ence d(y1, y2) =s(x, y1)−s(x, y2) to correlate with the side-by-side score. Indeed, if y1 is much more consistent than y2, we expect d to be a large positive value, whereas if the two responses are nearly equivalent in consistency,d should be near zero. We collect these side-by-side annotations on the summaries from CLIFF (37 pairs) and the biogra- phies from FactScore (16 pairs). 7 As mentioned above (§3), we define sQA(x, y)as the percent- age of supported QAs in y and compute this score using human labels. The Spearman correlation be- tween d(y1, y2) and the side-by-side consistency score yields strong positive results. For the sum- maries, we obtain a high correlation of ρ= 0.73 (p-value < 0.001), while the token-level evaluation in CLIFF (ρ= 0.11) and QGQA (ρ= 0.2) show no correlation. Figure 3 plots the difference in factual consistency scores derived from QASEM- CONSISTENCYand from token-level annotations against human side-by-side preferences. For the biographies, we obtain a high correlation for all metrics ( ρ= 0.71 for QASEMCONSISTENCY, ρ= 0.67 for FactScore, ρ= 0.81 for QGQA). However, due to the overlapping confidence inter- vals and the small sample size (N= 16), we cannot conclusively determine that one metric is statisti- cally superior to the others based solely on these results.These strong correlations demonstrate that QASEMCONSISTENCYeffectively reflects the degree of inconsistency in model-generated responses and can be reliably used for ranking models. 7We cannot do this analysis for verifiability because differ- ent models generate responses that point to different sources. 4.3.2 Qualitative Analysis We compare the localized annotations of supported QAs in the CLIFF's subset of our dataset to the original token-level annotations in CLIFF. Specifi- cally, we analyze the cases where summaries were annotated asfully supportedin CLIFF but not in our annotations and vice-versa. Notably, we observe that all ten summaries where all QAs were anno- tated assupportedin our dataset were also anno- tated asfully supportedin CLIFF. Conversely, we identified 10 out of 74 summaries marked asfully supportedin CLIFF whereas our annotators found unsupported QAs. Upon analyzing these cases, we discover that the majority of the summaries (7 out of 10) were indeed not fully supported by the ref- erence text, as reflected by the unsupported QAs. For instance, the summary sentence\"The US has suspended its participation in talks with Russia to try to broker a cessation of hostilities in Syria, the",
      "chunk_index": 13
    },
    {
      "index": 423,
      "chunk_id": "QASemConsistency2024_chunk_14",
      "source_id": "QASemConsistency2024",
      "text": "indeed not fully supported by the ref- erence text, as reflected by the unsupported QAs. For instance, the summary sentence\"The US has suspended its participation in talks with Russia to try to broker a cessation of hostilities in Syria, the State Department says. \"is not fully supported by the reference text\"The United States is suspending its participation in bilateral channels with Russia that were established to sustain the cessation of hostilities\"because the reference text mentions that the suspension of the U.S participation is\"to sustain the cessation\"and not\"to broker a cessa- tion\". This fine-grained factual inconsistency was indicated by several QAs marked asnot supported, such as\"Why has someone suspended something? To try to broker a cessation of hostilities in Syria\" about the predicate \"suspended\". Two other sum- maries could be interpreted as either supported or not supported, with our annotators showing local- ized disagreement on these cases. One summary was mistakenly annotated as not supported by our annotators. This analysis confirms that having hu- man annotators verify each predicate-argument re- lation is not only beneficial for localizing factual inconsistencies, but also effectively helps annota- tors toidentifythem more accurately. 5 Automatic Detection of Localized Factual Inconsistencies In this section we describe several models that auto- matically localize factual inconsistencies according to our methodology. We decompose the generated text y into a list of QA pairs {qa1, qa2, ..., qan}, and define the likelihood for each qai to be supported (entailed) by the reference text x as s(x, qai)∈[0,1] . We use the QASem parser from Section 3 to generate the QAs and conduct our experiments with different entailment classifiers. 5.1 Entailment Classifiers We experiment with two types of models: SupervisedWe apply three recent off-the-shelf NLI models to predict whether the QASem QAqai is entailed by the reference text. In these experi- ments, the premise is the reference text x and the hypothesis is the concatenation of the question qi and the answera i inqa i. The first supervised classifier isTRUE(Hon- ovich et al., 2022), an encoder-decoder model based on T5-XXL (11B parameters) (Raffel et al., 2020) and finetuned on diverse entailment datasets: SNLI (Bowman et al., 2015), MNLI (Williams et al., 2018), FEVER (Thorne et al., 2018), Sci- tail (Khot et al., 2018), PAWS (Zhang et al., 2019) and VitaminC (Schuster et al., 2021).8 The TRUE model, also known as AUTOAIS (Bohnet et al., 2022), was widely used in previous work for",
      "chunk_index": 14
    },
    {
      "index": 424,
      "chunk_id": "QASemConsistency2024_chunk_15",
      "source_id": "QASemConsistency2024",
      "text": "FEVER (Thorne et al., 2018), Sci- tail (Khot et al., 2018), PAWS (Zhang et al., 2019) and VitaminC (Schuster et al., 2021).8 The TRUE model, also known as AUTOAIS (Bohnet et al., 2022), was widely used in previous work for auto- matically measuring factual consistency (Bohnet et al., 2022; Gao et al., 2023a; Roit et al., 2023; Gao et al., 2023b; Slobodkin et al., 2024). This model was trained to generate \"1\" if the hypothesis is en- tailed by the premise and \"0\" otherwise. Hence, s(x, qai) is set as the probability of generating \"1\", given the likelihood score of \"1\" and \"0\". The sec- ond supervised model isTrueTeacher(Gekhman et al., 2023), a T5-XXL model finetuned on many model generated summaries annotated for factual consistency using an LLM. Similarly to TRUE, this model was trained to generated \"1\" if the hypothe- sis is entailed by the premise. The last supervised model isMiniCheck(Tang et al., 2024), a recent T5 Large (770M parameters) trained on synthetic data generated with GPT-4. This model outper- forms all systems of comparable size and reaches GPT-4 accuracy on LLM-AggreFact. LLM PromptingWe also use several LLMs to predict s(x, qai) using few-shot prompting. We instruct the model to write \"Yes\" if the proposition corresponding to the QA is entailed by the refer- ence text, and \"No\" otherwise. The prompt used in our experiments is presented in Appendix C. We first apply this prompt to recent open-source LLMs includingLlama 8B (v3.1)(Dubey et al., 2024), Gemma (v2) 2B, 9B and 27B(Riviere et al., 2024) andMistral Nemo Instruct 9. For all prompted 8https://huggingface.co/google/t5_xxl_true_ nli_mixture 9https://mistral.ai/news/mistral-nemo CLIFF FActScore Generative Search BAcc AUC BAcc AUC BAcc AUC Supervised NLI models TRUE (11B) 61.4 72.1 72.588.868.5 79.1 TrueTeacher (11B) 68.982.171.7 88.7 57.8 80.5 Minicheck (770M)70.480.878.087.376.3 83.4 LLM prompting Gemma 2 2B 65.9 73.7 74.6 81.9 70.8 80.2 Gemma 2 9B 70.0 82.5 78.0 88.1 77.187.1 Gemma 2 27B 72.385.275.290.471.6 85.5 Mistral Nemo (12.2B) 70.6 79.5 73.7 85.4 71.5 82.4 Llama 3.1 8B 65.8 79.0 76.5 89.4 68.5 83.7 GPT-4o75.7-82.4-79.2- Table 4: Performance of automatic models on the test set of our collected benchmark. We cannot report AUC for GPT-4o because it does not directly output probability scores for \"Yes\" or \"No\" answers. The highest BAcc and AUC within each category (Supervised NLI model and LLM prompting) are shown in bold. models, we define the entailment score s as the probability of generating \"Yes\"",
      "chunk_index": 15
    },
    {
      "index": 425,
      "chunk_id": "QASemConsistency2024_chunk_16",
      "source_id": "QASemConsistency2024",
      "text": "directly output probability scores for \"Yes\" or \"No\" answers. The highest BAcc and AUC within each category (Supervised NLI model and LLM prompting) are shown in bold. models, we define the entailment score s as the probability of generating \"Yes\" as the first gener- ated token, based on the likelihood score of gener- ating \"Yes\" and \"No\". Finally, we also use GPT-4o to predict entailment for the QAs but evaluate only \"hard\" predictions (see Section 5.2) because it does not output distributions over the vocabulary. 5.2 Evaluation of Automatic Detection We report two common metrics to evaluate the per- formance of automatic models to predict entail- ment of QAs. First, we follow standard evalua- tion practices (Honovich et al., 2022; Gekhman et al., 2023) and report the Area Under the Re- ceiver Operating Characteristic Curve (ROC AUC), which plots the true positive rate against the false positive rate for different possible thresholds of s(x, qai). Second, since the models are trained or instructed to classify a QA as eithersupportedor not supported, we also measure the performance of the hard prediction. Similarly to (Tang et al., 2024), we do not perform threshold tuning for each dataset and consider a QA as supported if s(x, qai)≥0.5 . Following previous work (La- ban et al., 2022; Fabbri et al., 2022; Tang et al., 2023, 2024), we report the Balanced accuracy met- ric (BAcc):BAcc= 1 2 ( TP TP+FN + TN TN+FP ). 5.3 Results Table 4 presents the results of our automatic mod- els. Notably, GPT-4o achieves the highest bal- anced accuracy (BAcc) across all datasets (75.7 on CLIFF, 82.4 on FactScore and 79.2 on Verifiability). Open source LLMs and supervised NLI models achieve also decent performance, with Minicheck emerging as the top-performing supervised model. These results suggest that even though these mod- els were trained on standard entailment datasets, where the hypothesis is a single sentence, they can effectively adapt to our setting, where hypotheses take the form of question-answer pairs representing predicate-argument relations (e.g,\"How someone died? from measles\"). This generalization ability likely stems from the models' training on massive text corpora, coupled with the fact that QASem question-answer pairs are expressed in natural lan- guage, unlike traditional semantic role labeling (SRL) schemes like PropBank or FrameNet that rely on predefined and complex taxonomies. Fi- nally, the consistently higher performance on the FactScore dataset can be explained by the nature of its",
      "chunk_index": 16
    },
    {
      "index": 426,
      "chunk_id": "QASemConsistency2024_chunk_17",
      "source_id": "QASemConsistency2024",
      "text": "in natural lan- guage, unlike traditional semantic role labeling (SRL) schemes like PropBank or FrameNet that rely on predefined and complex taxonomies. Fi- nally, the consistently higher performance on the FactScore dataset can be explained by the nature of its biographical content. These texts contain a high frequency of copular sentences (e.g.,\"Roselyn Sanchez is an actress. \"), which present a simpler verification challenge for the models. Indeed, 15% of the QAs in FactScore are copular constructions (e.g,\"Who is an actress? Roselyn Sanchez\", for which GPT-4o achieves a BAcc of 0.89, compared to 0.81 for non-copula QAs. Beyond standard metrics, we want to assess how well model's overall scores align with human judg- ment, similar to our approach with human scores (see Section 4.3.1). We compute the Spearman cor- relation between the difference of the individual QA scores d(y1, y2) =s QA(x, y2)−s QA(x, y1) and the side-by-side human preference Likert scale. Our experiments show that GPT-4o achieves a Spearman correlation of ρ= 0.54 (p-value < 0.05) on CLIFF.10 While GPT-4o achieves a high performance across datasets, there is still a gap between auto- matic and human performance, leaving much room for improvement in future work. 5.4 Analysis QA vs. affirmative.To investigate the impact of the question-answer (QA) format on entailment performance, we convert the QA pairs into affirma- tive sentences (e.g,\"Who ate something? John\"to \"John ate something\") using a small LLM (Gemma 2 2B). We then prompt Gemma 2 9B to determine whether these affirmative sentences are supported by the reference text. Surprisingly, this approach results in substantially lower balanced accuracy (BAcc) scores compared to the QA format: 63.7 (-6.3) on CLIFF, 74.7 (-3.3) on FActScore and 73.4 (-3.7) on Verifiability. We believe that the QA for- mat provides more structured information than sim- ple affirmative sentences because (1) it explicitly delineates the predicate (within the question) and the argument (within the answer), and (2) the ques- tion formulation (e.g., \"who\", \"where\", etc.) offers an additional layer of semantic information which can be valuable for deciding entailment. 6 Related Work Identifying factual inconsistencies in attributable text generation has become a prominent research area in recent years. Existing methodologies for this task can be categorized based on the granular- ity of the detection. Table 1 illustrates and com- pares the various decomposition methods, recently proposed in the literature. Starting with coarse-grained evaluation, Sum- mEval (Fabbri et al., 2021) asks annotators to",
      "chunk_index": 17
    },
    {
      "index": 427,
      "chunk_id": "QASemConsistency2024_chunk_18",
      "source_id": "QASemConsistency2024",
      "text": "task can be categorized based on the granular- ity of the detection. Table 1 illustrates and com- pares the various decomposition methods, recently proposed in the literature. Starting with coarse-grained evaluation, Sum- mEval (Fabbri et al., 2021) asks annotators to assign a 1-5 likert score to the entire summary, while some other works aim to produce a single score/label for the entire output (Yin et al., 2021; Rashkin et al., 2023; Honovich et al., 2022; Tang et al., 2023; Liu et al., 2023b; Gekhman et al., 2023). Several works evaluate each generated sentence separately with some (simple or sophis- ticated) form of aggregation (Falke et al., 2019; Kryscinski et al., 2020; Pagnoni et al., 2021; Utama et al., 2022; Tang et al., 2022; Laban et al., 2022; 10The results on FactScore were not statistically significant due to the limited number of pairwise preferences (n= 16). Mishra et al., 2024; Subbiah et al., 2024; Tang et al., 2024). To achieve sub-sentence evaluation, a few re- cent works decompose each sentence into \"claims\", \"facts\" or \"propositions\" (Table 1 Claim-level), whose support by the reference text is then as- sessed independently (Min et al., 2023; Krishna et al., 2023; Chen et al., 2023; Kamoi et al., 2023a; Wanner et al., 2024; Samir et al., 2024; Wei et al., 2024; Wan et al., 2024). However, these free text claims typically lack structure and systematicity. As a result, identifying unsupported claims does not effectivelylocalizefactual mistakes in the gen- erated text. In fact, these claims are not atomic and encompass multiple semantic relations. For instance, the claim \"The man died of measles\" in Table 1 is not supported by the article, but it re- mains unclear whether the issue is that the man died for a reason other than measles, or that there is no man who died. Another line of automatic evaluation methodol- ogy, typically referred to as question-generation and question-answering (QGQA), consists of generating questions and answers based on the model output and then comparing the answers to those obtained from the reference text by a QA model (Wang et al., 2020; Durmus et al., 2020; Nan et al., 2021; Scialom et al., 2021; Honovich et al., 2021; Fabbri et al., 2022). However, Kamoi et al. (2023b) demonstrate that this paradigm falls short in providing effectivelocalizationof factual inconsistency. This is primarily because the gen- erated questions often contain factual inconsisten- cies from the summary",
      "chunk_index": 18
    },
    {
      "index": 428,
      "chunk_id": "QASemConsistency2024_chunk_19",
      "source_id": "QASemConsistency2024",
      "text": "Honovich et al., 2021; Fabbri et al., 2022). However, Kamoi et al. (2023b) demonstrate that this paradigm falls short in providing effectivelocalizationof factual inconsistency. This is primarily because the gen- erated questions often contain factual inconsisten- cies from the summary itself. Indeed, as shown in Table 1 (QGQA), the generated question for the answer phrase \"an inquest\" assumes that the man died of measles and something was already opened and adjourned. In contrast, our method is based on predicate-argument level QAs, where each QA represents asinglesemantic relation. Some other works ask human annotators to high- light inconsistent tokens or spans in the gener- ated text (Maynez et al., 2020; Cao and Wang, 2021). This method often results in relatively poor inter-annotator agreement (e.g., 0.35 Fleiss Kappa in CLIFF), because span annotation is sub- jective (Mishra et al., 2024) and individual spans might serve multiple roles in the sentence, where some roles are in correct assertions and some are not. Indeed, the token-level evaluation in Table 1 includes the tokens\"how he got the illness\", al- though this phrase also implies that the man got the illness, which is supported by the reference text. In that summary, the mistake is that the goal of the examination should be to determine the cause of death, rather than how the man contracted the illness. This unsupported fact cannot be captured with span highlighting while it can effectively iden- tified using our QA\"What didn't something es- tablish? how he got the illness\"(Table 1). In the same line of work, Laban et al. (2023) create SUMMEDITS, a challenging benchmark with lo- calized factual inconsistencies. However, unlike our benchmark, the mistakes are limited to a single token and are not naturally occurring. Goyal and Durrett (2020) propose DAE, an auto- matic evaluation metric that operates at the level of semantic dependency arcs in a structured semantic dependency representation (Oepen et al., 2014) to localize inconsistent semantic relations in the gen- erated text. Similarly, FactGraph (Ribeiro et al., 2022) represents both the source article and the summary with AMR (Banarescu et al., 2013) then model the factual inconsistencies at the edge level. However, these methods have only been applied au- tomatically because obtaining human judgments at this level of granularity is challenging. Indeed, an- notators would need to understand dependency or AMR labels and isolate the semantics of individual arcs within sentences, making manual evaluation difficult. Similarly to",
      "chunk_index": 19
    },
    {
      "index": 429,
      "chunk_id": "QASemConsistency2024_chunk_20",
      "source_id": "QASemConsistency2024",
      "text": "applied au- tomatically because obtaining human judgments at this level of granularity is challenging. Indeed, an- notators would need to understand dependency or AMR labels and isolate the semantics of individual arcs within sentences, making manual evaluation difficult. Similarly to DAE and FactGraph, our QASEMCONSISTENCYalso operates at the level of individual semantic relations, while representing them with simple natural language question-answer pairs, enabling human evaluation as well and easing LLM processing. Cho et al. (2024) shown success at applying Neo-Davidsonian formal semantics to automatic text-to-image evaluation. We note though that while QASEMCONSIS- TENCYis a promising approach for both manual and automatic evaluation, it focuses solely on mea- suring factual consistency, a crucial aspect of at- tributable text generation. To provide a more com- prehensive assessment, we suggest that future work use QASEMCONSISTENCYin additionto target- ted metrics for capturing other aspects, such as BERTScore (Zhang* et al., 2020) for relevance, BookScore (Chang et al., 2024) for coherence, or with LLM as a judge (Zheng et al., 2023). Furthermore, it is worth noting that recent ad- vancements in reference-based metrics for the rel- evance aspect, such as Pyramid (Nenkova and Passonneau, 2004), LitePyramid (Shapira et al., 2019) and RoSE (Liu et al., 2023c), highlight the growing interest in more granular evaluation of relevance. Since QASEMCONSISTENCYdecom- poses text into fine-grained predicate-argument as- sertions, we conjecture that it could be adapted also for providing more fine-grained evaluation of relevance. 7 Limitations Our work has several limitations. First, as we con- sider only verbal and nominal predicates, factual inconsistencies that stem from other predication types, e.g. adjectives, will not be localized with the finest granularity. For example, consider the ref- erence text\"John needs to repair his new red car, after the accident\", and the generated text\"John's blue car was damaged after he made an accident\". QASEMCONSISTENCYwould identify the incon- sistency with the QA\"What was damaged? John's blue car\". However, this QA could be further di- vided into two smaller QAs\"What was damaged? John's car\"and\"What is blue? John's car\", while only the latter is not supported. In addition, simi- larly to previous decomposition approaches (Min et al., 2023; Krishna et al., 2023), QASEMCONSIS- TENCYdoes not capture factual inconsistencies that are due to implicit or inter-sentence discourse rela- tions. For example, the sentences\"John missed the train and arrived late\"and\"John arrived late and missed the train\"would be treated as equivalent, even though they describe opposite causes and con-",
      "chunk_index": 20
    },
    {
      "index": 430,
      "chunk_id": "QASemConsistency2024_chunk_21",
      "source_id": "QASemConsistency2024",
      "text": "factual inconsistencies that are due to implicit or inter-sentence discourse rela- tions. For example, the sentences\"John missed the train and arrived late\"and\"John arrived late and missed the train\"would be treated as equivalent, even though they describe opposite causes and con- sequences. These limitations could be potentially addressed in future work by enriching QASEM- CONSISTENCYwith additional semantic decompo- sitions, such as QADiscourse (Pyatkin et al., 2020) or QA-Adj (Pesahov et al., 2023), when improved parsers will be made available. 8 Conclusion We introduced QASEMCONSISTENCY, a method- ology that detects and localizes factual consistency errors to specific predicate-argument relations. Our error localization method is robust, as shown by our high human agreement and strong correlation with human preferences. Contrast with other methods that either created complex, non-granular claims in natural language, or relied on linguistic formalisms that excluded non-expert annotators, our method can be applied with ease by non-expert users and models alike. Moreover, it can help human con- sumers of generative models recognize which parts of a response should be double checked. We hope that future work will focus on extending our approach to detect inconsistencies in the wider discourse, and help generative models to correct their responses. Acknowledgments We thank the anonymous reviewers and the action editor for their valuable suggestions. We also thank all the Mturk workers that participate in our anno- tation project. Special thanks to Eran Hirsch, Aviv Slobodkin, Ayal Klein, Shuyang Cao, Vidhisha Balachandran, Tanya Goyal, Greg Durrett, Kyle Lo, Philippe Laban for their valuable discussions at various stages of the project and Ron Eliav for his considerable help in training the QASem parser. This research has been supported by a Google gift grant and the Israel Science Foundation (grant no. 2827/21). We also thank Google Cloud for provid- ing us with credits for running experiments on the Google Cloud Platform. Arie Cattan is partially supported by the PBC fellowship for outstanding PhD candidates in data science. References Collin F. Baker, Charles J. Fillmore, and John B. Lowe. 1998. The Berkeley FrameNet project. In 36th Annual Meeting of the Association for Com- putational Linguistics and 17th International Conference on Computational Linguistics, Vol- ume 1, pages 86-90, Montreal, Quebec, Canada. Association for Computational Linguistics. Laura Banarescu, Claire Bonial, Shu Cai, Madalina Georgescu, Kira Griffitt, Ulf Hermjakob, Kevin Knight, Philipp Koehn, Martha Palmer, and Nathan Schneider. 2013. Abstract Meaning Rep- resentation for sembanking. InProceedings of the 7th Linguistic",
      "chunk_index": 21
    },
    {
      "index": 431,
      "chunk_id": "QASemConsistency2024_chunk_22",
      "source_id": "QASemConsistency2024",
      "text": "Montreal, Quebec, Canada. Association for Computational Linguistics. Laura Banarescu, Claire Bonial, Shu Cai, Madalina Georgescu, Kira Griffitt, Ulf Hermjakob, Kevin Knight, Philipp Koehn, Martha Palmer, and Nathan Schneider. 2013. Abstract Meaning Rep- resentation for sembanking. InProceedings of the 7th Linguistic Annotation Workshop and In- teroperability with Discourse, pages 178-186, Sofia, Bulgaria. Association for Computational Linguistics. Bernd Bohnet, Vinh Q. Tran, Pat Verga, Roee Aha- roni, Daniel Andor, Livio Baldini Soares, Jacob Eisenstein, Kuzman Ganchev, Jonathan Herzig, Kai Hui, Tom Kwiatkowski, Ji Ma, Jianmo Ni, Tal Schuster, William W. Cohen, Michael Collins, Dipanjan Das, Donald Metzler, Slav Petrov, and Kellie Webster. 2022. Attributed question answering: Evaluation and modeling for attributed large language models.ArXiv, abs/2212.08037. Samuel R. Bowman, Gabor Angeli, Christopher Potts, and Christopher D. Manning. 2015. A large annotated corpus for learning natural lan- guage inference. InProceedings of the 2015 Conference on Empirical Methods in Natural Language Processing, pages 632-642, Lisbon, Portugal. Association for Computational Lin- guistics. Shuyang Cao and Lu Wang. 2021. CLIFF: Con- trastive learning for improving faithfulness and factuality in abstractive summarization. InPro- ceedings of the 2021 Conference on Empirical Methods in Natural Language Processing, pages 6633-6649, Online and Punta Cana, Dominican Republic. Association for Computational Lin- guistics. Yapei Chang, Kyle Lo, Tanya Goyal, and Mohit Iyyer. 2024. Booookscore: A systematic explo- ration of book-length summarization in the era of LLMs. InThe Twelfth International Conference on Learning Representations. Sihao Chen, Senaka Buthpitiya, Alex Fabrikant, Dan Roth, and Tal Schuster. 2023. PropSeg- mEnt: A large-scale corpus for proposition- level segmentation and entailment recognition. InFindings of the Association for Computa- tional Linguistics: ACL 2023, pages 8874-8893, Toronto, Canada. Association for Computational Linguistics. Wei-Lin Chiang, Lianmin Zheng, Ying Sheng, Anastasios N. Angelopoulos, Tianle Li, Dacheng Li, Banghua Zhu, Hao Zhang, Michael I. Jordan, Joseph E. Gonzalez, and Ion Stoica. 2024. Chat- bot arena: an open platform for evaluating llms by human preference. InProceedings of the 41st International Conference on Machine Learning, ICML'24. JMLR.org. Jaemin Cho, Yushi Hu, Jason Baldridge, Roopal Garg, Peter Anderson, Ranjay Krishna, Mohit Bansal, Jordi Pont-Tuset, and Su Wang. 2024. Davidsonian scene graph: Improving reliabil- ity in fine-grained evaluation for text-to-image generation. InICLR. Ido Dagan, Dan Roth, Mark Sammons, and Fabio Massimo Zanzotto. 2013. Recognizing textual entailment: Models and applications. In Recognizing Textual Entailment: Models and Applications. Herbert Aron David. 1963.The method of paired comparisons, volume 12. London. Donald Davidson. 1967. Truth and meaning.Syn- these, 17(1):304-323. Abhimanyu Dubey,",
      "chunk_index": 22
    },
    {
      "index": 432,
      "chunk_id": "QASemConsistency2024_chunk_23",
      "source_id": "QASemConsistency2024",
      "text": "Sammons, and Fabio Massimo Zanzotto. 2013. Recognizing textual entailment: Models and applications. In Recognizing Textual Entailment: Models and Applications. Herbert Aron David. 1963.The method of paired comparisons, volume 12. London. Donald Davidson. 1967. Truth and meaning.Syn- these, 17(1):304-323. Abhimanyu Dubey, Abhinav Jauhri, Abhinav Pandey, Abhishek Kadian, Ahmad Al-Dahle, Aiesha Letman, Akhil Mathur, Alan Schel- ten, Amy Yang, Angela Fan, Anirudh Goyal, Anthony Hartshorn, Aobo Yang, Archi Mitra, Archie Sravankumar, Artem Korenev, Arthur Hinsvark, Arun Rao, Aston Zhang, Aurelien Rodriguez, Austen Gregerson, Ava Spataru, Baptiste Rozière, Bethany Biron, Binh Tang, Bobbie Chern, Charlotte Caucheteux, Chaya Nayak, Chloe Bi, Chris Marra, Chris McConnell, Christian Keller, Christophe Touret, Chunyang Wu, Corinne Wong, Cristian Cantón Ferrer, Cyrus Nikolaidis, Damien Allonsius, Daniel Song, Danielle Pintz, Danny Livshits, David Es- iobu, Dhruv Choudhary, Dhruv Mahajan, Diego Garcia-Olano, Diego Perino, Dieuwke Hup- kes, Egor Lakomkin, Ehab A. AlBadawy, Elina Lobanova, Emily Dinan, Eric Michael Smith, Filip Radenovic, Frank Zhang, Gabriele Syn- naeve, Gabrielle Lee, Georgia Lewis Anderson, Graeme Nail, Grégoire Mialon, Guanglong Pang, Guillem Cucurell, Hailey Nguyen, Hannah Ko- revaar, Hu Xu, Hugo Touvron, Iliyan Zarov, Imanol Arrieta Ibarra, Isabel M. Kloumann, Is- han Misra, Ivan Evtimov, Jade Copet, Jaewon Lee, Jan Laurens Geffert, Jana Vranes, Jason Park, Jay Mahadeokar, Jeet Shah, Jelmer van der Linde, Jennifer Billock, Jenny Hong, Jenya Lee, Jeremy Fu, Jianfeng Chi, Jianyu Huang, Jiawen Liu, Jie Wang, Jiecao Yu, Joanna Bitton, Joe Spisak, Jongsoo Park, Joseph Rocca, Joshua Johnstun, Joshua Saxe, Ju-Qing Jia, Kalyan Va- suden Alwala, K. Upasani, Kate Plawiak, Keqian Li, Ken-591 neth Heafield, Kevin Stone, Khalid El-Arini, Krithika Iyer, Kshitiz Malik, Kuen- ley Chiu, Kunal Bhalla, Lauren Rantala-Yeary, Laurens van der Maaten, Lawrence Chen, Liang Tan, Liz Jenkins, Louis Martin, Lovish Madaan, Lubo Malo, Lukas Blecher, Lukas Landzaat, Luke de Oliveira, Madeline C. Muzzi, Ma- hesh Babu Pasupuleti, Mannat Singh, Manohar Paluri, Marcin Kardas, Mathew Oldham, Math- ieu Rita, Maya Pavlova, Melissa Hall Melanie Kambadur, Mike Lewis, Min Si, Mitesh Ku- mar Singh, Mona Hassan, Naman Goyal, Narjes Torabi, Nikolay Bashlykov, Nikolay Bogoychev, Niladri S. Chatterji, Olivier Duchenne, Onur cCelebi, Patrick Alrassy, Pengchuan Zhang, Pengwei Li, Petar Vasic, Peter Weng, Pra- jjwal Bhargava, Pratik Dubal, Praveen Krish- nan, Punit Singh Koura, Puxin Xu, Qing He, Qingxiao Dong, Ragavan Srinivasan, Raj Gana- pathy, Ramon Calderer, Ricardo Silveira Cabral, Robert Stojnic, Roberta Raileanu, Rohit Gird- har, Rohit Patel, Romain Sauvestre, Ronnie Polidoro, Roshan Sumbaly, Ross Taylor, Ruan Silva, Rui Hou, Rui",
      "chunk_index": 23
    },
    {
      "index": 433,
      "chunk_id": "QASemConsistency2024_chunk_24",
      "source_id": "QASemConsistency2024",
      "text": "Singh Koura, Puxin Xu, Qing He, Qingxiao Dong, Ragavan Srinivasan, Raj Gana- pathy, Ramon Calderer, Ricardo Silveira Cabral, Robert Stojnic, Roberta Raileanu, Rohit Gird- har, Rohit Patel, Romain Sauvestre, Ronnie Polidoro, Roshan Sumbaly, Ross Taylor, Ruan Silva, Rui Hou, Rui Wang, Saghar Hosseini, Sahana Chennabasappa, Sanjay Singh, Sean Bell, Seohyun Sonia Kim, Sergey Edunov, Shao- liang Nie, Sharan Narang, Sharath Chandra Raparthy, Sheng Shen, Shengye Wan, Shruti Bhosale, Shun Zhang, Simon Vandenhende, Soumya Batra, Spencer Whitman, Sten Sootla, Stephane Collot, Suchin Gururangan, Sydney Borodinsky, Tamar Herman, Tara Fowler, Tarek Sheasha, Thomas Georgiou, Thomas Scialom, Tobias Speckbacher, Todor Mihaylov, Tong Xiao, Ujjwal Karn, Vedanuj Goswami, Vibhor Gupta, Vignesh Ramanathan, Viktor Kerkez, Vincent Gonguet, Virginie Do, Vish V ogeti, Vladan Petrovic, Weiwei Chu, Wenhan Xiong, Wenyin Fu, Whitney Meers, Xavier Martinet, Xiaodong Wang, Xiaoqing Ellen Tan, Xinfeng Xie, Xuchao Jia, Xuewei Wang, Yaelle Gold- schlag, Yashesh Gaur, Yasmine Babaei, Yiqian Wen, Yiwen Song, Yuchen Zhang, Yue Li, Yun- ing Mao, Zacharie Delpierre Coudert, Zhengxu Yan, Zhengxing Chen, Zoe Papakipos, Aa- ditya K. Singh, Aaron Grattafiori, Abha Jain, Adam Kelsey, Adam Shajnfeld, Adi Gangidi, Adolfo Victoria, Ahuva Goldstand, Ajay Menon, Ajay Sharma, Alex Boesenberg, Alex Vaughan, Alexei Baevski, Allie Feinstein, Amanda Kallet, Amit Sangani, Anam Yunus, Andrei Lupu, An- dres Alvarado, Andrew Caples, Andrew Gu, Andrew Ho, Andrew Poulton, Andrew Ryan, Ankit Ramchandani, Annie Franco, Aparajita Saraf, Arkabandhu Chowdhury, Ashley Gabriel, Ashwin Bharambe, Assaf Eisenman, Azadeh Yazdan, Beau James, Ben Maurer, Ben Leon- hardi, Bernie Huang, Beth Loyd, Beto De Paola, Bhargavi Paranjape, Bing Liu, Bo Wu, Boyu Ni, Braden Hancock, Bram Wasti, Brandon Spence, Brani Stojkovic, Brian Gamido, Britt Montalvo, Carl Parker, Carly Burton, Catalina Mejia, Changhan Wang, Changkyu Kim, Chao Zhou, Chester Hu, Ching-Hsiang Chu, Chris Cai, Chris Tindal, Christoph Feichtenhofer, Da- mon Civin, Dana Beaty, Daniel Kreymer, Shang- Wen Li, Danny Wyatt, David Adkins, David Xu, Davide Testuggine, Delia David, Devi Parikh, Diana Liskovich, Didem Foss, Dingkang Wang, Duc Le, Dustin Holland, Edward Dowling, Eissa Jamil, Elaine Montgomery, Eleonora Presani, Emily Hahn, Emily Wood, Erik Brinkman, Es- teban Arcaute, Evan Dunbar, Evan Smothers, Fei Sun, Felix Kreuk, Feng Tian, Firat Oz- genel, Francesco Caggioni, Francisco Guzm'an, Frank J. Kanayet, Frank Seide, Gabriela Med- ina Florez, Gabriella Schwarz, Gada Badeer, Georgia Swee, Gil Halpern, Govind Thattai, Grant Herman, Grigory G. Sizov, Guangyi Zhang, Guna Lakshminarayanan, Hamid Sho- janazeri, Han Zou, Hannah Wang, Han Zha, Haroun Habeeb, Harrison Rudolph, Helen Suk, Henry",
      "chunk_index": 24
    },
    {
      "index": 434,
      "chunk_id": "QASemConsistency2024_chunk_25",
      "source_id": "QASemConsistency2024",
      "text": "Seide, Gabriela Med- ina Florez, Gabriella Schwarz, Gada Badeer, Georgia Swee, Gil Halpern, Govind Thattai, Grant Herman, Grigory G. Sizov, Guangyi Zhang, Guna Lakshminarayanan, Hamid Sho- janazeri, Han Zou, Hannah Wang, Han Zha, Haroun Habeeb, Harrison Rudolph, Helen Suk, Henry Aspegren, Hunter Goldman, Igor Moly- bog, Igor Tufanov, Irina-Elena Veliche, Itai Gat, Jake Weissman, James Geboski, James Kohli, Japhet Asher, Jean-Baptiste Gaya, Jeff Marcus, Jeff Tang, Jennifer Chan, Jenny Zhen, Jeremy Reizenstein, Jeremy Teboul, Jessica Zhong, Jian Jin, Jingyi Yang, Joe Cummings, Jon Carvill, Jon Shepard, Jonathan McPhie, Jonathan Torres, Josh Ginsburg, Junjie Wang, Kaixing(Kai) Wu, U KamHou, Karan Saxena, Karthik Prasad, Kartikay Khandelwal, Katay- oun Zand, Kathy Matosich, Kaushik Veeraragha- van, Kelly Michelena, Keqian Li, Kun Huang, Kunal Chawla, Kushal Lakhotia, Kyle Huang, Lailin Chen, Lakshya Garg, A Lavender, Lean- dro Silva, Lee Bell, Lei Zhang, Liangpeng Guo, Licheng Yu, Liron Moshkovich, Luca Wehrst- edt, Madian Khabsa, Manav Avalani, Manish Bhatt, Maria Tsimpoukelli, Martynas Mankus, Matan Hasson, Matthew Lennie, Matthias Reso, Maxim Groshev, Maxim Naumov, Maya Lathi, Meghan Keneally, Michael L. Seltzer, Michal Valko, Michelle Restrepo, Mihir Patel, Mik Vyatskov, Mikayel Samvelyan, Mike Clark, Mike Macey, Mike Wang, Miquel Jubert Her- moso, Mo Metanat, Mohammad Rastegari, Mu- nish Bansal, Nandhini Santhanam, Natascha Parks, Natasha White, Navyata Bawa, Nayan Singhal, Nick Egebo, Nicolas Usunier, Niko- lay Pavlovich Laptev, Ning Dong, Ning Zhang, Norman Cheng, Oleg Chernoguz, Olivia Hart, Omkar Salpekar, Ozlem Kalinli, Parkin Kent, Parth Parekh, Paul Saab, Pavan Balaji, Pedro Rittner, Philip Bontrager, Pierre Roux, Piotr Dol- lár, Polina Zvyagina, Prashant Ratanchandani, Pritish Yuvraj, Qian Liang, Rachad Alao, Rachel Rodriguez, Rafi Ayub, Raghotham Murthy, Raghu Nayani, Rahul Mitra, Raymond Li, Re- bekkah Hogan, Robin Battey, Rocky Wang, Rohan Maheswari, Russ Howes, Ruty Rinott, Sai Jayesh Bondu, Samyak Datta, Sara Chugh, Sara Hunt, Sargun Dhillon, Sasha Sidorov, Sa- tadru Pan, Saurabh Verma, Seiji Yamamoto, Sharadh Ramaswamy, Shaun Lindsay, Sheng Feng, Shenghao Lin, Shengxin Cindy Zha, Shiva Shankar, Shuqiang Zhang, Sinong Wang, Sneha Agarwal, Soji Sajuyigbe, Soumith Chin- tala, Stephanie Max, Stephen Chen, Steve Ke- hoe, Steve Satterfield, Sudarshan Govindaprasad, Sumit Gupta, Sung-Bae Cho, Sunny Virk, Suraj Subramanian, Sy Choudhury, Sydney Goldman, Tal Remez, Tamar Glaser, Tamara Best, Thilo Kohler, Thomas Robinson, Tianhe Li, Tianjun Zhang, Tim Matthews, Timothy Chou, Tzook Shaked, Varun V ontimitta, Victoria Ajayi, Vic- toria Montanez, Vijai Mohan, Vinay Satish Ku- mar, Vishal Mangla, Vlad Ionescu, Vlad An- drei Poenaru, Vlad T. Mihailescu, Vladimir Ivanov, Wei Li,",
      "chunk_index": 25
    },
    {
      "index": 435,
      "chunk_id": "QASemConsistency2024_chunk_26",
      "source_id": "QASemConsistency2024",
      "text": "Robinson, Tianhe Li, Tianjun Zhang, Tim Matthews, Timothy Chou, Tzook Shaked, Varun V ontimitta, Victoria Ajayi, Vic- toria Montanez, Vijai Mohan, Vinay Satish Ku- mar, Vishal Mangla, Vlad Ionescu, Vlad An- drei Poenaru, Vlad T. Mihailescu, Vladimir Ivanov, Wei Li, Wenchen Wang, Wenwen Jiang, Wes Bouaziz, Will Constable, Xia Tang, Xi- aofang Wang, Xiaojian Wu, Xiaolan Wang, Xide Xia, Xilun Wu, Xinbo Gao, Yanjun Chen, Ye Hu, Ye Jia, Ye Qi, Yenda Li, Yilin Zhang, Ying Zhang, Yossi Adi, Youngjin Nam, Yu Wang, Yuchen Hao, Yundi Qian, Yuzi He, Zach Rait, Zachary DeVito, Zef Rosnbrick, Zhaoduo Wen, Zhenyu Yang, and Zhiwei Zhao. 2024. The Llama 3 Herd of Models.ArXiv, abs/2407.21783. Esin Durmus, He He, and Mona Diab. 2020. FEQA: A question answering evaluation frame- work for faithfulness assessment in abstractive summarization. InProceedings of the 58th An- nual Meeting of the Association for Computa- tional Linguistics, pages 5055-5070, Online. As- sociation for Computational Linguistics. Alexander Fabbri, Chien-Sheng Wu, Wenhao Liu, and Caiming Xiong. 2022. QAFactEval: Im- proved QA-based factual consistency evalua- tion for summarization. InProceedings of the 2022 Conference of the North American Chap- ter of the Association for Computational Lin- guistics: Human Language Technologies, pages 2587-2601, Seattle, United States. Association for Computational Linguistics. Alexander R. Fabbri, Wojciech Kry´sci´nski, Bryan McCann, Caiming Xiong, Richard Socher, and Dragomir Radev. 2021. Summeval: Re- evaluating summarization evaluation.Transac- tions of the Association for Computational Lin- guistics, 9:391-409. Tobias Falke, Leonardo F. R. Ribeiro, Prasetya Ajie Utama, Ido Dagan, and Iryna Gurevych. 2019. Ranking generated summaries by correctness: An interesting but challenging application for natural language inference. InProceedings of the 57th Annual Meeting of the Association for Computational Linguistics, pages 2214-2220, Florence, Italy. Association for Computational Linguistics. Nicholas FitzGerald, Julian Michael, Luheng He, and Luke Zettlemoyer. 2018. Large-scale QA- SRL parsing. InProceedings of the 56th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pages 2051-2060, Melbourne, Australia. Association for Computational Linguistics. Luyu Gao, Zhuyun Dai, Panupong Pasupat, An- thony Chen, Arun Tejasvi Chaganty, Yicheng Fan, Vincent Zhao, Ni Lao, Hongrae Lee, Da- Cheng Juan, and Kelvin Guu. 2023a. RARR: Researching and revising what language mod- els say, using language models. InProceedings of the 61st Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pages 16477-16508, Toronto, Canada. Association for Computational Linguistics. Tianyu Gao, Howard Yen, Jiatong Yu, and Danqi Chen. 2023b. Enabling large language models to generate text with",
      "chunk_index": 26
    },
    {
      "index": 436,
      "chunk_id": "QASemConsistency2024_chunk_27",
      "source_id": "QASemConsistency2024",
      "text": "the 61st Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pages 16477-16508, Toronto, Canada. Association for Computational Linguistics. Tianyu Gao, Howard Yen, Jiatong Yu, and Danqi Chen. 2023b. Enabling large language models to generate text with citations. InProceedings of the 2023 Conference on Empirical Methods in Natural Language Processing, pages 6465- 6488, Singapore. Association for Computational Linguistics. Zorik Gekhman, Jonathan Herzig, Roee Aha- roni, Chen Elkind, and Idan Szpektor. 2023. TrueTeacher: Learning factual consistency eval- uation with large language models. InPro- ceedings of the 2023 Conference on Empirical Methods in Natural Language Processing, pages 2053-2070, Singapore. Association for Compu- tational Linguistics. Tanya Goyal and Greg Durrett. 2020. Evaluating factuality in generation with dependency-level entailment. InFindings of the Association for Computational Linguistics: EMNLP 2020, pages 3592-3603, Online. Association for Computa- tional Linguistics. Luheng He, Mike Lewis, and Luke Zettlemoyer. 2015. Question-answer driven semantic role la- beling: Using natural language to annotate natu- ral language. InProceedings of the 2015 Confer- ence on Empirical Methods in Natural Language Processing, pages 643-653, Lisbon, Portugal. Association for Computational Linguistics. James Higginbotham. 1983. The logic of per- ceptual reports: An extensional alternative to situation semantics.Journal of Philosophy, 80(February):100-127. Or Honovich, Roee Aharoni, Jonathan Herzig, Ha- gai Taitelbaum, Doron Kukliansy, Vered Co- hen, Thomas Scialom, Idan Szpektor, Avinatan Hassidim, and Yossi Matias. 2022. TRUE: Re- evaluating factual consistency evaluation. In Proceedings of the Second DialDoc Workshop on Document-grounded Dialogue and Conver- sational Question Answering, pages 161-175, Dublin, Ireland. Association for Computational Linguistics. Or Honovich, Leshem Choshen, Roee Aharoni, Ella Neeman, Idan Szpektor, and Omri Abend. 2021. q2: Evaluating factual consistency in knowledge-grounded dialogues via question gen- eration and question answering. InProceedings of the 2021 Conference on Empirical Methods in Natural Language Processing, pages 7856-7870, Online and Punta Cana, Dominican Republic. Association for Computational Linguistics. Lei Huang, Weijiang Yu, Weitao Ma, Weihong Zhong, Zhangyin Feng, Haotian Wang, Qiang- long Chen, Weihua Peng, Xiaocheng Feng, Bing Qin, and Ting Liu. 2023. A Survey on Hallucina- tion in Large Language Models: Principles, Tax- onomy, Challenges, and Open Questions.ArXiv, abs/2311.05232. Ryo Kamoi, Tanya Goyal, Juan Diego Rodriguez, and Greg Durrett. 2023a. WiCE: Real-world en- tailment for claims in Wikipedia. InProceedings of the 2023 Conference on Empirical Methods in Natural Language Processing, pages 7561- 7583, Singapore. Association for Computational Linguistics. Ryo Kamoi, Tanya Goyal, and Greg Durrett. 2023b. Shortcomings of question answering based fac- tuality frameworks for error",
      "chunk_index": 27
    },
    {
      "index": 437,
      "chunk_id": "QASemConsistency2024_chunk_28",
      "source_id": "QASemConsistency2024",
      "text": "in Wikipedia. InProceedings of the 2023 Conference on Empirical Methods in Natural Language Processing, pages 7561- 7583, Singapore. Association for Computational Linguistics. Ryo Kamoi, Tanya Goyal, and Greg Durrett. 2023b. Shortcomings of question answering based fac- tuality frameworks for error localization. InPro- ceedings of the 17th Conference of the European Chapter of the Association for Computational Linguistics, pages 132-146, Dubrovnik, Croatia. Association for Computational Linguistics. Tushar Khot, Ashish Sabharwal, and Peter Clark. 2018. Scitail: A textual entailment dataset from science question answering. InAAAI Confer- ence on Artificial Intelligence. Ayal Klein, Eran Hirsch, Ron Eliav, Valentina Pyatkin, Avi Caciularu, and Ido Dagan. 2022. QASem parsing: Text-to-text modeling of QA- based semantics. InProceedings of the 2022 Conference on Empirical Methods in Natural Language Processing, pages 7742-7756, Abu Dhabi, United Arab Emirates. Association for Computational Linguistics. Ayal Klein, Jonathan Mamou, Valentina Py- atkin, Daniela Stepanov, Hangfeng He, Dan Roth, Luke Zettlemoyer, and Ido Dagan. 2020. QANom: Question-answer driven SRL for nom- inalizations. InProceedings of the 28th Interna- tional Conference on Computational Linguistics, pages 3069-3083, Barcelona, Spain (Online). International Committee on Computational Lin- guistics. Kalpesh Krishna, Erin Bransom, Bailey Kuehl, Mo- hit Iyyer, Pradeep Dasigi, Arman Cohan, and Kyle Lo. 2023. LongEval: Guidelines for human evaluation of faithfulness in long-form summa- rization. InProceedings of the 17th Conference of the European Chapter of the Association for Computational Linguistics, pages 1650-1669, Dubrovnik, Croatia. Association for Computa- tional Linguistics. Wojciech Kryscinski, Bryan McCann, Caiming Xiong, and Richard Socher. 2020. Evaluating the factual consistency of abstractive text sum- marization. InProceedings of the 2020 Confer- ence on Empirical Methods in Natural Language Processing (EMNLP), pages 9332-9346, Online. Association for Computational Linguistics. Philippe Laban, Wojciech Kryscinski, Divyansh Agarwal, Alexander Fabbri, Caiming Xiong, Shafiq Joty, and Chien-Sheng Wu. 2023. SummEdits: Measuring LLM ability at factual reasoning through the lens of summarization. In Proceedings of the 2023 Conference on Empir- ical Methods in Natural Language Processing, pages 9662-9676, Singapore. Association for Computational Linguistics. Philippe Laban, Tobias Schnabel, Paul N. Ben- nett, and Marti A. Hearst. 2022. SummaC: Re- visiting NLI-based models for inconsistency de- tection in summarization.Transactions of the As- sociation for Computational Linguistics, 10:163- 177. Mike Lewis, Yinhan Liu, Naman Goyal, Marjan Ghazvininejad, Abdelrahman Mohamed, Omer Levy, Veselin Stoyanov, and Luke Zettlemoyer. 2020. BART: Denoising sequence-to-sequence pre-training for natural language generation, translation, and comprehension. InProceedings of the 58th Annual Meeting of the Association for Computational Linguistics, pages 7871-7880, Online. Association for",
      "chunk_index": 28
    },
    {
      "index": 438,
      "chunk_id": "QASemConsistency2024_chunk_29",
      "source_id": "QASemConsistency2024",
      "text": "Marjan Ghazvininejad, Abdelrahman Mohamed, Omer Levy, Veselin Stoyanov, and Luke Zettlemoyer. 2020. BART: Denoising sequence-to-sequence pre-training for natural language generation, translation, and comprehension. InProceedings of the 58th Annual Meeting of the Association for Computational Linguistics, pages 7871-7880, Online. Association for Computational Linguis- tics. Nelson Liu, Tianyi Zhang, and Percy Liang. 2023a. Evaluating verifiability in generative search en- gines. InFindings of the Association for Compu- tational Linguistics: EMNLP 2023, pages 7001- 7025, Singapore. Association for Computational Linguistics. Yixin Liu, Budhaditya Deb, Milagro Teruel, Aaron Halfaker, Dragomir Radev, and Ahmed Hassan Awadallah. 2023b. On improving summariza- tion factual consistency from natural language feedback. InProceedings of the 61st Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pages 15144-15161, Toronto, Canada. Association for Computational Linguistics. Yixin Liu, Alex Fabbri, Pengfei Liu, Yilun Zhao, Linyong Nan, Ruilin Han, Simeng Han, Shafiq Joty, Chien-Sheng Wu, Caiming Xiong, and Dragomir Radev. 2023c. Revisiting the gold standard: Grounding summarization evaluation with robust human evaluation. InProceedings of the 61st Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pages 4140-4170, Toronto, Canada. As- sociation for Computational Linguistics. Joshua Maynez, Shashi Narayan, Bernd Bohnet, and Ryan McDonald. 2020. On faithfulness and factuality in abstractive summarization. InPro- ceedings of the 58th Annual Meeting of the As- sociation for Computational Linguistics, pages 1906-1919, Online. Association for Computa- tional Linguistics. Sewon Min, Kalpesh Krishna, Xinxi Lyu, Mike Lewis, Wen-tau Yih, Pang Koh, Mohit Iyyer, Luke Zettlemoyer, and Hannaneh Hajishirzi. 2023. FActScore: Fine-grained atomic evalu- ation of factual precision in long form text gen- eration. InProceedings of the 2023 Conference on Empirical Methods in Natural Language Pro- cessing, pages 12076-12100, Singapore. Associ- ation for Computational Linguistics. Abhika Mishra, Akari Asai, Vidhisha Balachan- dran, Yizhong Wang, Graham Neubig, Yulia Tsvetkov, and Hannaneh Hajishirzi. 2024. Fine- grained hallucination detection and editing for language models. InFirst Conference on Lan- guage Modeling. Feng Nan, Cicero Nogueira dos Santos, Henghui Zhu, Patrick Ng, Kathleen McKeown, Ramesh Nallapati, Dejiao Zhang, Zhiguo Wang, An- drew O. Arnold, and Bing Xiang. 2021. Im- proving factual consistency of abstractive sum- marization via question answering. InProceed- ings of the 59th Annual Meeting of the Asso- ciation for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing (Volume 1: Long Papers), pages 6881-6894, Online. Association for Com- putational Linguistics. Shashi Narayan, Shay B. Cohen, and Mirella La- pata. 2018. Don't give me the details, just",
      "chunk_index": 29
    },
    {
      "index": 439,
      "chunk_id": "QASemConsistency2024_chunk_30",
      "source_id": "QASemConsistency2024",
      "text": "Computational Linguistics and the 11th International Joint Conference on Natural Language Processing (Volume 1: Long Papers), pages 6881-6894, Online. Association for Com- putational Linguistics. Shashi Narayan, Shay B. Cohen, and Mirella La- pata. 2018. Don't give me the details, just the summary! topic-aware convolutional neural networks for extreme summarization. InPro- ceedings of the 2018 Conference on Empirical Methods in Natural Language Processing, pages 1797-1807, Brussels, Belgium. Association for Computational Linguistics. Ani Nenkova and Rebecca Passonneau. 2004. Eval- uating content selection in summarization: The pyramid method. InProceedings of the Human Language Technology Conference of the North American Chapter of the Association for Compu- tational Linguistics: HLT-NAACL 2004, pages 145-152, Boston, Massachusetts, USA. Associa- tion for Computational Linguistics. Stephan Oepen, Marco Kuhlmann, Yusuke Miyao, Daniel Zeman, Dan Flickinger, Jan Haji ˇc, An- gelina Ivanova, and Yi Zhang. 2014. SemEval 2014 task 8: Broad-coverage semantic depen- dency parsing. InProceedings of the 8th In- ternational Workshop on Semantic Evaluation (SemEval 2014), pages 63-72, Dublin, Ireland. Association for Computational Linguistics. Artidoro Pagnoni, Vidhisha Balachandran, and Yu- lia Tsvetkov. 2021. Understanding factuality in abstractive summarization with FRANK: A benchmark for factuality metrics. InProceed- ings of the 2021 Conference of the North Amer- ican Chapter of the Association for Computa- tional Linguistics: Human Language Technolo- gies, pages 4812-4829, Online. Association for Computational Linguistics. Martha Palmer, Daniel Gildea, and Paul Kingsbury. 2005. The Proposition Bank: An annotated cor- pus of semantic roles.Computational Linguis- tics, 31(1):71-106. Terence Parsons. 1990.Events in the Semantics of English: A Study in Subatomic Semantics. MIT Press. Leon Pesahov, Ayal Klein, and Ido Dagan. 2023. QA-adj: Adding adjectives to QA-based seman- tics. InProceedings of the Fourth International Workshop on Designing Meaning Representa- tions, pages 74-88, Nancy, France. Association for Computational Linguistics. Valentina Pyatkin, Ayal Klein, Reut Tsarfaty, and Ido Dagan. 2020. QADiscourse - Discourse Relations as QA Pairs: Representation, Crowd- sourcing and Baselines. InProceedings of the 2020 Conference on Empirical Methods in Natu- ral Language Processing (EMNLP), pages 2804- 2819, Online. Association for Computational Linguistics. Colin Raffel, Noam Shazeer, Adam Roberts, Katherine Lee, Sharan Narang, Michael Matena, Yanqi Zhou, Wei Li, and Peter J Liu. 2020. Ex- ploring the limits of transfer learning with a uni- fied text-to-text transformer.Journal of machine learning research, 21(140):1-67. Hannah Rashkin, Vitaly Nikolaev, Matthew Lamm, Lora Aroyo, Michael Collins, Dipanjan Das, Slav Petrov, Gaurav Singh Tomar, Iulia Turc, and David Reitter. 2023. Measuring attribution in natural language generation models.Compu- tational",
      "chunk_index": 30
    },
    {
      "index": 440,
      "chunk_id": "QASemConsistency2024_chunk_31",
      "source_id": "QASemConsistency2024",
      "text": "uni- fied text-to-text transformer.Journal of machine learning research, 21(140):1-67. Hannah Rashkin, Vitaly Nikolaev, Matthew Lamm, Lora Aroyo, Michael Collins, Dipanjan Das, Slav Petrov, Gaurav Singh Tomar, Iulia Turc, and David Reitter. 2023. Measuring attribution in natural language generation models.Compu- tational Linguistics, 49(4):777-840. Leonardo F. R. Ribeiro, Mengwen Liu, Iryna Gurevych, Markus Dreyer, and Mohit Bansal. 2022. FactGraph: Evaluating factuality in sum- marization with semantic graph representations. InProceedings of the 2022 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, pages 3238-3253, Seattle, United States. Association for Computational Linguis- tics. Gemma Team Morgane Riviere, Shreya Pathak, Pier Giuseppe Sessa, Cassidy Hardin, Surya Bhupatiraju, L'eonard Hussenot, Thomas Mes- nard, Bobak Shahriari, Alexandre Ram'e, Johan Ferret, Peter Liu, Pouya Dehghani Tafti, Abe Friesen, Michelle Casbon, Sabela Ramos, Ravin Kumar, Charline Le Lan, Sammy Jerome, An- ton Tsitsulin, Nino Vieillard, Piotr Sta ´nczyk, Sertan Girgin, Nikola Momchev, Matt Hoff- man, Shantanu Thakoor, Jean-Bastien Grill, Behnam Neyshabur, Alanna Walton, Aliaksei Severyn, Alicia Parrish, Aliya Ahmad, Allen Hutchison, Alvin Abdagic, Amanda Carl, Amy Shen, Andy Brock, Andy Coenen, Anthony Laforge, Antonia Paterson, Ben Bastian, Bi- lal Piot, Boxi Wu, Brandon Royal, Charlie Chen, Chintu Kumar, Chris Perry, Christoper A. Welty, Christopher A. Choquette-Choo, Danila Sinopalnikov, David Weinberger, Dimple Vi- jaykumar, Dominika Rogozi'nska, D. Herbi- son, Elisa Bandy, Emma Wang, Eric Noland, Erica Moreira, Evan Senter, Evgenii Eltyshev, Francesco Visin, Gabriel Rasskin, Gary Wei, Glenn Cameron, Gus Martins, Hadi Hashemi, Hanna Klimczak-Pluci'nska, Harleen Batra, Harsh Dhand, Ivan Nardini, Jacinda Mein, Jack Zhou, James Svensson, Jeff Stanway, Jetha Chan, Jin Zhou, Joana Carrasqueira, Joana Il- jazi, Jocelyn Becker, Joe Fernandez, Joost R. van Amersfoort, Josh Gordon, Josh Lipschultz, Joshua Newlan, Junsong Ji, Kareem Mohamed, Kartikeya Badola, Kat Black, Katie Millican, Keelin McDonell, Kelvin Nguyen, Kiranbir Sod- hia, Kish Greene, Lars Lowe Sjoesund, Lau- ren Usui, L. Sifre, Lena Heuermann, Leticia Lago, Lilly McNealus, Livio Baldini Soares, Logan Kilpatrick, Lucas Dixon, Luciano Mar- tins, Machel Reid, Manvinder Singh, Mark Iverson, Martin Gorner, Mat Velloso, Mateo Wirth, Matt Davidow, Matt Miller, Matthew Rahtz, Matthew Watson, Meg Risdal, Mehran Kazemi, Michael Moynihan, Ming Zhang, Min- suk Kahng, Minwoo Park, Mofi Rahman, Mohit Khatwani, Natalie Dao, Nenshad Bardoliwalla, Nesh Devanathan, Neta Dumai, Nilay Chauhan, Oscar Wahltinez, Pankil Botarda, Parker Barnes, Paul Barham, Paul Michel, Pengchong Jin, Petko Georgiev, Phil Culliton, Pradeep Kuppala, Ramona Comanescu, Ramona Merhej, Reena Jana, Reza Rokni, Rishabh Agarwal, Ryan Mullins, Samaneh Saadat,",
      "chunk_index": 31
    },
    {
      "index": 441,
      "chunk_id": "QASemConsistency2024_chunk_32",
      "source_id": "QASemConsistency2024",
      "text": "Nenshad Bardoliwalla, Nesh Devanathan, Neta Dumai, Nilay Chauhan, Oscar Wahltinez, Pankil Botarda, Parker Barnes, Paul Barham, Paul Michel, Pengchong Jin, Petko Georgiev, Phil Culliton, Pradeep Kuppala, Ramona Comanescu, Ramona Merhej, Reena Jana, Reza Rokni, Rishabh Agarwal, Ryan Mullins, Samaneh Saadat, S. Mc Carthy, Sarah Perrin, S'ebastien Arnold, Sebastian Krause, Shengyang Dai, Shruti Garg, Shruti Sheth, Sue Ronstrom, Susan Chan, Timothy Jordan, Ting Yu, Tom Eccles, Tom Hennigan, Tomás Ko- ciský, Tulsee Doshi, Vihan Jain, Vikas Yadav, Vilobh Meshram, Vishal Dharmadhikari, War- ren Barkley, Wei Wei, Wenming Ye, Woohyun Han, Woosuk Kwon, Xiang Xu, Zhe Shen, Zhitao Gong, Zichuan Wei, Victor Cotruta, Phoebe Kirk, Anand Rao, Minh Giang, Lu- dovic Peran, Tris Brian Warkentin, Eli Collins, Joelle Barral, Zoubin Ghahramani, Raia Had- sell, D. Sculley, Jeanine Banks, Anca Dragan, Slav Petrov, Oriol Vinyals, Jeffrey Dean, Demis Hassabis, Koray Kavukcuoglu, Cl'ement Fara- bet, Elena Buchatskaya, Sebastian Borgeaud, Noah Fiedel, Armand Joulin, Kathleen Ke- nealy, Robert Dadashi, and Alek Andreev. 2024. Gemma 2: Improving open language models at a practical size. Paul Roit, Johan Ferret, Lior Shani, Roee Aharoni, Geoffrey Cideron, Robert Dadashi, Matthieu Geist, Sertan Girgin, Leonard Hussenot, Orgad Keller, Nikola Momchev, Sabela Ramos Garea, Piotr Stanczyk, Nino Vieillard, Olivier Bachem, Gal Elidan, Avinatan Hassidim, Olivier Pietquin, and Idan Szpektor. 2023. Factually consistent summarization via reinforcement learning with textual entailment feedback. InProceedings of the 61st Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Pa- pers), pages 6252-6272, Toronto, Canada. Asso- ciation for Computational Linguistics. Paul Roit, Ayal Klein, Daniela Stepanov, Jonathan Mamou, Julian Michael, Gabriel Stanovsky, Luke Zettlemoyer, and Ido Dagan. 2020. Con- trolled crowdsourcing for high-quality QA-SRL annotation. InProceedings of the 58th Annual Meeting of the Association for Computational Linguistics, pages 7008-7013, Online. Associa- tion for Computational Linguistics. Paul Roit, Aviv Slobodkin, Eran Hirsch, Arie Cat- tan, Ayal Klein, Valentina Pyatkin, and Ido Da- gan. 2024. Explicating the implicit: Argument detection beyond sentence boundaries. InPro- ceedings of the 62nd Annual Meeting of the As- sociation for Computational Linguistics (Volume 1: Long Papers), pages 16394-16409, Bangkok, Thailand. Association for Computational Lin- guistics. Farhan Samir, Chan Young Park, Anjalie Field, Vered Shwartz, and Yulia Tsvetkov. 2024. Lo- cating information gaps and narrative inconsis- tencies across languages: A case study of LGBT people portrayals on Wikipedia. InProceedings of the 2024 Conference on Empirical Methods in Natural Language Processing, pages 6747-6762, Miami, Florida, USA. Association for Computa- tional Linguistics. Tal Schuster, Adam",
      "chunk_index": 32
    },
    {
      "index": 442,
      "chunk_id": "QASemConsistency2024_chunk_33",
      "source_id": "QASemConsistency2024",
      "text": "and narrative inconsis- tencies across languages: A case study of LGBT people portrayals on Wikipedia. InProceedings of the 2024 Conference on Empirical Methods in Natural Language Processing, pages 6747-6762, Miami, Florida, USA. Association for Computa- tional Linguistics. Tal Schuster, Adam Fisch, and Regina Barzilay. 2021. Get your vitamin C! robust fact verifi- cation with contrastive evidence. InProceed- ings of the 2021 Conference of the North Amer- ican Chapter of the Association for Computa- tional Linguistics: Human Language Technolo- gies, pages 624-643, Online. Association for Computational Linguistics. Thomas Scialom, Paul-Alexis Dray, Sylvain Lam- prier, Benjamin Piwowarski, Jacopo Staiano, Alex Wang, and Patrick Gallinari. 2021. QuestE- val: Summarization asks for fact-based eval- uation. InProceedings of the 2021 Confer- ence on Empirical Methods in Natural Lan- guage Processing, pages 6594-6604, Online and Punta Cana, Dominican Republic. Association for Computational Linguistics. Ori Shapira, David Gabay, Yang Gao, Hadar Ro- nen, Ramakanth Pasunuru, Mohit Bansal, Yael Amsterdamer, and Ido Dagan. 2019. Crowd- sourcing lightweight pyramids for manual sum- mary evaluation. InProceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long and Short Papers), pages 682-687, Minneapo- lis, Minnesota. Association for Computational Linguistics. Aviv Slobodkin, Eran Hirsch, Arie Cattan, Tal Schuster, and Ido Dagan. 2024. Attribute first, then generate: Locally-attributable grounded text generation. InProceedings of the 62nd Annual Meeting of the Association for Compu- tational Linguistics (Volume 1: Long Papers), pages 3309-3344, Bangkok, Thailand. Associa- tion for Computational Linguistics. Melanie Subbiah, Faisal Ladhak, Akankshya Mishra, Griffin Thomas Adams, Lydia Chilton, and Kathleen McKeown. 2024. STORYSUMM: Evaluating faithfulness in story summarization. InProceedings of the 2024 Conference on Em- pirical Methods in Natural Language Process- ing, pages 9988-10005, Miami, Florida, USA. Association for Computational Linguistics. Liyan Tang, Tanya Goyal, Alex Fabbri, Philippe Laban, Jiacheng Xu, Semih Yavuz, Wojciech Kryscinski, Justin Rousseau, and Greg Durrett. 2023. Understanding factual errors in summa- rization: Errors, summarizers, datasets, error de- tectors. InProceedings of the 61st Annual Meet- ing of the Association for Computational Lin- guistics (Volume 1: Long Papers), pages 11626- 11644, Toronto, Canada. Association for Com- putational Linguistics. Liyan Tang, Tanya Goyal, Alexander R. Fabbri, Philippe Laban, Jiacheng Xu, Semih Yahvuz, Wojciech Kryscinski, Justin F. Rousseau, and Greg Durrett. 2022. Understanding factual er- rors in summarization: Errors, summarizers, datasets, error detectors. InAnnual Meeting of the Association for Computational Linguistics. Liyan Tang, Philippe Laban, and Greg Durrett. 2024. MiniCheck: Efficient",
      "chunk_index": 33
    },
    {
      "index": 443,
      "chunk_id": "QASemConsistency2024_chunk_34",
      "source_id": "QASemConsistency2024",
      "text": "Semih Yahvuz, Wojciech Kryscinski, Justin F. Rousseau, and Greg Durrett. 2022. Understanding factual er- rors in summarization: Errors, summarizers, datasets, error detectors. InAnnual Meeting of the Association for Computational Linguistics. Liyan Tang, Philippe Laban, and Greg Durrett. 2024. MiniCheck: Efficient fact-checking of LLMs on grounding documents. InProceedings of the 2024 Conference on Empirical Methods in Natural Language Processing, pages 8818-8847, Miami, Florida, USA. Association for Computa- tional Linguistics. James Thorne, Andreas Vlachos, Christos Christodoulopoulos, and Arpit Mittal. 2018. FEVER: a large-scale dataset for fact extrac- tion and VERification. InProceedings of the 2018 Conference of the North American Chapter of the Association for Computational Linguis- tics: Human Language Technologies, Volume 1 (Long Papers), pages 809-819, New Orleans, Louisiana. Association for Computational Lin- guistics. Louis L Thurstone. 1927. A law of comparative judgment. InScaling, pages 81-92. Routledge. Prasetya Utama, Joshua Bambrick, Nafise Moosavi, and Iryna Gurevych. 2022. Falsesum: Gener- ating document-level NLI examples for recog- nizing factual inconsistency in summarization. InProceedings of the 2022 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, pages 2763-2776, Seattle, United States. Association for Computational Linguis- tics. Manya Wadhwa, Xinyu Zhao, Junyi Jessy Li, and Greg Durrett. 2024. Learning to refine with fine- grained natural language feedback. InFindings of the Association for Computational Linguis- tics: EMNLP 2024, pages 12281-12308, Miami, Florida, USA. Association for Computational Linguistics. David Wan and Mohit Bansal. 2022. FactPE- GASUS: Factuality-aware pre-training and fine- tuning for abstractive summarization. InPro- ceedings of the 2022 Conference of the North American Chapter of the Association for Com- putational Linguistics: Human Language Tech- nologies, pages 1010-1028, Seattle, United States. Association for Computational Linguis- tics. David Wan, Mengwen Liu, Kathleen McKe- own, Markus Dreyer, and Mohit Bansal. 2023. Faithfulness-aware decoding strategies for ab- stractive summarization. InProceedings of the 17th Conference of the European Chapter of the Association for Computational Linguistics, pages 2864-2880, Dubrovnik, Croatia. Associa- tion for Computational Linguistics. David Wan, Koustuv Sinha, Srini Iyer, Asli Celiky- ilmaz, Mohit Bansal, and Ramakanth Pasunuru. 2024. ACUEval: Fine-grained hallucination evaluation and correction for abstractive sum- marization. InFindings of the Association for Computational Linguistics: ACL 2024, pages 10036-10056, Bangkok, Thailand. Association for Computational Linguistics. Alex Wang, Kyunghyun Cho, and Mike Lewis. 2020. Asking and answering questions to evalu- ate the factual consistency of summaries. InPro- ceedings of the 58th Annual Meeting of the As- sociation for Computational Linguistics, pages 5008-5020, Online. Association for Computa-",
      "chunk_index": 34
    },
    {
      "index": 444,
      "chunk_id": "QASemConsistency2024_chunk_35",
      "source_id": "QASemConsistency2024",
      "text": "Alex Wang, Kyunghyun Cho, and Mike Lewis. 2020. Asking and answering questions to evalu- ate the factual consistency of summaries. InPro- ceedings of the 58th Annual Meeting of the As- sociation for Computational Linguistics, pages 5008-5020, Online. Association for Computa- tional Linguistics. Miriam Wanner, Seth Ebner, Zhengping Jiang, Mark Dredze, and Benjamin Van Durme. 2024. A closer look at claim decomposition. InPro- ceedings of the 13th Joint Conference on Lexi- cal and Computational Semantics (*SEM 2024), pages 153-175, Mexico City, Mexico. Associa- tion for Computational Linguistics. Jerry Wei, Chengrun Yang, Xinying Song, Yifeng Lu, Nathan Zixia Hu, Jie Huang, Dustin Tran, Daiyi Peng, Ruibo Liu, Da Huang, Cosmo Du, and Quoc V Le. 2024. Long-form factuality in large language models. InThe Thirty-eighth Annual Conference on Neural Information Pro- cessing Systems. Adina Williams, Nikita Nangia, and Samuel Bow- man. 2018. A broad-coverage challenge corpus for sentence understanding through inference. InProceedings of the 2018 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long Papers), pages 1112-1122, New Orleans, Louisiana. Associa- tion for Computational Linguistics. Wen Xiao and Giuseppe Carenini. 2023. Entity- based SpanCopy for abstractive summarization to improve the factual consistency. InProceed- ings of the 4th Workshop on Computational Ap- proaches to Discourse (CODI 2023), pages 70- 81, Toronto, Canada. Association for Computa- tional Linguistics. Wenpeng Yin, Dragomir Radev, and Caiming Xiong. 2021. DocNLI: A large-scale dataset for document-level natural language inference. In Findings of the Association for Computational Linguistics: ACL-IJCNLP 2021, pages 4913- 4922, Online. Association for Computational Linguistics. Xiang Yue, Boshi Wang, Ziru Chen, Kai Zhang, Yu Su, and Huan Sun. 2023. Automatic evalua- tion of attribution by large language models. In Findings of the Association for Computational Linguistics: EMNLP 2023, pages 4615-4635, Singapore. Association for Computational Lin- guistics. Jingqing Zhang, Yao Zhao, Mohammad Saleh, and Peter Liu. 2020. PEGASUS: Pre-training with extracted gap-sentences for abstractive summa- rization. InProceedings of the 37th Interna- tional Conference on Machine Learning, vol- ume 119 ofProceedings of Machine Learning Research, pages 11328-11339. PMLR. Tianyi Zhang*, Varsha Kishore*, Felix Wu*, Kilian Q. Weinberger, and Yoav Artzi. 2020. Bertscore: Evaluating text generation with bert. InInternational Conference on Learning Repre- sentations. Yuan Zhang, Jason Baldridge, and Luheng He. 2019. PAWS: Paraphrase adversaries from word scrambling. InProceedings of the 2019 Con- ference of the North American Chapter of the Association for Computational Linguistics: Hu- man Language Technologies, Volume 1",
      "chunk_index": 35
    },
    {
      "index": 445,
      "chunk_id": "QASemConsistency2024_chunk_36",
      "source_id": "QASemConsistency2024",
      "text": "Learning Repre- sentations. Yuan Zhang, Jason Baldridge, and Luheng He. 2019. PAWS: Paraphrase adversaries from word scrambling. InProceedings of the 2019 Con- ference of the North American Chapter of the Association for Computational Linguistics: Hu- man Language Technologies, Volume 1 (Long and Short Papers), pages 1298-1308, Minneapo- lis, Minnesota. Association for Computational Linguistics. Lianmin Zheng, Wei-Lin Chiang, Ying Sheng, Siyuan Zhuang, Zhanghao Wu, Yonghao Zhuang, Zi Lin, Zhuohan Li, Dacheng Li, Eric Xing, Hao Zhang, Joseph E. Gonzalez, and Ion Stoica. 2023. Judging LLM-as-a-judge with MT- bench and chatbot arena. InThirty-seventh Con- ference on Neural Information Processing Sys- tems Datasets and Benchmarks Track. A Dataset Collection Filtering nonsensical QAsAlthough our QASem parser significantly outperforms the current SOTA model (Klein et al., 2022) by a large margin (see §3), it is still prone to errors that can introduce noise into the consistency evaluation. Specifically, for nominal predicates, the parser may generate QAs that are nonsensical or convey a different meaning from the intended predicate. For instance, in the sentence\"Sweden's former foreign minister Johan Gustafssoon, who was kidnapped by Islamist militants in Mali in 2013, has been released after more than two years in captivity\" (from CLIFF) the parser incorrectly generates 'Where did someonemilitantise?\" for the predicate \"militants\" and 'Who wascaptained?\" for the predicate \"captivity\". In some rare cases, the parser hallucinates predicate argument relation. For instance, in the sentence\"A man has been arrested on suspicion of murder after a man was found dead at a house in Cambridgeshire\", the parser generates the question\"Where was someone arrested? at a house in Cambridgeshire\", while this location refers to the predicate \"found\". To mitigate this issue, we add a preliminary annotation step, in which an annotator was asked to verify whether the QAs are both (1) semantically interpretable and (2) correctly represent the semantic relations in the generated text. This preliminary step is efficient and required less than an hour to validate 500 QAs. Annotation InterfaceFigure 4 shows the inter- face of the first annotation step (entity evaluation) and Figure 2 shows the interface of the second an- notation step (QA verification). Once annotators finish the first step, they can move the second step, but they can always come back and modify their annotation for the entity evaluation. Our annotation interface was developed with Vue.js11 as a WebComponent, which can embed into any website and popular annotation platforms, such as Amazon Mechanical Turk. Annotator",
      "chunk_index": 36
    },
    {
      "index": 446,
      "chunk_id": "QASemConsistency2024_chunk_37",
      "source_id": "QASemConsistency2024",
      "text": "but they can always come back and modify their annotation for the entity evaluation. Our annotation interface was developed with Vue.js11 as a WebComponent, which can embed into any website and popular annotation platforms, such as Amazon Mechanical Turk. Annotator CompensationFor XSUM and bi- ographies, each HIT consists of one or reference text, X, and two model outputs, Y1 and Y2. For XSUM, each HIT includes a source article with summaries generated by BART and PEGASUS, and compensated with $1.50 per HIT. For biogra- phies, we select two model outputs for each in- 11https://vuejs.org Instructions: In this task, you'll be given an ARTICLE and an extremely simple question-answer (QA), representing a predicate-argument relation. Please indicate whether the QA is supported by the ARTICLE or not, according to the following definition: A QA is supported by the ARTICLE if the meaning of the QA can be deduced from the ARTICLE. For example, given the article \"Real Madrid beats PSG in the UEFA final Champions League\": The QA \"Who won something? PSG\" is not supported because PSG didn't win anything. The QA \"What did someone win? The UEFA Champions League\" is supported because someone (here Real Madrid) did win the UEFA Champions League. ARTICLE: QA: Is the QA supported by the ARTICLE? Please answer only \"Yes\" or \"No\". Table 5: Our prompt for automatically verifying whether a QASem QA is supported by the reference text, brown indicates an example. stance, with a compensation of $3.00 per HIT, re- flecting the larger number of QAs to annotate. For Verifiability, each HIT consists of a single response with a compensation of $2.50 per HIT. B Prompts C Automatic Evaluation The prompt used in our experiments is shown in Table 5. Figure 4: An example of the first annotation step.",
      "chunk_index": 37
    },
    {
      "index": 447,
      "chunk_id": "TRUE_Benchmark2023_chunk_00",
      "source_id": "TRUE_Benchmark2023",
      "text": "Stochastic thermodynamics of multiple co-evolving systems - beyond multipartite processes Farita Tasnim∗ Massachusetts Institute of Technology, Cambridge, MA, USA David H. Wolpert † Santa Fe Institute, Santa Fe, NM, USA Complexity Science Hub, Vienna, Austria and Arizona State University, Tempe, AZ, USA (Dated: May 23, 2023) Many dynamical systems consist of multiple, co-evolving subsystems (degrees of freedom). These subsystems often depend upon each other in a way that restricts the overall system's dynamics. How does this network of dependencies aﬀect the system's thermodynamics? Prior studies in the stochastic thermodynamics of multipartite processes (MPPs) have approached this question by restricting the system to allow only one subsystem to change state at a time. However, in many real systems, such as chemical reaction networks or electronic circuits, multiple subsystems must change state together. Therefore, studies of MPPs do not apply to such systems. Here, we investigate the thermodynamics of composite processes, in which subsets of subsystems are allowed to change state simultaneously. These subsets correspond to the subsystems that interact with a single mechanism (e.g., a thermal or chemical reservoir) that is coupled to the system. An MPP is simply a (subcase of a) composite process in which all such subsets have cardinality one. We demonstrate the power of the composite systems framework to study the thermodynamics of multiple, co-evolving subsystems. In particular, we derive thermodynamic uncertainty relations for information ﬂows in composite processes. We also derive strengthened speed limits for composite processes. Our results apply to a much broader class of dynamical systems than do results for MPPs, and could guide future studies of the thermodynamics of distributed computational systems. Many dynamical systems can be decomposed into a set of multiple co-evolving subsystems. Each subsys- tem is a degree of freedom. For example, information- processing systems such as computers and brains con- sist of many separate components that evolve together and aﬀect each others' dynamics. In practice, such sys- tems are not at thermodynamic equilibrium. So, they require energy resources to operate. Research in the thermodynamics of such distributed computational systems seeks to address how their information-processing capabilities are constrained by the energy resources available to them. To conduct such analyses, we must ﬁrst specify the precise way in which the subsystems co-evolve. So far in stochastic thermo- dynamics, this has often been done by assuming the joint dynamics of the subsystems is a multipartite pro- cess (MPP) [1-3]. However, MPPs",
      "chunk_index": 0
    },
    {
      "index": 448,
      "chunk_id": "TRUE_Benchmark2023_chunk_01",
      "source_id": "TRUE_Benchmark2023",
      "text": "we must ﬁrst specify the precise way in which the subsystems co-evolve. So far in stochastic thermo- dynamics, this has often been done by assuming the joint dynamics of the subsystems is a multipartite pro- cess (MPP) [1-3]. However, MPPs require that every mechanism (a gen- eralized external system mediating the system's state transitions, e.g., a thermal or chemical reservoir) cou- pled to the system interacts with only one subsystem. So, in an MPP, only one subsystem can change state at any given time. Unfortunately, at most spatio-temporal scales that we observe, multiple subsystems do change state at the same time. As a canonical example, in chemical reaction networks (Fig. 1(a)), multiple species ∗ farita@mit.edu, web: farita.me † dhw@santafe.edu, web: davidwolpert.weebly.com counts must change state concurrently. As another ex- ample, the voltages on di ﬀerent conductors in a cir- cuit (Fig. 1(b)) must change state at the same time. There has been some preliminary work extending the stochastic thermodynamics of MPPs to address the broader scenario in which each mechanism couples to a set of multiple subsystems [4]. Systems with this na- ture are called composite systems, and their dynamics is called a composite process. Here we extend this preliminary work and obtain new results in the stochastic thermodynamics of compos- ite processes. We decompose key quantities (including probability ﬂows, entropy production, and dynamical activity) into contributions from each mechanism. We also analyze the network specifying which (set of) sub- systems can aﬀect the dynamics of each subsystem. This network gives rise to units, which are subsets of sub- systems whose joint dynamics does not depend on the state of the rest of the system. We then use the speciﬁ- cation of units and the decomposition of key quantities to derive a wealth of thermodynamic uncertainty rela- tions (TURs). Finally, we derive a strengthened thermo- dynamic speed limit theorem (SLT) for composite pro- cesses. This speed limit provides a tighter restriction on how much the probability distribution over system states can change during a ﬁxed time interval, using the contributions from each mechanism to entropy produc- tion and dynamical activity. These results also apply to MPPs, since they are a special case of a composite pro- cess. arXiv:2305.09571v2 [cond-mat.stat-mech] 22 May 2023 We begin by reviewing the preliminary work on com- posite processes, including the speciﬁcation of units. We then present how key quantities decompose into contributions from each mechanism",
      "chunk_index": 1
    },
    {
      "index": 449,
      "chunk_id": "TRUE_Benchmark2023_chunk_02",
      "source_id": "TRUE_Benchmark2023",
      "text": "special case of a composite pro- cess. arXiv:2305.09571v2 [cond-mat.stat-mech] 22 May 2023 We begin by reviewing the preliminary work on com- posite processes, including the speciﬁcation of units. We then present how key quantities decompose into contributions from each mechanism coupled to the sys- tem. We then present our results for TURs and strength- ened SLTs. We conclude by discussing our results in the broader contexts of the thermodynamics of constraints and the thermodynamics of computation and by sug- gesting avenues of future work. I. STOCHASTIC THERMODYNAMICS OF COMPOSITE PROCESSES A. Background on composite processes A composite process is a generalization of MPPs, de- scribing the co-evolution of a ﬁnite set of subsystems, N= {1,2,...,N }. Each subsystem i has a discrete state space Xi. xindicates a state vector in X =×i∈NXi, the joint state space of the full system. xA indicates a state vector in X=×i∈AXi, the joint state space of the subset A. The probability that the entire system is in a state x at time tevolves according to a master equation: d dtpx(t) = Kx′ x (t)px′(t) (1) This stochastic dynamics arises due to couplings of the system with a set of mechanisms V= {v1,v2,...,v M}. In general, each such mechanismvcouples to only a subset of the subsystems. We refer to the set of subsystems to which a mechanism vcouples as its puppet set, and write it as P(v) ⊆N. As an example, an MPP is a composite process where each mechanism couples to only one subsystem (al- though a single subsystem might be coupled to multiple mechanisms [1]). So in an MPP, the cardinality of every puppet set is 1. At any given time, a composite system changes state due to its interaction with at most one mechanism, just as with MPPs. Accordingly, the rate matrix of the overall system is a sum of mechanism-speciﬁc rate matrices: Kx′ x (t) = ∑ v∈V δ x′ N\\P(v) xN\\P(v) K x′ P(v),x′ N\\P(v) xP(v),xN\\P(v) (t) (2) := ∑ v∈V Kx′ x (v;t) (3) (Here and throughout, for any two variables, z,z′ con- tained in the same space,δz′ z is the Kronecker delta func- tion that equals 1 when z′= z, and equals 0 otherwise). We can illustrate composite processes using a toy stochastic chemical reaction network (Fig. 1a) [5- 7]. This network involves four co-evolving species {X1,X2,X3,X4} that change state according to three FIG. 1. Examples of systems",
      "chunk_index": 2
    },
    {
      "index": 450,
      "chunk_id": "TRUE_Benchmark2023_chunk_03",
      "source_id": "TRUE_Benchmark2023",
      "text": "z′= z, and equals 0 otherwise). We can illustrate composite processes using a toy stochastic chemical reaction network (Fig. 1a) [5- 7]. This network involves four co-evolving species {X1,X2,X3,X4} that change state according to three FIG. 1. Examples of systems whose dynamics can be modeled as composite processes. Each system consists of multiple sub- systems (blue circles). Mechanisms are denoted as r, and their puppet sets P(r) are indicated by translucent white bubbles. (a) An example stochastic chemical reaction network consists of four co-evolving species {X1,X2,X3,X4}that change state according to three chemical reactions{A,B,C}. (b) An example toy circuit consists of four conductors {1,2,3,4}that change state via interactions with three devices {A,B,C}. chemical reactions {A,B,C}(left). The system state is a vector consisting of the number of molecules of each species in the system. Only one reaction can occur at a time, but when a reaction does occur, multiple subsys- tems all change their state. For example, in the forward reaction A, species X1, X2, and X3 must change state at the same time, by counts of {−2,−1,+1}, respectively. Accordingly, this reaction network is not an MPP . How- ever, it is a composite process. We can illustrate this composite process in terms of the associated puppet sets (righthand side of ﬁgure). There are a total of three such puppet sets, one for each of the possible chemical reactions. These three puppet sets are indicated by translucent bubbles in the right- hand part of the ﬁgure. The mechanisms of the three puppet sets are denoted as rA, rB, and rC, and the pup- pet set of mechanism ris denoted as P(r). As another example, consider a toy electronic circuit (Fig. 1b) [8] consisting of four conductors (the four cir- cles in the left-hand side of the ﬁgure) and three devices (the three bidirectional arrows). The state of the system is a vector consisting of the voltage on each conductor. Two of the conductors (1 and 4) are \"regulated\", since they are tied directly to ﬁxed voltage sources ( V1 and V4). The other two conductors (2 and 3) are \"free\" to stochastically change state via the eﬀect of devices A, B, and C. The composite process capturing the dynamics of the state of this circuit is illustrated in the right-hand side of the ﬁgure. There are three puppet sets (each a translu- cent bubble), each corresponding to a mechanism asso- ciated with",
      "chunk_index": 3
    },
    {
      "index": 451,
      "chunk_id": "TRUE_Benchmark2023_chunk_04",
      "source_id": "TRUE_Benchmark2023",
      "text": "C. The composite process capturing the dynamics of the state of this circuit is illustrated in the right-hand side of the ﬁgure. There are three puppet sets (each a translu- cent bubble), each corresponding to a mechanism asso- ciated with one of the devices in the system. The mech- anisms are denoted as rA, rB, and rC, and the puppet set of mechanism ris denoted as P(r). In an MPP, even though the mechanisms that a ﬀect the dynamics of any subsystem i do not a ﬀect the dy- namics of any other subsystem, in general the dynamics of i will depend on the states of some set of other sub- systems. For example, in a bipartite process [9], both of the subsystems can be modeled as having their own set of mechanisms, but each subsystem's dynamics is gov- erned by the state of the other subsystem as well as its own state. Similarly, in a composite process, the dynamics of each subsystem i can depend on the state of other sub- systems in addition to its own state. Each such de- pendency can be represented as an edge in a directed graph. In the resulting dependency network each edge j→imeans that the state of subsystem jaﬀects the rate of state transitions in subsystem i. We refer to the set of subsystems whose state aﬀects the dynamics of i as the leaders of i. So j →i means that j is a leader of i. In any dependency network, the leaders of each subsystem iare its parents, pa(i). The leader setfor a mechanism v is deﬁned to be the union of the leaders of each subsystem in the puppet set of v: L(v) = ⋃ i∈P(v) pa(i). As an example, even though the puppet set of mechanism v2 in Fig. 2 is {A,C,D }, the leader set of v2 is {A,B,C,D }. FIG. 2. The dependency network speciﬁes how the dynam- ics of each subsystem is governed by the state of other sub- systems. This network deﬁnes the leader sets in a composite process. The leader set of any mechanism is a (perhaps proper) superset of its puppet set. Accordingly, we can write Kx′ x (v;t) = K x′ L(v),x′ N\\L(v) x′ L(v)\\P(v),xP(v),x′ N\\L(v) (v;t) (4) With abuse of notation, we can rewrite this in a way that explicitly embodies the fact that the instantaneous dy- namics of the puppet",
      "chunk_index": 4
    },
    {
      "index": 452,
      "chunk_id": "TRUE_Benchmark2023_chunk_05",
      "source_id": "TRUE_Benchmark2023",
      "text": "we can write Kx′ x (v;t) = K x′ L(v),x′ N\\L(v) x′ L(v)\\P(v),xP(v),x′ N\\L(v) (v;t) (4) With abuse of notation, we can rewrite this in a way that explicitly embodies the fact that the instantaneous dy- namics of the puppet set P(v) depends at most on the state of the leader set L(v), and not on the state of any of the subsystems in N\\L(v): K x′ L(v) x′ L(v)\\P(v),xP(v) (v;t) := Kx′ x (v;t) (5) A unit ω⊆N is a collection of subsystems such that as the full system's state evolves via a master equation ac- cording to K(t), the marginal distribution over the states of the unit also evolves according to its own CTMC: d dtpxω(t) = Kx′ω xω(ω;t)px′ω(t) (6) for some associated rate matrix K(ω;t). Intuitively, a unit is any set of subsystems whose evolution is inde- pendent of the states of the subsystems outside the unit. Typically, a unit is a union of leader sets. In such cases no subsystem in the unit has parents outside of the unit. Importantly though, this doesn't prevent there being a subsystem in the unit that is a leader for some sub- system outside of the unit. Informally speaking, the boundary of a unit in an dependency network can have outgoing edges, even though it cannot have any incom- ing edges. Any union of units is a unit, and any non-empty inter- section of units is a unit [4]. Note that the entire system Nitself is a unit. We denote the set of all units as N†. Since each separate unit evolves according to its own CTMC, all the usual theorems of stochastic thermody- namics apply to each unit separately. In particular, the Second Law [4] applies, as do the thermodynamic uncer- tainty relations [10-12], the speed limit theorems [13- 15], the ﬂuctuation theorems [16], ﬁrst-passage time bounds and bounds on stopping times [17-19], etc. We highlight that for any pair of nested units ωand α⊆ω, it is true that [3, 4]: ˙σω(t) ≥˙σα(t) (7) A set of units N∗is called a unit structureif it obeys the following properties [4]: • The union of the units in the set equals N. N∗= {ω1,ω2,...}: ⋃ ω∈N† ω= N • The set is closed under intersections of its units. ∀(ω1,ω2) ∈(N∗)2 = ω1 ∩ω2 ∈N∗ We deﬁne an inclusion-exclusion sum of a function fω evaluated on every unit ωin a unit structure",
      "chunk_index": 5
    },
    {
      "index": 453,
      "chunk_id": "TRUE_Benchmark2023_chunk_06",
      "source_id": "TRUE_Benchmark2023",
      "text": "N. N∗= {ω1,ω2,...}: ⋃ ω∈N† ω= N • The set is closed under intersections of its units. ∀(ω1,ω2) ∈(N∗)2 = ω1 ∩ω2 ∈N∗ We deﬁne an inclusion-exclusion sum of a function fω evaluated on every unit ωin a unit structure N∗as ˆ∑ ω∈N∗fω = ∑ ω′∈N∗ fω′ − ∑ ω′′∈N∗ fω′′ + ∑ ω′′′∈N∗ fω′′′ −... (8) For example, the time- t inclusion-exclusion (or \"in- ex\" for short) information reads IN∗ (t) := (ˆ∑ ω∈N∗Sω(t) ) −SN∗ (t) (9) Using the fact that the heat ﬂow into the unit struc- ture also decomposes into an in-ex sum, we can decom- pose the global EP incurred during a time period [0 ,τ] according to σN = ˆ∑ ω∈N∗σω−∆IN∗ (10) where ∆IN∗ is the change in the in-ex information dur- ing the time period [0 ,τ]. For a detailed proof of the in-ex decomposition of the global EP, see [2, 3]. One can use the in-ex sum decomposition of the EP in various ways depending on what degrees of freedom are accessible in the system of interest. For example, if one can calculate the mismatch cost [20, 21] λω for each unit in the unit structure, then the in-ex sum can be rewritten: σN = ˆ∑ ω∈N∗λω+ ˆ∑ ω∈N∗ξω−∆IN∗ (11) where ξω = σω −λω is the \"residual EP\" due to every- thing aside from the mismatch cost. Additionally, a very large number of lower bounds can be obtained on the global EP by replacing any posi- tive σω (or any such set of them) in the in-ex sum with any lower bound (e.g., TUR, SLT, etc.) on the value of that unit's EP . (See [2] for examples in the special case of MPPs.) B. Decomposition of thermodynamic and dynamical quantities in composite processes Since the entire systemNis itself a unit, we will write all our results in terms of units for the rest of the paper. The rate matrix of each unit ω in a composite pro- cess decomposes into rate matrices from each mecha- nism whose leader set is a subset of ω: Kx′ω xω(ω;t) = ∑ v:L(v)⊆ω δ x′ ω\\L(v) xω\\L(v) K x′ L(v),x′ ω\\L(v) xL(v),xω\\L(v) (t) (12) = ∑ v:L(v)⊆ω Kx′ω xω(v;t) (13) Similarly, we can decompose the EP rate of any unitω: ˙σω(t) = ∑ v:L(v)⊆ω, x′ω,xω̸=x′ω Kx′ω xω(v;t)px′ω(t)ln   Kx′ω xω(v;t)px′ω(t) Kxω x′ω (v;t)pxω(t)   (14) = ∑ v:L(v)⊆ω ˙ζv ω(t) (15) into",
      "chunk_index": 6
    },
    {
      "index": 454,
      "chunk_id": "TRUE_Benchmark2023_chunk_07",
      "source_id": "TRUE_Benchmark2023",
      "text": "= ∑ v:L(v)⊆ω Kx′ω xω(v;t) (13) Similarly, we can decompose the EP rate of any unitω: ˙σω(t) = ∑ v:L(v)⊆ω, x′ω,xω̸=x′ω Kx′ω xω(v;t)px′ω(t)ln   Kx′ω xω(v;t)px′ω(t) Kxω x′ω (v;t)pxω(t)   (14) = ∑ v:L(v)⊆ω ˙ζv ω(t) (15) into contributions ˙ζv ω(t) from each mechanism whose leader set is a subset of ω. In particular, since the entire system is a unit whose state transitions are mediated by every mechanism v∈V, the global EP rate decomposes as ˙σN(t) = ∑ v ˙ζv N(t). A unit's dynamical activity also decomposes: Aω(t) = ∑ v:L(v)⊆ω, x′ω,xω̸=x′ω Kx′ω xω(v;t)px′ω(t) = ∑ v:L(v)⊆ω A(v;t) (16) Similarly, the entire system's dynamical activity can be decomposed as AN(t) = ∑ vA(v;t). Note that the dy- namics of every pair of nested units ω,α ⊆ω must be consistent with one another [4], which means that Aα(v;t) = Aω(v;t) = A(v;t) for all αand ω. We denote the probability ﬂow from x′ ω →xω due to mechanism vas Ax′ω xω(v;t) = Kx′ω xω(v;t)px′ω(t). We write the net probability current from x′ ω →xω due to mechanism vas Jx′ω xω(v;t) = Ax′ω xω(v;t) −Axω x′ω (v;t). The total net proba- bility current from x′ω →xω equals the sum of the prob- ability currents due to each mechanism whose leader set is a subset of the unit ω: Jx′ω xω(t) = ∑ v:L(v)⊆ω Jx′ω xω(v;t) (17) Accordingly, we can decompose the master equation for the unit ω into probability currents induced by each mechanism: d dtpxω(t) = ∑ v:L(v)⊆ω, x′ω̸=xω Kx′ω xω(v;t)px′ω(t) = ∑ v:L(v)⊆ω, x′ω Jx′ω xω(v;t) (18) II. THERMODYNAMIC UNCERTAINTY RELATIONS FOR COMPOSITE PROCESSES For any unit ωthat is in an NESS, any linear function of probability currents Cω is a current. It can be divided into contributions from each mechanism: ˙Cω = ∑ x′ω,xω>x′ω Jx′ω xωCx′ω xω (19) = ∑ v:L(v)⊆ω, x′ω,xω>x′ω Jx′ω xω(v)Cx′ω xω (20) = ∑ v:L(v)⊆ω ˙Cω(v) (21) where Cx′ω xω = −Cxω x′ω is some anti-symmetric function of state transitions, and we have dropped the time depen- dence in the steady state. Importantly, the current contribution from each mechanism ˙Cω(v) is itself a current. So all of the ther- modynamic uncertainty relations (TURs) hold for the time-integrated version of any such mechanism-speciﬁc current. In an NESS running for a time period of length τ, this mechanism-speciﬁc time-integrated current is Cω(v) = τ˙Cω(v). Additionally, since every unit evolves according to its own CTMC,",
      "chunk_index": 7
    },
    {
      "index": 455,
      "chunk_id": "TRUE_Benchmark2023_chunk_08",
      "source_id": "TRUE_Benchmark2023",
      "text": "(TURs) hold for the time-integrated version of any such mechanism-speciﬁc current. In an NESS running for a time period of length τ, this mechanism-speciﬁc time-integrated current is Cω(v) = τ˙Cω(v). Additionally, since every unit evolves according to its own CTMC, the TURs hold for each unit. For example, the ﬁnite-time TUR bounds the preci- sion of any current in a CTMC with respect to its EP rate [10, 22]. For a composite process, this holds for any unit and any arbitrary time-integrated current: σω ≥2⟨Cω⟩2 Var(Cω (22) Additionally, for any mechanism v : L(v) ⊆ω and any associated current Cω(v), σω ≥ 2⟨Cω(v)⟩2 Var(Cω(v)) (23) The vector-valued TUR following [11] holds for a vec- tor ˙Cω of any set of (potentially mechanism-speciﬁc) currents {˙Cω}that are not linearly dependent: ˙CT ωΞ−1 ω ˙Cω ≤ ˙σω 2τ (24) where Ξ−1 ω is the inverse of the covariance matrix of the associated time-integrated currents {Cω}. Any of these TURs can be useful to bound the entropy production when one has limited access to the system in the sense that one can measure state transitions i) due only to some subset of the mechanisms inﬂuencing the system or state transitions or ii) involving some subset of units in the system. A. Information Flow TURs One important quantity in an MPP is information ﬂow [1, 9, 23]. Here, we extend the concept of infor- mation ﬂow to composite processes. For any unit ω in an NESS, a set of subsystems A⊂ω, and a set of subsys- tems B⊂ω(for which A∩B= ∅), the information ﬂow is the rate of decrease in the conditional entropy of the state of Bgiven the state of A, due to state transitions in A: ˙IA→B = ∑ x′ω,xω>x′ω Jx′ω xωδ x′ ω\\A xω\\A ln pxB|xA pxB|x′ A (25) So, when ωis in an NESS, the information ﬂow is a cur- rent for which Cx′ω xω = C x′ ω\\A,x′ A xω\\A,xA = δ x′ ω\\A xω\\A ln pxB|xA pxB|x′ A . The con- tribution to that information ﬂow that is due to interac- tions of the unit with reservoir v : L(v) ⊆ω is itself an information ﬂow ˙IA→B(v) = ∑ x′ω,xω>x′ω Jx′ω xω(v)δ x′ ω\\A xω\\A ln pxB|xA pxB|x′ A (26) Since these information ﬂows are currents, the TURs will apply to them. This observation in combination with Eq. (7) suggests that the precision of an informa- tion ﬂow is",
      "chunk_index": 8
    },
    {
      "index": 456,
      "chunk_id": "TRUE_Benchmark2023_chunk_09",
      "source_id": "TRUE_Benchmark2023",
      "text": "x′ω,xω>x′ω Jx′ω xω(v)δ x′ ω\\A xω\\A ln pxB|xA pxB|x′ A (26) Since these information ﬂows are currents, the TURs will apply to them. This observation in combination with Eq. (7) suggests that the precision of an informa- tion ﬂow is (best) bounded by the reciprocal of the en- tropy production of the smallest unit which contains A∪B. III. STRENGTHENED THERMODYNAMIC SPEED LIMITS FOR COMPOSITE PROCESSES Here we derive a speed limit similar to the one in [15], but for composite processes. This speed limit is tighter than the one presented in that paper. Our analysis will hold for an arbitrary unit ω(which could be the entire system Nitself): lω ≤ ∑ v:L(v)⊆ω Atot ω (v;τ)f ( ζv ω(τ) Atotω (v;τ) ) (27) where the dynamics occurs during the time period [0,τ]. Additionally, lω is the total variation distance between the initial (time-0) and ﬁnal (time- τ) probability distri- butions over states of the unit ω. Atotω (v;τ) is the total time-integrated dynamical activity due to mechanismv. ζvω(τ) is the total contribution to the entropy production of unit ωdue to interactions of ωwith mechanism v. We start by bounding the total variation distance be- tween the initial and ﬁnal (time-τ) probability distribu- tions over states of the unit ω: lω := L(pxω(0),pxω(τ)) = 1 ∑ xω ⏐⏐⏐pxω(τ) −pxω(0) ⏐⏐⏐ (28) = 1 ∑ xω ⏐⏐⏐⏐⏐ ∫ τ dt d dtpxω(t) ⏐⏐⏐⏐⏐ (29) ≤1 ∫ τ dt ∑ xω ⏐⏐⏐⏐⏐ d dtpxω(t) ⏐⏐⏐⏐⏐ (30) In a composite process, we can further bound the inte- grand: ∑ xω ⏐⏐⏐⏐⏐ d dtpxω(t) ⏐⏐⏐⏐⏐ = ∑ xω ⏐⏐⏐⏐⏐⏐⏐⏐ ∑ v:L(v)⊆ω ∑ x′ω̸=xω Jx′ω xω(v;t) ⏐⏐⏐⏐⏐⏐⏐⏐ (31) ≤ ∑ v:L(v)⊆ω ∑ xω,x′ω̸=xω ⏐⏐⏐⏐Jx′ω xω(v;t) ⏐⏐⏐⏐ (32) We write the time- t \"conditional probability distribu- tion\" of the forward process, under the counterfactual scenario that the process evolves with coupling only to mechanism v: L(v) ⊆ωas Wx′ω xω (v;t) = (1 −δx′ω xω)Kx′ω xω(v;t)px′ω(t) Aω(v;t) (33) Intuitively, this can be interpreted as a conditional prob- ability that if a jump occurs at tdue reservoir v: L(v) ⊆ ω, that the state before the jump was x′ω and the state afterwards was xω We write the same quantity for the reverse process as ˜Wx′ω xω (t) = (1 −δxω x′ω )Kxω x′ω (v;t)pxω(t) Aω(v;t) (34) The total variation distance between these matrices dTV(Wω(v;t),˜Wω(v;t)) represents how irreversible this counterfactual process (the one driven only by mech- anism v) is",
      "chunk_index": 9
    },
    {
      "index": 457,
      "chunk_id": "TRUE_Benchmark2023_chunk_10",
      "source_id": "TRUE_Benchmark2023",
      "text": "for the reverse process as ˜Wx′ω xω (t) = (1 −δxω x′ω )Kxω x′ω (v;t)pxω(t) Aω(v;t) (34) The total variation distance between these matrices dTV(Wω(v;t),˜Wω(v;t)) represents how irreversible this counterfactual process (the one driven only by mech- anism v) is at time t. Using these deﬁnitions, we can rewrite Eq. (32) as ∑ xω ⏐⏐⏐⏐⏐ d dtpxω(t) ⏐⏐⏐⏐⏐ ≤2 ∑ v:L(v)⊆ω Aω(v;t)dTV(Wω(v;t),˜Wω(v;t)) (35) Plugging into Eq. (30), we obtain lω ≤ ∫ τ dt ∑ v:L(v)⊆ω Aω(v;t)dTV(Wω(v;t),˜Wω(v;t)) (36) We next make use of the fact that mechanismv's con- tribution to the EP rate of unit ω(Eq. (15)) can be writ- ten in terms of the Kullback-Leibler (KL) divergence be- tween the conditional distributions of the forward and backward processes as ˙ζv ω(t) = Aω(v;t)DKL(Wω(v;t),˜Wω(v;t)) (37) Any positive monotonic concave function f relates the total variation distance to the KL divergence [15] ac- cording to: dTV(p;q) ≤f(DKL(p;q)) (38) We can use this relationship to relate Eq. (37) to lω. Combining Eqs. (36) to (38), lω ≤ ∫ τ dt ∑ v:L(v)⊆ω Aω(v;t)f ( ˙ζvω(t) Aω(v;t) ) (39) Next deﬁne ζv ω = ∫τ 0 dt˙ζv ω(t) as the total (ensemble- average) contribution to the EP of unit ωcaused by an interaction of the system with mechanism v during the time period [0 ,τ]. Also deﬁne Atotω (v;τ) = ∫τ 0 dtAω(v;t) as the total (ensemble-average) number of state transi- tions in the unit ωthat are caused by an interaction of the system with mechanism v. Then using the positivity of the dynamical activity and of the EP, together with the concavity of f, we can further bound the right hand side to obtain a general limit for composite processes: lω ≤ ∑ v:L(v)⊆ω Atot ω (v;τ)f ( ζvω(τ) Atotω (v;τ) ) (40) This result provides an upper bound on how much lω can change during the time interval [0 ,τ], in terms of the associated activity of ωand the contribution of ωto EP . So Eq. (40) is a thermodynamic speed limit theorem, involving By comparison, the speed limit in [15] applied to a unit ωreads lω ≤Atot ω (τ)f ( σω(τ) Atotω (τ) ) (41) For a composite process, the right hand side of this \"global\" bound expands to lω ≤   ∑ v:L(v)⊆ω Atot ω (v;τ)   f   ∑ v:L(v)⊆ωζv ω(τ) ∑ v:L(v)⊆ωAtotω (v;τ)   (42) By Jensen's inequality, the speed limit for compos-",
      "chunk_index": 10
    },
    {
      "index": 458,
      "chunk_id": "TRUE_Benchmark2023_chunk_11",
      "source_id": "TRUE_Benchmark2023",
      "text": "right hand side of this \"global\" bound expands to lω ≤   ∑ v:L(v)⊆ω Atot ω (v;τ)   f   ∑ v:L(v)⊆ωζv ω(τ) ∑ v:L(v)⊆ωAtotω (v;τ)   (42) By Jensen's inequality, the speed limit for compos- ite processes (Eq. (40)) is always tighter than the speed limit provided by [15] (Eq. (41)). For a concave func- tion f, a set of numbers a xv in its domain, and positive weights av, Jensen's inequality states that   ∑ v av  f (∑ vavxv∑ vav ) ≥ ∑ v avf(xv) (43) Setting av = Atotω (v;τ) and xv = ζvω(τ) Atotω (v;τ) proves that Eq. (40)) is always tighter than Eq. (41). Intuitively, this occurs because we're able to deﬁne the mechanism- speciﬁc contributions to the EP and activity in a com- posite process. [15] provides some examples of acceptable functions f. For example, if we follow Pinsker's inequality and choose f = √x 2 , then the speed limit provided by [15] collapses to the speed limit derived in [13]. If we plug in this choice of f to Eq. (40), extract the parameter τ by using the average frequency of state transitions ⟨Aω(v)⟩τ = Atotω (v;τ) τ , and rearrange terms, we obtain ∀ω∈N†: τ≥ (L(pxω(0),pxω(τ)))2 (∑ v:L(v)⊆ω √ ζvω(τ)⟨Avω⟩τ )2 (44) the tightest of which is given by: τ≥max ω∈N† (L(pxω(0),pxω(τ)))2 (∑ v:L(v)⊆ω √ ζvω(τ)⟨Av⟩τ )2 (45) This particular speed limit tells us that speed of the evo- lution of the system's probability distribution cannot be greater than the speed of evolution of the distribution over the coordinates of the \"slowest-evolving\" unit. IV. DISCUSSION Here we have introduced the stochastic thermody- namics of composite processes. This work presents a preliminary analysis of how information ﬂows in a com- posite process are constrained by the entropy produc- tions of units. It also demonstrates that bounds on the speed of transforming a system's probability distribu- tion over states can be tightened with knowledge of the contributions to the entropy production and dynamical activity from each mechanism with which the system interacts. This work ﬁts into a growing branch of research on the stochastic thermodynamics of constraints. One ex- ample of research in this area investigates the e ﬀect of constraints on the control protocol (time sequence of rate matrices evolving the probability distribution) [24] There has also been some important work where the \"constraint\" on",
      "chunk_index": 11
    },
    {
      "index": 459,
      "chunk_id": "TRUE_Benchmark2023_chunk_12",
      "source_id": "TRUE_Benchmark2023",
      "text": "constraints. One ex- ample of research in this area investigates the e ﬀect of constraints on the control protocol (time sequence of rate matrices evolving the probability distribution) [24] There has also been some important work where the \"constraint\" on such a many-degree-of-freedom clas- sical system is simply that it be some very narrowly deﬁned type of system, whose dynamics is speciﬁed by many di ﬀerent kinds of parameters. For example, there has been analysis of the stochastic thermodynam- ics of chemical reaction networks [6, 7], of electronic circuits [8, 25, 26], and of biological copying mecha- nisms [27]. This work analyzes the consequences of a major class of dynamical constraints that arises because many of these systems are most naturally modelled as a set of multiple co-evolving subsystems [1-4, 9, 28-30]. In particular, the main constraints on such systems are that only certain subsets of subsystems can simultane- ously change state a given time, and the dependencies between subsystems impose restrictions on their joint dynamics. There remain many avenues of potential future work, especially in the thermodynamics of computa- tion. Many computational processes consist of multiple, co-evolving systems with the broad set of constraints that allow them to be easily modeled as a composite pro- cess. Research in this direction would ﬁrst require for- malizing the notion of computation in a composite pro- cess. One such computation, which equates to the iden- tity map, is simply communication (information trans- mission). One could extend the recent study on the fun- damental thermodynamic costs of communication [31] to tie Shannon information theory to the stochastic ther- modynamics of composite processes. More generally, for any given computation, one could analyze the trade- oﬀs between the energy cost required to implement that computation and the performance (accuracy, time, etc.) of a composite process. In particular, there could be rich structure in how the properties of the dependency net- work in a composite process aﬀects these trade-oﬀs. V. ACKNOWLEDGEMENTS This work was supported by the MIT Media Lab Con- sortium, Santa Fe Institute, US NSF EAGER Grant CCF- 2221345. F.T. and D.H.W. thank Tarek Tohme for ini- tial discussions regarding TURs for information ﬂows in multipartite processes. F.T. thanks Nahuel Freitas for discussions regarding how circuits can be modeled as composite processes. [1] J. M. Horowitz, Multipartite information ﬂow for mul- tiple maxwell demons, Journal of Statistical Mechanics: Theory and Experiment 2015, P03006 (2015).",
      "chunk_index": 12
    },
    {
      "index": 460,
      "chunk_id": "TRUE_Benchmark2023_chunk_13",
      "source_id": "TRUE_Benchmark2023",
      "text": "in multipartite processes. F.T. thanks Nahuel Freitas for discussions regarding how circuits can be modeled as composite processes. [1] J. M. Horowitz, Multipartite information ﬂow for mul- tiple maxwell demons, Journal of Statistical Mechanics: Theory and Experiment 2015, P03006 (2015). [2] D. H. Wolpert, Combining lower bounds on entropy pro- duction in complex systems with multiple interacting components, in Frontiers in Entropy Across the Disciplines: Panorama of Entropy: Theory, Computation, and Applica- tions (World Scientiﬁc, 2023) pp. 405-453. [3] D. H. Wolpert, Minimal entropy production rate of in- teracting systems, New Journal of Physics 22, 113013 (2020). [4] D. H. Wolpert, Strengthened landauer bound for compos- ite systems, arXiV (2020). [5] A. Wachtel, R. Rao, and M. Esposito, Thermodynami- cally consistent coarse graining of biocatalysts beyond michaelis-menten, New Journal of Physics 20, 042002 (2018). [6] R. Rao and M. Esposito, Nonequilibrium thermodynam- ics of chemical reaction networks: wisdom from stochas- tic thermodynamics, Physical Review X6, 041064 (2016). [7] R. Rao and M. Esposito, Conservation laws and work ﬂuc- tuation relations in chemical reaction networks, The Jour- nal of chemical physics 149, 245101 (2018). [8] N. Freitas, J.-C. Delvenne, and M. Esposito, Stochastic thermodynamics of non-linear electronic circuits: A re- alistic framework for thermodynamics of computation, arXiv preprint arXiv:2008.10578 (2020). [9] J. M. Horowitz and M. Esposito, Thermodynamics with continuous information ﬂow, Physical Review X 4, 031015 (2014). [10] J. M. Horowitz and T. R. Gingrich, Proof of the ﬁnite-time thermodynamic uncertainty relation for steady-state cur- rents, Physical Review E 96, 020103 (2017). [11] A. Dechant, Multidimensional thermodynamic uncer- tainty relations, Journal of Physics A: Mathematical and Theoretical 52, 035001 (2018). [12] Y. Hasegawa and T. Van Vu, Fluctuation theorem un- certainty relation, Physical review letters 123, 110602 (2019). [13] N. Shiraishi, K. Funo, and K. Saito, Speed limit for clas- sical stochastic processes, Physical Review Letters 121, 10.1103/physrevlett.121.070601 (2018). [14] N. Shiraishi and K. Saito, Speed limit for open systems coupled to general environments, Physical Review Re- search 3, 023074 (2021). [15] J. S. Lee, S. Lee, H. Kwon, and H. Park, Speed limit for a highly irreversible process and tight ﬁnite-time landauer's bound, Physical review letters 129, 120603 (2022). [16] R. Rao and M. Esposito, Detailed ﬂuctuation theorems: A unifying perspective, Entropy 20, 635 (2018). [17] T. R. Gingrich and J. M. Horowitz, Fundamental bounds on ﬁrst passage time ﬂuctuations for currents, Physical review letters 119, 170601 (2017). [18]",
      "chunk_index": 13
    },
    {
      "index": 461,
      "chunk_id": "TRUE_Benchmark2023_chunk_14",
      "source_id": "TRUE_Benchmark2023",
      "text": "[16] R. Rao and M. Esposito, Detailed ﬂuctuation theorems: A unifying perspective, Entropy 20, 635 (2018). [17] T. R. Gingrich and J. M. Horowitz, Fundamental bounds on ﬁrst passage time ﬂuctuations for currents, Physical review letters 119, 170601 (2017). [18] I. Neri, É. Roldán, and F. Jülicher, Statistics of inﬁma and stopping times of entropy production and applications to active molecular processes, Physical Review X 7, 011019 (2017). [19] I. Neri, É. Roldán, S. Pigolotti, and F. Jülicher, Integral ﬂuctuation relations for entropy production at stopping times, Journal of Statistical Mechanics: Theory and Ex- periment 2019, 104006 (2019). [20] A. Kolchinsky and D. H. Wolpert, Dependence of dissipa- tion on the initial distribution over states, Journal of Sta- tistical Mechanics: Theory and Experiment 2017, 083202 (2017). [21] A. Kolchinsky and D. H. Wolpert, Dependence of inte- grated, instantaneous, and ﬂuctuating entropy produc- tion on the initial state in quantum and classical pro- cesses, Physical Review E 104, 054107 (2021). [22] P . Pietzonka, F. Ritort, and U. Seifert, Finite-time general- ization of the thermodynamic uncertainty relation, Phys- ical Review E 96, 012101 (2017). [23] D. Hartich, A. C. Barato, and U. Seifert, Sensory capac- ity: An information theoretical measure of the perfor- mance of a sensor, Physical Review E 93, 10.1103/phys- reve.93.022116 (2016). [24] A. Kolchinsky and D. H. Wolpert, Entropy production and thermodynamics of information under protocol con- straints, arXiv preprint arXiv:2008.10764 (2020). [25] C. Y. Gao and D. T. Limmer, Principles of low dissipa- tion computing from a stochastic circuit model, arXiv preprint arXiv:2102.13067 (2021). [26] D. H. Wolpert and A. Kolchinsky, Thermodynamics of computing with circuits, New Journal of Physics 22, 063047 (2020). [27] J. M. Poulton, P . R. Ten Wolde, and T. E. Ouldridge, Nonequilibrium correlations in minimal dynamical mod- els of polymer copying, Proceedings of the National Academy of Sciences 116, 1946 (2019). [28] R. M. D'Souza, Structure comes to random graphs, Na- ture Physics 5, 627 (2009). [29] S. Ito and T. Sagawa, Information thermodynamics on causal networks, Physical Review Letters 111, 180603 (2013). [30] D. H. Wolpert, Uncertainty relations and ﬂuctuation theorems for bayes nets, Physical Review Letters 125, 10.1103/physrevlett.125.200602 (2020). [31] F. Tasnim, N. Freitas, and D. H. Wolpert, The funda- mental thermodynamic costs of communication, arXiv preprint arXiv:2302.04320 (2023).",
      "chunk_index": 14
    },
    {
      "index": 462,
      "chunk_id": "TRUE_Benchmark2023_chunk_15",
      "source_id": "TRUE_Benchmark2023",
      "text": "F. Tasnim, N. Freitas, and D. H. Wolpert, The funda- mental thermodynamic costs of communication, arXiv preprint arXiv:2302.04320 (2023).",
      "chunk_index": 15
    },
    {
      "index": 463,
      "chunk_id": "NLI_Faithfulness2023_chunk_00",
      "source_id": "NLI_Faithfulness2023",
      "text": "MC-ViViT: Multi-branch Classifier-ViViT to Detect Mild Cognitive Impairment in Older Adults Using Facial Videos Jian Suna,1, Hiroko H. Dodge b,2 and Mohammad H. Mahoor c,∗,3 aDepartment Of Computer Science, University of Denver, 2155 E Wesley Ave, Denver, Colorado, 80210, United States of America bDepartment Of Neurology at Harvard Medical School, Harvard University, Massachusetts General Hospital, 55 Fruit St, Boston, Massachusetts, 02114, United States of America cDepartment Of Computer Engineering, University of Denver, 2155 E Wesley Ave, Denver, Colorado, 80210, United States of America A R T I C L E I N F O Keywords: Deep Learning, Facial Expression Features, Inter- and Intra-class imbalance, Mild Cognitive Impairment, Multi-branch Classifier, Transformer, ViViT. A B S T R A C T Deep machine learning models including Convolutional Neural Networks (CNN) have been suc- cessful in the detection of Mild Cognitive Impairment (MCI) using medical images, questionnaires, and videos. This paper proposes a novel Multi-branch Classifier-Video Vision Transformer (MC- ViViT) model to distinguish MCI from those with normal cognition by analyzing facial features. The data comes from the I-CONECT, a behavioral intervention trial aimed at improving cognitive function by providing frequent video chats. MC-ViViT extracts spatiotemporal features of videos in one branch and augments representations by the MC module. The I-CONECT dataset is challenging as the dataset is imbalanced containing Hard-Easy and Positive-Negative samples, which impedes the performance of MC-ViViT. We propose a loss function for Hard-Easy and Positive-Negative Samples (HP Loss) by combining Focal loss and AD-CORRE loss to address the imbalanced problem. Our experimental results on the I-CONECT dataset show the great potential of MC-ViViT in predicting MCI with a high accuracy of 90.63% accuracy on some of the interview videos. 1. Introduction Alzheimer's disease (AD) and related dementias (ADRD) are national public health issues with pervasive challenges for older adults, their families, and caregivers. ADRD is not only ranked as the sixth-leading cause of death in the U.S., but the COVID-19 pandemic has increased the number of deaths among those with AD (Alzheimer's_Association, 2021). Symptoms of AD/ADRD usually begin with Mild Cognitive Impairment (MCI) and include early onset mem- ory loss, cognitive decline, and impairments with verbal language and visual/spatial perception. Approximately 12- 18% of people age 60 or older are living with MCI in the U.S. Although older adults with MCI have the ability to independently perform most daily living activities such as eating, shopping, and bathing without help,",
      "chunk_index": 0
    },
    {
      "index": 464,
      "chunk_id": "NLI_Faithfulness2023_chunk_01",
      "source_id": "NLI_Faithfulness2023",
      "text": "perception. Approximately 12- 18% of people age 60 or older are living with MCI in the U.S. Although older adults with MCI have the ability to independently perform most daily living activities such as eating, shopping, and bathing without help, the National Institute on Aging (NIA) estimated that 10 to 20% of subjects with MCI will develop dementia over a one-year period (National_Institute_on_Aging, 2021). Magnetic Resonance Imaging (MRI) & Positron Emis- sion Tomography (PET) scanning and neuropsychic exam- ination are often used for identifying those with AD and dementia. These methods prove to be challenging for early MCI individuals given that the brain's structural changes ∗Corresponding author: Dr.Mohammad H. Mahoor Tel: +1-303-871-3745 | Fax: +1-303-871-2194 Jian.Sun86@du.edu (J. Sun); hdodge@mgh.harvard.edu (H.H. Dodge); mohammad.mahoor@du.edu (M.H. Mahoor) https://sites.google.com/view/sunjian/home (J. Sun); https://dodgelab.wixsite.com/dodge-lab (H.H. Dodge); http://mohammadmahoor.com (M.H. Mahoor) ORCID (s): 0000-0002-9367-0892 (J. Sun); 0000-0001-7290-8307 (H.H. Dodge); 0000-0001-8923-4660 (M.H. Mahoor) and cognitive test results at this point might be harder to differentiate from those with normal cognitive aging and the invasive nature of these assessments (Tang et al., 2020). Using accurate, non-invasive, and cost-efficient diagnostic technology has the potential to bolster the early detection of MCI and AD. A 2019 study proposed that early detection of AD would decrease healthcare costs, and improve quality of life (Eikelboom et al., 2019). Studies have shown that MCI can affect the patterns of speech, language, and face-to-face communication in older adults (Liu et al., 2022a). There are several studies on using traditional Machine Learn- ing (ML) approaches such as Decision Tree (Neelaveni & Devasana, 2020; Davuluri1 & Rengaswamy, 2020) and K-means clustering (Davuluri1 & Rengaswamy, 2020) to detect AD with a human using collected demographic in- formation (Neelaveni & Devasana, 2020) and MRI (Davu- luri1 & Rengaswamy, 2020). Other researchers used Cross- model Augmentation for automated detection of MCI from normal cognition (NC) using speech and language pat- terns (Liu et al., 2022a). However, there is fewer work on using ML models and particularly Deep Learning models for facial video analysis and then automated detection of MCI using visual features. This paper presents our recent work on developing and using Transformer-based models for the detection of MCI using facial videos collected in the Internet-Based Conversational Engagement Clinical Trial (I-CONECT) Study project 1 (Carr, 2019; Yu et al., 2021; Wu et al., 2022) (Clinicaltrials.gov #: NCT02871921). I- CONECT Study is a randomized controlled behavioral intervention trial aimed to enhance cognitive",
      "chunk_index": 1
    },
    {
      "index": 465,
      "chunk_id": "NLI_Faithfulness2023_chunk_02",
      "source_id": "NLI_Faithfulness2023",
      "text": "videos collected in the Internet-Based Conversational Engagement Clinical Trial (I-CONECT) Study project 1 (Carr, 2019; Yu et al., 2021; Wu et al., 2022) (Clinicaltrials.gov #: NCT02871921). I- CONECT Study is a randomized controlled behavioral intervention trial aimed to enhance cognitive functions by providing frequent social interactions using video chats. Semi-structured 30-minute conversations with interviewers 1Website: https://www.i-conect.org/. J.Sun et al.: Preprint submitted to Elsevier Page 1 of 13 arXiv:2304.05292v4 [cs.CV] 5 Jan 2024 MC-ViViT: Multi-branch Classifier-ViViT to Detect Mild Cognitive Impairment in Older Adults Using Facial Videos Figure 1:The structure of the proposed MC-ViViT. D is the model depth. It represents the layer number of MC-ViViT, which is the total number of Spatial Transformers and Temporal Transformers. Layer Norm is Layer Normalization. MC-ViViT takes tubelet embedding to divide the video clips into cubes and loads these cubes into Transformer Encoder sequentially. Then, Transformer Encoder takes Factorised Encoder to extract spatio-temporal features. Finally, the output features go into Multi-branch Classifier to finish classification. were provided 4 times per week for 6 months to older adults. The study was funded by the National Institute on Aging (NIA). Facial videos contain spatial features like facial expres- sions and head poses. It can also capture facial expression change, track eye gaze motion, head pose movement, and lips activity, which are the temporal features and the key patterns in verbal and non-verbal communication (Tanaka et al., 2019; Nam et al., 2020). We hypothesize that facial videos collected during face-to-face communication in so- cial settings can provide rich information for the detection of MCI from NC subjects. To capture facial features out of facial videos, we develop a variant of the Video Vision Transformer-based model (ViViT (Arnab et al., 2021)). This model takes Vision Transformer (ViT) (Dosovitskiy et al., 2020) as its backbone and captures spatio-temporal features. ViViT has proven its value on Facial Expression Recognition (FER) (Huang et al., 2021) and violence detec- tion (Singh et al., 2022). The I-CONECT dataset has both inter- and intra-class variations, which makes it a challenging video dataset. First, it is an imbalanced dataset as the distributions of MCI and NC subjects are uneven. This makes the dataset inter-class imbalance (aka Hard-Easy sample problem). The intra- class imbalanced issue happens within each class as the I- CONECT dataset consists of videos in different lengths. Also, the quality of the videos in I-CONECT may vary from subject to subject. The occlusion",
      "chunk_index": 2
    },
    {
      "index": 466,
      "chunk_id": "NLI_Faithfulness2023_chunk_03",
      "source_id": "NLI_Faithfulness2023",
      "text": "(aka Hard-Easy sample problem). The intra- class imbalanced issue happens within each class as the I- CONECT dataset consists of videos in different lengths. Also, the quality of the videos in I-CONECT may vary from subject to subject. The occlusion and lightness problems in the videos decrease the quality of extracted spatio-temporal features. Furthermore, some subjects may not behave any symptoms of MCI, and act like NC in some video clips, which restricts ViViT from extracting enough MCI-related features. This is the so-called Positive-Negative sample problem. To address these problems, we propose a Multi-branch Classifier (MC) module to augment the representation ca- pability of ViViT. The proposed MC has four levels of Fully Connected (FC) layers, whereas the third one has 4 branches. MC provides more features but gives all features identical weight. Purely depending on MC contributes lim- itedly to the accuracy. We decide to assign different weights to features and classes while computing loss. Subsequently, we combined Focal loss (Lin et al., 2017) and AD-CORRE loss (Fard & Mahoor, 2022) into the loss function for Hard- Easy and Positive-Negative Samples (HP loss) to relief the negative effect of inter- and intra-class imbalanced issues. Specifically, the Focal Loss is responsible for the Hard-Easy sample problem, while AD-CORRE loss (Fard & Mahoor, 2022) handles the Positive-Negative sample problem. In summary, we present a new model called Multi- branch Classifier-ViViT (MC-ViViT), by integrating the aforementioned ViViT, MC, and HP Loss. We validated MC-ViViT on several themes of the I-CONECT dataset. Our experimental results show that MC-ViViT is highly capable to detect MCI from NC subjects. The overall contributions are summarized as follows: • We propose MC-ViViT to detect MCI from the inter- view videos provided by the I-CONECT Study. • We design the MC module to enrich the extracted spatio-temporal features. Its multi-branch structure helps ViViT to capture the visual features from dif- ferent perspectives. • We develop the HP loss by combining Focal loss and AD-CORRE loss. The HP loss addresses the inter- and intra-class imbalanced issues and helps the model pay attention to classes with less samples and subjects with short video lengths. J.Sun et al.: Preprint submitted to Elsevier Page 2 of 13 MC-ViViT: Multi-branch Classifier-ViViT to Detect Mild Cognitive Impairment in Older Adults Using Facial Videos The remainder of this paper is organized as follows. In Section 2, we discuss the previous works on analyzing and recognizing",
      "chunk_index": 3
    },
    {
      "index": 467,
      "chunk_id": "NLI_Faithfulness2023_chunk_04",
      "source_id": "NLI_Faithfulness2023",
      "text": "Elsevier Page 2 of 13 MC-ViViT: Multi-branch Classifier-ViViT to Detect Mild Cognitive Impairment in Older Adults Using Facial Videos The remainder of this paper is organized as follows. In Section 2, we discuss the previous works on analyzing and recognizing facial expressions and patterns of MCI people in videos using machine learning approaches. We also review the methods for augmenting features and han- dling Inter- and Intra-class imbalanced problems. Then, in Section 3, we present our new proposed model, includ- ing Tubelet Embedding, Factorised Encoder, Multi-branch Classifier, and HP Loss. Section 4 gives the details of our experiments including the dataset, data processing, evalu- ation metrics, implementation, results, discussion, and the ablation study. We finally conclude the paper with some suggestions for future research in Section 5. 2. Related Work 2.1. Detection of Mild Cognitive Impairment (MCI) Using Machine Learning Methods Detection of MCI in older adults using innovative ma- chine learning approaches has received great attention in the research community (Cavedoni et al., 2020). It is known that cognitive impairment and dementia adversely affect people's verbal and nonverbal behaviors, memory, cogni- tive functions, etc. Since the focus of this paper is on the detection of MCI using nonverbal visual patterns and behaviors expressed in facial videos of older adults using deep machine learning methods, we review the literature on using ML methods for the detection of MCI. Some researchers have utilized traditional ML approaches to de- tect MCI or more advanced stages of cognitive impairment such as Alzheimer's disease and dementia. ML methods such as Support Vector Machines (SVM), Decision Trees, PCA+SVM, K-means cluster, hierarchical clustering, and Density-based spatial clusters of applications with noise (DBSCAN) (Asim et al., 2018; Neelaveni & Devasana, 2020; Davuluri1 & Rengaswamy, 2020; Pang et al., 2023; de Mendonça et al., 2023) are the most common algorithms used in this domain. Some researchers have exploited deep learning models to detect MCI and dementia, such as CNN models with In- ception modules (Ding et al., 2019), linking a fully convolu- tional network (FCN) to a traditional multilayer perceptron (MLP) (Qiu et al., 2020), Inception-ResNet-V2 (Lu et al., 2021), and CNN-based models (Salehi et al., 2020; Poloni et al., 2022; De & Chowdhury, 2021; de Mendonça et al., 2023). Researchers have proposed that cognitive impairment and AD causes severe face recognition deficits and emotion detection deficits (Sapey-Triomphe et al., 2015; Martinez et al., 2018; Torres Mendonça De",
      "chunk_index": 4
    },
    {
      "index": 468,
      "chunk_id": "NLI_Faithfulness2023_chunk_05",
      "source_id": "NLI_Faithfulness2023",
      "text": "et al., 2022; De & Chowdhury, 2021; de Mendonça et al., 2023). Researchers have proposed that cognitive impairment and AD causes severe face recognition deficits and emotion detection deficits (Sapey-Triomphe et al., 2015; Martinez et al., 2018; Torres Mendonça De Melo Fádel et al., 2019; Dourado et al., 2019; Mazzi et al., 2020; Meléndez et al., 2020; Gil & Arroyo-Anlló, 2021). They collected the re- action of participants to specific emotions to estimate the degree of recognition deficits, which is a kind of MCI. The aforementioned work did experiments on either brain Magnetic Resonance Imaging (MRI) scans, CT scans, and X-ray Scans (Islam & Zhang, 2018; Asim et al., 2018; Ding et al., 2019; Davuluri1 & Rengaswamy, 2020; Salehi et al., 2020; Mazzi et al., 2020; Rehouma et al., 2021; Mercioni & Stavarache, 2022; Lu et al., 2021; Poloni et al., 2022; De & Chowdhury, 2021; de Mendonça et al., 2023) or demographic information, patient interviews, and structured datasets that are available on line (Sapey-Triomphe et al., 2015; Asim et al., 2018; Martinez et al., 2018; Torres Mendonça De Melo Fádel et al., 2019; Dourado et al., 2019; Neelaveni & Devasana, 2020; Davuluri1 & Rengaswamy, 2020; Qiu et al., 2020; Mazzi et al., 2020; Gil & Arroyo- Anlló, 2021; Hammoudeh et al., 2022; Pang et al., 2023). Collecting brain MRI and CT Scans are expensive. Other data modalities such as demographic information, patient interviews, and online structured datasets contain less com- plex, subtle, and subjective features. On the other hand, audiovisual collected during social interviews and face-to- face communication either in person or virtually contain rich verbal and nonverbal information. Some researchers (Nam et al., 2020) video recorded the reaction of participants to different commands and introduced eye gaze and head pose in their study. Jiang et al. (2022) indicated the influence of eye-tracking on predicting cognitive impairment too. Tanaka et al. (2019) filmed the video during the human-agent interaction and considered action units, eye gaze, and lip activity in the study. It asked three fixed queries: Q1) What's the date today?, Q2) Tell me something interesting about yourself, Q3) How did you come here today? Then, Fei et al. (2019) argued that extracting facial features in dynamic approaches benefits analyzing the evolution of facial expressions and getting spatio-temporal features. It also showed different facial feature extraction techniques, such as Geometric, Appear- ance, Holistic, and Local. Finally, Umeda-Kameyama et al.",
      "chunk_index": 5
    },
    {
      "index": 469,
      "chunk_id": "NLI_Faithfulness2023_chunk_06",
      "source_id": "NLI_Faithfulness2023",
      "text": "(2019) argued that extracting facial features in dynamic approaches benefits analyzing the evolution of facial expressions and getting spatio-temporal features. It also showed different facial feature extraction techniques, such as Geometric, Appear- ance, Holistic, and Local. Finally, Umeda-Kameyama et al. (2021) explored the capability of Xception and other CNN- based models to distinguish between people with cognitive impairment and those without dementia. Intuitively, our work explores the feasibility of an advanced deep learning model to predict MCI on the I-CONECT dataset. 2.2. Facial Video Analysis Using Deep Neural Models Automated analysis of facial videos using deep ma- chine learning and computer vision has been studied in the last decades. The applications vary from facial expression recognition (FER) (Jin et al., 2020; Liu et al., 2021a; Khan et al., 2017), surveillance (Patrikar & Parate, 2022; Ling et al., 2021; Song et al., 2022), pose estimation (Liu et al., 2022c; Sümer et al., 2021), head gesture recog- nition (Khan et al., 2022; Gashi et al., 2021; Li et al., 2022; Xia et al., 2022), medical applications (Sonawane & Sharma, 2021; Sibley et al., 2021; Villa et al., 2021), among others. This section presents related works that used deep neural network-based algorithms for facial video analysis. When it comes to FER, some works used the traditional technique of converting videos into frames. For example, Liu et al. (2021b) compressed videos into an apex frame by the encoding algorithm. Jin et al. (2020) simply cut J.Sun et al.: Preprint submitted to Elsevier Page 3 of 13 MC-ViViT: Multi-branch Classifier-ViViT to Detect Mild Cognitive Impairment in Older Adults Using Facial Videos videos into images, but it enriched features by using facial landmark points. Loading videos directly is another method. FER-GCN (Liu et al., 2021a), TimeSformer (Mohan & Popa, 2021), and three-stream network (Kim et al., 2022) simulated to analyze the video directly by taking multi frames as input. They stated that a sequence-based model helps extract temporal features. Based on multi frames, some researchers implemented a multi-frame optical flow method to get the difference between two frames (Pan et al., 2020; Liang et al., 2021). They claimed that multi-frame optical flow enriched temporal features. The aforemen- tioned studies defended their idea with good experimental results. Furthermore, Jin et al. (2020) focused on diagnosing Parkinson's Disease (PD), which inspires us to believe that Deep Learning-based models are able to detect MCI. Using more deep neural methods,",
      "chunk_index": 6
    },
    {
      "index": 470,
      "chunk_id": "NLI_Faithfulness2023_chunk_07",
      "source_id": "NLI_Faithfulness2023",
      "text": "aforemen- tioned studies defended their idea with good experimental results. Furthermore, Jin et al. (2020) focused on diagnosing Parkinson's Disease (PD), which inspires us to believe that Deep Learning-based models are able to detect MCI. Using more deep neural methods, Transformer-based models utilize modules to help capture spatio-temporal features in one stream, such as Convolutional Spatiotem- poral Encoding layer from TimeConvNets (Hou Lee & Wong, 2020) and Temporal Shift Module (TSM) from TimeSformer (Mohan & Popa, 2021) and ViViT-B/16×2 FE (Arnab et al., 2021) (A ViT-Base backbone with a tubelet size of ℎ ×𝑤 ×𝑡 = 16 × 16 × 2, FE represents factor- ized encoder). Other works utilized spatial-only attention and temporal-only attention consecutively to extract spatio- temporal features in one branch (Mohan & Popa, 2021; Bulat et al., 2021; Liu et al., 2022d). On the other hand, CNN-based models usually take multi-stream structure to extract spatio-temporal features separately, such as deep temporal-spatial networks (Pan et al., 2020), TMSAU-Net (Liang et al., 2021), and three- stream network (Kim et al., 2022). These models take optical flow, frame difference, and motion vector methods to help create temporal information instead of learning from the raw images through the model. Models with 3D Convo- lutional operation can extract spatio-temporal information in one stream, but their large hyperparameter scale improves computational complexity. Comprehensively, to avoid the extra operation and im- plement the model thoroughly and efficiently, this research selected ViViT FE as the backbone of the proposed model. 2.3. Feature Enrichment using MLP Head Inception Net (Szegedy et al., 2015) is a classic CNN model. It consists of repeated Inception Modules, which ap- ply various scaled filters to extract features on the same layer and broaden the network. The advantage of the Inception Module is to enrich the features. Multi-models also help augment representation and improve accuracy. Moreover, some researchers have shown the effect of multi-models on MCI detection (Zhang et al., 2019; Lee et al., 2019; Liu et al., 2020; Kang et al., 2021; Naz et al., 2022). In addition, a good Fully Connected layer benefits the model performance, such as XnODR and XnIDR (Sun et al., 2021). The Inception Net and multi-models inspired the design of the Multi-branch Classifier (MC) (see Section 3.3). 2.4. Inter-class and Intra-class Imbalanced Problems Inter-class and intra-class imbalanced problems usually prevent ML models from predicting well. In (He et al., 2019) the authors explained that the",
      "chunk_index": 7
    },
    {
      "index": 471,
      "chunk_id": "NLI_Faithfulness2023_chunk_08",
      "source_id": "NLI_Faithfulness2023",
      "text": "multi-models inspired the design of the Multi-branch Classifier (MC) (see Section 3.3). 2.4. Inter-class and Intra-class Imbalanced Problems Inter-class and intra-class imbalanced problems usually prevent ML models from predicting well. In (He et al., 2019) the authors explained that the inter-class imbalanced problem occurs when the number of samples in some classes is much larger than those in other classes, which causes the misclassification of rare class examples. The intra-class imbalanced dataset means that a class consists of several sub-concepts or sub-clusters. Moreover, at least one of the concepts or clusters is represented by a significantly less number of samples than the others (He et al., 2019). The number of features from each sub-concept are un- equal, which weakens the performance of classifier (Sam- path et al., 2021). Focal Loss is powerful to equalize inter-class imbal- anced datasets (Lin et al., 2017). Moreover, classes with more features are easier to detect than those with less features. This is also called the Hard-Easy sample problem. To address the intra-class imbalanced issue, researchers have proposed methods such as feature selection (He et al., 2019), resampling (He et al., 2019; Liu et al., 2021c), and designing loss function (Farzaneh & Qi, 2020; Ngo & Yoon, 2020; Farzaneh & Qi, 2021; Fard & Mahoor, 2022). For example, EPIMTS (early prediction on the imbalanced mul- tivariate time series) fused feature selection and resampling, which calls MUDSG to resample based on the extracted core shapelets (He et al., 2019). Liu et al. (2021c) uti- lized Soft Hard Example Mining (SHEM) to re-balance the error density distribution. To achieve intra-class bal- ancing, they assigned instance-wise sampling probabilities according to the prediction of the current ensemble model 𝐹𝑡−1(⋅)(Liu et al., 2021c). Other works focus on upgrading loss function. They proposed Discriminant Distribution- Agnostic loss (DDA loss) (Farzaneh & Qi, 2020), Weighted Center Loss (WCL) (Ngo & Yoon, 2020), Deep Attentive Center Loss (DACL) (Farzaneh & Qi, 2021), and Quadru- plet loss (Tian et al., 2021). These loss functions are tightly related to center loss. The goal of these methods is to achieve intra-class compactness and inter-class separation. The AD-CORRE loss presented in (Fard & Mahoor, 2022) aims to handle inter- and intra-class imbalanced issues, but it focuses on addressing the problem by digging the correlation between embeddings in the mini-batch level. Then, it embeds the influence of whole training samples into batches incrementally instead of directly (Fard &",
      "chunk_index": 8
    },
    {
      "index": 472,
      "chunk_id": "NLI_Faithfulness2023_chunk_09",
      "source_id": "NLI_Faithfulness2023",
      "text": "handle inter- and intra-class imbalanced issues, but it focuses on addressing the problem by digging the correlation between embeddings in the mini-batch level. Then, it embeds the influence of whole training samples into batches incrementally instead of directly (Fard & Mahoor, 2022). This has less computational cost. In the I-CONECT dataset, the number of samples with MCI is apparently more than that with NC. Thus, subjects labeled in MCI are easy samples, while those labeled in NC are hard ones. To emphasize and increase the weight of NC, we apply Focal Loss to address the Hard-Easy sample problem. Within each class, the number of frames from each video is unequal. This is particularly important as interviewees with MCI may behave cognitively normal sometimes or show the symptoms of MCI in other parts of the video. J.Sun et al.: Preprint submitted to Elsevier Page 4 of 13 MC-ViViT: Multi-branch Classifier-ViViT to Detect Mild Cognitive Impairment in Older Adults Using Facial Videos The proportion of normal and symptom parts is imbalanced and skewed too. This can affect the accuracy of the final classification as we use majority voting to assign a video to MCI or NC based on the number of classified clips. In general, the I-CONECT dataset has one inter-class imbalanced problem and two intra-class imbalanced prob- lems. Based on the above discussion, we aim to achieve both inter- and intra-class separation, which is different from the target of the aforementioned loss functions such as DDA (Farzaneh & Qi, 2020), WCL (Ngo & Yoon, 2020), and DACL (Farzaneh & Qi, 2021). To avoid suboptimal performance, after comprehensive thought, we are inclined to use the Adaptive Correlation (AD-CORRE) loss (Fard & Mahoor, 2022) to solve the Positive-Negative sample problem. 3. Multi-branch Classifier-ViViT (MC-ViViT) The proposed Multi-branch Classifier-ViViT (MC-ViViT) is an end-to-end model, which predicts whether a video segment in the I-CONECT dataset belongs to the MCI group or normal cognition (NC). The backbone of the network is ViViT FE (Arnab et al., 2021) as its Factorised Encoder structure can extract Spatio-Temporal features efficiently. To enrich the features and improve performance, MC-ViViT utilizes multi branches for classification. Specifically, MC- ViViT splits each video into several video cubes by Tubelet Embedding. Then, it embeds the video cubes into tokens and prepares the input tokens by concatenating class tokens and positional embedding. Subsequently, MC-ViViT applies the Transformer Encoder with Factorised Encoder to extract Spatio-Temporal features",
      "chunk_index": 9
    },
    {
      "index": 473,
      "chunk_id": "NLI_Faithfulness2023_chunk_10",
      "source_id": "NLI_Faithfulness2023",
      "text": "into several video cubes by Tubelet Embedding. Then, it embeds the video cubes into tokens and prepares the input tokens by concatenating class tokens and positional embedding. Subsequently, MC-ViViT applies the Transformer Encoder with Factorised Encoder to extract Spatio-Temporal features from the input tokens. Finally, the model computes the prediction score of each class using multi-branch classifiers. The rest of this section explains the model components in detail. 3.1. Tubelet Embedding Review Figure 2:The structure of Tubelet Embedding. Following the pattern of ViViT (Arnab et al., 2021), the cubic patch is non-overlapping too. Suppose that the tensor shape of one video clip is [𝑇,𝐻,𝑊, 3], where 𝑇 is the frame numbers, 𝐻 and 𝑊 are the height and the width of each frame, and 3represents the RGB channels. Then, the tensor size of each cubic patch is [𝑡,ℎ,𝑤, 3]. 𝑡, ℎ, and 𝑤are the size of the corresponding temporal, height, and width dimensions. 𝑛𝑡 =⌊𝑇 𝑡 ⌋2, 𝑛ℎ =⌊𝐻 ℎ ⌋, and 𝑛𝑤 =⌊𝑊 𝑤 ⌋ denote the token number of respective temporal, height, and width dimensions. The Tubelet Embedding changes the input unit from a 2D patch to a 3D cube, which contains temporal information (see Fig. 2). Therefore, it is the foundation of offering Spatio-Temporal information in MC-ViViT. 3.2. Factorised Encoder (FE) Review Figure 3: The structure of FE. CLS is class token. The green and purple capsules are positional embeddings. The rest capsules are tubelet embedding. MC is a Multi-branch classifier. Fig. 1 shows that in ViViT (Arnab et al., 2021), the Transformer Encoder takes the combined positional and embedded cubic tokens as input, and it contains the Self- Attention module followed by the FeedForward module. Eq. (1) presents the details of the input tokens. 𝐳 =[𝑧𝑐𝑙𝑠,𝐄𝑥1,𝐄𝑥2,⋯,𝐄𝑥𝑛ℎ𝑛𝑤𝑛𝑡]+ 𝐩, (1) where 𝑧𝑐𝑙𝑠 is a learnable class token, [𝑥1,𝑥2,⋯,𝑥𝑛𝑡𝑛ℎ𝑛𝑤] is the tensor of cubic patches, 𝑥𝑖 ∈ ℝℎ×𝑤×𝑡 𝑛ℎ𝑛𝑤𝑛𝑡 is the number of cubic patches.𝐸is the linear projection to embed cubic patches. In addition, 𝐩 ∈ ℝ(𝑛ℎ𝑛𝑤𝑛𝑡+1)×𝑑 represents a learnable positional embedding. 𝑑 is the dimension of the embedded token. Given the layer 𝑙, the Self-Attention module consists of Layer Normalization (LN) and Multi-Head Dot-Product Attention, while the FeedForward (FF) module includes LN and MLP. Both modules use skip-connection to enrich fea- tures and prevent gradient vanishing. Moreover, Arnab et al. (2021) proposed four Multi-Head Self-Attention (MHSA) modules. This work selected FE because Arnab et al. (2021) showed that",
      "chunk_index": 10
    },
    {
      "index": 474,
      "chunk_id": "NLI_Faithfulness2023_chunk_11",
      "source_id": "NLI_Faithfulness2023",
      "text": "(FF) module includes LN and MLP. Both modules use skip-connection to enrich fea- tures and prevent gradient vanishing. Moreover, Arnab et al. (2021) proposed four Multi-Head Self-Attention (MHSA) modules. This work selected FE because Arnab et al. (2021) showed that ViViT FE performed the best of all four versions. Fig. 3 shows that FE has two parts, Spatial Trans- former Encoder ( 𝐿𝑠) and Temporal Transformer Encoder (𝐿𝑡). Spatial Transformer Encoder extracts latent represen- tations on the different tokens with the same temporal index. Therefore, these latent representations are spatial features. Then, these latent representations with different temporal indexes come to the Temporal Transformer Encoder, which 2⌊⋅∕⋅⌋ means to get the largest integer not greater than ⋅∕⋅. J.Sun et al.: Preprint submitted to Elsevier Page 5 of 13 MC-ViViT: Multi-branch Classifier-ViViT to Detect Mild Cognitive Impairment in Older Adults Using Facial Videos studies the interactions between tokens from different time steps. Thus, the Temporal Transformer Encoder can mine temporal features. Assuming that the 𝐿𝑠 and 𝐿𝑡 repeats 𝑛𝑠𝑝 and 𝑛𝑡𝑝 times, respectively. FF processes the output of FE and returns the Sequence-Level Spatio-Temporal feature. The above explanation can be summarized as Eqs. 2-5. 𝐲𝑙 𝑠 =𝐿𝑙 𝑠𝑛𝑠𝑝 (⋯𝐿𝑙 𝑠1 (𝐿𝑁(𝐳𝑙))⋯) (2) 𝐲𝑙 𝑡 =𝐿𝑙 𝑡𝑛𝑡𝑝 (⋯𝐿𝑙 𝑡1 (𝐲𝑙 𝑠)⋯) (3) 𝐲𝑙 𝐹𝐸 =𝐲𝑙 𝑡 +𝐳𝑙 (4) 𝐲𝑙 𝐹𝐹 =𝑀𝐿𝑃(𝐿𝑁(𝐲𝑙 𝐹𝐸))+ 𝐲𝑙 𝐹𝐸 (5) 3.3. Multi-branch Classifier (MC) Inspired by Inception Module (Szegedy et al., 2015) and multi-models (Zhang et al., 2019; Lee et al., 2019; Liu et al., 2020; Kang et al., 2021; Naz et al., 2022), MC takes multi-branch structure to provide different views and enrich representation as well. Different from Inception Module and multi-models, MC only does linear projection at each branch instead of convolutional operation or the neural network. In detail, MC consists of four FC layers. The dimension change is 64→ 16→ [8,8,8,8]→ concatenate to 32 → 𝑛𝑢𝑚_𝑐𝑙𝑎𝑠𝑠, where 𝑛𝑢𝑚_𝑐𝑙𝑎𝑠𝑠 represents the number of class. In our experiment, 𝑛𝑢𝑚_𝑐𝑙𝑎𝑠𝑠 is 2. When we convert the dimension from 16 to 32, Fig. 4 shows that we apply a multi-branch structure to convert the dimension to 8 and repeat 4 times. Then, we concatenate them as a 32-dimensional tensor. With this structure, MC can provide more features and view the object from different angles. Figure 4:The structure of MC. ⊕represents concatenation. 3.4. Loss function for Hard-Easy and Positive-Negative Samples (HP Loss) HP Loss has two components, Focal Loss",
      "chunk_index": 11
    },
    {
      "index": 475,
      "chunk_id": "NLI_Faithfulness2023_chunk_12",
      "source_id": "NLI_Faithfulness2023",
      "text": "tensor. With this structure, MC can provide more features and view the object from different angles. Figure 4:The structure of MC. ⊕represents concatenation. 3.4. Loss function for Hard-Easy and Positive-Negative Samples (HP Loss) HP Loss has two components, Focal Loss (Lin et al., 2017) and AD-CORRE(FD) (Fard & Mahoor, 2022). AD- CORRE(FD) is the FD (Feature Discriminator) component of AD-CORRE Loss. 3.4.1. Focal Loss Review Focal Loss addresses the imbalance aspect of Hard-Easy samples by generating the weight based on sample numbers. It derives from𝛼-balanced Cross Entropy loss. Then, adding a modulating factor (1 −𝑝𝑚𝑐𝑖)𝛾 defined in Eq. (7) to the cross entropy loss, with tunable focusing parameter 𝛾 >0, makes the final Focal Loss. The above can be summarized as Eq. (6). 𝐹𝐿(𝑝𝑚𝑐𝑖)=− 𝛼𝑚𝑐𝑖(1− 𝑝𝑚𝑐𝑖)𝛾𝑙𝑜𝑔(𝑝𝑚𝑐𝑖) (6) where 𝛼is a weighting factor. 𝛼 ∈[0,1]for class MCI and 1− 𝛼 for class NC. 𝑝𝑚𝑐𝑖 represents the probability of the frame sequence attributing to MCI. 𝑝𝑚𝑐𝑖 = { 𝑝 𝑖𝑓 𝑦 =𝑀𝐶𝐼 1− 𝑝 𝑜𝑡ℎ𝑒𝑟𝑤𝑖𝑠𝑒 (7) where 𝑝is the model's predict score,𝑦is the predicted label. 3.4.2. AD-CORRE(FD) Review AD-CORRE(FD) focuses on the correlation between the samples within a mini-batch. This prevents the im- balanced dataset from affecting the prediction. Therefore, the model pays attention to the class with less samples too. AD-CORRE(FD) comprises four key components of AD-CORRE(FD), Variance Eraser (Beta Matrix 𝛽𝑛×𝑛), At- tention Map Matrix ( Ω𝑛×𝑛), Harmony Matrix ( Φ𝑛×𝑛), and Correlation Matrix (CORM). In short, AD-CORRE(FD) is a weighted mean absolute error and is shown in Eq. (8). 𝐹𝐷 = 1 𝑘𝑛2 𝑘∑ 𝑙=0 𝑛∑ 𝑖=0 𝑛∑ 𝑗=0 𝛽[𝑖,𝑗]Ω[𝑖,𝑗] |Φ[𝑖,𝑗]− 𝐶𝑂𝑅𝑀𝑙[𝑖,𝑗]|, (8) where 𝑘is the class number, 𝑛is the size of mini-batch. The difference between Φ𝑛×𝑛 and CORM𝑛×𝑛 is the core of the AD-CORRE(FD) loss. In general, AD-CORRE(FD) focuses on the correlation between samples within the mini-batch. This prevents the imbalanced dataset from affecting the prediction. AD-CORRE(FD) Analysis Intuitively, there are two ways to address the intra-class imbalance. Given a class, we assign trainable weight to each subject based on frame numbers. Or, we attribute different weights to positive and negative sequences within the same video. Either way, however, will stimulate more dispute and drive the problem complexity going exacerbation. On this condition, AD-CORRE(FD)'s intention is to ignore the frames-imbalanced issue and the positive and negative sample problem and to only focus on the classi- fication task at the mini-batch level. The intervention of AD-CORRE(FD) avoids the deeper",
      "chunk_index": 12
    },
    {
      "index": 476,
      "chunk_id": "NLI_Faithfulness2023_chunk_13",
      "source_id": "NLI_Faithfulness2023",
      "text": "going exacerbation. On this condition, AD-CORRE(FD)'s intention is to ignore the frames-imbalanced issue and the positive and negative sample problem and to only focus on the classi- fication task at the mini-batch level. The intervention of AD-CORRE(FD) avoids the deeper conflict. In addition, AD-CORRE(FD) substantially decreases the computation complexity because it only analyzes the similarity between embeddings within the mini-batch. It accumulatively col- lects class distribution information from previous batches to improve its adaptive weight. Calculating within the mini- batch is also the reason that AD-CORRE(FD) benefits from detecting minority samples. J.Sun et al.: Preprint submitted to Elsevier Page 6 of 13 MC-ViViT: Multi-branch Classifier-ViViT to Detect Mild Cognitive Impairment in Older Adults Using Facial Videos Table 1 The details of researched themes. Subject Number is the total number of videos in the corresponding theme. Male/Female and MCI/NC show the gender distribution and category distribution. Frame Number is the total image number of each theme after converting videos into frames. Crafts Hobbies Day Time TV Shows Movie Genres School Subjects Subject Number 32 41 35 39 Male/Female 11/21 11/30 10/25 11/28 MCI/NC 20/12 20/21 21/14 22/17 Frame Number 691872 859872 770656 797968 3.4.3. Combine Two Loss Functions Finally, Eq. (9) shows the HP Loss is the sum of Focal Loss and AD-CORRE (FD). We follow the pattern of AD- CORRE Loss and set 𝜆as 0.5as well. 𝐻𝑃 𝐿𝑜𝑠𝑠=𝐹𝐿(𝑝𝑚𝑐𝑖)+𝜆∗𝐴𝐷−𝐶𝑂𝑅𝑅𝐸(𝐹𝐷) (9) 4. Experiments In this section, we first introduce the I-CONECT dataset, evaluation metrics, and implementation details. Then, we explain our experiments, present the results, report the ablation study, and evaluate the results. 4.1. Dataset The Internet-Based Conversational Engagement Clini- cal Trial (I-CONECT) is to explore how social conversation can help improve memory and may prevent dementia or Alzheimer's disease in older adults. The study followed research participants aged 75 and older recruited from Port- land, Oregon, or Detroit, Michigan in the USA. The study randomized 186 participants. The participants had a 30- minute long chat per session with standardized interviewers (conversational staff) 4 times a week over a period of 6 months. The control group received only weekly 10-minute phone check-ins. All participants connected with the con- versational staff using study-provided user-friendly devices. Conversations were semi-structured with some standard prompts and daily topics, but once the conversation started, it flowed naturally for fun and engaging conversations. Out of 186 randomized participants, 86 participants are diagnosed as NC, while 100 people are",
      "chunk_index": 13
    },
    {
      "index": 477,
      "chunk_id": "NLI_Faithfulness2023_chunk_14",
      "source_id": "NLI_Faithfulness2023",
      "text": "user-friendly devices. Conversations were semi-structured with some standard prompts and daily topics, but once the conversation started, it flowed naturally for fun and engaging conversations. Out of 186 randomized participants, 86 participants are diagnosed as NC, while 100 people are diagnosed as MCI. In each 30 minutes-session, participants discussed one of the 161 selected themes, such as Summer Time, Health Care, Military Service, Television Technology, etc. The conversational interactions were recorded as videoes. In the current study, we selected the following 4 themes: Crafts Hobbies, Day Time TV Shows, Movie Genres, and School Subjects. The interviewees talked about their crafts hobbies. They listed their favorite daytime TV shows, discussed preferred movie genres, and showed examples. Then, they also recalled their campus lives in School Subjects. Table 1 shows data exploration. We used sequence-based approaches rather than frame- based approaches to process the dataset and extracted facial features via a dynamic approach. 4.2. Data Processing (a) (b) Figure 5:Two sample frames from the video dataset. In (a), the window of the interviewee is bigger than that of the inter- viewer because the interviewer was speaking. Conversely, in (b), the interviewer was talking so that her window is bigger than the interviewee's. The facial features of interviewees are valuable to our research. Usually, the subject talks like NC at the start and the end, which prevents the model from predicting accurately. Furthermore, Fig. 5 shows that every video has a complex background and interviewers' faces, which has a negative influence on predicting results. To address this problem, we decide to drop the first 3 minutes and the last 2.5 minutes of each video. Then, EasyOCR helps detect whether or not the upper half frame contains the subject ID in the given frame. For frames with the subject ID on the top, we implemented RetineFace (Deng et al., 2020) to crop the participants' facial images and dropped all irrelevant and interfering information. In detail, we calculate the area of detected faces and the Intersection over Union (IoU) of them. If IoU < 0.05, we kept the bigger face like Fig. 5a. Otherwise, we continue to process the next frame and drop the frame like Fig. 5b. Finally, we browse the kept images and manually delete the wrongly saved ones. We used this method to process different themes. The four themes that are studied in this paper may have videos from 40 participants or",
      "chunk_index": 14
    },
    {
      "index": 478,
      "chunk_id": "NLI_Faithfulness2023_chunk_15",
      "source_id": "NLI_Faithfulness2023",
      "text": "like Fig. 5b. Finally, we browse the kept images and manually delete the wrongly saved ones. We used this method to process different themes. The four themes that are studied in this paper may have videos from 40 participants or less. For the sake of taking advantage of videos thoroughly and predicting better, we select K-fold Cross Validation and use Eq. (10) to calculate 𝐾 for each theme. 𝐾 =⌊𝑁𝑉𝑖𝑑𝑒𝑜 ∕𝐿𝐹𝑜𝑙𝑑⌋ (10) where 𝑁𝑉𝑖𝑑𝑒𝑜 and 𝐿𝐹𝑜𝑙𝑑 are the video number of the selected theme and that of each fold. We set 𝐿𝐹𝑜𝑙𝑑 = 3in our study. Moreover, videos in each fold belong to different participants. K-fold Cross Validation sets one fold as the test set and the resting folds as the training sets every time. It does not choose any fold as the validation set. Here, we use the test set as the validation one during the training process. This is to avoid overfitting the model (Ashtiani et al., 2021). In fact, attributing some of the subjects to a third subset inevitably shrinks the size of the training set. Thus, the model may face monotonous facial features and perform weakly in recognizing different samples (Ashtiani et al., 2021). Furthermore, the model may also take valuable representations as noise or outliers due to insufficient data samples, which adversely affects the prediction. Hence, we do not use a validation set in our experiments. J.Sun et al.: Preprint submitted to Elsevier Page 7 of 13 MC-ViViT: Multi-branch Classifier-ViViT to Detect Mild Cognitive Impairment in Older Adults Using Facial Videos Table 2 The prediction accuracy of detecting MCI on 4 themes using K-fold evaluation. Test Theme Crafts Hobbies Day Time TV Shows Movie Genres School Subjects Fold Num 11 14 11 13 Accuracy 29/32 36/41 30/35 35/39 90.63% 85.37% 85.71% 89.74% F1 93.03% 85.00% 88.37% 90.47% AUC 0.6042 0.4952 0.6122 0.5294 MCI/NC 20/12 20/21 21/14 22/17 Sensitivity/ 100%/ 85%/ 90.48%/ 86.36%/ Specificity 75% 85.71% 78.57% 94.12% Simultaneously, inspired by Hou Lee & Wong (2020); Liu et al. (2021a); Mohan & Popa (2021); Kim et al. (2022), this study used multi frames as the input. Thus, we cut each video into a certain number of fixed-length segments as inputs. Let 𝐿3 be the number of consecutive frames in a divided segment, 𝑁 represents the number of total frames. ⌊𝑁∕𝐿⌋ is the number of segments for each video. 4.3. Evaluation Metrics Prediction accuracy, F1 score, AUC",
      "chunk_index": 15
    },
    {
      "index": 479,
      "chunk_id": "NLI_Faithfulness2023_chunk_16",
      "source_id": "NLI_Faithfulness2023",
      "text": "fixed-length segments as inputs. Let 𝐿3 be the number of consecutive frames in a divided segment, 𝑁 represents the number of total frames. ⌊𝑁∕𝐿⌋ is the number of segments for each video. 4.3. Evaluation Metrics Prediction accuracy, F1 score, AUC (Area Under the Receiver Operating Characteristic Curve), Sensitivity, and Specificity are the evaluation metrics in the experiment. McNamara and Martin stated that sensitivity and specificity are intrinsic measures of a case definition or diagnostic test, whereas predictive values vary with the prevalence of a condition within a population (McNamara & Martin, 2023). Eq. (11) presents the statistical definition of sensitivity, which is similar to recall. Eq. (12) is the equation of specificity. Sensitivity = True Positive True Positive + False Negative (11) Specificity = True Negative True Negative + False Positive (12) 4.4. Implementation Details We performed data augmentation to each segment, which contains the random horizontal and vertical flip, random rotation, and center crop. All frames within one set do exactly same augmentation. The batch size is 80. Following the pattern of (Hou Lee & Wong, 2020; Liu et al., 2021a; Mohan & Popa, 2021; Kim et al., 2022),𝐿is 16. The initialized learning rate is 1e-6. We used Adam optimizer and Cyclic scheduler with the mode of triangular2. The loss function is HP loss. The epoch number is 30. We coded the network by PyTorch 1.12.0+cu116 and ran experiments on the NVIDIA GTX 1080Ti GPU. To completely evaluate the new proposed architecture, we designed three experiments (See Sections 4.5 - 4.7). 4.5. Experiment: K-fold Validation This experiment is to test the capability of MC-ViViT on predicting MCI in each theme. The inputs are all the corresponding videos. 3𝐿and 𝑇 from Section 3.1 are identical. Table 3 The prediction accuracy of cross-theme fashion. Three themes are used for training a model and the left theme is used for test. Test Theme Crafts Hobbies Day Time TV Shows Movie Genres School Subjects Fold Num 11 14 11 10 Accuracy 27/32 33/41 30/35 32/39 84.38% 80.49% 85.71% 82.05% F1 87.80% 82.61% 88.89% 89.36% AUC 0.7 0.7452 0.6701 0.4773 MCI/NC 20/12 20/21 21/14 22/17 Sensitivity/ 90%/ 95%/ 95.24%/ 95.45%/ Specificity 75% 66.67% 71.43% 76.47% We report the results in Table 2. MC-ViViT achieves over 85% accuracy on all four themes. For example, MC- ViViT predicts 29 out of 32 subjects correctly on the theme Crafts Hobbies, which reaches the highest accuracy, 90.63%. The",
      "chunk_index": 16
    },
    {
      "index": 480,
      "chunk_id": "NLI_Faithfulness2023_chunk_17",
      "source_id": "NLI_Faithfulness2023",
      "text": "76.47% We report the results in Table 2. MC-ViViT achieves over 85% accuracy on all four themes. For example, MC- ViViT predicts 29 out of 32 subjects correctly on the theme Crafts Hobbies, which reaches the highest accuracy, 90.63%. The accuracy of School Subjects is 89.74%, which is close to 90%. Three of the F1 Scores are more than 88%, and only the F1 score of Day Time TV Shows is 85%. The accuracy and F1 Score solidly support that MC-ViViT has robust performance. The other evidence comes from AUC Score. The average AUC of all four themes is 0.56. Given that not all the clips have MCI features in the video, even for subjects with MCI, it is reasonable to predict parts of their video sequences as NC. Due to this reason, some subjects will be correctly predicted as MCI with low scores. Thus, the average AUC value is around 0.6. Table 2 also shows that, on all four themes, MCI/NC has a related balanced prediction distribution. Their Sensitivity and Specificity are high as well. For instance, the sample of Day Time TV Shows comprises 20 MCI subjects and 21 NC ones. The Sensitivity over Specificity is 85%/85.71%. 4.6. Experiment: cross themes evaluation This experiment is to test if MC-ViViT can integrate and learn complex features from a given set of training themes and detect MCI from different test themes correctly. We train the video on three themes and predict the rest. This time, we resize all the frames as96×96 to train faster. There are no overlapped subjects between the training sets and the test sets. Table 3 shows all the results. MC-ViViT still provides stable performance. It achieves over 82% accuracy and over 87% F1 Score on the three themes. The highest accuracy is 85.71% (predict 30 out of 35 samples correctly) on the Movie Genres, while the highest F1 Score is 89.36% on the School Subject. The accuracy and F1 Score of this experiment are still robust to show that MC-ViViT can predict well. The average AUC Score of this experiment is 0.648, which is higher than 0.56 from Section 4.5. This value also accords with the analysis from Section 4.5. It supports that MC-ViViT can make objective decisions. In the meanwhile, MC-ViViT only reaches 80.49% accuracy and 82.61% F1 Score on the Day Time TV Shows. This is due to the smaller frame size. Subsequently, Fig.",
      "chunk_index": 17
    },
    {
      "index": 481,
      "chunk_id": "NLI_Faithfulness2023_chunk_18",
      "source_id": "NLI_Faithfulness2023",
      "text": "analysis from Section 4.5. It supports that MC-ViViT can make objective decisions. In the meanwhile, MC-ViViT only reaches 80.49% accuracy and 82.61% F1 Score on the Day Time TV Shows. This is due to the smaller frame size. Subsequently, Fig. 6a presents that, the smaller frame size affects the Sensitivity over Specificity of MCI/NC. J.Sun et al.: Preprint submitted to Elsevier Page 8 of 13 MC-ViViT: Multi-branch Classifier-ViViT to Detect Mild Cognitive Impairment in Older Adults Using Facial Videos (a) (b) (c) (d) Figure 6: (a): The prediction accuracy of MC-ViViT on four themes. The blue line represents the result of Section 4.5. Experiment: K-fold Validation, while the yellow one is that of Section 4.6. Experiment: cross themes evaluation. (b): The visualization of tuning temporal dimension 𝑡. 𝑡=4, MC-ViViT performs the best. (c): The visualization of tuning classifier MC. With MC, the model performs better. (d): The visualization of tuning loss function. With HP Loss, MC-ViViT performs better. Table 4 The prediction accuracy on 4 themes with different t size. Test Theme Crafts Hobbies Day Time TV Shows Movie Genres School Subjects t = 2 84.38% 58.54% 82.86% 71.79% t = 4 90.63% 85.37% 85.71% 89.74% t = 8 81.25% 75.61% 85.71% 76.92% Specifically, it weakens the capability of detecting negative samples, which is NC in our experiment. Thereby, the results of Sensitivity/Specificity are less balance than those from Section 4.5. For example, the Sensitivity/Specificity of movie genres increases from 90.48%/78.57% (Table 2) to 95.24%/71.43%. Smaller frame size decreasing the impor- tant features of NC makes the NC harder to detect. There- fore, Specificity in this experiment is lower than Section 4.5. 4.7. Ablation Study This section discusses the effect of temporal dimension 𝑡, MC, and HP Loss on the performance of MC-ViViT (Sections 4.7.1-4.7.3). 4.7.1. Study the most proper temporal dimension𝑡 In MC-ViViT, the frame number, 𝑇, is 16, which can be divided by [1,2,4,8,16]. To keep Tubelet Embedding and reduce the burden of computation, we set the temporal dimension 𝑡as [2,4,8]alternatively to study the best value for this research. Table 4 and Fig. 6b show that setting 𝑡 = 4 gives the best performance under the current configuration on four themes. When 𝑡 = 2, the tubelet lacks adequate temporal features for making good predictions. When𝑡=8, the tubelet, as a long sequence or long memory, includes many useful temporal representations for sure. But it also requires tubelet embedding",
      "chunk_index": 18
    },
    {
      "index": 482,
      "chunk_id": "NLI_Faithfulness2023_chunk_19",
      "source_id": "NLI_Faithfulness2023",
      "text": "on four themes. When 𝑡 = 2, the tubelet lacks adequate temporal features for making good predictions. When𝑡=8, the tubelet, as a long sequence or long memory, includes many useful temporal representations for sure. But it also requires tubelet embedding to have a larger fixed dimension length. However, increasing this fixed length will boost the model's total parameters, which makes the MC-ViViT computationally expensive. In addition, other Transformer-based models also set 𝑡 = 4while analyzing facial videos such as MTV (Yan et al., 2022) and MSVT (Yu et al., 2023). Using 𝑡=4 helps Transformer-based models capture local-consecutive incon- sistency efficiently and reach sufficient performance (Yu Table 5 The prediction accuracy on 4 themes with or without MC. No multi-branch means that we drop the multi-branch during the experiment. Test Theme Crafts Hobbies Day Time TV Shows Movie Genres School Subjects MC 90.63% 85.37% 85.71% 89.74% No MC 78.13% 63.41% 74.29% 74.36% Table 6 The prediction accuracy on 4 themes with different loss functions. Test Theme Crafts Hobbies Day Time TV Shows Movie Genres School Subjects HP Loss 90.63% 85.37% 85.71% 89.74% Focal Loss 68.75% 68.29% 68.57% 66.67% AD-CORRE(FD) 53.13% 60.98% 60.00% 56.41% et al., 2023). It is also to get the best trade-off between performance and efficiency (Yan et al., 2022). Thereby, we choose 𝑡=4. 4.7.2. Study the effect of MC This study is to evaluate the effect of MC structure. In the experiment, we dropped the multi-branch structure and changed the dimension from 16 to 𝑛𝑢𝑚_𝑐𝑙𝑎𝑠𝑠 directly. The dimension changes will be 64 → 16 → 𝑛𝑢𝑚_𝑐𝑙𝑎𝑠𝑠. We collected the related experimental results in Table 5. It shows that, for each theme, the accuracy of ViViT with MC is at least 10% higher than that without MC. Fig. 6c bolsters this view. Therefore, the MC module provides more features and benefits the ViViT in predicting better. 4.7.3. Study the effect of HP Loss This study is to test the influence of each component of HP Loss on the prediction. We first did experiments with Focal Loss only. Then, we did experiments with AD- CORRE(FD) Loss only. Table 6 contains all the results and shows that MC-ViViT using Focal loss performs better than that using AD-CORRE(FD). In the meanwhile, the accuracy of MC-ViViT using HP loss surpasses at least 14% than that using either Focal loss or AD-CORRE(FD). Fig. 6d clearly displays this huge disparity. Thereby, Table 6 and Fig.",
      "chunk_index": 19
    },
    {
      "index": 483,
      "chunk_id": "NLI_Faithfulness2023_chunk_20",
      "source_id": "NLI_Faithfulness2023",
      "text": "performs better than that using AD-CORRE(FD). In the meanwhile, the accuracy of MC-ViViT using HP loss surpasses at least 14% than that using either Focal loss or AD-CORRE(FD). Fig. 6d clearly displays this huge disparity. Thereby, Table 6 and Fig. 6d uphold that HP Loss helps the MC-ViViT perform better than each individual component does. J.Sun et al.: Preprint submitted to Elsevier Page 9 of 13 MC-ViViT: Multi-branch Classifier-ViViT to Detect Mild Cognitive Impairment in Older Adults Using Facial Videos Table 7 Chen et al. (2020) and Liu et al. (2022b) mostly focused on language and medical image data in detecting MCI using the I-CONECT dataset, while we are one of the first groups to use non-medical visual data. These two papers have tried different subsets of I-CONECT and different data modalities. Nevertheless, our model (MC-ViViT) outperforms their work. Data Modality Accuracy F1 Chen et al. (2020) Text 79.15% - Liu et al. (2022b) Text, MRI 87.00% 89.00% MC-ViViT (Ours) Video 90.63% 93.03% 5. Discussion The experiments presented in Sections 4.5 and 4.6 and the comparison in Table 7 indicate that MC-ViViT can detect MCI with promising accuracy. For a selected theme, MC-ViViT can predict subjects not included in the model training very well. It can also infer the representations learned from many themes to correctly predict the sub- jects from unseen themes. The MC-ViViT's robust perfor- mance convinces us that analyzing semi-structured inter- view videos is useful to detect MCI. In the meanwhile, ViViT, as the backbone, mainly pro- vides good representations to the final prediction. These good representations include spatial features and tempo- ral ones. Different from manually described and learned patterns such as head pose, eye gaze, eye-tracking, and lip activities, Fig. 7 shows that MC-ViViT pays attention to the different areas of the face. For example, it focuses on regions of the forehead, eyelids, nose, cheek, and jaw. This correlates with the statement that ViViT is capable to monitor the motion of all facial muscles and discover many slight changes and features. Furthermore, Sections 4.5 and 4.6 indicate that these features are very important to decide if the subject has MCI or not. In short, ViViT takes video clips as the input and extracts spatio-temporal features in one stream, which enables the model to make a com- prehensive decision. Subsequently, the ablation study from Section 4.7 suggests that both MC and HP Loss improve the accuracy.",
      "chunk_index": 20
    },
    {
      "index": 484,
      "chunk_id": "NLI_Faithfulness2023_chunk_21",
      "source_id": "NLI_Faithfulness2023",
      "text": "takes video clips as the input and extracts spatio-temporal features in one stream, which enables the model to make a com- prehensive decision. Subsequently, the ablation study from Section 4.7 suggests that both MC and HP Loss improve the accuracy. MC broadens the network through the multi- branch structure, which provides different perspectives to analyze the subject's features. MC strengthens the fea- tures' richness, which benefits in making correct decisions. HP Loss, then, concentrates on prediction equality. The I- CONECT dataset is a very imbalanced dataset. Table 1 indicates that every theme has more MCI subjects than NC ones. The focal Loss part of HP loss assigns more weight to the class with less subjects and aims to solve the inter- class imbalanced issues. Within each class, the different video length causes the inequivalence of extracted frames from each subject. The AD-CORRE(FD) part of HP Loss changes the background of the problem from the intra-class level to the mini-batch level. Within the mini-batch, AD- CORRE(FD) tackles the classification task by measuring the similarity between embedding features, which wisely resolves the imbalance crisis and reduces the computational complexity. Thereby, HP loss is essential to the MC-ViViT. The experimental results from Tables 5 and 6 uphold the above discussion. In addition, Table 2 indicates that MC- ViViT can learn features well under one particular theme, while Table 3 convinces us that features from other themes can help detect MCI from NC on the left one. Subjects perform similar features on different themes. Figure 7: The extracted spatio-temporal feature maps on 16 consecutive frames from two subjects. MC-ViViT focuses more on the bright areas and pays less attention on the dark ones. Given that there are 𝑛𝑡 tubelets, where 𝑛𝑡 = ⌊𝑇 𝑡 ⌋ = ⌊16 4 ⌋ = 4, there are four feature maps. To simplify the visualization, we pick the first frame of each tubelet to represent it. In general, there are two key findings. The first one is that Transformer-based models are capable to detect MCI in the early stage only by analyzing facial features; the second one is that semi-structured interview videos can provide plenty of features to help the model make quality predictions while merely requiring the tablet computer or mobile phone to collect. This costs much less than collecting conventional medical images like MRI. These two findings reveal that MC-ViViT has the potential to detect other diseases,",
      "chunk_index": 21
    },
    {
      "index": 485,
      "chunk_id": "NLI_Faithfulness2023_chunk_22",
      "source_id": "NLI_Faithfulness2023",
      "text": "the model make quality predictions while merely requiring the tablet computer or mobile phone to collect. This costs much less than collecting conventional medical images like MRI. These two findings reveal that MC-ViViT has the potential to detect other diseases, such as dementia and Parkinson's Disease (PD), in the early stage. Moreover, the corresponding data collection becomes convenient and easy to access. Even a cell phone can help film facial videos. On the other side, due to the lack of a consistent video recording environment, the videos' qualities vary a lot. From what we observed in this research, the video quality significantly affects the prediction. We notice that MC- ViViT usually predicts high-clarity videos rightly with high prediction scores. Conversely, MC-ViViT is easy to give unclear videos with low prediction scores. Or even MC- ViViT classifies those videos with the wrong labels. This explains the reason that Sensitivity versus Specificity is far from equivalence. Future work will focus on establishing a mechanism to evaluate the video quality score. The future model will implement this score to design a new loss function that helps tune the training process. J.Sun et al.: Preprint submitted to Elsevier Page 10 of 13 MC-ViViT: Multi-branch Classifier-ViViT to Detect Mild Cognitive Impairment in Older Adults Using Facial Videos 6. Conclusion This paper presented MC-ViViT to detect MCI from NC via facial videos of the semi-structured interviews (I- CONECT dataset). The experiments show that MC-ViViT can predict well with the help of MC and HP Loss. Simul- taneously, it shows that the Transformer-based model can tell whether or not people have MCI merely by analyzing facial features from the semi-structured interview videos. Compared to the other datasets mentioned in Section 1, the I-CONECT dataset is easy to access, low cost, time-saving, flexible, and less restricted. It is very valuable to spread and propagate. Acknowledgement This research was partially funded by a grant to the University of Denver by Colorado Office of Economic Development and International Trade and two grants from National Institutes of Health (NIH) (R01AG051628 and R01AG056102). We would like to thank Miss. Jisu Lee for proofreading this paper. Author Contributions Jian Sun: Conceptualization, Methodology, Software, Validation, Formal analysis, Investigation, Data Curation, Writing - Original Draft, Writing - Review & Editing, Visualization. Hiroko H. Dodge:Investigation, Resources, Data Cura- tion, Writing-Review and Editing. Mohammad H. Mahoor:Resources, Writing-Review and Editing, Supervision, Project Administration. References Alzheimer's_Association (2021). 2021 alzheimer's",
      "chunk_index": 22
    },
    {
      "index": 486,
      "chunk_id": "NLI_Faithfulness2023_chunk_23",
      "source_id": "NLI_Faithfulness2023",
      "text": "Validation, Formal analysis, Investigation, Data Curation, Writing - Original Draft, Writing - Review & Editing, Visualization. Hiroko H. Dodge:Investigation, Resources, Data Cura- tion, Writing-Review and Editing. Mohammad H. Mahoor:Resources, Writing-Review and Editing, Supervision, Project Administration. References Alzheimer's_Association (2021). 2021 alzheimer's disease facts and figures. Alzheimer's & Dementia, 17, 327-406. Arnab, A., Dehghani, M., Heigold, G., Sun, C., Lu ˇci´c, M., & Schmid, C. (2021). Vivit: A video vision transformer. In Proceedings of the IEEE/CVF International Conference on Computer Vision (pp. 6836- 6846). Ashtiani, S.-H. M., Javanmardi, S., Jahanbanifard, M., Martynenko, A., & Verbeek, F. J. (2021). Detection of mulberry ripeness stages using deep learning models. IEEE Access, 9, 100380-100394. Asim, Y ., Raza, B., Malik, A. K., Rathore, S., Hussain, L., & Iftikhar, M. A. (2018). A multi-modal, multi-atlas-based approach for alzheimer detection via machine learning. International Journal of Imaging Systems and Technology, 28, 113-123. Bulat, A., Perez Rua, J. M., Sudhakaran, S., Martinez, B., & Tzimiropou- los, G. (2021). Space-time mixing attention for video transformer. In M. Ranzato, A. Beygelzimer, Y . Dauphin, P. Liang, & J. W. Vaughan (Eds.), Advances in Neural Information Processing Systems(pp. 19594- 19607). Curran Associates, Inc. volume 34. Carr, D. (2019). How to successfully navigate a revise-and-resubmit decision and handle rejections. Innovation in Aging, 3, S224. Cavedoni, S., Chirico, A., Pedroli, E., Cipresso, P., & Riva, G. (2020). Digital biomarkers for the early detection of mild cognitive impairment: artificial intelligence meets virtual reality. Frontiers in Human Neuro- science, 14, 245. Chen, L., Dodge, H. H., & Asgari, M. (2020). Topic-based measures of conversation for detecting mild cognitive impairment. In Proceedings of the conference. Association for Computational Linguistics. Meeting (p. 63). NIH Public Access volume 2020. Davuluri1, R., & Rengaswamy, R. (2020). A survey of different machine learning models for alzheimer disease prediction. International Journal of Emerging Trends in Engineering Research, 8. De, A., & Chowdhury, A. S. (2021). Dti based alzheimer's disease classification with rank modulated fusion of cnns and random forest. Expert Systems with Applications, 169, 114338. Deng, J., Guo, J., Ververas, E., Kotsia, I., & Zafeiriou, S. (2020). Reti- naface: Single-shot multi-level face localisation in the wild. In Pro- ceedings of the IEEE/CVF conference on computer vision and pattern recognition (pp. 5203-5212). Ding, Y ., Sohn, J. H., Kawczynski, M. G., Trivedi, H., Harnish, R., Jenkins, N. W., Lituiev, D., Copeland, T. P., Aboian, M. S., Mari Aparici, C., Behr, S. C.,",
      "chunk_index": 23
    },
    {
      "index": 487,
      "chunk_id": "NLI_Faithfulness2023_chunk_24",
      "source_id": "NLI_Faithfulness2023",
      "text": "IEEE/CVF conference on computer vision and pattern recognition (pp. 5203-5212). Ding, Y ., Sohn, J. H., Kawczynski, M. G., Trivedi, H., Harnish, R., Jenkins, N. W., Lituiev, D., Copeland, T. P., Aboian, M. S., Mari Aparici, C., Behr, S. C., Flavell, R. R., Huang, S.-Y ., Zalocusky, K. A., Nardo, L., Seo, Y ., Hawkins, R. A., Hernandez Pampaloni, M., Hadley, D., & Franc, B. L. (2019). A deep learning model to predict a diagnosis of alzheimer disease by using 18f-fdg pet of the brain. Radiology, 290, 456-464. PMID: 30398430. Dosovitskiy, A., Beyer, L., Kolesnikov, A., Weissenborn, D., Zhai, X., Unterthiner, T., Dehghani, M., Minderer, M., Heigold, G., Gelly, S. et al. (2020). An image is worth 16x16 words: Transformers for image recognition at scale. arXiv preprint arXiv:2010.11929, abs/2010.11929. Dourado, M., Fdel, B., Neto, J., Alves, G., & Alves, C. (2019). Facial ex- pression recognition patterns in mild and moderate alzheimer's disease. Journal of Alzheimer's Disease, 69, 1-11. Eikelboom, W. S., Singleton, E., Van Den Berg, E., Coesmans, M., Mattace Raso, F., Van Bruchem, R. L., Goudzwaard, J. A., De Jong, F. J., Koopmanschap, M., Den Heijer, T. et al. (2019). Early recognition and treatment of neuropsychiatric symptoms to improve quality of life in early alzheimer's disease: protocol of the beat-it study. Alzheimer's research & therapy, 11, 1-12. Fard, A. P., & Mahoor, M. H. (2022). Ad-corre: Adaptive correlation-based loss for facial expression recognition in the wild. IEEE Access , 10, 26756-26768. Farzaneh, A. H., & Qi, X. (2020). Discriminant distribution-agnostic loss for facial expression recognition in the wild. In 2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition Workshops (CVPRW) (pp. 1631-1639). Farzaneh, A. H., & Qi, X. (2021). Facial expression recognition in the wild via deep attentive center loss. In Proceedings of the IEEE/CVF Winter Conference on Applications of Computer Vision (WACV) (pp. 2402-2411). Fei, Z., Yang, E., Li, D. D.-U., Butler, S., Ijomah, W., & Zhou, H. (2019). A survey on computer vision techniques for detecting facial features towards the early diagnosis of mild cognitive impairment in the elderly. Systems Science & Control Engineering, 7, 252-263. Gashi, S., Saeed, A., Vicini, A., Di Lascio, E., & Santini, S. (2021). Hier- archical classification and transfer learning to recognize head gestures and facial expressions using earbuds. In Proceedings of the 2021 International Conference on Multimodal Interaction (pp. 168-176). Gil, R., & Arroyo-Anlló, E. M. (2021). Alzheimer's disease and face",
      "chunk_index": 24
    },
    {
      "index": 488,
      "chunk_id": "NLI_Faithfulness2023_chunk_25",
      "source_id": "NLI_Faithfulness2023",
      "text": "S. (2021). Hier- archical classification and transfer learning to recognize head gestures and facial expressions using earbuds. In Proceedings of the 2021 International Conference on Multimodal Interaction (pp. 168-176). Gil, R., & Arroyo-Anlló, E. M. (2021). Alzheimer's disease and face masks in times of covid-19. Journal of Alzheimer's disease : JAD, 79, 9-14. Hammoudeh, M., Nagavelli, U., Samanta, D., & Chakraborty, P. (2022). Machine learning technology-based heart disease detection models. Journal of Healthcare Engineering, 2022. He, G., Zhao, W., Xia, X., Peng, R., & Wu, X. (2019). An ensemble of shapelet-based classifiers on inter-class and intra-class imbalanced multivariate time series at the early stage. Soft Computing, 23, 6097- 6114. Hou Lee, J. R., & Wong, A. (2020). Timeconvnets: A deep time windowed convolution neural network design for real-time video facial expression recognition. In 2020 17th Conference on Computer and Robot Vision (CRV) (pp. 9-16). Huang, Z., Qing, Z., Wang, X., Feng, Y ., Zhang, S., Jiang, J., Xia, Z., Tang, M., Sang, N., & Ang Jr, M. H. (2021). Towards training stronger video J.Sun et al.: Preprint submitted to Elsevier Page 11 of 13 MC-ViViT: Multi-branch Classifier-ViViT to Detect Mild Cognitive Impairment in Older Adults Using Facial Videos vision transformers for epic-kitchens-100 action recognition. arXiv preprint arXiv:2106.05058, . Islam, J., & Zhang, Y . (2018). Early diagnosis of alzheimer's disease: A neuroimaging study with deep learning architectures. In 2018 IEEE/CVF Conference on Computer Vision and Pattern Recognition Workshops (CVPRW) (pp. 1962-19622). Jiang, Z., Seyedi, S., Haque, R. U., Pongos, A. L., Vickers, K. L., Manzanares, C. M., Lah, J. J., Levey, A. I., & Clifford, G. D. (2022). Automated analysis of facial emotions in subjects with cognitive im- pairment. PLOS ONE, 17, 1-19. Jin, B., Qu, Y ., Zhang, L., & Gao, Z. (2020). Diagnosing parkinson disease through facial expression recognition: video analysis. Journal of medical Internet research, 22, e18697. Kang, W., Lin, L., Zhang, B., Shen, X., Wu, S., Initiative, A. D. N. et al. (2021). Multi-model and multi-slice ensemble learning architecture based on 2d convolutional neural networks for alzheimer's disease diagnosis. Computers in Biology and Medicine, 136, 104678. Khan, A. R., Saba, T., Khan, M. Z., Fati, S. M., & Khan, M. U. G. (2022). Classification of human's activities from gesture recognition in live videos using deep learning. Concurrency and Computation: Practice and Experience, 34, e6825. Khan, S., Xu, G., Chan, R., & Yan, H. (2017). An online",
      "chunk_index": 25
    },
    {
      "index": 489,
      "chunk_id": "NLI_Faithfulness2023_chunk_26",
      "source_id": "NLI_Faithfulness2023",
      "text": "M., & Khan, M. U. G. (2022). Classification of human's activities from gesture recognition in live videos using deep learning. Concurrency and Computation: Practice and Experience, 34, e6825. Khan, S., Xu, G., Chan, R., & Yan, H. (2017). An online spatio-temporal tensor learning model for visual tracking and its applications to facial expression recognition. Expert Systems with Applications, 90, 427-438. Kim, J.-H., Kim, N., & Won, C. S. (2022). Facial expression recognition with swin transformer. arXiv preprint arXiv:2203.13472, . Lee, G., Nho, K., Kang, B., Sohn, K.-A., & Kim, D. (2019). Predicting alzheimer's disease progression using multi-modal deep learning ap- proach. Scientific reports, 9, 1952. Li, J., Xu, S., & Qin, X. (2022). A hierarchical model for learning to understand head gesture videos. Pattern Recognition, 121, 108256. Liang, L., Lang, C., Li, Y ., Feng, S., & Zhao, J. (2021). Fine-grained facial expression recognition in the wild. IEEE Transactions on Information Forensics and Security, 16, 482-494. Lin, T.-Y ., Goyal, P., Girshick, R., He, K., & Dollár, P. (2017). Focal loss for dense object detection. In Proceedings of the IEEE international conference on computer vision (pp. 2980-2988). Ling, X., Liang, J., Wang, D., & Yang, J. (2021). A facial expres- sion recognition system for smart learning based on yolo and vision transformer. In 2021 7th International Conference on Computing and Artificial Intelligence ICCAI 2021 (pp. 178-182). New York, NY , USA: Association for Computing Machinery. Liu, D., Zhang, H., & Zhou, P. (2021a). Video-based facial expression recognition using graph convolutional networks. In 2020 25th Interna- tional Conference on Pattern Recognition (ICPR)(pp. 607-614). IEEE. Liu, G., Xue, Z., Zhan, L., Dodge, H. H., & Zhou, J. (2022a). Detection of mild cognitive impairment from language markers with crossmodal augmentation. In PACIFIC SYMPOSIUM ON BIOCOMPUTING 2023: Kohala Coast, Hawaii, USA, 3-7 January 2023 (pp. 7-18). World Scientific. Liu, G., Xue, Z., Zhan, L., Dodge, H. H., & Zhou, J. (2022b). Detection of mild cognitive impairment from language markers with crossmodal augmentation. In PACIFIC SYMPOSIUM ON BIOCOMPUTING 2023: Kohala Coast, Hawaii, USA, 3-7 January 2023 (pp. 7-18). World Scientific. Liu, H., Fang, S., Zhang, Z., Li, D., Lin, K., & Wang, J. (2022c). Mfdnet: Collaborative poses perception and matrix fisher distribution for head pose estimation. IEEE Transactions on Multimedia, 24, 2449-2460. Liu, M., Li, F., Yan, H., Wang, K., Ma, Y ., Shen, L., Xu, M., Initiative, A. D. N. et al. (2020). A",
      "chunk_index": 26
    },
    {
      "index": 490,
      "chunk_id": "NLI_Faithfulness2023_chunk_27",
      "source_id": "NLI_Faithfulness2023",
      "text": "Collaborative poses perception and matrix fisher distribution for head pose estimation. IEEE Transactions on Multimedia, 24, 2449-2460. Liu, M., Li, F., Yan, H., Wang, K., Ma, Y ., Shen, L., Xu, M., Initiative, A. D. N. et al. (2020). A multi-model deep convolutional neural network for automatic hippocampus segmentation and classification in alzheimer's disease. Neuroimage, 208, 116459. Liu, X., Jin, L., Han, X., & You, J. (2021b). Mutual information regularized identity-aware facial expression recognition in compressed video. Pattern Recognition, 119, 108105. Liu, Z., Ning, J., Cao, Y ., Wei, Y ., Zhang, Z., Lin, S., & Hu, H. (2022d). Video swin transformer. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)(pp. 3202-3211). Liu, Z., Wei, P., Wei, Z., Yu, B., Jiang, J., Cao, W., Bian, J., & Chang, Y . (2021c). Handling inter-class and intra-class imbalance in class- imbalanced learning. arXiv preprint arXiv:2111.12791, . Lu, B., Li, H.-X., Chang, Z.-K., Li, L., Chen, N.-X., Zhu, Z.-C., Zhou, H.-X., Li, X.-Y ., Wang, Y .-W., Cui, S.-X., Deng, Z.-Y ., Fan, Z., Yang, H., Chen, X., Thompson, P. M., Castellanos, F. X., Yan, C.-G., & for the Alzheimer's Disease Neuroimaging Initiative (2021). A practical alzheimer disease classifier via brain imaging-based deep learning on 85,721 samples. bioRxiv, . Martinez, M., Multani, N., Anor, C. J., Misquitta, K., Tang-Wai, D. F., Keren, R., Fox, S., Lang, A. E., Marras, C., & Tartaglia, M. C. (2018). Emotion detection deficits and decreased empathy in patients with alzheimer's disease and parkinson's disease affect caregiver mood and burden. Frontiers in Aging Neuroscience, 10. Mazzi, C., Massironi, G., Sanchez-Lopez, J., De Togni, L., & Savazzi, S. (2020). Face recognition deficits in a patient with alzheimer's disease: Amnesia or agnosia? the importance of electrophysiological markers for differential diagnosis. Frontiers in Aging Neuroscience, 12. McNamara, L. A., & Martin, S. W. (2023). 1 - principles of epidemiology and public health. In S. S. Long (Ed.), Principles and Practice of Pediatric Infectious Diseases (Sixth Edition)(pp. 1-9.e1). Philadelphia: Elsevier. (Sixth edition ed.). Meléndez, J. C., Satorres, E., & Oliva, I. (2020). Comparing the effect of interference on an emotional stroop task in older adults with and without alzheimer's disease. Journal of Alzheimer's disease, 73, 1445-1453. de Mendonça, L. J. C., Ferrari, R. J., Initiative, A. D. N. et al. (2023). Alzheimer's disease classification based on graph kernel svms con- structed with 3d texture features extracted from mr images.",
      "chunk_index": 27
    },
    {
      "index": 491,
      "chunk_id": "NLI_Faithfulness2023_chunk_28",
      "source_id": "NLI_Faithfulness2023",
      "text": "disease. Journal of Alzheimer's disease, 73, 1445-1453. de Mendonça, L. J. C., Ferrari, R. J., Initiative, A. D. N. et al. (2023). Alzheimer's disease classification based on graph kernel svms con- structed with 3d texture features extracted from mr images. Expert Systems with Applications, 211, 118633. Mercioni, M.-A., & Stavarache, L. L. (2022). Disease diagnosis with medical imaging using deep learning. In Future of Information and Communication Conference (pp. 198-208). Springer Springer Interna- tional Publishing. Mohan, B., & Popa, M. (2021). Temporal based emotion recognition inspired by activity recognition models. In 2021 9th International Con- ference on Affective Computing and Intelligent Interaction Workshops and Demos (ACIIW) (pp. 01-08). Nam, U., Lee, K., Ko, H., Lee, J.-Y ., & Lee, E. C. (2020). Analyzing facial and eye movements to screen for alzheimer's disease. Sensors, 20. National_Institute_on_Aging (2021). What is mild cognitive impairment? Naz, S., Ashraf, A., & Zaib, A. (2022). Transfer learning using freeze fea- tures for alzheimer neurological disorder detection using adni dataset. Multimedia Systems, 28, 85-94. Neelaveni, J., & Devasana, M. (2020). Alzheimer disease prediction using machine learning algorithms. In 2020 6th International Conference on Advanced Computing and Communication Systems (ICACCS)(pp. 101- 104). Ngo, Q. T., & Yoon, S. (2020). Weighted-center loss for facial expressions recognition. In 2020 International Conference on Information and Communication Technology Convergence (ICTC)(pp. 54-56). Pan, X., Zhang, S., Guo, W., Zhao, X., Chuang, Y ., Chen, Y ., & Zhang, H. (2020). Video-based facial expression recognition using deep temporal- spatial networks. IETE Technical Review, 37, 402-409. Pang, Y ., Kukull, W., Sano, M., Albin, R., Shen, C., Zhou, J., & Dodge, H. H. (2023). Predicting progression from normal to mci and from mci to ad using clinical variables in the national alzheimer's coordinating center uniform data set version 3: Application of machine learning models and a probability calculator. The journal of prevention of Alzheimer's disease, 10, 301-313. Patrikar, D. R., & Parate, M. R. (2022). Anomaly detection using edge computing in video surveillance system. International Journal of Multimedia Information Retrieval, 11, 85-110. Poloni, K. M., Ferrari, R. J., Initiative, A. D. N. et al. (2022). A deep ensemble hippocampal cnn model for brain age estimation applied to alzheimer's diagnosis. Expert Systems with Applications, 195, 116622. Qiu, S., Joshi, P. S., Miller, M. I., Xue, C., Zhou, X., Karjadi, C., Chang, G. H., Joshi, A. S., Dwyer, B., Zhu, S. et al. (2020). Development and validation",
      "chunk_index": 28
    },
    {
      "index": 492,
      "chunk_id": "NLI_Faithfulness2023_chunk_29",
      "source_id": "NLI_Faithfulness2023",
      "text": "applied to alzheimer's diagnosis. Expert Systems with Applications, 195, 116622. Qiu, S., Joshi, P. S., Miller, M. I., Xue, C., Zhou, X., Karjadi, C., Chang, G. H., Joshi, A. S., Dwyer, B., Zhu, S. et al. (2020). Development and validation of an interpretable deep learning framework for alzheimer's J.Sun et al.: Preprint submitted to Elsevier Page 12 of 13 MC-ViViT: Multi-branch Classifier-ViViT to Detect Mild Cognitive Impairment in Older Adults Using Facial Videos disease classification. Brain, 143, 1920-1933. Rehouma, R., Buchert, M., & Chen, Y .-P. P. (2021). Machine learning for medical imaging-based covid-19 detection and diagnosis. International Journal of Intelligent Systems, 36, 5085-5115. Salehi, A. W., Baglat, P., Sharma, B. B., Gupta, G., & Upadhya, A. (2020). A cnn model: Earlier diagnosis and classification of alzheimer disease using mri. In 2020 International Conference on Smart Electronics and Communication (ICOSEC) (pp. 156-161). Sampath, V ., Maurtua, I., Aguilar Martín, J. J., & Gutierrez, A. (2021). A survey on generative adversarial networks for imbalance problems in computer vision tasks. Journal of big Data, 8, 1-59. Sapey-Triomphe, L.-A., Heckemann, R. A., Boublay, N., Dorey, J.-M., Hénaff, M.-A., Rouch, I., Padovan, C., Hammers, A., Krolak-Salmon, P., & Initiative, A. D. N. (2015). Neuroanatomical correlates of recognizing face expressions in mild stages of alzheimer's disease. PLoS ONE, 10, e0143586. Sibley, K. G., Girges, C., Hoque, E., & Foltynie, T. (2021). Video-based analyses of parkinson's disease severity: A brief review. Journal of Parkinson's disease, 11, S83-S93. Singh, S., Dewangan, S., Krishna, G. S., Tyagi, V ., & Reddy, S. (2022). Video vision transformers for violence detection. arXiv preprint arXiv:2209.03561, . Sonawane, B., & Sharma, P. (2021). Review of automated emotion-based quantification of facial expression in parkinson's patients. The Visual Computer, 37, 1151-1167. Song, Y ., Tang, H., Meng, F., Wang, C., Wu, M., Shu, Z., & Tong, G. (2022). A transformer-based low-resolution face recognition method via on-and-offline knowledge distillation. Neurocomputing, 509, 193-205. Sümer, O., Goldberg, P., D'Mello, S., Gerjets, P., Trautwein, U., & Kasneci, E. (2021). Multimodal engagement analysis from facial videos in the classroom. IEEE Transactions on Affective Computing, (pp. 1-1). Sun, J., Fard, A. P., & Mahoor, M. H. (2021). Xnodr and xnidr: Two accurate and fast fully connected layers for convolutional neural networks. arXiv preprint arXiv:2111.10854, . Szegedy, C., Liu, W., Jia, Y ., Sermanet, P., Reed, S., Anguelov, D., Erhan, D., Vanhoucke, V ., & Rabinovich, A. (2015). Going deeper with",
      "chunk_index": 29
    },
    {
      "index": 493,
      "chunk_id": "NLI_Faithfulness2023_chunk_30",
      "source_id": "NLI_Faithfulness2023",
      "text": "Two accurate and fast fully connected layers for convolutional neural networks. arXiv preprint arXiv:2111.10854, . Szegedy, C., Liu, W., Jia, Y ., Sermanet, P., Reed, S., Anguelov, D., Erhan, D., Vanhoucke, V ., & Rabinovich, A. (2015). Going deeper with convolutions. In 2015 IEEE Conference on Computer Vision and Pattern Recognition (CVPR) (pp. 1-9). Tanaka, H., Adachi, H., Kazui, H., Ikeda, M., Kudo, T., & Nakamura, S. (2019). Detecting dementia from face in human-agent interaction. Ad- junct of the 2019 International Conference on Multimodal Interaction , (pp. 1-6). Tang, F., Uchendu, I., Wang, F., Dodge, H. H., & Zhou, J. (2020). Scalable diagnostic screening of mild cognitive impairment using ai dialogue agent. Alzheimer's & Dementia, 16. Tian, Y ., Li, M., & Wang, D. (2021). Dfer-net: Recognizing facial expression in the wild. In 2021 IEEE International Conference on Image Processing (ICIP) (pp. 2334-2338). Torres Mendonça De Melo Fádel, B., Santos De Carvalho, R. L., Belfort Almeida Dos Santos, T. T., & Dourado, M. C. N. (2019). Facial expression recognition in alzheimer's disease: A systematic review. Journal of clinical and experimental neuropsychology, 41, 192-203. Umeda-Kameyama, Y ., Kameyama, M., Tanaka, T., Son, B.-K., Kojima, T., Fukasawa, M., Iizuka, T., Ogawa, S., Iijima, K., & Akishita, M. (2021). Screening of alzheimer's disease by facial complexion using artificial intelligence. Aging, 13, 1765-1772. Villa, A., Sankar, V ., & Shiboski, C. (2021). Tele (oral) medicine: A new approach during the covid-19 crisis. Oral Diseases, 27, 744. Wu, C.-Y ., Mattek, N., Wild, K., Miller, L. M., Kaye, J. A., Silbert, L. C., & Dodge, H. H. (2022). Can changes in social contact (frequency and mode) mitigate low mood before and during the covid-19 pandemic? the i-conect project. Journal of the American Geriatrics Society , 70, 669-676. Xia, J., Zhang, H., Wen, S., Yang, S., & Xu, M. (2022). An efficient multitask neural network for face alignment, head pose estimation and face tracking. Expert Systems with Applications, 205, 117368. Yan, S., Xiong, X., Arnab, A., Lu, Z., Zhang, M., Sun, C., & Schmid, C. (2022). Multiview transformers for video recognition. InProceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR) (pp. 3333-3343). Yu, K., Wild, K., Potempa, K., Hampstead, B. M., Lichtenberg, P. A., Struble, L. M., Pruitt, P., Alfaro, E. L., Lindsley, J., MacDonald, M. et al. (2021). The internet-based conversational engagement clinical trial (i- conect) in socially isolated adults 75+ years",
      "chunk_index": 30
    },
    {
      "index": 494,
      "chunk_id": "NLI_Faithfulness2023_chunk_31",
      "source_id": "NLI_Faithfulness2023",
      "text": "K., Wild, K., Potempa, K., Hampstead, B. M., Lichtenberg, P. A., Struble, L. M., Pruitt, P., Alfaro, E. L., Lindsley, J., MacDonald, M. et al. (2021). The internet-based conversational engagement clinical trial (i- conect) in socially isolated adults 75+ years old: randomized controlled trial protocol and covid-19 related study modifications. Frontiers in digital health, 3, 714813. Yu, Y ., Ni, R., Zhao, Y ., Yang, S., Xia, F., Jiang, N., & Zhao, G. (2023). Msvt: Multiple spatiotemporal views transformer for deepfake video detection. IEEE Transactions on Circuits and Systems for Video Technology, (pp. 1-1). Zhang, F., Li, Z., Zhang, B., Du, H., Wang, B., & Zhang, X. (2019). Multi-modal deep learning model for auxiliary diagnosis of alzheimer's disease. Neurocomputing, 361, 185-195. J.Sun et al.: Preprint submitted to Elsevier Page 13 of 13",
      "chunk_index": 31
    },
    {
      "index": 495,
      "chunk_id": "DALLEval2024_chunk_00",
      "source_id": "DALLEval2024",
      "text": "arXiv:2306.06504v1 [math.DG] 10 Jun 2023 Hadamard-type variation formulae for the eigenvalues of a class of second-order elliptic operators and its applications C. L. Cunha 1, J. N. Gomes 2 and M. A. M. Marrocos 3 Abstract. We use variational methods to derive Hadamard-type formula e for the eigenvalues of a class of elliptic operators on a comp act Riemannian manifold M. We then apply the latter in the following context. Consider a family of elliptic operators which is parametrized by eithe r the set of all Cr- Riemannian metrics on M or the set of all Cr-diﬀeomorphisms on a domain into M. In either case we prove that if a subset of the parametrizati ons set yields a simple spectrum of the operator, then it is necessar ily a generic subset. We also analyse the behavior of the eigenvalues when the metr ic evolves along the Ricci ﬂow on a closed Riemannian manifold, and we prove, u nder a suitable hypothesis, that they increase. 1. Introduction Let (Mn,g) be an n-dimensional compact Riemannian manifold with boundary ∂M, and let L be the class of elliptic diﬀerential operators given by L f := div η(T∇f) := div( T∇f) − ⟨∇η,T ∇f⟩, (1.1) where T is a symmetric positive deﬁnite (1 ,1)-tensor on Mn and f,η ∈ C∞(M). Here, div stands for the divergence of smooth vector ﬁelds and ∇ for the gradient of smooth functions. We allow both T and η depend on the metric g = ⟨,⟩. This class of operators uniﬁes several particular cases that are w ell-studied in the literature, e.g., the Laplace-Beltrami operator, the drifted Lapla cian, the Cheng- Yau operator, among other geometric operators which naturally a rise in geometric analysis. For instance, when T is divergence free, then the operator L is likely to have applications in physics, see, e.g., Serre [ 19]. We highlight that Serre's work deals with divergence free positive deﬁnite symmetric tensors and ﬂ uid dynamics, there we can ﬁnd examples and know where these tensors occur. W e refer the reader to Alencar, Neto and Zhou [ 1], Ara´ ujo Filho and Gomes [ 2], Gomes and Miranda [ 9] or Navarro [ 18] for more related discussions. In this paper, we study the behavior of the spectrum of L by means of Hadamard-type variational formulae for its eigenvalues. It is orga nized as follows. We give some preliminaries in",
      "chunk_index": 0
    },
    {
      "index": 496,
      "chunk_id": "DALLEval2024_chunk_01",
      "source_id": "DALLEval2024",
      "text": "or Navarro [ 18] for more related discussions. In this paper, we study the behavior of the spectrum of L by means of Hadamard-type variational formulae for its eigenvalues. It is orga nized as follows. We give some preliminaries in Section 2 and some general comments on v aria- tional formulae in Section 3. In Section 4, we consider the eigenvalue problem for the operator Lg with the Dirichlet boundary condition. Theorem 1 deal with the 2010 Mathematics Subject Classiﬁcation. Primary 47A05, 47A75; Secondary 47A55. Key words and phrases. Elliptic operator, Hadamard-type formulas, Eigenvalues. 2 C. L. CUNHA 1, J. N. GOMES 2 AND M. A. M. MARROCOS 3 generic simplicity of the eigenvalues of the operator Lg(·) = div η(Tg∇·), where the Tg-family satisﬁes the property P. A precise description of this latter property is given in Section 5. Taking into account property P, we prove that all eigenvalues of Lg are generically simple. The perturbation of the domain case is settled by Propo- sition 6 in which the η-dependency on the parameter t is crucial. In Section 6, we deal with the generic simplicity of the eigenvalues of the operator Lg with the Neu- mann boundary condition. In Section 7, we deal with domain variation considering both boundary conditions, Dirichlet and Neumann, and proving that the eigenval- ues are generically simple. Especially, Theorem 2 shows that the multip licity of an eigenvalue can be reduced by small perturbation of the domain. In S ection 8, we deal with extremal metrics. A domain Ω is local minimizer (local maximize r) for the kth-eigenvalue µk of L if for any analytic volume-preserving deformation Ω t, the function t↦→µk(t) admits a local minimum (local maximum) at t= 0. For the Dirichlet Laplacian case, it is known that if a domain is local minimizer (res p. local maximizer) for µk with µk > µk−1 (resp. µk < µk+1), then the norm |∂φ/∂ν| of the normal derivative of the eigenfunction φof µk is constant (see [ 20]). We prove a similar result for L in Theorem 3. In Section 9, we analyze the behavior of the eigenvalues of the operator Lg(t) along the Ricci Flow. For instance, we prove a monotonicity for one-parameter family of eigenvalues λ(t) of Lg(t) along the Ricci ﬂow on a closed homogeneous n-dimensional Riemannian manifold, see Theorem 4. We also prove the monotonicity of the eigenvalues of div",
      "chunk_index": 1
    },
    {
      "index": 497,
      "chunk_id": "DALLEval2024_chunk_02",
      "source_id": "DALLEval2024",
      "text": "Ricci Flow. For instance, we prove a monotonicity for one-parameter family of eigenvalues λ(t) of Lg(t) along the Ricci ﬂow on a closed homogeneous n-dimensional Riemannian manifold, see Theorem 4. We also prove the monotonicity of the eigenvalues of div η(ψ∇u) envolving by the Ricci ﬂow on three-dimensional Riemannian manifold having strictly po sitive Ricci curvature initially, see Theorem 5. In particular, since the solution t o the Ricci ﬂow becomes extinct in ﬁnite time, with additional hypothesis over the fu nction ψ, we prove that lim t→δ λ(t) = ∞, see Theorem 6. It is worth mentioning here some of the main articles related to our wo rk. Uhlenbeck [21] showed that on a closed Riemannian manifold ( Mn,g) there exists a residual subset Γ into Mr for which the spectrum of the Laplace-Beltrami operator ∆ g is simple, where Mr, 2 ≤ r < ∞, stands for the set of all Cr-Riemannian metrics g on Mn. Canzani [ 5] considered elliptic, formally self-adjoint, conformally covariant op - erator Pg of order macting on smooth sections over a closed Riemannian manifold (Mn,g), and proved, under special conditions over the eigenspaces of Pg, that the set of all functions f ∈ C∞(M) for which Pef g has only simple nonzero eigenvalues is a residual subset of C∞(M). El Souﬁ and Ilias [ 20] established necessary and suﬃcient conditions for a do- main to be critical, locally minimizing or locally maximizing the kth-eigenvalue of the Laplace-Beltrami operator. As an application they obtained a c haracteriza- tion for critical domains of the trace of the heat kernel under Diric hlet boundary condition. Cao, Hou and Ling [ 6] showed a monotonicity Hadamard-type formula for the ﬁrst eigenvalue of −∆ + aR (0 < a <1/2) on a closed surface with nonnega- tive scalar curvature R under the Ricci ﬂow. Recall that, this latter ﬂow was introduced by Hamilton [ 11] to study the geometry of positive Ricci curvature on three-dimensional closed Riemannian manifolds. Currently, the Ricc i ﬂow stands as a powerful tool in studying the geometry and topology for lower dimensional manifolds. ON EIGENV ALUES OF A CLASS OF SECOND ORDER ELLIPTIC OPERATORS 3 2. Preliminaries Let dm = e−ηdM be the weighted volume form on ( Mn,g), d µ = e−ηdσ be the weighted area form induced on ∂M, where dM is the Riemannian volume form of g = ⟨,⟩,",
      "chunk_index": 2
    },
    {
      "index": 498,
      "chunk_id": "DALLEval2024_chunk_03",
      "source_id": "DALLEval2024",
      "text": "ELLIPTIC OPERATORS 3 2. Preliminaries Let dm = e−ηdM be the weighted volume form on ( Mn,g), d µ = e−ηdσ be the weighted area form induced on ∂M, where dM is the Riemannian volume form of g = ⟨,⟩, and η : M → R be a smooth function. If T is a (0 ,2)-tensor on Mn, then we can associate it with a unique (1 ,1)-tensor, which will also be denoted T, according to ⟨TX,Y ⟩ = T(X,Y ) for all X,Y ∈ X(M). Thus, for an (1 ,1)-tensor S, we have ST(X,Y ) = ⟨STX,Y ⟩ = S(TX,Y ). Recall that the divergence of an (1 ,1)-tensor T is the (0 ,1)-tensor (divT)(v)(p) = tr g ( w↦→(∇wT)(v)(p) ) , for p∈ Mn and v,w ∈ TpM. Recall also that the inner product induced by gon the space of (0 ,2)-tensors on Mn is ⟨T,S⟩ = tr g ( TS∗) , where S∗ denotes the adjoint tensor of S. Clearly, we have in a local coordinates ⟨T,S⟩ = gikgjl TijSkl. In this way, if ∇2f = ∇dfstands for the Hessian of f, then ∆ gf = ⟨∇2f,g⟩. Now, we deﬁne the ( η,Tg)-divergence operator Lgf = div( Tg∇f) − Tg(∇η,∇f) = eηdiv(e−ηT∇f), where Tg is a symmetric positive deﬁnite (0 ,2)-tensor which depends smoothly on g. If we consider the drifted divergence operator div ηX := div X− ⟨∇η,X⟩, then the ( η,Tg)-divergence operator takes the form Lgf = div η(T∇f), moreover, ap- plying the divergence theorem, one has∫ M divη(TX)dm = ∫ ∂M ⟨TX,ν ⟩dµ, where ν is the outward pointing unit normal vector ﬁeld along ∂M. Since div η(T(fX)) = fdivη(TX) + ⟨∇f,TX ⟩ for f,ℓ ∈ C∞(M), we obtain ∫ M ℓdivη(T∇f)dm = − ∫ M Tg(∇ℓ,∇f)dm + ∫ ∂M ℓTg(∇f,ν)dµ (2.1) and the following formulae hold Lg(fℓ) = fLgℓ+ ℓLgf + 2Tg(∇f,∇ℓ). It follows from (2.1) that ( η,Tg)-divergence operator is a formally self-adjoint operator in the Hilbert space of all real functions f ∈ L2(M,dm) that vanish (or Tg(∇f,ν) = 0) on ∂M in the sense of the trace. Thus, the Dirichlet (or Tg- Neumann) boundary eigenvalue problem for Lg has real and discrete spectrum 0 = µ0(g) <µ1(g) ≤ µ2(g) ≤ · · · ≤ µk(g) ≤ · · · → ∞ , where each µi(g) is repeated according to its multiplicity. Moreover, since Mn is compact there exist two positive",
      "chunk_index": 3
    },
    {
      "index": 499,
      "chunk_id": "DALLEval2024_chunk_04",
      "source_id": "DALLEval2024",
      "text": "and discrete spectrum 0 = µ0(g) <µ1(g) ≤ µ2(g) ≤ · · · ≤ µk(g) ≤ · · · → ∞ , where each µi(g) is repeated according to its multiplicity. Moreover, since Mn is compact there exist two positive constants α and β, such that 0 <α|Y|2 ≤ Tg(Y,Y ) ≤ β|Y|2, for all Y ∈ X(M). Now, as Lg is a closed operator, we can use perturbation theory for linear operators as in Kato [ 14]. Let Sr(M) be the Banach spaces of Cr-symmetric (0 ,2)-tensors on Mn, and let Mr be the Banach spaces of Cr-Riemannian metrics on Mn, 2 ≤ r <∞. A 4 C. L. CUNHA 1, J. N. GOMES 2 AND M. A. M. MARROCOS 3 subset of Mr is called residual if it contains a countable intersection of open dense sets. A property of metrics in Mr is called generic if it holds on a residual subset. 3. Variation formulae by means of the metrics We start by considering t ↦→g(t) a smooth variation of an initial metric g. Let dm t = e−ηdMt be the weighted volume form of g(t), and d µt = e−ηdσt be the weighted area form induced on ∂M. Denoting by H the (0 ,2)-tensor given by Hij = d dt ⏐ ⏐ t=0gij(t) and writing h = gijHij, we get d dt dmt = 1 2 hdm. Similarly, if ˜H stands for the (0 ,2)-tensor induced by the derivative of g(t) restricted to ∂M, and ˜h denotes its trace, then d dt dµt = 1 ˜hdµ. Since there is no danger of confusion, we also write ⟨,⟩ for the metric g(t). Thus, for each X ∈ X(M), we can write Xt = gij(t)xi(t)∂j , where xi(t) = ⟨X,∂i⟩, and we deﬁne ˙X := gijx′ i∂j , where x′ i = d dt xi(t). Now, we consider t ↦→Tg(t) =: Tt to be a smooth variation of the initial symmetric (0 ,2)-tensor T = Tg. Then, we obtain the identity d dtTt(Xt,Yt) = T( ˙X,Y ) + T(X, ˙Y) + HT (X,Y ), (3.1) where HT := −(TH + HT) + T′ and T ′(∂i,∂j ) = d dtTt(∂i,∂j ) = ( Tt)′ ij . Indeed, a straightforward computation gives us Tt(Xt,Yt) = gij(t)gkl(t)xi(t)yk(t)(Tt)jl and d dtgij(t) = −gir(t)gjs (t)Hrs(t). Hence d dtTt(Xt,Yt) = gijgklx′ i(t)yk(t)(Tt)jl + gijgklxi(t)y′ k(t)(Tt)jl − girgjsgklHrsxiyk(Tt)jl − gijgkrglsHrsxiyk(Tt)jl + gijgklxiyk(Tt)′ jl which is",
      "chunk_index": 4
    },
    {
      "index": 500,
      "chunk_id": "DALLEval2024_chunk_05",
      "source_id": "DALLEval2024",
      "text": "dtTt(∂i,∂j ) = ( Tt)′ ij . Indeed, a straightforward computation gives us Tt(Xt,Yt) = gij(t)gkl(t)xi(t)yk(t)(Tt)jl and d dtgij(t) = −gir(t)gjs (t)Hrs(t). Hence d dtTt(Xt,Yt) = gijgklx′ i(t)yk(t)(Tt)jl + gijgklxi(t)y′ k(t)(Tt)jl − girgjsgklHrsxiyk(Tt)jl − gijgkrglsHrsxiyk(Tt)jl + gijgklxiyk(Tt)′ jl which is enough to obtain (3.1). In the special case of gradient vector ﬁelds, we denote by ∇tf the gradient of f ∈ C∞(M) computed in the metric g(t). Observe that ∇tf = gij(t)fi∂j , where fi = ⟨∇f,∂i⟩ = ∂if does not depend on t. Thus, from (3.1) we have d dtTt(∇tf,∇tℓ) = HT (∇f,∇ℓ) (3.2) for all f,ℓ ∈ C∞(M). In particular, d dt⟨∇tf,∇tℓ⟩ = −H(∇f,∇ℓ). Moreover, if we consider the smooth curve t ↦→ℓ(t) ∈ C∞(M) and the smooth vector ﬁeld νt := ∇tf |∇tf| , then d dtTt(νt,∇tℓ(t)) = 1 2H(ν,ν)T(ν,∇ℓ) + T(ν,∇ℓ′) + HT (ν,∇ℓ). (3.3) Indeed, ﬁrst note that νi = 1 |∇f| ⟨∇f,∂i⟩ implies ν′ i = 1 1 |∇f|H(ν,ν)∂if, (3.4) ON EIGENV ALUES OF A CLASS OF SECOND ORDER ELLIPTIC OPERATORS 5 and then T( ˙ν,∇ℓ) = 1 2H(ν,ν)T(ν,∇ℓ). Thus, by taking Xt = νt and Yt = ∇tℓ(t) into (3.1), we obtain (3.3). Now, we consider a smooth function η : I× M → R and write ˙η := d dt ⏐ ⏐ t=0η(t) to deﬁne ¯Ltφ:= div( Tt∇φ) − Tt(∇η(t),∇φ) (3.5) for all φ∈ C∞(M). We observe that the family of operators ¯Lt is very convenient to deal with domain deformations. Lemma 1. For all f ∈ C∞ c (M), ¯L ′f = div η(HT ∇f) + 1 2 T(∇h,∇f) − T(∇ ˙η,∇f) (3.6) where ¯L ′ := d dt ⏐ ⏐ t=0 ¯Lt. Proof. We begin by considering η to be independent of t. In this case, ¯Lt becomes Lt. Integration by parts formula gives us ∫ M ℓLtfdmt = − ∫ M Tt(∇ℓ,∇f)dmt for ℓ∈ C∞ c (M). Hence, from equation (3.2), we have at t= 0 ∫ M ℓL ′fdm = ∫ M ( − 1 2ℓhL f − HT (∇f,∇ℓ) − 1 2hT(∇ℓ,∇f) ) dm. (3.7) Observe that divη(HT (ℓ∇f)) = ℓdivη(HT ∇f) + HT (∇f,∇ℓ) (3.8) and divη(ℓhT∇f) = ℓhL f + ℓT(∇h,∇f) + hT(∇ℓ,∇f). (3.9) Plugging (3.8) and (3.9) in (3.7), we obtain ∫ M ℓL ′fdm = ∫ M ℓ ( 1 2 T(∇h,∇f) + div η(HT ∇f) ) dm, which is suﬃcient to prove (3.6) for this particular case. For the gen eral case,",
      "chunk_index": 5
    },
    {
      "index": 501,
      "chunk_id": "DALLEval2024_chunk_06",
      "source_id": "DALLEval2024",
      "text": "(3.8) and (3.9) in (3.7), we obtain ∫ M ℓL ′fdm = ∫ M ℓ ( 1 2 T(∇h,∇f) + div η(HT ∇f) ) dm, which is suﬃcient to prove (3.6) for this particular case. For the gen eral case, it is enough to note that ¯L ′f = L ′f − T(∇ ˙η,∇f) (3.10) to conclude the proof of the lemma. □ 4. Hadamard-type variation formula In what follows, we assume that all manifolds are oriented and those that are compact we assume to have boundary. The closed manifolds are ass umed to be compact without boundary. As in the previous section, we continue by considering t↦→Tg(t) =: Tt a smooth variation of the initial symmetric (0 ,2)-tensor T = Tg, and the family of operators ¯Lt as in (3.5), besides, we denote d¯mt = e−η(t)dMt. The next result is an extension of Lemma 3 .15 in Berger [ 4] for the ( η,Tg)- divergence operator Lg. 6 C. L. CUNHA 1, J. N. GOMES 2 AND M. A. M. MARROCOS 3 Lemma 2. Let (M,g) be a compact Riemannian manifold. Consider a real analytic one-parameter family of Riemannian metrics g(t) on M with g= g(0). If λ is an eigenvalue of multiplicity m> 1 for Lg, then there exist ε> 0, scalars λi (i= 1 ,...,m ) and functions φi varying analytically in t, such that, for all |t| < ε the following holds: (1) λi(0) = λ; (2) {φi(t)} is orthonormal in L2(M,d ¯mt); (3) { ¯Ltφi(t) = λi(t)φi(t) in M φi(t) = 0 on ∂M. Proof. Write g(z) and Tz for the extensions of g(t) and Tt to a domain D0 of the complex plane C, respectively. So, we can naturally extend the operator ¯Lt to ¯Lz := ¯La+ib = ¯La + i ¯Lb. We observe that the domain D = H2(M) ∩ H1 0 (M) of ¯Lz do not depend on z, and since M is compact, for any two Riemannian metrics g1,g2 the associated Sobolev norms ||.||W 1,2 (M,gi) in H2(M) are equivalent. Moreover, the map z↦→ ¯Lzφ is holomorphic for z ∈ D0 and for every φ∈ D. Thus, ¯Lz is a holomorphic family of type ( A) as in Kato [ 14]. Once the family ¯Lz is not self-adjoint in its domain, we can consider the isometry Pt : L2(M,dm) → L2(M,d ¯mt) given by Pt(u) = √ det(gij) √ det(gij(t)) u,",
      "chunk_index": 6
    },
    {
      "index": 502,
      "chunk_id": "DALLEval2024_chunk_07",
      "source_id": "DALLEval2024",
      "text": "family of type ( A) as in Kato [ 14]. Once the family ¯Lz is not self-adjoint in its domain, we can consider the isometry Pt : L2(M,dm) → L2(M,d ¯mt) given by Pt(u) = √ det(gij) √ det(gij(t)) u, so that each operator ˜Lt := P−1 t ◦ ¯Lt ◦ Pt is self-adjoint in L2(M,dm), since∫ M v ˜Ltudm = ∫ M Pt(v) ¯LtPt(u)d¯mt = ∫ M Pt(u) ¯LtPt(v)d¯mt = ∫ M P−1 t Pt(u)P−1 t ¯LtPt(v)dm = ∫ M u ˜Ltvdm. Evidently, ˜Lt share the same spectrum with ¯Lt. So, applying Theorem 3.9 in [ 14] for the operator ˜Lt we obtain the families of eigenvalues and eigenfunctions with the required properties. □ We now consider a smooth map F : Mr → S 2 +(M) that associates to each g∈ M r a symmetric positive deﬁnite tensor F(g) = Tg. Naturally, dFg stands for the diﬀerential of F at g, and dF∗ g for the adjoint of dFg. The next proposition constitutes the ﬁrst Hadamard-type variational formula. Proposition 1. Let (M,g) be a compact Riemannian manifold and g(t) be a smooth variation of g. Consider {φi(t)} ⊂ C∞(M) a diﬀerentiable family of functions and λi(t) a diﬀerentiable family of real numbers such that λi(0) = λ for each i= 1 ,...,m and for all t{ − ¯Ltφi(t) = λi(t)φi(t) in M φi(t) = 0 on ∂M, with ⟨φi(t),φj (t)⟩L2(M,d ¯mt) = δij . Then, we have (λi + λj )′δij = ∫ M ⟨1 ¯L (φiφj )g− 2(T∇φi)♭ ⊗ dφj − 2dφi ⊗ (T∇φj )♭,H ⟩ dm + ∫ M [ 2⟨dF∗ g Sym(dφi ⊗ dφj ),H⟩+ T(∇ ˙η,∇(φiφj )) ] dm, (4.1) ON EIGENV ALUES OF A CLASS OF SECOND ORDER ELLIPTIC OPERATORS 7 where (T∇φi)♭ is the 1-form associated to T∇φi induced by g, and Sym is the symmetrization operator. Proof. Diﬀerentiating with respect to the variable t the equation − ¯Ltφi(t) = λi(t)φi(t) at t= 0, one has − ¯L ′φi − ¯L φ′ i = λ′ iφi + λφ′ i. Multiplying by φj and using equation (3.10), we get −φj L ′φi + φj T(∇ ˙η,∇φi) − φj ¯L φ′ i = λ′ iφj φi − φ′ i ¯L φj . Using integration by parts, we obtain λ′ iδij = − ∫ M φj L ′φidm + ∫ M T(∇ ˙η,φj ∇φi)dm. So, we deduce from (3.6) that (λi + λj )′δij =",
      "chunk_index": 7
    },
    {
      "index": 503,
      "chunk_id": "DALLEval2024_chunk_08",
      "source_id": "DALLEval2024",
      "text": "iφj φi − φ′ i ¯L φj . Using integration by parts, we obtain λ′ iδij = − ∫ M φj L ′φidm + ∫ M T(∇ ˙η,φj ∇φi)dm. So, we deduce from (3.6) that (λi + λj )′δij = − ∫ M φj L ′φidm − ∫ M φiL ′φj dm + ∫ M T(∇ ˙η,∇(φj φi))dm = − ∫ M [ 1 2 T(∇h,∇(φiφj )) + φj divη(HT ∇φi) + φidivη(HT ∇φj ) ] dm + ∫ M T(∇ ˙η,∇(φj φi))dm. Again, integration by parts implies (λi + λj )′δij = ∫ M ( h ¯L (φiφj ) + 2 HT (∇φi,∇φj ) + T(∇ ˙η,∇(φj φi)) ) dm. (4.2) Now, we compute HT (∇φi,∇φj ) = −T(H∇φi,∇φj ) − H(T∇φi,∇φj ) + ⟨T′,dφi ⊗ dφj ⟩ = ⟨−(T∇φi)♭ ⊗ dφj − dφi ⊗ (T∇φj )♭,H⟩+ ⟨dFg(H),Sym(dφi ⊗ dφj )⟩ = ⟨−(T∇φi)♭ ⊗ dφj − dφi ⊗ (T∇φj )♭ + (dFg)∗Sym(dφi ⊗ dφj ),H⟩. We substitute this into (4.2) to complete the proof. □ We point out that Hadamard-type variation formula (4.1) generalize s the Berger formula in [ 4]. Moreover, we highlight that equation (4.2) will be useful in diﬀerent ways in this paper. 5. On generic properties of eigenvalues Here, we give an application of the Hadamard-type formula that we h ave ob- tained in the previous section. Let F : Mr → S 2 +(M) be a smooth map that associates to each g ∈ M r a symmetric positive deﬁnite tensor F(g) = Tg such that Lg(·) = div η(Tg∇·) is uniformly elliptic. We say that the Tg-family satisﬁes the property P if, for all g ∈ M r, the tensor Gg = ( n− 4)Tg + 2 dFg(g) satisﬁes either Gg(X,X) > 0 or Gg(X,X) <0 or Gg(X,X) ≡ 0 for all X ∈ X(M). For example, if ψ is a positive smooth function then Tg = ψg satisﬁes the property P. 8 C. L. CUNHA 1, J. N. GOMES 2 AND M. A. M. MARROCOS 3 Theorem 1. Let (M,g0) be a compact Riemannian manifold. Consider a family of operators Lg(·) = div η(Tg∇·) where the Tg-family satisﬁes the property P, and let λ be an eigenvalue of the problem { −Lg0 φ = λφ in M φ = 0 on ∂M (5.1) with multiplicity m >1. Then given any neighborhood U ⊂ M r of g0 and ǫ >0, there exists",
      "chunk_index": 8
    },
    {
      "index": 504,
      "chunk_id": "DALLEval2024_chunk_09",
      "source_id": "DALLEval2024",
      "text": "let λ be an eigenvalue of the problem { −Lg0 φ = λφ in M φ = 0 on ∂M (5.1) with multiplicity m >1. Then given any neighborhood U ⊂ M r of g0 and ǫ >0, there exists g ∈ U and a simple eigenvalue λ(g) such that |λ(g) − λ(g0)| < ǫ. In particular, the subset Γ ⊂ M r where the eigenvalues of (5.1) are simple, is residual. Proof. We will argue by contradiction. Let U ⊂ M r be a neighborhood of g0 such that ∀g∈ U with |λ(g) − λ(g0)| <ǫ the multiplicity of λ(g) is m. Consider g(t) = g0 + tH, where H is any symmetric (0 ,2)-tensor on ( Mn,g(t)) with λ(t) and φ(t) given by Lemma 2 such that { −Ltφi(t) = λ(t)φi(t) in M φi(t) = 0 on ∂M. By Proposition 1 λ′δij = ∫ M ⟨1 4L (φiφj )g0 − Sym((T∇φi)♭ ⊗ dφj ),H⟩dm + ∫ M ⟨−Sym(dφi ⊗ (T∇φj )♭) + dF∗ g0 Sym(dφi ⊗ dφj ),H⟩dm. If i̸= j, we have 0 = 1 4 L (φiφj )g0 − Sym((T∇φi)♭ ⊗ dφj ) − Sym(dφi ⊗ (T∇φj )♭) +dF∗ g0 Sym(dφi ⊗ dφj ). (5.2) Furthermore, taking traces in equation (5.2), we get 0 = nL (φiφj ) − 8T(∇φi,∇φj ) + 4 dFg0 (g0)(∇φi,∇φj ) = −λnφiφj + ((n− 4)T + 2dFg0 (g0))(∇φi,∇φj ). (5.3) We so obtain λnφiφj = Gg(∇φi,∇φj ). (5.4) Motivated by a similar argument given by Uhlenbeck [ 21] we ﬁx p∈ M and consider an integral curve α in M such that α(0) = p and α′(s) = Gg∇φi(α(s)). Deﬁning β(s) := φj (α(s)), we compute β′(s) = ⟨∇φj (α(s)),α′(s)⟩ = Gg(∇φj ,∇φi)(α(s)) = λnφi(α(s))φj (α(s)) = λnφi(α(s))β(s). This gives us that β(s) = cenλ ∫ s 0 φi(α(t))dt, where c >0. If Gg is positive, we have that φi(α(s)) is increasing in s and then β(s) ր ∞ , which is a contradiction, since M is compact. If Gg is negative the argument is analogous. If Gg ≡ 0, then from equation (5.3) we get φiφj = 0, hence, by the principle of the unique continuation [13] we have that at least one of the eigenfunctions vanishes, which is a gain a contradiction. It proves the ﬁrst part of the theorem. Now, let Γ m be the set of metrics g∈ M r such that the ﬁrst m eigenvalues",
      "chunk_index": 9
    },
    {
      "index": 505,
      "chunk_id": "DALLEval2024_chunk_10",
      "source_id": "DALLEval2024",
      "text": "that at least one of the eigenfunctions vanishes, which is a gain a contradiction. It proves the ﬁrst part of the theorem. Now, let Γ m be the set of metrics g∈ M r such that the ﬁrst m eigenvalues of Lg with respect to Problem (5.1) are simple. It is known that those eigen values depend continually of the metric (see [ 3]), hence Γ m is open in Mr. On the other ON EIGENV ALUES OF A CLASS OF SECOND ORDER ELLIPTIC OPERATORS 9 hand, it follows from the ﬁrst part of the theorem that Γ m is dense in Mr. Since Mr is a complete metric space on the topology Cr, the set Γ = ∩∞ m=1Γ m is residual. □ 6. Tg-Neumann boundary eigenvalue problem Our goal in this section is to prove the generic properties for eigenv alues of the operator L with the Neumann boundary condition. More speciﬁcally, { (L + λ)φ = 0 in M T(∇φ,ν) = 0 on ∂M, (6.1) where ν is the outward pointing unit normal vector ﬁeld along ∂M. 6.1. Hadamard-type variation formulas. Here, we proceed as in Section 4. Let us consider a smooth family of Riemannian metrics g(t) and Tt = Tg(t). Proposition 2. The following holds for any f,ℓ ∈ C∞(M). ∫ M ℓ ¯L ′fdm = ∫ M ℓ ( 1 2T(∇h,∇f) + div η(HT ∇f) − T(∇ ˙η,∇f) ) dm, where ¯L ′ := d dt ⏐ ⏐ t=0 ¯Lg(t). Proof. As in the proof of Lemma 1 we begin by considering η to be indepen- dent of t. In this case, ¯Lt becomes Lt. By integration by parts∫ M ℓLtfdmt = − ∫ M T(∇f,∇ℓ)dmt + ∫ ∂M ℓT(∇f,νt)dµt. Thus, from (3.3) at t= 0, ∫ M ℓL ′fdm + 1 ∫ M ℓhL fdm = − ∫ M HT (∇f,∇ℓ)dm − 1 ∫ M hT(∇f,∇ℓ)dm + ∫ ∂M ℓ ( HT (ν,∇f) + 1 2H(ν,ν)T(∇f,ν) ) dµ + ∫ ∂M ℓ ˜h 2 T(∇f,ν)dµ. Rearranging the above equation, we have∫ M ℓL ′fdm = − ∫ M HT (∇f,∇ℓ)dm + ∫ ∂M ℓHT (ν,∇f)dµ − 1 ∫ M ( hT(∇f,∇ℓ) + ℓhL f ) dm + 1 ∫ ∂M ℓ ( ˜h+ H(ν,ν) ) T(∇f,ν)dµ. (6.2) Finally, plugging (3.8) and (3.9) in (6.2), we obtain ∫ M lL ′fdm = ∫ M ℓ ( 1 2T(∇h,∇f) + div η(HT ∇f) ) dm",
      "chunk_index": 10
    },
    {
      "index": 506,
      "chunk_id": "DALLEval2024_chunk_11",
      "source_id": "DALLEval2024",
      "text": "f ) dm + 1 ∫ ∂M ℓ ( ˜h+ H(ν,ν) ) T(∇f,ν)dµ. (6.2) Finally, plugging (3.8) and (3.9) in (6.2), we obtain ∫ M lL ′fdm = ∫ M ℓ ( 1 2T(∇h,∇f) + div η(HT ∇f) ) dm + 1 ∫ ∂M ℓ ( − h+ H(ν,ν) + ˜h ) T(∇f,ν)dµ, for all ℓ∈ C∞(M), which is suﬃcient for completing the proof of our proposition, since ˜h= tr g(H|∂M ) = h− H(ν,ν). For the general case, we use equation (3.10) to conclude the proof. □ 10 C. L. CUNHA 1, J. N. GOMES 2 AND M. A. M. MARROCOS 3 Below we will obtain Hadamard-type formulas for the eigenvalues of L with Tg-Neumann boundary condition. Proposition 3. Let (M,g0) be a compact Riemannian manifold, and g(t) be a smooth variation of g0. Consider {φi(t)} ⊂ C∞(M) a diﬀerentiable family of functions and λi(t) a diﬀerentiable family of real numbers such that λi(0) = λ for each i= 1 ,...,m and for all t { − ¯Lg(t)φi(t) = λi(t)φi(t) in M Tt(νt,∇φi(t)) = 0 on ∂M, which ⟨φi(t),φj (t)⟩L2(M,dmt) = δij. Then, we obtain the following variation formula (λi + λj )′δij = ∫ M ⟨1 2 Lg0 (φiφj )g0 − 2(Tg0 ∇φi)♭ ⊗ dφj − 2dφi ⊗ (Tg0 ∇φj )♭,H⟩dm + ∫ M [ 2⟨dF∗ g0 Sym(dφi ⊗ dφj ),H⟩+ T(∇ ˙η,∇(φiφj )) ] dm, where (Tg0 ∇φi)♭ is the 1-form associated to Tg0 ∇φi induced by g0, and Sym denotes the symmetrization operator. Proof. Taking the derivative with respect to t in both sides of the identity − ¯Lg(t)φi(t) = λi(t)φi(t), one has at t= 0 − ¯L ′φi − ¯L φ′ i = λ′ iφi + λφ′ i. Thus − ∫ M (φj ¯L ′φi + φj ¯L φ′ i)dm = ∫ M (λ′ iφj φi − φ′ i ¯L φj )dm. Now, since T(νt,∇tφi(t)) = 0 on ∂M, we deduce from (3.3) that T(ν,∇φ′ i) = −HT (ν,∇φi). Moreover, integration by parts gives λ′ iδij = − ∫ M φj ¯L ′φidm − ∫ ∂M φj T(ν,∇φ′ i)dµ = − ∫ M φj ¯L ′φidm + ∫ ∂M φj HT (ν,∇φi)dµ. Whence, −(λi + λj )′δij = ∫ M φj ¯L ′φidm + ∫ M φi ¯L ′φj dm − ∫ ∂M φiHT (ν,∇φj )dµ − ∫ ∂M φj HT (ν,∇φi)dµ = ∫ M 2 T(∇h,∇(φiφj ))dm + ∫ M φidivη(HT ∇φj )dm +",
      "chunk_index": 11
    },
    {
      "index": 507,
      "chunk_id": "DALLEval2024_chunk_12",
      "source_id": "DALLEval2024",
      "text": "λj )′δij = ∫ M φj ¯L ′φidm + ∫ M φi ¯L ′φj dm − ∫ ∂M φiHT (ν,∇φj )dµ − ∫ ∂M φj HT (ν,∇φi)dµ = ∫ M 2 T(∇h,∇(φiφj ))dm + ∫ M φidivη(HT ∇φj )dm + ∫ M φj divη(HT ∇φi)dm + ∫ M T(∇ ˙η,∇(φiφj ))dm − ∫ ∂M φiHT (ν,∇φj )dµ− ∫ ∂M φj HT (ν,∇φi)dµ. ON EIGENV ALUES OF A CLASS OF SECOND ORDER ELLIPTIC OPERATORS 11 Next, we use the divergence theorem to compute (λi +λj )′δij = ∫ M h ¯L (φiφj )dm+2 ∫ M HT (∇φi,∇φj )dm+ ∫ M T(∇ ˙η,∇(φiφj ))dm (6.3) or equivalently (λi + λj )′δij =2 ∫ M ⟨1 4 L (φiφj )g− (T∇φi)♭ ⊗ dφj − dφi ⊗ (T∇φj )♭,H⟩dm + 2 ∫ M ⟨dF∗ g Sym(dφi ⊗ dφj ),H⟩dm + ∫ M T(∇ ˙η,∇(φiφj ))dm as in Proposition 1. □ Now, we will prove the existence of analytic curves of eigenvalues. Proposition 4. Let (M,g0) be a compact Riemannian manifold and g(t) be a real analytic one-parameter family of Riemannian metrics o n M with g(0) = g0. Assume λis an eigenvalue of multiplicity mfor the operator Lg0 with Tg0 -Neumann boundary condition. Then, there exist ε >0, and t-analytic functions λi(t) and φi(t), with i= 1 ,...,m and |t| <ε, such that ⟨φi(t),φj (t)⟩L2(M,dmt) = δj i , λi(0) = λ and { −Ltφi(t) = λi(t)φi(t) in M Tt(∇φi(t),νt) = 0 on ∂M. The following remark is due at this point. The main technique utilized in th e proof of Proposition 4 is the Liapunov-Schmidt method. In particula r, we will fol- low along the same lines of proof as in Henry [ 12], Marrocos and Pereira [ 17], and Gomes and Marrocos [ 10], for they already successfully employed this method in contexts similar to ours. The proof of Proposition 4 requires the fo llowing propo- sition. Proposition 5. Let (M,g0) be a compact Riemannian manifold, and λ0 be an eigenvalue of multiplicity m >1 of the operator L with Tg0 -Neumann boundary condition. Then for every ǫ >0 there is δ > 0 so that for each |t| < δ, there exist exactly m eigenvalues (counting their multiplicities) to Problem (6.1) in the interval (λ0 − ǫ,λ0 + ǫ). Proof. Let {φj }m j=1 be an orthonormal basis associated with λ0, and consider the orthonormal projection on the corresponding eigenspace Pu",
      "chunk_index": 12
    },
    {
      "index": 508,
      "chunk_id": "DALLEval2024_chunk_13",
      "source_id": "DALLEval2024",
      "text": "exist exactly m eigenvalues (counting their multiplicities) to Problem (6.1) in the interval (λ0 − ǫ,λ0 + ǫ). Proof. Let {φj }m j=1 be an orthonormal basis associated with λ0, and consider the orthonormal projection on the corresponding eigenspace Pu = m∑ j=1 φj ∫ M φj udm0. Note that P induces the splitting L2(M,dm0) = R(P)⊕N (P) so that any function u ∈ L2(M,dm0) can be written as u = φ+ ψ, where φ ∈ R (P) = ker(L + λ0) and ψ ∈ N (P). With this in mind, we consider the family of metrics g(t) on M and the family of eigenvalue problems, where the operator L remains ﬁxed and the families Tt and νt change with the parameter t along the boundary,    (I− P)(L + λ)(φ+ ψ) = 0 in M P(L + λ)(φ+ ψ) = 0 in M Tt(νt,∇(φ+ ψ)) = 0 on ∂M. (6.4) 12 C. L. CUNHA 1, J. N. GOMES 2 AND M. A. M. MARROCOS 3 To solve problem (6.4), we ﬁrst observe that since φj and ψ are orthonormal, by the divergence theorem we must have P(L + λ)ψ = m∑ j=1 φj ∫ M φj (L + λ)ψdm0 = p∑ j=1 φj ∫ M φj (L + λ)ψ− ψ(L + λ)φj dm0 = p∑ j=1 φj ∫ ∂M φj T(∇ψ,ν) − ψT(∇φ,ν)dµ0 = m∑ j=1 φj ∫ ∂M φj T(∇ψ,ν)dµ0 which implies (L + λ)ψ= ( I− P) ( (L + λ)ψ ) + m∑ j=1 φj ∫ ∂M φj T(∇ψ,ν)dµ0. Thus, we get (L + λ)ψ+ (I− P)(Lt − L )(φ+ ψ) − m∑ j=1 φj ∫ ∂M φj T(∇ψ,ν)dµ0 = 0 . Moreover, the part concerning the boundary in (6.4) can be rewrit ten as T(ν,∇ψ) + Tt(νt,∇(φ+ ψ)) − T(ν,∇(φ+ ψ)) = 0 . Hence, to solve the ﬁrst and the third equations of (6.4), is equivale nt to ﬁnding the zeros of the map F : R × R × R(P) × H2(M) ∩ N (P) − → N (P) × H 2 (M) (t,λ,φ,ψ ) ↦→ ( F1(t,λ,φ,ψ ),F2(t,λ,φ,ψ ) ) , where     F1 = ( L + λ)ψ+ (I− P)(Lt − L )(φ+ ψ) − m∑ j=1 φj ∫ ∂M φj T(∇ψ,ν)dµ0 F2 = T(ν,∇ψ) + Tt(νt,∇(φ+ ψ)) − T(ν,∇(φ+ ψ)). Note that F depends diﬀerentially on the variables λ, t, ψ e φ. Our intention",
      "chunk_index": 13
    },
    {
      "index": 509,
      "chunk_id": "DALLEval2024_chunk_14",
      "source_id": "DALLEval2024",
      "text": "+ λ)ψ+ (I− P)(Lt − L )(φ+ ψ) − m∑ j=1 φj ∫ ∂M φj T(∇ψ,ν)dµ0 F2 = T(ν,∇ψ) + Tt(νt,∇(φ+ ψ)) − T(ν,∇(φ+ ψ)). Note that F depends diﬀerentially on the variables λ, t, ψ e φ. Our intention is to use the implicit function theorem to show that F(t,λ,φ,ψ ) = (0 ,0) admits a solution ψ as function of λ, t and φ. To this end, we observe that if t= 0 ,λ = λ0 and ψ= 0, then ∂F ∂ψ(0,λ0,0,0) ˙ψ= ( (L + λ0) ˙ψ− m∑ j=1 φj ∫ ∂M φj T(∇ ˙ψ,ν)dµ0 , ∂ ˙ψ ∂ν ) . (6.5) We claim now that the map given in (6.5) is an isomorphism from H2(M) ∩ N (P) onto N (P) × H 2 (M). Indeed, the proof of this fact can be found in [ 16]. Hence, by the implicit function theorem there exist positive numbers δ, ǫ and a function S(t,λ)φ of class C1 at the variables ( t,λ) such that for every |t| <δ and λ ∈ (λ0 − ǫ,λ0 + ǫ), F(t,λ,φ,S (t,λ)φ) = (0 ,0). Furthermore, S(t,λ)φ is analytic at λ and linear at φ. This solves the equation (6.4) in relation to ψ. ON EIGENV ALUES OF A CLASS OF SECOND ORDER ELLIPTIC OPERATORS 13 We now observe that for every φ∈ R (P) there exist real numbers c1,c2,...,c m so that φ = ∑ m j=1 cj φj . Thus, the second equation in (6.4) can be equivalently perceived as a system of equations in the variables c1,...,c m as below m∑ j=1 cj ∫ M φk(Lt + λ)(φj + S(t,λ)φj )dm0 = 0 , k = 1 ,2,...,m. In this way, λ is an eigenvalue of Lt if and only if det A(t,λ) = 0, where A(t,λ) is given by Akj (t,λ) = ∫ M φk(Lt + λ)(φj + S(t,λ)φj )dm0. Furthermore, the associated eigenfunctions are given by u(t,λ) = m∑ j=1 cj (φj + S(t,λ)φj ). In other words, c = ( c1,...,c m) must satisfy A(t,λ)c = 0. It turns out that by Rouch´ e theorem, we have that: For every ǫ> 0 there is δ >0 so that if |t− t0| <δ, then there exist exactly m-roots of det A(t,λ) = 0 in the interval ( λ0 −ǫ,λ0 +ǫ). □ Proof of Proposition 4. Assume the same hypotheses as in Proposition 5. We must show",
      "chunk_index": 14
    },
    {
      "index": 510,
      "chunk_id": "DALLEval2024_chunk_15",
      "source_id": "DALLEval2024",
      "text": ">0 so that if |t− t0| <δ, then there exist exactly m-roots of det A(t,λ) = 0 in the interval ( λ0 −ǫ,λ0 +ǫ). □ Proof of Proposition 4. Assume the same hypotheses as in Proposition 5. We must show that there exist m analytic curves of eigenvalues λj (t) for (6 .1) associated to m analytic curves of eigenfunctions φj (t). The strategy of proof is to reduce the problem to a ﬁnite-dimensional one and to apply Kato's Selection theorem, see [ 14]. With this in mind, we will make a slightly diﬀerent construction than that of Proposition 5. This will result, as we will see shortly, in ob taining a symmetric matrix. Let {φj }m j=1 be orthonormal eigenfunctions of the Laplace-Neumann operato r associated with λ0. For each j = 1 ,...,m consider the following problem          (L + λ0)u = 0 in M Tt(νt,∇(φj + u)) = 0 on ∂M Pu = m∑ j=1 φj ∫ M φj udm0 = 0 in M. (6.6) Consider now the orthogonal complement [ φj ]⊥ of ker(L + λ0) in L2(M,dm0) and deﬁne F : ( −δ,δ) × H2(M,dm0) − →[φj ]⊥ × R(P) × H 2 (M,dm0) by F(t,w) = ( (L + λ0)w, Pw, Tt(νt,∇(φj + w)) ) . Exactly as before we get that ∂F ∂w (0,0) is an isomorphism, so by the implicit function theorem there exist δ >0 and an analytic function wj (t) deﬁned on |t− t0| < δ such that F(t,wj (t)) = 0. In addition, we obtain for each |t− t0| < δa linearly independent set of functions {ϕj (t)}m j=1, given by ϕj (t) = φj + wj (t), that satisfy the equation (6 .6). By using the Gram-Schmidt orthonormalization process with respect to the inner product (u,v) := ∫ M uvdmt, 14 C. L. CUNHA 1, J. N. GOMES 2 AND M. A. M. MARROCOS 3 we can without loss of generality assume that {ϕj (t)}m j=1 is orthonormal. Note that the functions ϕj (t) belong to Dt = {u∈ H2(M,dm0),Tt(νt,∇u) = 0 }. Moreover, since Lt is self-adjoint with respect to the inner product deﬁned above, it f ollows that the matrix ∫ M ϕj Ltϕkdmt is symmetric. For a given T ∈ S 2,k, we deﬁne a family of Riemannian metrics on M by g(t) = g0 + tT",
      "chunk_index": 15
    },
    {
      "index": 511,
      "chunk_id": "DALLEval2024_chunk_16",
      "source_id": "DALLEval2024",
      "text": "to the inner product deﬁned above, it f ollows that the matrix ∫ M ϕj Ltϕkdmt is symmetric. For a given T ∈ S 2,k, we deﬁne a family of Riemannian metrics on M by g(t) = g0 + tT and let P(t) be given by P(t)u= m∑ j=1 ϕj (t) ∫ M uϕj (t)dmt. We ﬁnally deﬁne for each j = 1 ,...,m , Gj : ( −ǫ,ǫ) × R × H2(M) − → [φj ]⊥ × H 2 (M,dm0) × R(P) (t,λ,w ) ↦→ ( Gj1(t,λ,w ),Gj2(t,λ,w ),Gj3(t,λ,w ) ) by    Gj1 = ( I− P(t))((Lt + λ))(w+ ϕj (t)) Gj2 = Tt(νt,∇w); Gj3 = P(t)w. Once again, the implicit function theorem yields a number δ >0 and functions wj (t,λ) such that for any |t − t0| < δ and every |λ − λ0| < δ, the equality Gj (t,λ,w j (t,λ)) = (0 ,0,0) holds. As we know, λ is an eigenvalue for (6.1) if and only if there exists a nonzero m-uple c= ( c1,...,c m) of real numbers such that A(t,λ)c= 0, where Aij (t,λ) = ∫ M ϕi(t)(Lt + λ)(ϕj (t) + wj (t,λ))dmt. That is, λ is an eigenvalue of (6.1) if det A(t,λ) = 0. By Rouch´ e's theorem, there exist m roots near λ0 counting multiplicity, for each t. So by Puiseux's theo- rem [ 22] there exist m-analytic functions t → λi(t) which locally solve the equa- tion det A(t,λ) = 0. It can be easily seen that A is symmetric and hence, by Kato's Selection theorem [ 14], we can ﬁnd an analytic curve ci(t) ∈ Rm such that A(t,λi(t))ci(t) = 0, for each i= 1 ,...,m . Thus ψi(t) = m∑ j=1 ci j (t)(ϕj + ωj (t,λi(t))) is an analytic curve of eigenfunctions for (6.1) associated with λi(t). Reasoning exactly as Kato in [ 14, p. 98] we can obtain m-analytic curves of eigenvalues {φi(t)}m i=1 such that ∫ M φi(t)φj (t)dmt = δj i . □ Remark 1. In the case of m = m(λ0) = 1 , the existence of a diﬀerentiable curve of eigenvalues through λ0 follows directly from the implicit function theorem applied to the map F : Sk × H2(M,dm0) × R → L2(M,dm0) × R deﬁned by F(g,u,λ ) = ( (Lg + λ)u, ∫ M u2dm0 ) . The corresponding formula to the derivative λ′(t) can",
      "chunk_index": 16
    },
    {
      "index": 512,
      "chunk_id": "DALLEval2024_chunk_17",
      "source_id": "DALLEval2024",
      "text": "implicit function theorem applied to the map F : Sk × H2(M,dm0) × R → L2(M,dm0) × R deﬁned by F(g,u,λ ) = ( (Lg + λ)u, ∫ M u2dm0 ) . The corresponding formula to the derivative λ′(t) can be obtained by letting i= j = 1 in Proposition 3. ON EIGENV ALUES OF A CLASS OF SECOND ORDER ELLIPTIC OPERATORS 15 Remark 2. We observe that Theorem 1 remains valid if we replace the Diri ch- let boundary condition by the Tg-Neumann boundary condition. 7. Domain variation In this section we consider the case of domain deformation. For this , let ( M,g) be a Riemannian manifold, Ω ⊂ M be a bounded domain with smooth boundary ∂Ω, and T be a symmetric (0 ,2)-tensor on M. We consider a deformation Ω t by the family of diﬀeomorphisms ft : Ω → Ω t, where f0 = idΩ is the identity map. The smooth variations of g and T are given by g(t) = f∗ t g and Tt = f∗ t T. We denote V = d dt ⏐ ⏐ t=0ft, H = d dt ⏐ ⏐ t=0f∗ t g= LV g and T′ = d dt ⏐ ⏐ t=0f∗ t T = LV T. The next two technical lemmas are necessary tools to obtain the Ha damard- types formulas given by Proposition 7. Lemma 3. For X,Y ∈ X(M), we have HT (X,Y ) = −⟨∇T X V,Y ⟩ − ⟨X,∇T Y V⟩+ ⟨(∇V T)X,Y ⟩. (7.1) Proof. T′(X,Y ) = ( LV T)(X,Y ) = V⟨TX,Y ⟩ − T(∇V X− ∇X V,Y ) − T(X,∇V Y − ∇Y V) = T(X,∇Y V) + T(∇X V,Y ) + ⟨∇V (TX),Y ⟩ − T(∇V X,Y ) = T(X,∇Y V) + T(∇X V,Y ) + ⟨(∇V T)X,Y ⟩. In particular, for T = g, we get H(X,Y ) = ⟨∇X V,Y ⟩+ ⟨X,∇Y V⟩. Follows that HT(X,Y ) = H(TX,Y ) = ⟨∇T X V,Y ⟩+ ⟨TX, ∇Y V⟩ and TH(X,Y ) = H(X,TY ) = ⟨∇X V,TY ⟩+ ⟨X,∇T Y V⟩. By using the identity HT = T′ − (HT + TH) we conclude our proof. □ Lemma 4. For any eigenfunctions associated to the eigenvalue λ we have HT (∇φi,∇φj ) = −divη(⟨V,∇φj ⟩T∇φi +⟨V,∇φi⟩T∇φj )+ 1 2 ⟨V,∇L (φiφj )⟩. (7.2) Proof. Equation (7.1) implies HT (∇φi,∇φj ) = − ⟨∇T ∇φi V,∇φj ⟩ −",
      "chunk_index": 17
    },
    {
      "index": 513,
      "chunk_id": "DALLEval2024_chunk_18",
      "source_id": "DALLEval2024",
      "text": "Lemma 4. For any eigenfunctions associated to the eigenvalue λ we have HT (∇φi,∇φj ) = −divη(⟨V,∇φj ⟩T∇φi +⟨V,∇φi⟩T∇φj )+ 1 2 ⟨V,∇L (φiφj )⟩. (7.2) Proof. Equation (7.1) implies HT (∇φi,∇φj ) = − ⟨∇T ∇φi V,∇φj ⟩ − ⟨∇T ∇φj V,∇φi⟩+ ⟨(∇V T)∇φi,∇φj ⟩. Now, we observe that ⟨∇T ∇φi V,∇φj ⟩ = div η(⟨V,∇φj ⟩T∇φi) + λ⟨V,∇φj ⟩φi − ∇2φj (V,T ∇φi). Moreover, ∇2φj (V,T ∇φi) + ∇2φi(V,T ∇φj ) + ⟨(∇V T)∇φi,∇φj ⟩ = ⟨∇(T(∇φi,∇φj )),V ⟩. 16 C. L. CUNHA 1, J. N. GOMES 2 AND M. A. M. MARROCOS 3 Thus, HT (∇φi,∇φj ) = − divη(⟨V,∇φj ⟩T∇φi + ⟨V,∇φi⟩T∇φj ) − λ⟨V,∇(φj φi)⟩+ + ⟨∇(T(∇φi,∇φj )),V ⟩ = −divη(⟨V,∇φj ⟩T∇φi + ⟨V,∇φi⟩T∇φj ) + 1 2⟨V,∇L (φiφj )⟩. It ﬁnishes the proof. □ The next proposition provides Hadamard-type formulas for the eig envalues of the operator L considering both Dirichlet and Neumann boundary conditions. Proposition 6. Let (M,g) be a Riemannian manifold, Ω ⊂ M be a bounded domain, ft : Ω → (M,g) be an analytic family of diﬀeomorphisms (Ω t = ft(Ω)) and λ be an eigenvalue of Lg with multiplicity m >1. Then there exist a family of m functions {φi(t)} ∈ C∞(Ω t) with ⟨φi(t),φj (t)⟩L2(Ω t,dm) = δij and a diﬀerentiable family of real numbers λi(t) with λi(0) = λ, such that they satisfy { −Ltφi(t) = λ(t)φi(t) in Ω Bα(φi(t)) = 0 on ∂Ω , (7.3) for all t and i= 1 ,...,m , where Bα(φi) = α⟨T∇φi,νt⟩+ (1 − α)φi for α∈ { 0,1}. Moreover, we obtain the following variation formula (λi + λj )′ ⏐ ⏐ ⏐ t=0 δij = ∫ ∂Ω [ L (φiφj )⟨V,ν⟩ − 2⟨V,∇φj ⟩⟨∇φi,Tν ⟩ − 2⟨V,∇φi⟩⟨∇φj ,Tν ⟩ + ⟨V,∇η⟩⟨∇(φiφj ),Tν ⟩ ] dµ (7.4) where V = d dt ⏐ ⏐ t=0ft. Remark 3. We point out that if T = id and η is constant, then formula (7.4) has been obtained by Souﬁ and Ilias [20], moreover, Henry [12] also gave a similar formula for the Laplacian on domains in Rn. Proof. Consider the family of metrics g(t) = f∗ t gand symmetric (0 ,2)-tensors Tt = f∗ t T on Ω. It is not diﬃcult to see that Lemma 2 is still valid for the operator ¯Lt with η(t) = η◦ ft, that is, there exists a family { ¯φi(t)} ⊂ C∞(Ω) of analytic functions in",
      "chunk_index": 18
    },
    {
      "index": 514,
      "chunk_id": "DALLEval2024_chunk_19",
      "source_id": "DALLEval2024",
      "text": "f∗ t T on Ω. It is not diﬃcult to see that Lemma 2 is still valid for the operator ¯Lt with η(t) = η◦ ft, that is, there exists a family { ¯φi(t)} ⊂ C∞(Ω) of analytic functions in t satisfying ⟨¯φi(t),¯φj (t)⟩L2(Ω ,dmt) = δij and − ¯Lt ¯φi(t) = λi(t) ¯φi(t) in Ω . (7.5) We claim that λi(t) and φi(t) := ¯φi(t) ◦ f−1 t satisfy (7.3). Initially, we have g(df(TtX),dfei) = gt(TtX,ei) = Tt(X,ei) = T(dfX,dfei) = g(TdfX,dfei), for all X ∈ X(M). So dfTt∇t ¯φ= T∇φ and divgT∇φ= div gdfTt∇t ¯φ= div g(t)Tt∇t ¯φ. Thus, for each q= f(p) ∈ Ω t we obtain ( Lgφi(t) ) (q) = [ divg ( T∇φi(t) ) − T ( ∇η,∇φi(t) )] f(p) = [ divg(t) ( Tt∇t ¯φi(t) ) − Tt ( ∇tη(t),∇t ¯φi(t) )] p = [ ¯Lt ¯φi(t) ] p = −λi(t) ( ¯φi(t) ) p = −λi(t) ( φi(t) ◦ f ) p = −λi(t)φi(t)(q). ON EIGENV ALUES OF A CLASS OF SECOND ORDER ELLIPTIC OPERATORS 17 Since ¯φi(0) = φi(0), ¯L0 = L and h= ⟨H,g⟩ = 2div V, by equations (4.2) and (6.3) we have sijδij = ∫ Ω L (φiφj )divV + 2HT (∇φi,∇φj ) + T(∇ ˙η,∇(φiφj ))dm, where sij = ( λi + λj )′ and ˙η = d dt ⏐ ⏐ t=0(η◦ ft) = ⟨∇η,V ⟩. By Lemma 4 and the identity divη( ˙ηT(∇(φiφj ))) = ˙ηL (φiφj ) + T(∇ ˙η,∇(φiφj )) = ⟨L (φiφj )V,∇η⟩+ T(∇ ˙η,∇(φiφj )), we obtain sijδij = ∫ Ω [ L (φiφj )divV − 2divη(⟨V,∇φj ⟩T∇φi + ⟨V,∇φi⟩T∇φj ) + ⟨V,∇L (φiφj )⟩+ divη(⟨V,∇η⟩T∇(φiφj )) − ⟨L (φiφj )V,∇η⟩ ] dm = ∫ Ω [ divη(L (φiφj )V − 2⟨V,∇φj ⟩T∇φi − 2⟨V,∇φi⟩T∇φj + ⟨V,∇η⟩T∇(φiφj ) ] dm = ∫ ∂Ω [ L (φiφj )⟨V,ν⟩ − 2⟨V,∇φj ⟩⟨T∇φi,ν⟩ − 2⟨V,∇φi⟩⟨T∇φj ,ν⟩ + ⟨V,∇η⟩⟨T∇(φiφj ),ν⟩ ] dµ, which is (7.4). □ Proposition 6 under the boundary conditions becomes Proposition 7. Let ft,λ(t),φi(t) be as in Proposition 6, and let us consider the eigenvalue boundary problem { −Ltφi(t) = λ(t)φi(t) in Ω Bα(φi(t)) = 0 on ∂Ω . (7.6) Then we have for α= 0 case: ( λi + λj )′δij = −2 ∫ ∂Ω ∂φi ∂ν ∂φj ∂ν T(ν,ν)⟨V,ν⟩dµ, (7.7) for α= 1 case: ( λi + λj )′δij = 2 ∫ ∂Ω (T(∇φi,∇φj ) − λφiφj )⟨V,ν⟩dµ. (7.8) Proof. It",
      "chunk_index": 19
    },
    {
      "index": 515,
      "chunk_id": "DALLEval2024_chunk_20",
      "source_id": "DALLEval2024",
      "text": "for α= 0 case: ( λi + λj )′δij = −2 ∫ ∂Ω ∂φi ∂ν ∂φj ∂ν T(ν,ν)⟨V,ν⟩dµ, (7.7) for α= 1 case: ( λi + λj )′δij = 2 ∫ ∂Ω (T(∇φi,∇φj ) − λφiφj )⟨V,ν⟩dµ. (7.8) Proof. It is enough to use (7.4) together with the identities L (φiφj ) = φiL φj + φj L φi + 2T(∇φi,∇φj ) and ∇φi = ∇∂Ω φi + ∂φi ∂ν ν on ∂ Ω to obtain (7.7) and (7.8). □ It is known that the set Diﬀ r(Ω) of Cr-diﬀeomorphisms of Ω is an aﬃne sub- manifold of a Banach Space [ 8]. The result below shows that the multiplicity of an eigenvalue can be reduced by small perturbation of the domain. 18 C. L. CUNHA 1, J. N. GOMES 2 AND M. A. M. MARROCOS 3 Theorem 2. Let (M,g) be a Riemannian manifold and let Ω ⊂ M be a bounded domain. If λ is an eigenvalue with multiplicity m> 1 of the problem{ −Lgφ = λφ in Ω Bαφ = 0 on ∂Ω , (7.9) where Bα(φi) = α⟨T∇φi,νt⟩+ (1 − α)φi for α∈ { 0,1}, then in every neighborhood of the identity idΩ with respect to the Cr topology, there exist a diﬀeomorphism f and a positive ǫsuch that |λ(f)−λ(idΩ )| <ǫ, and λ(f) is simple. In particular, the subset of diﬀeomorphisms D ⊂ Diﬀ r(Ω) that make the eigenvalues of (7.9) simple, is residual. Proof. Let λbe an eigenvalue of multiplicity m> 1 and suppose that, for all perturbation by diﬀeomorphism of Ω, the multiplicity of λcannot be reduced. The proceeding follows the same lines as in proof of Theorem 1. For α= 0 case, from (7.7) we have ∂φi ∂ν ∂φj ∂ν = 0 on ∂Ω. This way either ∂φi ∂ν = 0 or ∂φj ∂ν = 0 in some open set U of ∂Ω. If ∂φi ∂ν = 0 in U, since φi = 0 on ∂Ω, it follows from the unique continuation principle [ 13] that φi = 0 on Ω, which is a contradiction. For α= 1 case. From (7.8) we have T(∇φi,∇φj ) − λφiφj = 0 on ∂Ω. By the Uhlenbeck's argument once again we get a contradiction. I t proves the ﬁrst part of the theorem. The second part follows from the an alogous argument as in the proof of Theorem 1. □ 8. Application to extremal domains",
      "chunk_index": 20
    },
    {
      "index": 516,
      "chunk_id": "DALLEval2024_chunk_21",
      "source_id": "DALLEval2024",
      "text": "the Uhlenbeck's argument once again we get a contradiction. I t proves the ﬁrst part of the theorem. The second part follows from the an alogous argument as in the proof of Theorem 1. □ 8. Application to extremal domains for the kth-eigenvalue Before to claim the main results of this section we will begin with some de ﬁ- nitions and remarks. Here, we will consider only analytic volume-pres erving defor- mation of Ω ⊂ M. Let µk(t) be the kth-eigenvalue of Lt with Dirichlet boundary condition. If Ω t = ft(Ω) is an analytic volume-preserving deformation of Ω, then it is not diﬃcult to see that v:= ⟨ d dt ⏐ ⏐ ⏐ t=0 ft,ν ⟩ must satisfy ∫ ∂Ω vdµ= 0. We denote A0(∂Ω) the set of all regular functions on ∂Ω such that ∫ ∂Ω vdµ= 0. Souﬁ and Ilias proved that given a v ∈ A 0(∂Ω) there exists an analytic volume-preserving deformation Ω t = ft(Ω) such that v:= ⟨ d dt ⏐ ⏐ ⏐ t=0 ft,ν ⟩ ∈ A 0(∂Ω) (see [ 20]). Recall that a domain Ω ⊂ M is a local minimizer (local maximizer) for the kth-eigenvalue µk of L if for any analytic volume-preserving deformation Ω t, the function t ↦→µk(t) admits a local minimum (local maximum) at t= 0. Theorem 3. Let k be a positive integer such that the kth-eigenvalue µk of L φ = div η(T∇φ) with Dirichlet boundary condition satisﬁes µk > µk−1 (resp. µk < µk+1). If Ω is a local minimizer (resp. local maximizer) for µk, then, it is simple and for some constant c its associated eigenfuction φ satisﬁes⏐ ⏐ ⏐ ⏐ ∂φ ∂ν √ T(ν,ν) ⏐ ⏐ ⏐ ⏐= c on ∂Ω . Proof. Suppose µk > µk−1 and consider v = ⟨ d dt ⏐ ⏐ ⏐ t=0 ft,ν ⟩ ∈ A 0(∂Ω) such that Ω t = ft(Ω) is a volume-preserving analytic deformation of Ω. Let {λi(t)} and ON EIGENV ALUES OF A CLASS OF SECOND ORDER ELLIPTIC OPERATORS 19 {φi(t)} be analytic families of eigenvalues and eigenfunctions given by Lemma 2 such that λi(0) = µk. By continuity λi(t) > µk−1(t) for suﬃciently small t, since λi(0) = µk >µk−1. By hypothesis, the function t↦→µk(t) admits a local minimum at t= 0. So, λi(t) also admits a local minimum at t= 0 and then d dt λi(t) ⏐ ⏐ t=0",
      "chunk_index": 21
    },
    {
      "index": 517,
      "chunk_id": "DALLEval2024_chunk_22",
      "source_id": "DALLEval2024",
      "text": "µk−1(t) for suﬃciently small t, since λi(0) = µk >µk−1. By hypothesis, the function t↦→µk(t) admits a local minimum at t= 0. So, λi(t) also admits a local minimum at t= 0 and then d dt λi(t) ⏐ ⏐ t=0 = 0. Eq. (7.7) of Proposition 7 give us ∫ ∂Ω v ( ∂φ ∂ν ) 2 T(ν,ν)dµ= 0 for all φ ∈ Ek and v ∈ A 0(∂Ω), where Ek is the eigenspace associated to kth- eigenvalue µk. Hence, ∂φ ∂ν √ T(ν,ν) is locally constant on ∂Ω for any φ ∈ Ek. Let φ1 and φ2 be two eigenfunctions in Ek, one can ﬁnd a linear combination αφ1 + βφ2 =: φ so that ∂φ ∂ν vanishes, at least, on one connected component of Ω. To this end, it is suﬃcient to choose α and β such that α∂φ1 ∂ν √ T(ν,ν) = −β∂φ2 ∂ν √ T(ν,ν). Applying the principle of the unique continuation [ 13], we deduce that φ is identi- cally zero and that µk is simple. To complete the proof, we must show that, for all φ∈ Ek, ( ∂φ ∂ν ) 2 T(ν,ν) takes the same constant value on all the connected components o f ∂Ω. For it, let Σ 1 and Σ 2 be two distinct connected components of ∂Ω and let v∈ A 0(∂Ω) be the function given by v= vol(Σ 2) on Σ 1, v= −vol(Σ 1) on Σ 2 and v= 0 on the others components. Then the condition ∫ ∂Ω v ( ∂φ ∂ν ) 2 T(ν,ν)dµ= 0 implies that ( ∂φ ∂ν ) 2 T(ν,ν) ⏐ ⏐ ⏐ ⏐ ⏐ Σ 1 = ( ∂φ ∂ν ) 2 T(ν,ν) ⏐ ⏐ ⏐ ⏐ ⏐ Σ 2 which completes the proof of our assertion. Notice that the same a rguments hold in the case µk <µk+1. □ 9. Applications to Ricci ﬂow on closed manifold In this section, we study some properties of the eigenvalues of Lg(t) along the Ricci ﬂow equation d dtg(t) = −2Ricg(t) on an ( n ≥ 3)-dimensional closed smooth manifold Mn, where Ricg(t) stands for the Ricci curvature of the Riemannian manifold ( M,g(t)). Hamilton [ 11] established the existence and the uniqueness of solutions to the Ricci ﬂow equation in a maximal interval [0 ,δ), δ ≤ +∞, for any given initial Riemannian metric g0 = g(0). This maximal solution is then called",
      "chunk_index": 22
    },
    {
      "index": 518,
      "chunk_id": "DALLEval2024_chunk_23",
      "source_id": "DALLEval2024",
      "text": "Hamilton [ 11] established the existence and the uniqueness of solutions to the Ricci ﬂow equation in a maximal interval [0 ,δ), δ ≤ +∞, for any given initial Riemannian metric g0 = g(0). This maximal solution is then called the Ricci ﬂow with initial condition g0, and δ (whenever ﬁnite) is called the blow-up time of the ﬂow. We start by observing that the eigenvalues of Lg(t)(·) = div η(Tg(t)∇(·)) can be parametrized C2-diﬀerentiable in t, see Theorem ( C) in [ 15]. Now, we derive a general evolution formula for the eigenvalues of Lg(t) along the Ricci ﬂow. 20 C. L. CUNHA 1, J. N. GOMES 2 AND M. A. M. MARROCOS 3 Proposition 8. If λ(t) denotes the evolution of an eigenvalue of Lg(t) along the Ricci ﬂow on Mn, then λ′ = ∫ M R(λu2 − T(∇u,∇u))dm + ∫ M [ 4Ric(T∇u,∇u) + T′(∇u,∇u) ] dm, (9.1) where u stands for an eigenfunction associated to the eigenvalue λ, and R is the scalar curvature. Proof. Taking H = −2Ricg(t) we get TH(∇u,∇u) = HT(∇u,∇u) = −2Ric(T∇u,∇u). Thus, by deﬁnition of HT we have HT (∇u,∇u) = 4 Ric(T∇u,∇u) + T′(∇u,∇u). Since h= −2Rand L u2 = 2 uL u+ 2T(∇u,∇u), we obtain from (4.2) λ′(t) = ∫ M h 4 L u2 + HT (∇u,∇u)dm = ∫ M R(λu2 − T(∇u,∇u))dm + ∫ M 4Ric(T∇u,∇u) + T′(∇u,∇u)dm, which is (9.1). □ Let us now consider the behavior of the spectrum when we evolve an initial metric that is homogeneous. Hamilton showed that the Ricci ﬂow pre serves the isometries of the initial Riemannian manifold. Hence, the evolving metr ic remains homogeneous along the ﬂow. This important observation implies the f ollowing: Theorem 4. Let λ(t) be the evolution of an eigenvalue of Lg(t) along the Ricci ﬂow on a closed homogeneous Riemannian manifold Mn. If T′ ≥ − 4Ric(T,·), then λ(t) is non-decreasing along the ﬂow. Proof. Since the evolving metric remains homogeneous along the ﬂow, we have that R is constant for all time. So, from equation (2.1) and Proposition 8 we deduce that λ′ = ∫ M 4Ric(T∇u,∇u) + T′(∇u,∇u)dm and the result follows from the assumption that T′ ≥ − 4Ric(T,·). □ Example 1. Let g(t) be a solution to the Ricci ﬂow on a closed homogeneous Riemannian manifold Mn such that the initial Riemannian metric has strictly pos- itive Ricci curvature and",
      "chunk_index": 23
    },
    {
      "index": 519,
      "chunk_id": "DALLEval2024_chunk_24",
      "source_id": "DALLEval2024",
      "text": "from the assumption that T′ ≥ − 4Ric(T,·). □ Example 1. Let g(t) be a solution to the Ricci ﬂow on a closed homogeneous Riemannian manifold Mn such that the initial Riemannian metric has strictly pos- itive Ricci curvature and it continues so for all time. Consi der Tt = ψg(t), for some positive smooth function ψ on Mn. Then, T′ > −4Ric(T,·) and hence the eigenvalue λ(t) of divη(ψ∇(·)) is non-decreasing along the ﬂow. For a three-dimensional closed smooth manifold, Hamilton also prove d that if the initial Riemannian metric has strictly positive Ricci curvature, th en it continues so for all time. Theorem 5. Let g(t) be a solution to the Ricci ﬂow on a three-dimensional closed Riemannian manifold M3 with initial Ricci curvature that is strictly positive. Then, there exist t0 ∈ [0,δ) such that the eigenvalue λ(t) of divη(ψ∇(·)) increase for t∈ [t0,δ). ON EIGENV ALUES OF A CLASS OF SECOND ORDER ELLIPTIC OPERATORS 21 Proof. Let g(t) be a solution to the Ricci ﬂow on a closed three-manifold M with strictly positive initial Ricci curvature, then R >0 and there is ε >0 such that Ric ≥ εRg, and we can assume 2 ε− 1 ≤ 0. So both conditions remain valid on 0 ≤ t<δ (see [ 11, Theorem 9.4]), and then by (9.1) λ′(t) = λ ∫ M u2Rdm − ∫ M Rψ|∇u|2dm + 2 ∫ M ψRic(∇u,∇u)dm ≥ λRmin(t) − ∫ M Rψ|∇u|2dm + 2 ε ∫ M Rψ|∇u|2dm ≥ λ(Rmin(t) + (2 ε− 1)Rmax(t)). By Theorem 15.1 in [ 11], we have Rmax(t) Rmin(t) → 1 as t → δ. Then there is t0 ≥ 0 so that −(2ε− 1) < Rmin(t) Rmax(t) ≤ 1 ∀t0 ≤ t<δ, from which we get λ′(t) >0 for all t∈ [t0,δ), which proves our theorem. □ Let L u = div η(∇u) be the drifted Laplacian and Ricη := Ric+ ∇2η be the Bakry-´Emery-Ricci tensor. From the classical Bochner's formula, we obt ain 2L|∇u|2 = Ricη(∇u,∇u) + ⟨∇(Lu),∇u⟩+ |∇2u|2. (9.2) Besides, it is true that ⟨∇ψ,∇|∇u|2⟩ = 2 ⟨∇∇u∇u,∇ψ⟩ = 2( ∇u)⟨∇ψ,∇u⟩ − 2∇2ψ(∇u,∇u), for any smooth function ψ on M. So, ψL|∇u|2 = L |∇u|2 − ⟨∇ψ,∇|∇u|2⟩ = L |∇u|2 − 2(∇u)⟨∇ψ,∇u⟩+ 2∇2ψ(∇u,∇u). (9.3) But, (∇u)⟨∇ψ,∇u⟩ = ⟨∇u,∇⟨∇u,∇ψ⟩⟩ = ⟨∇u,∇(L u)⟩ − ⟨∇u,∇(ψLu)⟩ = div η(L u∇u) − (L u)Lu− divη ( ψLu∇u ) + ψ(Lu)2 (9.4) and ψ⟨∇(Lu),∇u⟩ = div η (",
      "chunk_index": 24
    },
    {
      "index": 520,
      "chunk_id": "DALLEval2024_chunk_25",
      "source_id": "DALLEval2024",
      "text": "L |∇u|2 − ⟨∇ψ,∇|∇u|2⟩ = L |∇u|2 − 2(∇u)⟨∇ψ,∇u⟩+ 2∇2ψ(∇u,∇u). (9.3) But, (∇u)⟨∇ψ,∇u⟩ = ⟨∇u,∇⟨∇u,∇ψ⟩⟩ = ⟨∇u,∇(L u)⟩ − ⟨∇u,∇(ψLu)⟩ = div η(L u∇u) − (L u)Lu− divη ( ψLu∇u ) + ψ(Lu)2 (9.4) and ψ⟨∇(Lu),∇u⟩ = div η ( ψLu∇u ) − (L u)Lu. (9.5) Joining (9.2), (9.3), (9.4) and (9.4), we get the Bochner type formu la for the oper- ator L u= div η(ψ∇u) as follows 2L |∇u|2 = div η(L u∇u) + ψRicη(∇u,∇u) − 2(L u)Lu+ ψ(Lu)2 +ψ|∇2u|2 − ∇2ψ(∇u,∇u). (9.6) This formula will be useful in the proof of our next result. Remark 4. Alencar, Neto and Zhou [1] showed a Bochner type formula for the operator that has been introduced by Cheng and Yau [7]. The Bochner type formula for the more general expression of L has been proved by Gomes and Miranda [9]. We know that in the hypothesis of Theorem 5 the solution to the Ricci ﬂow becomes extinct in ﬁnite time, in particular, lim t→δ Rmin(t) = ∞. 22 C. L. CUNHA 1, J. N. GOMES 2 AND M. A. M. MARROCOS 3 Theorem 6. Let g(t) be the solution to the Ricci ﬂow on a three-dimensional closed Riemannian manifold M3 with strictly positive initial Ricci curvature and λ(t) the evolution of an eigenvalue of divη(ψ∇(·)), with ψ ≥ c for some constant c> 0. If ∇2ψ≤ 0, then lim t→δ λ(t) = ∞. Proof. Integrating (9.6) gives ∫ M ψRic(∇u,∇u)dm = 2 ∫ M (L u)Ludm − ∫ M ψ(Lu)2dm − ∫ M ψ|∇2u|2dm + ∫ M ∇2ψ(∇u,∇u)dm − ∫ M ψ∇2η(∇u,∇u)dm ≤ − 2λ ∫ M u(Lu)dm − ∫ M ψ∇2η(∇u,∇u)dm = 2 λ ∫ M |∇u|2dm − ∫ M ψ∇2η(∇u,∇u)dm ≤ 2λc−1 ∫ M ψ|∇u|2dm − ∫ M ψ∇2η(∇u,∇u)dm = 2 λ2c−1 − ∫ M ψ∇2η(∇u,∇u)dm. We already know that for any solution of the Ricci ﬂow on a closed thr ee- manifold with positive Ricci curvature, there exists ǫ >0 such that Ric ≥ ǫRg is preserved along the ﬂow. Thus 2λ2c−1 − ∫ M ψ∇2η(∇u,∇u)dm ≥ ǫ ∫ M Rψ|∇u|2dm ≥ ǫRminλ. Now, note that ∇2η(∇u,∇u) ≥ d|∇u|2, for some constant d(t) >0, and then λ(t) ≥ c(ǫRmin(t) + d(t))/2. Since lim t→δ Rmin(t) = ∞, the proof is complete. □ Acknowledgements: The ﬁrst author is wholeheartedly acknowledging the ﬁnancial support by Funda¸ c˜ ao de Amparo ` a Pesquisa do Estado do Amazonas - FAPEAM",
      "chunk_index": 25
    },
    {
      "index": 521,
      "chunk_id": "DALLEval2024_chunk_26",
      "source_id": "DALLEval2024",
      "text": "≥ c(ǫRmin(t) + d(t))/2. Since lim t→δ Rmin(t) = ∞, the proof is complete. □ Acknowledgements: The ﬁrst author is wholeheartedly acknowledging the ﬁnancial support by Funda¸ c˜ ao de Amparo ` a Pesquisa do Estado do Amazonas - FAPEAM in terms of a doctoral scholarship. The second author has been par- tially supported by Conselho Nacional de Desenvolvimento Cient´ ıﬁco e Tecnol´ ogico (CNPq), of the Ministry of Science, Technology and Innovation of B razil (Grants 428299/2018-0 and 310458/2021-8). The third author has been partially supported by S˜ ao Paulo Research Foundation (FAPESP, Grant 2016/10009- 3), by CNPq (Grant 428299/2018-0) and fully supported by his beloved wife. Th e authors would like to express their sincere thanks to D. Tsonev for useful comme nts, discussions and constant encouragement. References [1] H. Alencar, G. S. Neto and D. Zhou, Eigenvalue estimates f or a class of elliptic diﬀerential operators on compact manifolds, Bull. Braz. Math. Soc. (N.S .) 46 (3) (2015), 491-514. [2] M.C. Ara´ ujo Filho and J.N.V. Gomes, Estimates of eigenv alues of an elliptic diﬀerential system in divergence form, Z. Angew. Math. Phys. 73 (2022) 21 0. ON EIGENV ALUES OF A CLASS OF SECOND ORDER ELLIPTIC OPERATORS 23 [3] S. Bando and H. Urakawa, Generic properties of the eigenv alue of the Laplacian for compact Riemannian manifolds, Tohoku Math. J. 35 (1983) 155-172. [4] M. Berger, Sur les premi` eres valeurs propres des vari´ e t´ es Riemanniennes, Compos. Math. 26 (1973) 129-149. [5] Y. Canzani, On the multiplicity of eigenvalues of confor mally covariant operators, Ann. Inst. Fourier 64 (3) (2014) 947-970. [6] X. Cao, S. Hou and J. Ling, Estimate and monotonicity of th e ﬁrst eigenvalue under the Ricci ﬂow, Math. Ann. 354 (2) (2012) 451-463. [7] S. Y. Cheng and S. T. Yau, Hypersurfaces with constant sca lar curvature, Math. Ann. 225 (1977), 195-204. [8] M.C. Delfour and J.-P. Zolesio, Shapes and Geometries: A nalysis, Diﬀerential Calculus, and Optimization. Society for Industrial and Applied Mathemat ics, 2001. [9] J.N.V. Gomes and J.F.R. Miranda, Eigenvalue estimates f or a class of elliptic diﬀerential operators in divergence form, Nonlinear Anal. 176 (2018) 1- 19. [10] J.N.V. Gomes and M.A.M. Marrocos, On eigenvalue generi c properties of the Laplace- Neumann operator, J. Geom. Phys. 135 (2019) 21-31. [11] R.S. Hamilton, Three-manifolds with positive Ricci cu rvature, J. Diﬀerential Geom. 17 (1982) 255-306. [12] D.B. Henry,",
      "chunk_index": 26
    },
    {
      "index": 522,
      "chunk_id": "DALLEval2024_chunk_27",
      "source_id": "DALLEval2024",
      "text": "[10] J.N.V. Gomes and M.A.M. Marrocos, On eigenvalue generi c properties of the Laplace- Neumann operator, J. Geom. Phys. 135 (2019) 21-31. [11] R.S. Hamilton, Three-manifolds with positive Ricci cu rvature, J. Diﬀerential Geom. 17 (1982) 255-306. [12] D.B. Henry, Perturbation of the boundary in boundary-v alue problems of partial diﬀerential equations, Cambridge University Press, 2005. [13] L. H¨ ormander, Linear partial diﬀerential operators, Springer, New York, 1963. [14] T. Kato, Perturbation Theory for Linear Operators, Spr inger, 1980. [15] A. Kriegl and P.W. Michor, Diﬀerentiable perturbation of unbounded operators, Math. Ann. 327 (2003) 191-201. [16] J.L. Lions and E. Magenes, Non-homogeneous boundary va lue problems and applications, Die Grundlehren der mathematischen Wissenschaften 181 (19 72) Springer, New York. [17] M.A.M. Marrocos and A.L. Pereira, Eigenvalues of the Ne umann Laplacian in symmetric regions, J. Math. Phys. 56 (2015) 111502. [18] J. Navarro, On second-order, divergence-free tensors , J. Math. Phys. 55 (2014) 062501. [19] D. Serre, Divergence-free positive symmetric tensors and ﬂuid dynamic, Ann. Inst. H. Poincar´ e Anal. Non Lin´ eaire 35 (5) (2018), 1209-1234. [20] A. El Soulﬁ and S. Ilias, Domain deformations and eigenv alues of the Dirichlet Laplacian in a Riemannian manifolds, Illinois J. Math. 51 (2007) 645-666 . [21] K. Uhlenbeck, Generic Properties of Eigenfunctions, A mer. J. Math. 98 (4) (1976) 1059-1078. [22] C.T.C. Wall, Singular Points of Plane Curves, London Ma thematical Society Student Texts, 2004. 1Instituto de Educac ¸˜ao, Agricultura e Ambiente, Universidade Federal do Ama- zonas, Rua 29 de Agosto, 786, 69.800-000 Humait ´a, Amazonas, Brazil. 2Departamento de Matem ´atica, Universidade Federal de S ˜ao Carlos, Rod. W ash- ington Lu´ıs, Km 235, 13.565-905 S ˜ao Carlos, S ˜ao Paulo, Brazil. 3Departamento de Matem ´atica, Universidade Federal do Amazonas, A v. General Rodrigo Oct ´avio, 6200, 69.080-900 Manaus, Amazonas, Brazil Email address : 1ncleiton@ufam.edu.br Email address : 2jnvgomes@ufscar.br Email address : 3marcusmarrocos@ufam.edu.br URL: 1,3https://www.ufam.edu.br URL: 2https://www2.ufscar.br",
      "chunk_index": 27
    },
    {
      "index": 523,
      "chunk_id": "LongForm_Faithfulness2023_chunk_00",
      "source_id": "LongForm_Faithfulness2023",
      "text": "An Examination of the Robustness of Reference-Free Image Captioning Evaluation Metrics Saba Ahmadi Mila Université de Montréal saba.ahmadi@mila.quebec Aishwarya Agrawal Mila Université de Montréal Canada CIFAR AI Chair aishwarya.agrawal@mila.quebec Abstract Recently, reference-free metrics such as CLIP- Score (Hessel et al., 2021), UMIC (Lee et al., 2021), and PAC-S (Sarto et al., 2023) have been proposed for automatic reference-free evalua- tion of image captions. Our focus lies in eval- uating the robustness of these metrics in sce- narios that require distinguishing between two captions with high lexical overlap but very dif- ferent meanings. Our findings reveal that de- spite their high correlation with human judg- ments, CLIPScore, UMIC, and PAC-S struggle to identify fine-grained errors. While all met- rics exhibit strong sensitivity to visual ground- ing errors, their sensitivity to caption implausi- bility errors is limited. Furthermore, we found that all metrics are sensitive to variations in the size of image-relevant objects mentioned in the caption, while CLIPScore and PAC-S are also sensitive to the number of mentions of image- relevant objects in the caption. Regarding lin- guistic aspects of a caption, all metrics show weak comprehension of negation, and CLIP- Score and PAC-S are insensitive to the struc- ture of the caption to a great extent. We hope our findings will guide further improvements in reference-free evaluation of image caption- ing. Our code and dataset are publicly available at: https://github.com/saba96/img-cap-metrics- robustness. 1 Introduction Image caption quality has been traditionally eval- uated using a reference-based approach, with met- rics like BLEU (Papineni et al., 2002), ROUGE (Lin, 2004), METEOR (Banerjee and Lavie, 2005), and CIDEr (Vedantam et al., 2014) assessing the lexical overlap between generated and reference captions. However, this approach is restrictive as the set of references may not capture the full range of valid captions, and furthermore, lexical overlap- based metrics tend to favor captions with similar vo- cabulary but different meanings. To address these limitations, recent studies like CLIPScore (Hessel . Figure 1: Recently proposed reference-free image cap- tioning evaluation metrics such as CLIPScore, UMIC, and PAC-S are far from perfect. This figure shows how these metrics cannot tell apart an incorrect caption (shown in red) from a correct caption when there is a high lexical overlap between them. et al., 2021), UMIC (Lee et al., 2021) and PAC-S (Sarto et al., 2023) have proposed reference-free approaches for evaluating image caption quality, which more closely aligns with human",
      "chunk_index": 0
    },
    {
      "index": 524,
      "chunk_id": "LongForm_Faithfulness2023_chunk_01",
      "source_id": "LongForm_Faithfulness2023",
      "text": "correct caption when there is a high lexical overlap between them. et al., 2021), UMIC (Lee et al., 2021) and PAC-S (Sarto et al., 2023) have proposed reference-free approaches for evaluating image caption quality, which more closely aligns with human judgments. These approaches leverage large pretrained image- text matching models to measure the similarity be- tween a given image and a candidate caption. How- ever, the evaluation benchmarks for these metrics do not necessarily involve differentiating between captions with significant lexical overlap but vastly different meanings (Fig. 1). In this work, we evalu- ate the robustness of these reference-free metrics in scenarios where the correct and incorrect cap- tions have high lexical overlap. To our surprise, we found that all metrics fail to distinguish be- tween correct and incorrect captions ∼46% of the time. In a pursuit to identify what aspects of a cap- tion (e.g., plausibility, visual grounding, number and size of objects mentioned in the caption, nega- tion and sentence structure) these metrics are most sensitive to, we conduct several controlled exper- iments, varying one aspect at a time. We found that: • All metrics show limited sensitivity to caption implausibility errors but a heightened sensitiv- ity to visual grounding errors. arXiv:2305.14998v2 [cs.CL] 6 Feb 2024 • CLIPScore and PAC-S show high sensitivity to the number of image-relevant objects men- tioned in the caption while UMIC shows lim- ited sensitivity. • All metrics are sensitive to the size of image- relevant objects mentioned in the caption. • All metrics exhibit a weak understanding of negation. • UMIC is sensitive to sentence structure, whereas CLIPScore and PAC-S demonstrate limited sensitivity. • UMIC prioritizes correct sentence structure over mentions of larger objects or number of objection mentions in captions, whereas CLIP- Score and PAC-S exhibit the opposite behav- ior. Our primary contribution is highlighting specific areas where reference-free metrics exhibit limita- tions so that caution can be exercised when using these metrics for image captioning evaluation. We hope our findings will guide further improvements in reference-free evaluation of image captioning. 2 Related Works Reference-free metrics: We study the robustness of CLIPScore (Hessel et al., 2021), UMIC (Lee et al., 2021) and PAC-S (Sarto et al., 2023). CLIP- Score measures the similarity between the image and the candidate caption using a scaled cosine similarity of the image and text representations from the CLIP (Radford et al., 2021) model. On the other",
      "chunk_index": 1
    },
    {
      "index": 525,
      "chunk_id": "LongForm_Faithfulness2023_chunk_02",
      "source_id": "LongForm_Faithfulness2023",
      "text": "and PAC-S (Sarto et al., 2023). CLIP- Score measures the similarity between the image and the candidate caption using a scaled cosine similarity of the image and text representations from the CLIP (Radford et al., 2021) model. On the other hand, UMIC utilizes the UNITER (Chen et al., 2020) model, which is pretrained to align im- age and text pairs, and finetunes it via contrastive learning to distinguish reference captions from its hard negatives. PAC-S (Sarto et al., 2023) intro- duces a novel metric that strategically curates pos- itive pairs for contrastive learning, enhancing the multimodal embedding space of CLIP. PAC-S em- ploys scaled cosine similarity, akin to CLIPScore, to evaluate the similarity between the candidate caption and the provided image. SMURF (Fein- glass and Yang, 2021) is another recently proposed metric for image caption evaluation, which has a reference-free evaluation of the fluency of the caption; however, the evaluation of the semantic correctness of the caption is still reference-based. Also, InfoMetIC (Hu et al., 2023) has the capability Long Answer: The color of the shirt this tennis player is wearing is red. Completed By Model Please summarize the question and answer in one sentence. Question: What color is the table? Answer: brown Long answer: The color of table is brown. Question: What color is the front of the train? Answer: red and black Long Answer: The color of the front of the train is red and black. Support Examples Question: What color of shirt is this tennis player wearing? Answer: redPrompt Figure 2: Generating caption-like sentences by trans- forming visual question-answer pairs using GPT-J. to pinpoint incorrect words and overlooked image areas at a fine-grained level while also providing an overall quality score at a coarse-grained level. Vision-language benchmarks: Recently, a number of vision-language benchmarks have been proposed to evaluate the fine-grained understanding of relations, attributes, actions, and visio-linguistic compositionality in vision-language models, such as CAB (Yamada et al., 2022), Winoground (Thrush et al., 2022), ARO (Yuksekgonul et al., 2023), VL-checklist (Zhao et al., 2022), CREPE (Ma et al., 2023) and V ALSE (Parcalabescu et al., 2022). Although these evaluations also highlight the limitations of current models towards fine- grained understanding, our focus is specifically on evaluating the robustness of recently proposed reference-free image-captioning metrics. Our goal is to identify the scenarios where these metrics fail to distinguish between correct and incorrect cap- tions to ensure the cautious use",
      "chunk_index": 2
    },
    {
      "index": 526,
      "chunk_id": "LongForm_Faithfulness2023_chunk_03",
      "source_id": "LongForm_Faithfulness2023",
      "text": "grained understanding, our focus is specifically on evaluating the robustness of recently proposed reference-free image-captioning metrics. Our goal is to identify the scenarios where these metrics fail to distinguish between correct and incorrect cap- tions to ensure the cautious use of these metrics in such scenarios. 3 Datasets Used to Conduct the Examination 3.1 Dataset Creation To conduct our examination of the robustness of the metrics, we use a dataset of generated image captions. We generate image captions in one of the following ways, depending on the question we are trying to answer (see section 4 for more details): QA to caption conversion : We employ GPT- J prompting to transform visual question-answer pairs into caption-like sentences. We use the ques- tions from the popular VQAv2 (Goyal et al., 2016) dataset, and the answers could either be ground- truth answers or model-generated, depending on the analysis. Figure 2 shows an example caption- like sentence generated by GPT-J along with the prompt and support examples. The support exam- ples are specific to the question type of the input question. More details about support example se- lection can be found in Appendix A.1. To clarify the motivation to generate captions in this manner, it is essential to outline the limita- tions of existing captioning datasets such as FOIL (Shekhar et al., 2017), ARO, and Winoground. These datasets mostly rely on modifying ground- truth captions by shuffling or swapping words to create incorrect captions. While these evaluation methods offer valuable insights, they are limited in their ability to comprehensively assess image- captioning metrics as these incorrect captions are out-of-distribution and easy for models to identify as incorrect (Hsieh et al., 2023). For our study, we generate captions from VQA question-answer pairs instead of using these exist- ing datasets for two primary reasons. Firstly, lever- aging the VQAv2 dataset facilitates a comprehen- sive evaluation of image-captioning metrics' robust- ness across various skills, such as color recognition, counting, etc. Moreover, using model-generated answers to create incorrect captions helps us con- struct a dataset that mirrors real-world use cases of image captioning metrics, i.e., using metrics to evaluate model-generated responses (note that the VQA answers are obtained from a model that was first pretrained for image captioning and then fine- tuned for VQA). Specifically, the incorrect captions generated using our approach contain plausible er- rors. This is attributed to the model's tendency to produce reasonable responses,",
      "chunk_index": 3
    },
    {
      "index": 527,
      "chunk_id": "LongForm_Faithfulness2023_chunk_04",
      "source_id": "LongForm_Faithfulness2023",
      "text": "obtained from a model that was first pretrained for image captioning and then fine- tuned for VQA). Specifically, the incorrect captions generated using our approach contain plausible er- rors. This is attributed to the model's tendency to produce reasonable responses, such as providing a color for a color-related question or a numerical answer for a counting inquiry. Furthermore, the model typically generates answers that are visually relevant to the image, even if they do not precisely match the query. For example, for an image con- taining a person wearing yellow pants and a red car, the model might incorrectly respond with \"red.\" to a question asking about the color of the pants. Thus, our dataset holds value as the generated captions are plausible as well as contain visually relevant errors. For a detailed comparison of our dataset with FOIl, ARO, and Winoground, please refer to Appendix A.2. Caption templates : To conduct a controlled study of robustness of image captioning metrics towards specific factors such as number and size of objects mentioned in the caption, we gener- ate captions using templates in the format of the \"There is a/an [object name].\". We utilized the COCO detection dataset (Lin et al., 2014) to extract the names of objects in each image. This dataset provides object tags across 90 categories and attributes like objects' areas. The sentence con- struction process is elaborated within each baseline description. We will make the dataset containing all the gen- erated captions publicly available for the purpose of reproducibility and future use by the community. 3.2 Dataset Analysis We conduct the following analyses of our generated captions dataset: Human verification: We collected human an- notations for 2000 captions: 1000 corresponding to correct VQA answers and 1000 incorrect ones. We asked five workers to determine whether the sentence is correct or incorrect. If it is incorrect, we additionally asked them to identify all relevant issues: 1) it is grammatically incorrect, 2) it is in- complete, i.e., it misses some information present in the original question-answer pair, 3) it hallu- cinates information, i.e., it contains information not present in the original question-answer pair or misrepresents information present in the question- answer pair. The majority voting across the work- ers' responses for each caption indicated that 255 instances were incorrect. Among these, 30 cap- tions were identified as grammatically incorrect, 24 captions were deemed incomplete, and 17 cap- tions were",
      "chunk_index": 4
    },
    {
      "index": 528,
      "chunk_id": "LongForm_Faithfulness2023_chunk_05",
      "source_id": "LongForm_Faithfulness2023",
      "text": "question- answer pair. The majority voting across the work- ers' responses for each caption indicated that 255 instances were incorrect. Among these, 30 cap- tions were identified as grammatically incorrect, 24 captions were deemed incomplete, and 17 cap- tions were flagged for hallucinating information, where a caption was counted towards a particular incorrectness category if at least two annotators voted for that category. We extended this analysis to 100 randomly sam- pled captions generated using the caption template method, and all samples were found to be correct, benefiting from their straightforward format. Comparing generated captions with human written captions: For the captions generated using the QA to caption conversionmethod, it is worth asking how the distribution of such captions com- pares with that of human written captions in exist- ing datasets, such as, COCO captions (Chen et al., 2015). To throw light on this, we refer to (An- tol et al., 2015) where they compared the distribu- tions of nouns, verbs, and adjectives mentioned in COCO captions with those mentioned in the VQA questions and answers, and found that they are statistically significantly different from each other (Kolmogorov-Smirnov test, p < 0.001). Conse- quently, we expect the captions generated through our QA to caption conversionmethod to exhibit dif- ferent distributions of nouns, verbs, and adjectives compared to the human-written captions. How- ever, (Antol et al., 2015) also show that the VQA questions and answers require a deeper understand- ing of images beyond what (human written) image captions typically capture. Thus, in spite of the differing word distributions between our generated captions and human written captions, we posit that our captions can be extremely valuable in stress testing the robustness of image caption evalua- tion metrics. 4 Experiments and Results Preliminary experiment: First, we describe our preliminary experiment that served as a motiva- tion for the rest of the study. We were interested in examining how different the scores assigned by reference-free image captioning metrics are for correct/incorrect captions created by converting questions and correct/incorrect answers from the VQAv2 dataset to caption-like sentences. Cap- tions generated in this way are unique in that even for incorrect captions, a significant portion of it (corresponding to the question part) is still correct. Thus, such a dataset of captions serves as a good stress testdataset for examining the robustness of reference-free image captioning metrics. To obtain correct and incorrect answers, we ob- tained predictions",
      "chunk_index": 5
    },
    {
      "index": 529,
      "chunk_id": "LongForm_Faithfulness2023_chunk_06",
      "source_id": "LongForm_Faithfulness2023",
      "text": "it (corresponding to the question part) is still correct. Thus, such a dataset of captions serves as a good stress testdataset for examining the robustness of reference-free image captioning metrics. To obtain correct and incorrect answers, we ob- tained predictions from the ALBEF (Li et al., 2021) visual question answering model on the validation splits of the VQAv2(Goyal et al., 2016) dataset. We fine-tuned ALBEF on this dataset and con- ducted IID evaluation. We then converted each question and its corresponding ALBEF answer into a caption-like sentence as described in Section 3. We only use answers that match with either three or more human answers (and we classify them as correct answers) or that do not match with any hu- man answers (and we classify them as incorrect answers), resulting in a total of 179,297 answers (43389 incorrect and 135908 correct). The his- tograms of results for the VQAv2 dataset are pre- sented in Figure 3. We see a significant overlap between the distributions of scores for correct and incorrect captions for all metrics, highlighting the Answer Type CLIPScore UMIC PAC-S VQAv2- Correct 0.480 0 .394 0 .558 VQAv2- Incorrect 0.481 0 .403 0 .549 Table 1: CLIPScore, UMIC, and PAC-S comparison for caption-like sentences for incorrect and correct answers generated by ALBEF model for VQAv2 dataset. Answer Type CLIPScore UMIC PAC-S Correct yes/no 0.457 0 .355 0 .540 Incorrect yes/no 0.470 0 .392 0 .547 Correct numbers 0.468 0 .354 0.561 Incorrect numbers 0.477 0 .387 0 .553 Correct others 0.512 0 .452 0 .578 Incorrect others 0.485 0 .411 0 .548 Table 2: CLIPScore, UMIC, and PAC-S comparison for correct and incorrect caption-like sentences generated with different answer types from VQAv2 dataset. limitations of these metrics in precisely assessing caption quality. Score normalization: The UMIC final score, which is an output of a sigmoid function, has a value range between 0 and 1. On the other hand, the CLIPScore and PAC-S use the cosine similarity score scaled by a factor of 2.5 and 2, respectively. Although theoretically, CLIPScore can vary be- tween -2.5 and 2.5, and PAC-S can vary between -2 and 2, we have not observed negative scores, and they rarely exceed 1.0. The distributions of metrics are illustrated in Figure 3. While we do not directly compare the values of these metrics in this paper, we aim to contrast their sensitivity to different factors. To achieve",
      "chunk_index": 6
    },
    {
      "index": 530,
      "chunk_id": "LongForm_Faithfulness2023_chunk_07",
      "source_id": "LongForm_Faithfulness2023",
      "text": "scores, and they rarely exceed 1.0. The distributions of metrics are illustrated in Figure 3. While we do not directly compare the values of these metrics in this paper, we aim to contrast their sensitivity to different factors. To achieve this, we apply the min-max normalization separately to each metric for every experiment. This method allows us to evaluate the respective sensitivities of the metrics effectively. Please note that all reported scores are normalized, but the histograms are plotted using the original scores to accurately represent the original distributions. Score normalized results: As shown in Table 1, CLIPScore and UMIC assign higher average scores to incorrect captions compared to correct captions; however, PAC-S assigns higher average scores to correct captions. We conducted further analysis by examining the average scores assigned by these metrics for different answer types of the VQAv2 dataset (please refer to Table 2 for detailed scores). Specifically, we observed that for the 'yes/no' an- swer type, on average, all the metrics assign higher scores to incorrect captions. For the 'number' an- Fig. a: CLIPScore Fig. b: UMIC Fig. c: PAC-S Figure 3: Histograms of CLIPScore (Fig. a), UMIC (Fig. b), and PAC-S (Fig. c) for correct and incorrect caption-like sentences created using correct and incorrect answers from ALBEF for VQAv2 questions. Question CLIPScore CLIPScore UMIC UMIC PAC-S PAC-S Type Incorrect Correct Incorrect Correct Incorrect Correct how many 0.475 0 .468 0 .372 0 .354 0 .559 0 .562 what color 0.454 0 .466 0 .420 0 .517 0 .514 0 .542 what sport 0.480 0 .584 0 .299 0 .342 0 .513 0 .628 what animal 0.436 0 .544 0 .257 0 .322 0 .488 0 .623 what time 0.469 0 .405 0 .333 0 .282 0 .528 0 .492 what brand 0.440 0 .458 0 .481 0 .511 0 .497 0 .508 what type/kind 0.485 0 .537 0 .382 0 .417 0 .544 0 .594 where 0.501 0 .551 0 .380 0 .435 0 .561 0 .620 which 0.495 0 .529 0 .419 0 .414 0 .556 0 .581 what is/are the 0.497 0 .543 0 .436 0 .468 0 .559 0 .605 others 0.480 0 .471 0 .412 0 .370 0 .549 0 .550 Table 3: CLIPScore, UMIC, and PAC-S for correct and incorrect caption-like sentences generated for different question types of VQAv2. swer type, only PAC-S was able to assign higher average scores to",
      "chunk_index": 7
    },
    {
      "index": 531,
      "chunk_id": "LongForm_Faithfulness2023_chunk_08",
      "source_id": "LongForm_Faithfulness2023",
      "text": ".471 0 .412 0 .370 0 .549 0 .550 Table 3: CLIPScore, UMIC, and PAC-S for correct and incorrect caption-like sentences generated for different question types of VQAv2. swer type, only PAC-S was able to assign higher average scores to correct captions. However, for the 'others' answer type, all the metrics assign higher average scores to correct captions. For further investigation, we look at results for specific question types for VQAv2. As illustrated in Table 3), for CLIPScore, we observe that in- correct captions received higher scores on average for three question types: 'how many', 'what time' and 'others'. Also, UMIC assigns higher scores on average to incorrect captions for four question types: 'how many', 'what time', 'which', and 'oth- ers'. On the other hand, PAC-S assigns higher scores on average to incorrect captions for 'what time' and 'others' question types, suggesting all metrics show poor performance for 'what time' questions, which is considered to be a hard ques- tion type. Moreover, CLIPScore and UMIC show poor performance for 'how many' questions. Al- though PAC-S assigns higher average to correct captions over incorrect captions for 'how many' question type, the gap between the absolute values of average scores for correct and incorrect captions for 'how many' question is less than that for other question types. Controlled investigation to identify sensitiv- ity to various factors: Having established that these metrics struggle to distinguish the set of in- correct captions from the set of correct captions, in the following sections, we delve deeper into un- derstanding the underlying reasons for their failure. To validate the comparisons made between differ- ent group means and ensure the reliability of our claims, we conducted a t-test for each comparison, using a p-value threshold of 0.01 (p-value < 0.01). Notably, all reported comparisons successfully sat- isfied this predetermined threshold, affirming the robustness of our statistical analyses. 4.1 Sensitivity to fine-grained errors The primary objective of this section is to deter- mine the sensitivity of these metrics to fine-grained Answer Type CLIPScore UMIC PAC-S Ground Truth 0.479 0 .422 0 .542 Incorrect from ALBEF 0.468 0 .404 0 .535 Table 4: CLIPScore, UMIC, and PAC-S comparison for caption-like sentences for incorrect answers generated by ALBEF model for VQAv2 and captions generated with its ground truth counterpart. errors. An incorrect caption is said to have \"fine- grained errors\" if it has high lexical overlap with a correct caption.",
      "chunk_index": 8
    },
    {
      "index": 532,
      "chunk_id": "LongForm_Faithfulness2023_chunk_09",
      "source_id": "LongForm_Faithfulness2023",
      "text": "caption-like sentences for incorrect answers generated by ALBEF model for VQAv2 and captions generated with its ground truth counterpart. errors. An incorrect caption is said to have \"fine- grained errors\" if it has high lexical overlap with a correct caption. To obtain such pairs of correct and incorrect captions, we first generate incorrect captions corresponding to the questions for which ALBEF produced incorrect responses. Then, we generate correct captions using ground-truth an- swers for the same set of questions. We convert the questions and answers into captions using the method described in Section 3. We excluded ques- tions with yes/no answers from this study as we discuss them in Section 4.4. In total, we analyzed 38383 samples for this experiment. We quantify the degree of lexical overlap be- tween a pair of correct and incorrect captions in our dataset by measuring the F1 score between them. The mean F1 score across all such pairs in our dataset is 0.725. To place this in context, we measure the F1 score between pairs of correct (human-written) and incorrect (generated by image captioning models) captions from the Composite dataset (Aditya et al., 2017), a widely-used dataset for evaluating image captioning metrics (see Ap- pendix A.3 for more details on F1 score computa- tion for Composite dataset). The mean F1 score across all such pairs from the Composite dataset is 0.224, which is significantly lower than that for our dataset. This highlights the difficulty of our dataset making it suitable for stress testing the robustness of image captioning metrics. As demonstrated in Table 4, for all metrics, captions with ground truth answers received a higher average score compared to captions with fine-grained errors. Despite the higher average scores assigned to correct captions, the ranking re- sults reveal that these metrics often fail to prioritize correct captions over incorrect ones. CLIPScore fails to rank correct captions above incorrect cap- tions in 46.34% of cases, while UMIC fails to do so in 45.99% of cases. Also, PAC-S ranks incor- rect captions over correct captions in 46.84% of times. Thus, all metrics show weak sensitivity to detecting fine-grained errors. We also report a human baseline for the task of distinguishing correct captions from the ones with fine-grained errors. We collected five human annotations for 2000 examples using the Amazon Mechanical Turk platform, each example consist- ing of an image, a correct caption and an incorrect caption. We",
      "chunk_index": 9
    },
    {
      "index": 533,
      "chunk_id": "LongForm_Faithfulness2023_chunk_10",
      "source_id": "LongForm_Faithfulness2023",
      "text": "task of distinguishing correct captions from the ones with fine-grained errors. We collected five human annotations for 2000 examples using the Amazon Mechanical Turk platform, each example consist- ing of an image, a correct caption and an incorrect caption. We asked humans to indicate the best matching description. Majority voting across the worker responses for each caption revealed humans fail to identify correct caption from incorrect cap- tion in 15.4% cases. This shows human perfor- mance is far better than the metrics' performance which fail to rank correct captions above incorrect captions around 46% of the time. 4.2 Are metrics differently sensitive to different kinds of fine-grained errors? Figure 4: Captions from ground truth, plausible an- swer, an object from the image and a random asnwer of VQAv2. The main aim of this experiment is to assess if the metrics exhibit varying sensitivity to different types of fine-grained errors, in particular visual grounding errors and caption implausibility errors. To assess this, we generated three types of incor- rect captions for each correct caption by replacing the ground-truth answer in the correct caption with: a plausible but incorrect answer (visual ground- ing error), an object found in the image (caption implausibility error), and a random answer (see Figure 4 for an example and see Appendix A.4 for more details on plausible answers). For this experiment, we limited our investigation to the following question types: 'what number is', 'what time', 'what color', and 'what brand', as their answers are non-object entities and, therefore, are not present in the COCO Detection dataset. Thus, when constructing a sentence using an object in the image, we can be sure that it would result in an incorrect caption for the image. We analyzed 23841 sets of 4 captions each for this experiment. Answer Type CLIPScore UMIC PAC-S Ground Truth 0.501 0 .487 0 .576 Plausible 0.474 0 .242 0 .527 Object from Image 0.526 0 .354 0 .601 Random 0.458 0 .275 0 .522 Table 5: CLIPScore, UMIC, and PAC-S comparison for caption-like sentences from VQAv2 ground truth, plausible, object from image and random answers. As illustrated in Table 5, the score difference be- tween the correct captions and the captions with im- plausibility errors is significantly smaller than the difference between the correct captions and the cap- tions with visual grounding errors. This indicates that the metrics exhibit lower sensitivity to caption implausibility errors and",
      "chunk_index": 10
    },
    {
      "index": 534,
      "chunk_id": "LongForm_Faithfulness2023_chunk_11",
      "source_id": "LongForm_Faithfulness2023",
      "text": "correct captions and the captions with im- plausibility errors is significantly smaller than the difference between the correct captions and the cap- tions with visual grounding errors. This indicates that the metrics exhibit lower sensitivity to caption implausibility errors and higher sensitivity to vi- sual grounding errors. Notably, both CLIPScore and PAC-S assigned higher average scores to cap- tions with implausibility errors compared to ground truth answers, and only UMIC assigned higher av- erage score to captions with ground truth answers. In the following sections, we further examine the sensitivity of the metrics to various visual and lin- guistic aspects. 4.3 Visual Aspects In this section, our objective is to assess the sen- sitivity of the metrics to the size and number of objects mentioned in the caption. Importantly, we would like to highlight that our focus is on analyz- ing how the size and number of objects mentioned in captions affect metric robustness and sensitiv- ity. We refrain from making value judgments about whether these effects are good or bad. 4.3.1 Sensitivity to the number of object mentions in the caption In this section, we aim to evaluate the sensitivity of the metrics to the number of objects mentioned in the caption. To conduct this evaluation, we filter images from COCO Detection dataset (Lin et al., 2014) having a minimum of three object tags and randomly select three object tags for each image and utilize their corresponding object names to form sentences, depicting one, two, and three ob- jects presented in the image (see Figure 5). We analyzed 19412 images for this experiment. As presented in the first three rows of Table 6, CLIPScore and PAC-S scores for captions with three objects are significantly higher than for cap- tions with two objects. Also, captions with two ob- Number of Objects CLIPScore UMIC PAC-S One Object 0.449 0 .205 0 .500 Two Objects 0.512 0 .212 0 .540 Three Objects 0.561 0 .195 0 .578 Shuffled One Object 0.445 0 .139 0 .503 Shuffled Two Objects 0.499 0 .148 0 .541 Shuffled Three Objects 0.540 0 .169 0 .576 Table 6: CLIPScore, UMIC, and PAC-S comparison for sentences with various number of objects name, and their shuffled counterparts. Figure 5: Captions referring to different number of objects from the image. jects score significantly higher than those with one object. In contrast, for UMIC, captions with one, two, and three objects",
      "chunk_index": 11
    },
    {
      "index": 535,
      "chunk_id": "LongForm_Faithfulness2023_chunk_12",
      "source_id": "LongForm_Faithfulness2023",
      "text": "number of objects name, and their shuffled counterparts. Figure 5: Captions referring to different number of objects from the image. jects score significantly higher than those with one object. In contrast, for UMIC, captions with one, two, and three objects received average scores of 0.205, 0.212, and 0.195, respectively. Although the t-test indicated statistically significant differences between scores across different object counts, the gap between absolute score values is smaller for UMIC than for CLIPScore and PAC-S. In conclu- sion, CLIPScore and PAC-S display a height- ened sensitivity to the number of image-relevant objects mentioned in the caption, while UMIC shows limited sensitivity towards this factor. 4.3.2 Sensitivity to size of objects mentioned in the caption In this experiment, our primary goal is to examine the effect of object size mentioned in captions on the metrics. To achieve this, we utilize the COCO Detection dataset (Lin et al., 2014) to select one Candidate Captions CLIPSore UMIC PAC-S Small Object: There is a knife. 0.460 0.507 0.561 Big Object: There is a pizza. 0.632 0.469 0.718 Shuffled Small Object: A there knife is. 0.480 0.268 0.561 Shuffled Big Object: A there pizza is. 0.664 0.250 0.719 Figure 6: Captions referring to small and large area of the image and their shuffled counterparts. Object Size CLIPScore UMIC PAC-S Small Object 0.396 0 .317 0 .492 Big Object 0.434 0 .232 0 .580 Shuffled Small Object 0.390 0 .205 0 .495 Shuffled Big Object 0.436 0 .170 0 .590 Table 7: CLIPScore, UMIC, and PAC-S comparison for captions referring to small and a big objects in the image, and their shuffled counterparts. small and one large object from the same image with a noticeable difference in the area (see Figure 6 for an example and for detailed explanation see Appendix A.5.). As a result, we selected 24610 images for further analysis. As demonstrated in the first two rows of Table, 7, for CLIPScore and PAC-S, captions with smaller objects received a lower average score than those with bigger objects. On the other hand, UMIC assigned a higher average score to captions with smaller objects compared to captions with bigger objects. Overall, all metrics demonstrate sensi- tivity to the size of image-relevant objects men- tioned in the caption. 4.4 Linguistic Aspects 4.4.1 Sensitivity to negation To assess the ability of metrics to distinguish be- tween correct captions and their negated versions, we created 80530",
      "chunk_index": 12
    },
    {
      "index": 536,
      "chunk_id": "LongForm_Faithfulness2023_chunk_13",
      "source_id": "LongForm_Faithfulness2023",
      "text": "demonstrate sensi- tivity to the size of image-relevant objects men- tioned in the caption. 4.4 Linguistic Aspects 4.4.1 Sensitivity to negation To assess the ability of metrics to distinguish be- tween correct captions and their negated versions, we created 80530 captions-like sentences by us- ing the questions with 'yes' or 'no' ground-truth answers from the validation split of VQAv2. Addi- tionally, we generated negated captions by negating the ground truth answer. For CLIPScore, correct captions received a higher score of 0.457, and their negated versions got 0.450 on average. For UMIC, correct cap- tions received a higher average of 0.359, and their negated versions got 0.335 on average. Correct captions received a higher average of 0.556 for PAC-S, and their negated versions got 0.548 on average. Although the correct captions scored sta- tistically significantly higher than the negated ones, CLIPScore, UMIC, and PAC-S ranked the negated caption above the correct caption incorrectly in 41.36%, 44.24%, and 41.83% of cases, respectively. Thus, all metrics exhibit a weak understanding of negation. 4.4.2 Sensitivity to the sentence structure To evaluate the sensitivity of the metrics to sentence structure, we generated 214354 caption-like sen- tences with VQAv2 ground truth answers and then shuffled them. For CLIPScore, correct captions re- ceived 0.469, and their shuffled version got 0.450 on average. For UMIC, correct captions received 0.400, and their shuffled version got 0.211 on av- erage. Correct captions received 0.548 for PAC-S, and their shuffled version got 0.539 on average. Despite higher average scores assigned to correct captions, the ranking results reveal that CLIPScore fails to rank the correct caption higher than the shuffled one in 34.32% of cases, contrasting with UMIC, where this occurs in only 9.18% of cases. Additionally, PAC-S falls short, assigning a higher score to the correct caption than the shuffled one in 43.05% of cases. This indicates that UMIC is more responsive to the structure of the sentence compared to CLIPScore and PAC-S. 4.5 Visio-Linguistic Aspects 4.5.1 Sentence Structure versus Visual Aspects In order to compare the sensitivity of metrics to sentence structure and object size, we conducted a sentence shuffling experiment using captions that contained objects of varying sizes, as described in Section 4.3. We shuffle both big and small object captions in the same order (see Figure 6). As shown in Table 7, our results demonstrate that CLIPScore and PAC-S assign the highest scores to captions referring to a",
      "chunk_index": 13
    },
    {
      "index": 537,
      "chunk_id": "LongForm_Faithfulness2023_chunk_14",
      "source_id": "LongForm_Faithfulness2023",
      "text": "described in Section 4.3. We shuffle both big and small object captions in the same order (see Figure 6). As shown in Table 7, our results demonstrate that CLIPScore and PAC-S assign the highest scores to captions referring to a larger area of the image, regardless of whether they are shuffled or not. In contrast, UMIC exhibits the opposite trend, with the highest scores assigned to correct (i.e., unshuffled) sentences, re- gardless of the size of the objects mentioned in the captions. This highlights that UMIC is more sen- sitive to sentence structure than the size of the objects mentioned in the caption, whereas for CLIPScore and PAC-S, the behavior is just the opposite. To compare the sensitivity of metrics to sentence structure and the number of object mentions, we conducted a sentence shuffling experiment using captions that varied in the number of object men- tions. As shown in Table 6, UMIC assigns the lowest scores to shuffled captions, regardless of the number of objects mentioned in the captions. This indicates that UMIC prioritizes sentence structure over the number of object mentions . In contrast, CLIPScore and PAC-S assign the high- est scores to captions with three objects, regardless of whether they are shuffled or not. Similarly, the captions with two objects have the second highest CLIPScore and PAC-S, regardless of the correct- ness of the sentence structure. This reveals that CLIPScore and PAC-S places greater impor- tance on the number of object mentions than the sentence structure. 5 Conclusion and Discussion In conclusion, recently proposed reference-free im- age captioning evaluation metrics are far from per- fect; they cannot distinguish an incorrect caption from a correct caption when the difference between them is fine-grained. The sensitivity of CLIPScore, UMIC, and PAC-S varies across different error types: they are less affected by plausibility errors yet more by visual grounding errors. All metrics struggle with understanding negation. All metrics are influenced by the size of the relevant objects mentioned in the caption, and CLIPScore and PAC- S also responds to the number of object mentions. UMIC is responsive to sentence structure, while CLIPScore and PAC-S disregards it often. More- over, UMIC prioritizes sentence structure over the number and size of objects mentioned in the cap- tion; in contrast CLIPScore and PAC-S prioritize the object size and number of object mentions over sentence structure. Our primary contribution is highlighting specific areas where reference-free",
      "chunk_index": 14
    },
    {
      "index": 538,
      "chunk_id": "LongForm_Faithfulness2023_chunk_15",
      "source_id": "LongForm_Faithfulness2023",
      "text": "sentence structure over the number and size of objects mentioned in the cap- tion; in contrast CLIPScore and PAC-S prioritize the object size and number of object mentions over sentence structure. Our primary contribution is highlighting specific areas where reference-free metrics exhibit limita- tions. The root cause of these limitations is traced to the insufficient fine-grained understanding of the CLIP and UNITER models upon which these reference-free metrics rely. In order to improve the reference-free metrics, we believe that underly- ing models need to become better at fine-grained understanding of objects, attributes, relationships etc., so that they can better distinguish fine-grained differences between captions. Promising avenues for enhancing this understanding include explor- ing object-centric representations (Locatello et al., 2020; Greff et al., 2019; Burgess et al., 2019) and incorporating training with hard negatives (Yuksek- gonul et al., 2023; Zhang et al., 2023; Bugliarello et al., 2023), allowing the model to learn to dis- cern fine-grained differences and errors. Given the restricted fine-grained understanding of the under- lying models shaping these metrics, caution is ad- vised when employing them as evaluation metrics for image captioning. Limitations As a limitation, it is important to consider that re- sponses marked as incorrect may not always be incorrect due to the stringent nature of VQA evalu- ation metrics (Agrawal et al., 2023). Our approach does not account for this factor. However, for our experiments, since we fine-tune ALBEF for each domain, the risk of this issue is low. To get a quan- titative sense, we randomly sampled 100 incorrect answers (as deemed by the VQA automatic met- ric) generated by ALBEF for VQAv2, and in only 10% of cases, the answer was actually correct (as deemed by an expert human). Furthermore, it is important to note that we do not account for the saliency of objects mentioned in the caption, which could be a confounding factor in our evaluation. Ethics Statement To enhance transparency and explainability, we conducted experiments aimed at shedding light on the evaluation process of the metric. By doing so, we aimed to provide insights and explanations that enable users to better comprehend and trust the metric's evaluations. Furthermore, we evaluated the robustness of the metrics, contributing towards the development of less biased evaluation metrics. While we assess various aspects of existing met- rics, it is important to note that our evaluation does not specifically examine metrics' potential biases across different",
      "chunk_index": 15
    },
    {
      "index": 539,
      "chunk_id": "LongForm_Faithfulness2023_chunk_16",
      "source_id": "LongForm_Faithfulness2023",
      "text": "the robustness of the metrics, contributing towards the development of less biased evaluation metrics. While we assess various aspects of existing met- rics, it is important to note that our evaluation does not specifically examine metrics' potential biases across different demographics, including gender or race. While our research does not include an explicit experiment on bias perpetuation or ampli- fication, we strongly encourage future studies to investigate how metrics may interact with biases present in datasets. This research direction is cru- cial in developing metrics that are less biased and more inclusive towards diverse demographics. Acknowledgements We express our gratitude to Stefan Lee for provid- ing constructive feedback. The technical support extended by the Mila IDT team in managing the computational infrastructure is greatly appreciated. The authors acknowledge the material support of NVIDIA in the form of computational resources. Throughout this project, Aishwarya Agrawal re- ceived support from the Canada CIFAR AI Chair award. References Somak Aditya, Yezhou Yang, Chitta Baral, Yiannis Aloimonos, and Cornelia Fermüller. 2017. Image understanding using vision and reasoning through scene description graph. Computer Vision and Image Understanding, 173. Aishwarya Agrawal, Ivana Kajic, Emanuele Bugliarello, Elnaz Davoodi, Anita Gergely, Phil Blunsom, and Aida Nematzadeh. 2023. Reassessing evaluation practices in visual question answering: A case study on out-of-distribution generalization. In Findings of the Association for Computational Linguistics: EACL 2023, pages 1201-1226, Dubrovnik, Croatia. Association for Computational Linguistics. Stanislaw Antol, Aishwarya Agrawal, Jiasen Lu, Mar- garet Mitchell, Dhruv Batra, C. Lawrence Zitnick, and Devi Parikh. 2015. VQA: Visual Question An- swering. In International Conference on Computer Vision (ICCV). Satanjeev Banerjee and Alon Lavie. 2005. METEOR: An automatic metric for MT evaluation with im- proved correlation with human judgments. In Pro- ceedings of the ACL Workshop on Intrinsic and Ex- trinsic Evaluation Measures for Machine Transla- tion and/or Summarization, pages 65-72, Ann Arbor, Michigan. Association for Computational Linguis- tics. Emanuele Bugliarello, Aida Nematzadeh, and Lisa Anne Hendricks. 2023. Weakly-supervised learning of visual relations in multimodal pretraining. Christopher P. Burgess, Loïc Matthey, Nicholas Watters, Rishabh Kabra, Irina Higgins, Matthew M. Botvinick, and Alexander Lerchner. 2019. Monet: Unsuper- vised scene decomposition and representation. ArXiv, abs/1901.11390. Xinlei Chen, Hao Fang, Tsung-Yi Lin, Ramakr- ishna Vedantam, Saurabh Gupta, Piotr Dollár, and C Lawrence Zitnick. 2015. Microsoft coco captions: Data collection and evaluation server. arXiv preprint arXiv:1504.00325. Yen-Chun Chen, Linjie Li, Licheng Yu, Ahmed El Kholy, Faisal Ahmed, Zhe Gan, Yu Cheng, and Jingjing",
      "chunk_index": 16
    },
    {
      "index": 540,
      "chunk_id": "LongForm_Faithfulness2023_chunk_17",
      "source_id": "LongForm_Faithfulness2023",
      "text": "Ramakr- ishna Vedantam, Saurabh Gupta, Piotr Dollár, and C Lawrence Zitnick. 2015. Microsoft coco captions: Data collection and evaluation server. arXiv preprint arXiv:1504.00325. Yen-Chun Chen, Linjie Li, Licheng Yu, Ahmed El Kholy, Faisal Ahmed, Zhe Gan, Yu Cheng, and Jingjing Liu. 2020. Uniter: Universal image-text rep- resentation learning. In Computer Vision - ECCV 2020, pages 104-120, Cham. Springer International Publishing. Anuj Diwan, Layne Berry, Eunsol Choi, David Harwath, and Kyle Mahowald. 2022. Why is winoground hard? investigating failures in visuolinguistic compo- sitionality. In Proceedings of the 2022 Conference on Empirical Methods in Natural Language Processing, pages 2236-2250, Abu Dhabi, United Arab Emirates. Association for Computational Linguistics. Joshua Feinglass and Yezhou Yang. 2021. Smurf: Se- mantic and linguistic understanding fusion for cap- tion evaluation via typicality analysis. In Proceed- ings of the 58th Annual Meeting of the Association for Computational Linguistics. Yash Goyal, Tejas Khot, Douglas Summers-Stay, Dhruv Batra, and Devi Parikh. 2016. Making the v in vqa matter: Elevating the role of image understanding in visual question answering. International Journal of Computer Vision, 127:398-414. Klaus Greff, Raphaël Lopez Kaufman, Rishabh Kabra, Nick Watters, Christopher Burgess, Daniel Zoran, Loic Matthey, Matthew Botvinick, and Alexander Lerchner. 2019. Multi-object representation learning with iterative variational inference. In Proceedings of the 36th International Conference on Machine Learn- ing, volume 97 of Proceedings of Machine Learning Research, pages 2424-2433. PMLR. Jack Hessel, Ari Holtzman, Maxwell Forbes, Ronan Le Bras, and Yejin Choi. 2021. CLIPScore: A reference-free evaluation metric for image captioning. In Proceedings of the 2021 Conference on Empiri- cal Methods in Natural Language Processing, pages 7514-7528, Online and Punta Cana, Dominican Re- public. Association for Computational Linguistics. Cheng-Yu Hsieh, Jieyu Zhang, Zixian Ma, Aniruddha Kembhavi, and Ranjay Krishna. 2023. Sugarcrepe: Fixing hackable benchmarks for vision-language compositionality. arXiv preprint arXiv:2306.14610. Anwen Hu, Shizhe Chen, Liang Zhang, and Qin Jin. 2023. Infometic: An informative metric for reference-free image caption evaluation. Andrej Karpathy and Fei-Fei Li. 2015. Deep visual- semantic alignments for generating image descrip- tions. In CVPR, pages 3128-3137. IEEE Computer Society. Hwanhee Lee, Seunghyun Yoon, Franck Dernoncourt, Trung Bui, and Kyomin Jung. 2021. UMIC: An unreferenced metric for image captioning via con- trastive learning. In Proceedings of the 59th An- nual Meeting of the Association for Computational Linguistics and the 11th International Joint Confer- ence on Natural Language Processing (Volume 2: Short Papers), pages 220-226, Online. Association for Computational Linguistics. Junnan Li, Ramprasaath R. Selvaraju, Akhilesh",
      "chunk_index": 17
    },
    {
      "index": 541,
      "chunk_id": "LongForm_Faithfulness2023_chunk_18",
      "source_id": "LongForm_Faithfulness2023",
      "text": "of the 59th An- nual Meeting of the Association for Computational Linguistics and the 11th International Joint Confer- ence on Natural Language Processing (Volume 2: Short Papers), pages 220-226, Online. Association for Computational Linguistics. Junnan Li, Ramprasaath R. Selvaraju, Akhilesh Deepak Gotmare, Shafiq Joty, Caiming Xiong, and Steven Hoi. 2021. Align before fuse: Vision and language representation learning with momentum distillation. In Advances in Neural Information Processing Sys- tems. Chin-Yew Lin. 2004. ROUGE: A package for auto- matic evaluation of summaries. In Text Summariza- tion Branches Out, pages 74-81, Barcelona, Spain. Association for Computational Linguistics. Tsung-Yi Lin, Michael Maire, Serge Belongie, James Hays, Pietro Perona, Deva Ramanan, Piotr Dollár, and C. Lawrence Zitnick. 2014. Microsoft coco: Common objects in context. In Computer Vision - ECCV 2014, pages 740-755, Cham. Springer Inter- national Publishing. Francesco Locatello, Dirk Weissenborn, Thomas Un- terthiner, Aravindh Mahendran, Georg Heigold, Jakob Uszkoreit, Alexey Dosovitskiy, and Thomas Kipf. 2020. Object-centric learning with slot atten- tion. In Advances in Neural Information Processing Systems, volume 33, pages 11525-11538. Curran As- sociates, Inc. Zixian Ma, Jerry Hong, Mustafa Omer Gul, Mona Gandhi, Irena Gao, and Ranjay Krishna. 2023. Crepe: Can vision-language foundation models reason com- positionally? Kishore Papineni, Salim Roukos, Todd Ward, and Wei- Jing Zhu. 2002. Bleu: a method for automatic evalu- ation of machine translation. In Proceedings of the 40th Annual Meeting of the Association for Compu- tational Linguistics, pages 311-318, Philadelphia, Pennsylvania, USA. Association for Computational Linguistics. Letitia Parcalabescu, Michele Cafagna, Lilitta Murad- jan, Anette Frank, Iacer Calixto, and Albert Gatt. 2022. V ALSE: A task-independent benchmark for vision and language models centered on linguistic phenomena. In Proceedings of the 60th Annual Meet- ing of the Association for Computational Linguistics (Volume 1: Long Papers), pages 8253-8280, Dublin, Ireland. Association for Computational Linguistics. Alec Radford, Jong Wook Kim, Chris Hallacy, Aditya Ramesh, Gabriel Goh, Sandhini Agarwal, Girish Sas- try, Amanda Askell, Pamela Mishkin, Jack Clark, Gretchen Krueger, and Ilya Sutskever. 2021. Learn- ing transferable visual models from natural language supervision. Sara Sarto, Manuele Barraco, Marcella Cornia, Lorenzo Baraldi, and Rita Cucchiara. 2023. Positive- Augmented Contrastive Learning for Image and Video Captioning Evaluation. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pat- tern Recognition. Ravi Shekhar, Sandro Pezzelle, Yauhen Klimovich, Au- relie Herbelot, Moin Nabi, Enver Sangineto, and Raf- faella Bernardi. 2017. \"foil it! find one mismatch between image and language caption\". In Proceed- ings of the",
      "chunk_index": 18
    },
    {
      "index": 542,
      "chunk_id": "LongForm_Faithfulness2023_chunk_19",
      "source_id": "LongForm_Faithfulness2023",
      "text": "on Computer Vision and Pat- tern Recognition. Ravi Shekhar, Sandro Pezzelle, Yauhen Klimovich, Au- relie Herbelot, Moin Nabi, Enver Sangineto, and Raf- faella Bernardi. 2017. \"foil it! find one mismatch between image and language caption\". In Proceed- ings of the 55th Annual Meeting of the Association for Computational Linguistics (ACL) (Volume 1: Long Papers), pages 255-265. Tristan Thrush, Ryan Jiang, Max Bartolo, Amanpreet Singh, Adina Williams, Douwe Kiela, and Candace Ross. 2022. Winoground: Probing vision and lan- guage models for visio-linguistic compositionality. In Proceedings of the IEEE/CVF Conference on Com- puter Vision and Pattern Recognition (CVPR), pages 5238-5248. Ramakrishna Vedantam, C. Lawrence Zitnick, and Devi Parikh. 2014. Cider: Consensus-based image descrip- tion evaluation. 2015 IEEE Conference on Computer Vision and Pattern Recognition (CVPR), pages 4566- 4575. Ben Wang and Aran Komatsuzaki. 2021. GPT-J- 6B: A 6 Billion Parameter Autoregressive Lan- guage Model. https://github.com/kingoflolz/ mesh-transformer-jax. Yutaro Yamada, Yingtian Tang, and Ilker Yildirim. 2022. When are lemons purple? the concept association bias of clip. Mert Yuksekgonul, Federico Bianchi, Pratyusha Kalluri, Dan Jurafsky, and James Zou. 2023. When and why vision-language models behave like bags-of-words, and what to do about it? In The Eleventh Interna- tional Conference on Learning Representations. Le Zhang, Rabiul Awal, and Aishwarya Agrawal. 2023. Contrasting intra-modal and ranking cross- modal hard negatives to enhance visio-linguistic fine- grained understanding. Tiancheng Zhao, Tianqi Zhang, Mingwei Zhu, Haozhan Shen, Kyusong Lee, Xiaopeng Lu, and Jianwei Yin. 2022. An explainable toolbox for evaluating pre- trained vision-language models. In Proceedings of the 2022 Conference on Empirical Methods in Nat- ural Language Processing: System Demonstrations, pages 30-37, Abu Dhabi, UAE. Association for Com- putational Linguistics. A Appendix A.1 Generating Caption-like Sentences To generate caption-like sentences from each ques- tion and answer pair of VQA datasets, we utilize pretrained GPT-J (Wang and Komatsuzaki, 2021) in a few-shot manner. To accomplish this, we first constructed a support examples dataset using the VQAv2 (Goyal et al., 2016) training split. For each of the sixty-four predefined question types in the VQAv2 dataset, we randomly selected four exam- ples from the VQAv2 training split. Then, we trans- formed both the questions and answers into single sentences, which we wrote ourselves. When gener- ating captions for VQAv2 validation split, we first match the question type to one of the predefined sixty-four question types. Then, we select four sup- port examples associated with that question type and prompt GPT-J to generate a",
      "chunk_index": 19
    },
    {
      "index": 543,
      "chunk_id": "LongForm_Faithfulness2023_chunk_20",
      "source_id": "LongForm_Faithfulness2023",
      "text": "When gener- ating captions for VQAv2 validation split, we first match the question type to one of the predefined sixty-four question types. Then, we select four sup- port examples associated with that question type and prompt GPT-J to generate a transformed sen- tence. If the question type does not match any of our predefined question types, we randomly select eight support examples from the entire pool of sup- port examples. Please see Figure 2 and note that we visualized a 2-shot prompt for simplification. A.2 Comparison with FOIL, Winoground and ARO • FOIL: The distinction between our dataset and the FOIL dataset lies in their respective approaches to altering captions. While FOIL primarily focuses on changing nouns in MS- COCO captions, encompassing 73 out of the 91 MS-COCO categories, our setup, utiliz- ing the VQA dataset, allows for a more di- verse analysis. In our study, we go beyond changing nouns and explore variations in cap- tions related to colors, time, count, and more. Notably, even in terms of nouns, our dataset exhibits greater diversity as we are not con- strained to object types present in MS-COCO annotated categories. • ARO: ARO dataset incorporates tests focusing on attribution, relations, and order. In the at- tribution test, distinctions are drawn between phrases like \"The paved road and the white house.\" and \"The white road and the paved house.\". Meanwhile, the relation test explores understanding relationships, as seen in exam- ples like \"The horse is eating the grass.\" and the contrasting, implausible statement \"The grass is eating the horse.\". As shown by (Hsieh et al., 2023), the hard-negative cap- tions present in these benchmarks are eas- ily identifiable by vision-language models as they are out-of-distribution (OOD) w.r.t the training data seen by the language encoder in these models. While our correct and incorrect pairs of captions are both plausible sentences where only the incorrect caption exhibits a fine-grained error that stems from a lack of precise visual grounding. • Winoground: Winoground dataset is meticu- lously curated by humans specifically for test- ing visio-linguistic compositionality. While it maintains a high level of quality, it comprises only 1600 samples, which, regrettably, is in- sufficient for robust statistical analyses. Fur- thermore, it lacks detailed annotations for as- pects such as color, time, and counting in com- parison to VQAv2. Importantly, as indicated by (Diwan et al., 2022), this dataset introduces challenges that go beyond fine-grained",
      "chunk_index": 20
    },
    {
      "index": 544,
      "chunk_id": "LongForm_Faithfulness2023_chunk_21",
      "source_id": "LongForm_Faithfulness2023",
      "text": "for robust statistical analyses. Fur- thermore, it lacks detailed annotations for as- pects such as color, time, and counting in com- parison to VQAv2. Importantly, as indicated by (Diwan et al., 2022), this dataset introduces challenges that go beyond fine-grained under- standing, including issues like out-of-domain challenges and ambiguous captions. These challenges significantly confound the study's results. A.3 F1 score computation for the Composite Dataset We calculated the F1 score between the human- written correct captions and model-generated in- correct captions in the Composite dataset (Aditya et al., 2017). We used the captions generated by the Karparthy model (Karpathy and Li, 2015) as they were better in quality. In the Composite dataset, each model-generated caption has an associated correctness score (provided by humans) ranging from 1 ('The description has no relevance to the image') to 5 ('The description relates perfectly to the image'). For our F1 score computation, we con- sidered all captions with score less than or equal to 4 as incorrect captions. A.4 Plausible Answers To generate plausible captions for each question type, we first compiled a list of plausible answers derived from the ground truth multiple-choice an- swer of the same question type in the validation split of VQAv2. Subsequently, an answer was ran- domly selected from this list of plausible answers. This chosen answer was used to replace the ground truth answer in the original caption, thus generating a plausible alternate caption. A.5 Picking a large and small object from the image In this experiment, our primary objective is to inves- tigate how the object size mentioned in captions af- fects the scores assigned by CLIPScore and UMIC. To select small and large objects that are distinctly different in size, we could sort the objects by their associated area in the COCO Detection dataset. However, this approach may not always yield accu- rate results because multiple objects with the same name may appear in an image. For instance, if there are two cars in an image, one smaller but fur- ther away and the other larger but closer, sorting by area would lead to incorrect identification of the smallest and largest objects. This would result in identical captions for both objects, such as \"There is a car.\" which is not ideal for comparison. To overcome this issue, we added up the area of all object categories with the same name and sorted the total areas of",
      "chunk_index": 21
    },
    {
      "index": 545,
      "chunk_id": "LongForm_Faithfulness2023_chunk_22",
      "source_id": "LongForm_Faithfulness2023",
      "text": "identical captions for both objects, such as \"There is a car.\" which is not ideal for comparison. To overcome this issue, we added up the area of all object categories with the same name and sorted the total areas of each object category in the image. We then calculated the difference between the areas associated with the largest and smallest categories. If the difference exceeded our threshold, we selected those objects for analysis. As a result, we selected 24610 images for further analysis (See Figure 6). A.6 Computational Resources In all experiments detailed in this paper, we em- ployed a single NVIDIA Quadro RTX 8000 with 48 GB GDDR6 GPU Memory. Specifically, for the primary task of generating caption-like sentences from the VQAv2 dataset, we performed inference using the GPT-J model with 6 billion parameters, executing the process over a duration of 24 hours. A.7 Dataset Terms of Use We will distribute our datasets (both generated with caption template and QA to caption conversion method) under the Creative Commons Attribution 4.0 License. It is noteworthy to mention that this licensing choice aligns with the terms of use gov- erning both the COCO and VQAv2 datasets, foun- dational to the creation of our datasets. A.8 Editorial Assistance We would like to disclose that ChatGPT was uti- lized for refining the language and structure of this academic paper. While the primary content and research remain the work of the authors, the as- sistance provided by ChatGPT was limited to the improvement of writing quality.",
      "chunk_index": 22
    },
    {
      "index": 546,
      "chunk_id": "REPLUG2023_chunk_00",
      "source_id": "REPLUG2023",
      "text": "REPLUG : Retrieval-Augmented Black-Box Language Models Weijia Shi,1 * Sewon Min,1 Michihiro Yasunaga,2 Minjoon Seo,3 Rich James,4 Mike Lewis,4 Luke Zettlemoyer1 4 Wen-tau Yih4 Abstract We introduce REPLUG , a retrieval-augmented lan- guage modeling framework that treats the lan- guage model (LM) as a black box and augments it with a tuneable retrieval model. Unlike prior retrieval-augmented LMs that train language mod- els with special cross attention mechanisms to en- code the retrieved text, REPLUG simply prepends retrieved documents to the input for the frozen black-box LM. This simple design can be eas- ily applied to any existing retrieval and language models. Furthermore, we show that the LM can be used to supervise the retrieval model, which can then find documents that help the LM make better predictions. Our experiments demonstrate that REPLUG with the tuned retriever significantly improves the performance of GPT-3 (175B) on language modeling by 6.3%, as well as the perfor- mance of Codex on five-shot MMLU by 5.1%. 1. Introduction Large language models (LLMs) such as GPT-3 (Brown et al., 2020a) and Codex (Chen et al., 2021a), have demonstrated impressive performance on a wide range of language tasks. These models are typically trained on very large datasets and store a substantial amount of world or domain knowledge implicitly in their parameters. However, they are also prone to hallucination and cannot represent the full long tail of knowledge from the training corpus. Retrieval-augmented language models (Khandelwal et al., 2020; Borgeaud et al., 2022; Izacard et al., 2022b; Yasunaga et al., 2022), in con- trast, can retrieve knowledge from an external datastore when needed, potentially reducing hallucination and increas- ing coverage. Previous approaches of retrieval-augmented language models require access to the internal LM repre- sentations (e.g., to train the model (Borgeaud et al., 2022; 1University of Washington2Stanford University3KAIST 4Meta AI. * Work done while the first author was interning at Meta AI. Correspondence to: Weijia Shi <swj0419@uw.edu>. Figure 1. Different from previous retrieval-augmented ap- proaches (Borgeaud et al., 2022) that enhance a language model with retrieval by updating the LM's parameters, REPLUG treats the language model as a black box and augments it with a frozen or tunable retriever. This black-box assumption makes REPLUG applicable to large LMs (i.e., >100B parameters), which are often served via APIs. Izacard et al., 2022b) or to index the datastore (Khandelwal et al., 2020)), and are thus difficult to be applied to",
      "chunk_index": 0
    },
    {
      "index": 547,
      "chunk_id": "REPLUG2023_chunk_01",
      "source_id": "REPLUG2023",
      "text": "retriever. This black-box assumption makes REPLUG applicable to large LMs (i.e., >100B parameters), which are often served via APIs. Izacard et al., 2022b) or to index the datastore (Khandelwal et al., 2020)), and are thus difficult to be applied to very large LMs. In addition, many best-in-class LLMs can only be accessed through APIs. Internal representations of such models are not exposed and fine-tuning is not supported. In this work, we introduce REPLUG (Retrieve and Plug), a new retrieval-augmented LM framework where the lan- guage model is viewed as a black box and the retrieval component is added as a tuneable plug-and-play module. Given an input context, REPLUG first retrieves relevant documents from an external corpus using an off-the-shelf retrieval model. The retrieved documents are prepended to the input context and fed into the black-box LM to make the final prediction. Because the LM context length limits the number of documents that can be prepended, we also introduce a new ensemble scheme that encodes the retrieved documents in parallel with the same black-box LM, allow- ing us to easily trade compute for accuracy. As shown in arXiv:2301.12652v4 [cs.CL] 24 May 2023 REPLUG : Retrieval-Augmented Black-Box Language Models Figure 1, REPLUG is extremely flexible and can be used with any existing black-box LM and retrieval model. We also introduce REPLUG LSR (REPLUG with LM- Supervised Retrieval), a training scheme that can further improve the initial retrieval model in REPLUG with super- vision signals from a black-box language model. The key idea is to adapt the retriever to the LM, which is in contrast to prior work (Borgeaud et al., 2022) that adapts language models to the retriever. We use a training objective which prefers retrieving documents that improve language model perplexity, while treating the LM as a frozen, black-box scoring function. Our experiments show that REPLUG can improve the perfor- mance of diverse black-box LMs on both language modeling and downstream tasks, including MMLU (Hendrycks et al., 2021) and open-domain QA (Kwiatkowski et al., 2019; Joshi et al., 2017). For instance, REPLUG can improve Codex (175B) performance on MMLU by 4.5%, achieving compa- rable results to the 540B, instruction-finetuned Flan-PaLM. Furthermore, tuning the retriever with our training scheme (i.e., REPLUG LSR ) leads to additional improvements, in- cluding up to 6.3% increase in GPT-3 175B language mod- eling. To the best of our knowledge, our work is the first to show",
      "chunk_index": 1
    },
    {
      "index": 548,
      "chunk_id": "REPLUG2023_chunk_02",
      "source_id": "REPLUG2023",
      "text": "tuning the retriever with our training scheme (i.e., REPLUG LSR ) leads to additional improvements, in- cluding up to 6.3% increase in GPT-3 175B language mod- eling. To the best of our knowledge, our work is the first to show the benefits of retrieval to large LMs (>100B model parameters), for both reducing LM perplexity and and im- proving in-context learning performance. We summarize our contributions as follows: • We introduce REPLUG (§3), the first retrieval- augmented language modeling framework for enhanc- ing large black-box language models with retrieval. • We propose a training scheme (§4) to further adapt an off-the-shelf retrieval model to the LM, using the lan- guage modeling scores as supervision signals, resulting in improved retrieval quality. • Evaluations on language modeling (§6), open-domain QA and MMLU demonstrate that REPLUG can im- prove the performance of various language models such as GPT, OPT and BLOOM, including very large models with up to 175B parameters. 2. Background and Related Work Black-box Language Models Large language models (i.e., >100B), such as GPT-3 (Brown et al., 2020a), Codex (Chen et al., 2021a), and Yuan 1.0 (Wu et al., 2021), are not open-sourced due to commercial considerations and are only available as black-box APIs, through which users can send queries and receive responses. On the other hand, even open sourced language models such as OPT-175B (Zhang et al., 2022a) and BLOOM-176B (Scao et al., 2022) require significant computational resources to run and finetune locally. For example, finetuning BLOOM-176B requires 72 A100 GPUs (80GB memory, $15k each (Younes Belkda, 2022)), making them inac- cessible to researchers and developers with limited re- sources. Traditionally, retrieval-augmented model frame- works (Khandelwal et al., 2020; Borgeaud et al., 2022; Yu, 2022; Izacard et al., 2022b; Goyal et al., 2022) have fo- cused on the white-box setting, where language models are fine-tuned to incorporate retrieved documents. How- ever, the increasing scale and black-box nature of large language models makes this approach infeasible. To ad- dress the challenges posed by large language models, we investigate retrieval-augmentation in the black-box setting, where users only have access to the model predictions and cannot access or modify its parameters. Retrieval-augmented Models Augmenting language models with relevant information retrieved from various knowledge stores has shown to be effective in improving performance on various NLP tasks, including language mod- eling (Min et al., 2022; Borgeaud et al., 2022; Khandelwal et al., 2020) and",
      "chunk_index": 2
    },
    {
      "index": 549,
      "chunk_id": "REPLUG2023_chunk_03",
      "source_id": "REPLUG2023",
      "text": "Augmenting language models with relevant information retrieved from various knowledge stores has shown to be effective in improving performance on various NLP tasks, including language mod- eling (Min et al., 2022; Borgeaud et al., 2022; Khandelwal et al., 2020) and open-domain question answering (Lewis et al., 2020; Izacard et al., 2022b; Hu et al., 2022). Specifi- cally, using the input as query, (1) a retriever first retrieves a set of documents (i.e., sequences of tokens) from a corpus and then (2) a language model incorporates the retrieved documents as additional information to make a final predic- tion. This style of retrieval can be added to both encoder- decoder (Yu, 2022; Izacard et al., 2022b) and decoder-only models (Khandelwal et al., 2020; Borgeaud et al., 2022; Shi et al., 2022; Rubin et al., 2022). For example, Atlas (Izacard et al., 2022b) finetunes an encoder-decoder model jointly with the retriever by modeling documents as latent variables, while RETRO (Borgeaud et al., 2022) changes the decoder- only architecture to incorporate retrieved texts and pretrains the language model from scratch. Both methods require updating the model parameters through gradient descent, which cannot be applied to black-box LMs. Another line of retrieval-augmented LMs such as kNN-LM (Khandelwal et al., 2020; Zhong et al., 2022) retrieves a set of tokens and interpolates between the LM's next token distribution and kNN distributions computed from the retrieved tokens at inference. Although kNN-LM does not require additional training, it requires access to internal LM representations to compute the kNN distribution, which are not always available for large LMs such as GPT-3. In this work, we in- vestigate ways to improve large black-box language models with retrieval. While concurrent work (Mallen et al., 2022; Si et al., 2023; Yu et al., 2023; Khattab et al., 2022) has demonstrated that using a frozen retriever can improve GPT- 3 performance on open-domain question answering, we approach the problem in a more general setting, including language modeling and understanding tasks. We also pro- pose an ensemble method to incorporate more documents REPLUG : Retrieval-Augmented Black-Box Language Models Figure 2.REPLUG at inference (§3). Given an input context, REPLUG first retrieves a small set of relevant documents from an external corpus using a retriever (§3.1 Document Retrieval). Then it prepends each document separately to the input context and ensembles output probabilities from different passes (§3.2 Input Reformulation). and a training scheme to further adapt the",
      "chunk_index": 3
    },
    {
      "index": 550,
      "chunk_id": "REPLUG2023_chunk_04",
      "source_id": "REPLUG2023",
      "text": "relevant documents from an external corpus using a retriever (§3.1 Document Retrieval). Then it prepends each document separately to the input context and ensembles output probabilities from different passes (§3.2 Input Reformulation). and a training scheme to further adapt the retriever to large LMs. 3. REPLUG We introduce REPLUG (Retrieve and Plug), a new retrieval- augmented LM paradigm where the language model is treated as black box and the retrieval component is added as a potentially tuneable module. As shown in Figure 2, given an input context, REPLUG first retrieves a small set of relevant documents from an external corpus using a retriever (§3.1). Then we pass the concate- nation of each retrieved document with the input context through the LM in parallel, and ensemble the predicted probabilities (§3.2). 3.1. Document Retrieval Given an input context x, the retriever aims to retrieve a small set of documents from a corpus D = {d1...dm} that are relevant to x. Following prior work (Qu et al., 2021; Izacard & Grave, 2021b; Ni et al., 2021), we use a dense retriever based on the dual encoder architecture, where an encoder is used to encode both the input context x and the document d. Specifically, the encoder maps each document d ∈ D to an embedding E(d) by taking the mean pooling of the last hidden representation over the tokens in d. At query time, the same encoder is applied to the input context x to obtain a query embedding E(x). The similarity between the query embedding and the document embedding is computed by their cosine similarity: s(d, x) = cos(E(d), E(x)) (1) The top-k documents that have the highest similarity scores when compared with the input x are retrieved in this step. For efficient retrieval, we precompute the embedding of each document d ∈ D and construct FAISS index (Johnson et al., 2019) over these embeddings. 3.2. Input Reformulation The retrieved top- k documents provide rich information about the original input context x and can potentially help the LM to make a better prediction. One simple way to incorporate the retrieved documents as part of the input to the LM is to prepend x with all k documents. However, this simple scheme is fundamentally restricted by the number of documents (i.e., k) we can include, given the language model's context window size. To address this limitation, we adopt an ensemble strategy described as",
      "chunk_index": 4
    },
    {
      "index": 551,
      "chunk_id": "REPLUG2023_chunk_05",
      "source_id": "REPLUG2023",
      "text": "x with all k documents. However, this simple scheme is fundamentally restricted by the number of documents (i.e., k) we can include, given the language model's context window size. To address this limitation, we adopt an ensemble strategy described as follows. Assume D′ ⊂ Dconsists of k most relevant documents to x, ac- cording to the scoring function in Eq. (1). We prepend each document d ∈ D′ to x, pass this concatenation to the LM separately, and then ensemble output probabilities from all k passes. Formally, given the input context x and its top-k relevant documents D′, the output probability of the next token y is computed as a weighted average ensemble: p(y | x, D′) = X d∈D′ p(y | d ◦ x) · λ(d, x), where ◦ denotes the concatenation of two sequences and the weight λ(d, x) is based on the similarity score between the document d and the input context x: λ(d, x) = es(d,x) P d∈D′ es(d,x) Although our ensemble method requires running the LM k times, the cross attention is performed between each re- trieved document and the input context. Therefore, com- pared with the method of prepending all the retrieved docu- REPLUG : Retrieval-Augmented Black-Box Language Models ments, our ensemble methods do not incur additional com- putational cost overhead. 4. REPLUG LSR: Training the Dense Retriever Instead of relying only on existing neural dense retrieval models (Karpukhin et al., 2020a; Izacard et al., 2022a; Su et al., 2022), we further propose REPLUG LSR (REPLUG with LM-Supervised Retrieval), which adapts the retriever in REPLUG by using the LM itself to provide supervision about which documents should be retrieved. Inspired by Sachan et al. (2022), our approach can be seen as adjusting the probabilities of the retrieved documents to match the probabilities of the output sequence perplexi- ties of the language model. In other words, we would like the retriever to find documents that result in lower perplex- ity scores. As shown in Figure 3, our training algorithm consists of the four steps: (1) retrieving documents and computing the retrieval likelihood (§4.1), (2) scoring the retrieved documents by the language model (§4.2), (3) up- dating the retrieval model parameters by minimizing the KL divergence between the retrieval likelihood and the LM's score distribution (§4.3), and (4) asynchronous update of the datastore index (§4.4). 4.1. Computing Retrieval Likelihood We retrieve k documents D′ ⊂ Dwith",
      "chunk_index": 5
    },
    {
      "index": 552,
      "chunk_id": "REPLUG2023_chunk_06",
      "source_id": "REPLUG2023",
      "text": "dating the retrieval model parameters by minimizing the KL divergence between the retrieval likelihood and the LM's score distribution (§4.3), and (4) asynchronous update of the datastore index (§4.4). 4.1. Computing Retrieval Likelihood We retrieve k documents D′ ⊂ Dwith the highest simi- larity scores from a corpus D given an input context x, as described in §3.1. We then compute the retrieval likelihood of each retrieved document d: PR(d | x) = es(d,x)/γ P d∈D′ es(d,x)/γ where γ is a hyperparameter that controls the temerature of the softmax. Ideally, the retrieval likelihood is computed by marginalizing over all the documents in the corpusD, which is intractable in practice. Therefore, we approximate the retrieval likelihood by only marginalizing over the retrieved documents D′. 4.2. Computing LM likelihood We use the LM as a scoring function to measure how much each document could improve the LM perplexity. Specifi- cally, we first compute PLM (y | d, x), the LM probability of the ground truth output y given the input context x and a document d. The higher the probability, the better the document di is at improving the LM's perplexity. We then compute the LM likelihood of each document d as follows: Q(d | x, y) = ePLM(y|d,x)/β P d∈D′ ePLM(y|d,x)/β where β is another hyperparameter. 4.3. Loss Function Given the input context x and the corresponding ground truth continuation y, we compute the retrieval likelihood and the language model likelihood. The dense retriever is trained by minimizing the KL divergence between these two distributions: L = 1 |B| X x∈B KL \u0010 PR(d | x) ∥ QLM(d | x, y) \u0011 , where B is a set of input contexts. When minimizing the loss, we can only update the retrieval model parameters. The LM parameters are fixed due to our black-box assumption. 4.4. Asynchronous Update of the Datastore Index Because the parameters in the retriever are updated during the training process, the previously computed document em- beddings are no longer up to date. Therefore, following Guu et al. (2020), we recompute the document embeddings and rebuild the efficient search index using the new embeddings every T training steps. Then we use the new document embeddings and index for retrieval, and repeat the training procedure. 5. Training Setup In this section, we describe the details of our training proce- dure. We first describe the model setting in REPLUG (§5.1) and then",
      "chunk_index": 6
    },
    {
      "index": 553,
      "chunk_id": "REPLUG2023_chunk_07",
      "source_id": "REPLUG2023",
      "text": "use the new document embeddings and index for retrieval, and repeat the training procedure. 5. Training Setup In this section, we describe the details of our training proce- dure. We first describe the model setting in REPLUG (§5.1) and then describe the procedure for training the retriever in REPLUG LSR (§5.2). 5.1. REPLUG In theory, any type of retriever, either dense (Karpukhin et al., 2020b; Ni et al., 2021) or sparse (Robertson et al., 2009), could be used for REPLUG . Following prior work (Izacard et al., 2022b), we use the Contriever (Izacard et al., 2022a) as the retrieval model for REPLUG , as it has demonstrated strong performance. 5.2. REPLUG LSR For REPLUG LSR , we initialize the retriever with the Contriever model (Izacard et al., 2022a). We use GPT-3 Curie (Brown et al., 2020b) as the supervision LM to com- pute the LM likelihood. Training data We use 800K sequences of 256 tokens each, sampled from the Pile training data (Gao et al., 2020), as our training queries. Each query is split into two parts: the first 128 tokens are used as the input context x, and the last 128 tokens are used as the ground truth continuation y. For the external corpus D, we sample 36M documents REPLUG : Retrieval-Augmented Black-Box Language Models Figure 3.REPLUG LSR training process (§4). The retriever is trained using the output of a frozen language model as supervision signals. of 128 tokens from the Pile training data. To avoid trivial retrieval, we ensure that the external corpus documents do not overlap with the documents from which the training queries are sampled. Training details To make the training process more ef- ficient, we pre-compute the document embeddings of the external corpus D and create a FAISS index (Johnson et al., 2019) for fast similarity search. Given a queryx, we retrieve the top 20 documents from the FAISS index and compute the retrieval likelihood and the LM likelihood with a tem- perature of 0.1. We train the retriever using the Adam opti- mizer (Kingma & Ba, 2015) with a learning rate of 2e-5, a batch size of 64, and a warmup ratio of 0.1. We re-compute the document embeddings every 3k steps and fine-tune the retriever for a total of 25k steps. 6. Experiments We perform evaluations on both language modeling (§6.1) and downstream tasks such as MMLU (§6.2) and open- domain QA (§6.3).",
      "chunk_index": 7
    },
    {
      "index": 554,
      "chunk_id": "REPLUG2023_chunk_08",
      "source_id": "REPLUG2023",
      "text": "We re-compute the document embeddings every 3k steps and fine-tune the retriever for a total of 25k steps. 6. Experiments We perform evaluations on both language modeling (§6.1) and downstream tasks such as MMLU (§6.2) and open- domain QA (§6.3). In all settings, REPLUG ˜improve the performance of various black-box language models, show- ing the effectiveness and generality of our approach. 6.1. Language Modeling Datasets The Pile (Gao et al., 2020) is a language mod- eling benchmark that consists of text sources from diverse domains such as web pages, code and academic papers. Fol- lowing prior work, we report bits per UTF-8 encoded byte (BPB) as the metric on each subset domain. Baselines We consider GPT-3 and GPT-2 family language model as the baselines. The four models from GPT-3 (Davinci, Curie, Baddage and Ada) are black-box models that are only accessible through API Our model We add REPLUG and REPLUG LSR to the baselines. We randomly subsampled Pile training data (367M documents of 128 tokens) and use them as the re- trieval corpus for all models. As the Pile dataset has made efforts to deduplicate documents across train, validation and test splits (Gao et al., 2020), we did not do additional filter- ing. For both REPLUG and REPLUG LSR, we use a length of 128-token context to do retrieval and adopt the ensem- ble method (Section 3.2) to incorporate top 10 retrieved documents during inference. Results Table 1 reports the results of the original base- lines, baselines augmented with the REPLUG , and baselines augmented with the REPLUG LSR . We observe that both REPLUG and REPLUG LSR significantly outperform the baselines. This demonstrates that simply adding a retrieval module to a frozen language model (i.e., the black-box set- ting) is effective at improving the performance of different sized language models on language modeling tasks. Fur- thermore, REPLUG LSR consistently performs better than REPLUG by a large margin. Specifically, REPLUG LSR results in 7.7% improvement over baselines compared to 4.7% improvement of REPLUG averaged over the 8 models. This indicates that further adapting the retriever to the target LM is beneficial. 6.2. MMLU Datasets Massive Multi-task Language Understanding (MMLU (Hendrycks et al., 2021)) is a multiple choice QA dataset that covers exam questions from 57 tasks includ- ing mathematics, computer science, law, US history and etc. The 57 tasks are grouped into 4 categories: humani- ties, STEM, social sciences and",
      "chunk_index": 8
    },
    {
      "index": 555,
      "chunk_id": "REPLUG2023_chunk_09",
      "source_id": "REPLUG2023",
      "text": "et al., 2021)) is a multiple choice QA dataset that covers exam questions from 57 tasks includ- ing mathematics, computer science, law, US history and etc. The 57 tasks are grouped into 4 categories: humani- ties, STEM, social sciences and other. Following Chung REPLUG : Retrieval-Augmented Black-Box Language Models Model # Parameters Original + REPLUG Gain % + REPLUG LSR Gain % GPT-2 Small 117M 1.33 1.26 5.3 1.21 9.0 Medium 345M 1.20 1.14 5.0 1.11 7.5 Large 774M 1.19 1.15 3.4 1.09 8.4 XL 1.5B 1.16 1.09 6.0 1.07 7.8 GPT-3 Ada 350M 1.05 0.98 6.7 0.96 8.6 (black-box) Babbage 1.3B 0.95 0.90 5.3 0.88 7.4 Curie 6.7B 0.88 0.85 3.4 0.82 6.8 Davinci 175B 0.80 0.77 3.8 0.75 6.3 Table 1.Both REPLUG and REPLUG LSR consistently enhanced the performance of different language models. Bits per byte (BPB) of the Pile using GPT-3 and GPT-2 family models (Original) and their retrieval-augmented versions (+REPLUG and +REPLUG LSR. The gain % shows the relative improvement of our models compared to the original language model. Model # Parameters Humanities Social. STEM Other All Codex 175B 74.2 76.9 57.8 70.1 68.3 PaLM 540B 77.0 81.0 55.6 69.6 69.3 Flan-PaLM 540B - - - - 72.2 Atlas 11B 46.1 54.6 38.8 52.8 47.9 Codex + REPLUG 175B 76.0 79.7 58.8 72.1 71.4 Codex + REPLUG LSR 175B 76.5 79.9 58.9 73.2 71.8 Table 2.REPLUG and REPLUG LSR improves Codex by 4.5% and 5.1% respectively. Performance on MMLU broken down into 4 categories. The last column averages the performance over these categories. All models are evaluated based on 5-shot in-context learning with direct prompting. et al. (2022a), we evaluate REPLUG in the 5-shot in-context learning setting. Baselines We consider two groups of strong previous mod- els as baselines for comparisons. The first group of base- lines is the state-of-the-art LLMs including Codex1 (Chen et al., 2021b), PaLM (Chowdhery et al., 2022), and Flan- PaLM (Chung et al., 2022b). According to Chung et al. (2022b), these three models rank top-3 in the leaderboard of MMLU. The second group of baselines consists of retrieval-augmented language models. We only include At- las (Izacard et al., 2022b) in this group, as no other retrieval- augmented LMs have been evaluated on the MMLU dataset. Atlas trains both the retriever and the language model, which we consider a white-box retrieval LM setting. Our model We add REPLUG and REPLUG LSR only",
      "chunk_index": 9
    },
    {
      "index": 556,
      "chunk_id": "REPLUG2023_chunk_10",
      "source_id": "REPLUG2023",
      "text": "group, as no other retrieval- augmented LMs have been evaluated on the MMLU dataset. Atlas trains both the retriever and the language model, which we consider a white-box retrieval LM setting. Our model We add REPLUG and REPLUG LSR only to Codex because other models such as PaLM and Flan-PaLM are not accessible to the public. We use the test question as the query to retrieve 10 relevant documents from Wikipedia (2018, December) and prepend each retrieved document to the test question, resulting in 10 separate inputs. These inputs are then separately fed into the language models, and the output probabilities are ensemble together. 1Code-Davinci-002 Results Table 2 presents the results from the baselines, REPLUG , and REPLUG LSR on the MMLU dataset. We observe that both the REPLUG and REPLUG LSR improve the original Codex model by 4.5% and 5.1%, respectively. In addition, REPLUG LSR largely outperforms the previous retrieval-augmented language model, Atlas, demonstrating the effectiveness of our black-box retrieval language model setting. Although our models slightly underperform Flan- PaLM, this is still a strong result because Flan-PaLM has three times more parameters. We would expect that the REPLUG LSR could further improve Flan-PaLM, if we had access to the model. Another interesting observation is that the REPLUG LSR outperforms the original model by 1.9% even in the STEM category. This suggests that retrieval may improve a lan- guage model's problem-solving abilities. 6.3. Open Domain QA Lastly, we conduct evaluation on two open-domain QA datasets: Natural Questions (NQ) (Kwiatkowski et al., 2019) and TriviaQA (Joshi et al., 2017). Datasets NQ and TriviaQA are two open-domain QA datasets consisting of questions, answers collected from 2Si et al. (2022) augment Codex with concatenation of 10 documents retrieved by contriever. REPLUG : Retrieval-Augmented Black-Box Language Models NQ TQA Model Few-shot Full Few-shot Full Chinchilla 35.5 - 64.6 - PaLM 39.6 - - - Codex 40.6 - 73.6 - RETRO† - 45.5 - - R2-D2† - 55.9 - 69.9 Atlas† 42.4 60.4 74.5 79.8 Codex + Contrievercc 2 44.2 - 76.0 - Codex + REPLUG 44.7 - 76.8 - Codex + REPLUG LSR 45.5 - 77.3 - Table 3.Performance on NQ and TQA. We report results for both few-shot (64 shots for Chinchilla, PaLM, and Atlas; 16 shots for Codex-based models) and full training data settings. REPLUG LSR improves Codex by 12.0% on NQ and 5.0% on TQA, making it the best-performing model",
      "chunk_index": 10
    },
    {
      "index": 557,
      "chunk_id": "REPLUG2023_chunk_11",
      "source_id": "REPLUG2023",
      "text": "We report results for both few-shot (64 shots for Chinchilla, PaLM, and Atlas; 16 shots for Codex-based models) and full training data settings. REPLUG LSR improves Codex by 12.0% on NQ and 5.0% on TQA, making it the best-performing model in the few-shot setting. Note that models with † are finetuned using training examples, while other models use in-context learning. Wikipedia and the Web. Following prior work (Izacard & Grave, 2021a; Si et al., 2022), we report results for the filtered set of TriviaQA. For evaluation, we consider the few- shot setting where the model is only given a few training examples and full data where the model is given all the training examples. Baselines We compare our model with several state-of- the-art baselines, both in a few-shot setting and with full training data. The first group of models consists of power- ful large language models, including Chinchilla (Hoffmann et al., 2022), PaLM (Chowdhery et al., 2022), and Codex. These models are all evaluated using in-context learning under the few-shot setting, with Chinchilla and PaLM eval- uated using 64 shots, and Codex using 16 shots. The sec- ond group of models for comparison includes retrieval- augmented language models such as RETRO (Borgeaud et al., 2021), R2-D2 (Fajcik et al., 2021), and Atlas (Izacard et al., 2022b). All of these retrieval-augmented models are finetuned on the training data, either in a few-shot setting or with full training data. Specifically, Atlas is finetuned on 64 examples in the few-shot setting. Our model We add REPLUG and REPLUG LSR to Codex with Wikipedia (2018, December) as the retrieval corpus to evaluate the model in a 16-shot in context learning. Sim- ilar to the setting in language modeling and MMLU, we incorporate top-10 retrieved documents using our proposed ensemble method. Results As shown in Table 3, REPLUG LSR significantly improves the performance of the original Codex by 12.0% on NQ and 5.0% on TQA. It outperforms the previous best Figure 4. Ensembling random documents does not result in improved performance. BPB of Curie augmented with different methods (random, REPLUG and REPLUG LSR) when varying the number of documents (i.e.; number of ensemble times.) model, Atlas, which was fine-tuned with 64 training exam- ples, achieving a new state-of-the-art in the few-shot setting. However, this result still lags behind the performance of retrieval-augmented language models fine-tuned on the full training data. This is likely due to",
      "chunk_index": 11
    },
    {
      "index": 558,
      "chunk_id": "REPLUG2023_chunk_12",
      "source_id": "REPLUG2023",
      "text": "Atlas, which was fine-tuned with 64 training exam- ples, achieving a new state-of-the-art in the few-shot setting. However, this result still lags behind the performance of retrieval-augmented language models fine-tuned on the full training data. This is likely due to the presence of near- duplicate test questions in the training set (e.g., Lewis et al. (2021) found that 32.5% of test questions overlap with the training sets in NQ). 7. Analysis 7.1. REPLUG performance gain does not simply come from the ensembling effect The core of our method design is the use of an ensem- ble method that combines output probabilities of different passes, in which each retrieved document is prepended sep- arately to the input and fed into a language model. To study whether the gains come solely from the ensemble method, we compare our method to ensembling random documents. For this, we randomly sample several documents, concate- nated each random document with the input, and ensemble the outputs of different runs (referred to as \"random\"). As shown in Figure 6, we evaluated the performance of GPT-3 Curie on Pile when augmented with random documents, documents retrieved by REPLUG , and documents retrieved by REPLUG LSR . We observed that ensembling random documents leads to worse performance, indicating that the performance gains of REPLUG do not solely come from the ensembling effect. Instead, ensembling the relevant docu- ments is crucial for the success of REPLUG . Additionally, as more documents were ensembled, the performance of REPLUG and REPLUG LSR improved monotonically. How- ever, a small number of documents (e.g., 10) was sufficient to achieve large performance gains. REPLUG : Retrieval-Augmented Black-Box Language Models Perplexity 14.00 16.60 19.20 21.80 24.40 27.00 Parameters (Million) 100 475 850 1225 1600 Original + RE-PLUG GPT-2 (a) Perplexity 11.00 13.60 16.20 18.80 21.40 24.00 Parameters (Million) 100 2075 4050 6025 8000 Original + RE-PLUG BLOOM (b) Perplexity 9.00 13.00 17.00 21.00 25.00 29.00 Parameters (Million) 100 1000 10000 100000 1000000 Original + RE-PLUG OPT (c) Figure 5. GPT-2, BLOOM and OPT models of varying sizes consistently benefit from REPLUG . The x-axis indicates the size of the language model and the y-axis is its perplexity on Wikitext-103. 7.2. REPLUG is applicable to diverse language models Here we further study whether REPLUG could enhance di- verse language model families that have been pre-trained using different data and methods. Specifically, we focus on three groups of",
      "chunk_index": 12
    },
    {
      "index": 559,
      "chunk_id": "REPLUG2023_chunk_13",
      "source_id": "REPLUG2023",
      "text": "perplexity on Wikitext-103. 7.2. REPLUG is applicable to diverse language models Here we further study whether REPLUG could enhance di- verse language model families that have been pre-trained using different data and methods. Specifically, we focus on three groups of language models with varying sizes: GPT- 2 (117M, 345M, 774M, 1.5B parameters) (Brown et al., 2020a), OPT (125M, 350M, 1.3B, 2.7B, 6.7B, 13B, 30B, 66B) (Zhang et al., 2022b) and BLOOM (560M, 1.1B, 1.7B, 3B and 7B) (Scao et al., 2022). We evaluate each model on Wikitext-103 (Stephen et al., 2017) test data and report its perplexity. For comparison, we augment each model with REPLUG that adopts the ensemble method to incorporate top 10 retrieved documents. Following prior work (Khan- delwal et al., 2020), we use Wikitext-103 training data as the retrieval corpus. Figure 5 shows the performance of different-sized language models with and without REPLUG . We observe that the performance gain brought by REPLUG stays consistent with model size. For example, OPT with 125M parameters achieves 6.9% perplexity improvement, while OPT with 66B parameters achieves 5.6% perplexity improvement. Ad- ditionally, REPLUG improves the perplexity of all the model families. This indicates that REPLUG is applicable to di- verse language models with different sizes. 7.3. Qualitative Analysis: rare entities benefit from retrieval To understand why the REPLUG improves language model- ing performance, we conducted manual analysis of exam- ples in which the REPLUG results in a decrease in perplexity. We find that REPLUG is more helpful when texts contain rare entities. Figure 6 shows a test context and its contin- uation from the Wikitext-103 test set. For REPLUG , we use the test context as a query to retrieve a relevant docu- ment from Wikitext-103 training data. We then compute the perplexity of the continuation using the original GPT-2 1.5B and its REPLUG enhanced version. After incorporating the retrieved document, the perplexity of the continuation improves by 11%. Among all tokens in the continuation, we found that REPLUG is most helpful for the rare entity name \"Li Bai\". This is likely because the original LM does not have sufficient information about this rare entity name. How- ever, by incorporating the retrieved document,REPLUG was able to match the name with the relevant information in the retrieved document, resulting in better performance. Figure 6.Rare entities benefit from retrieval. After incorporat- ing the retrieved document during inference, the entity \" Li Bai\"",
      "chunk_index": 13
    },
    {
      "index": 560,
      "chunk_id": "REPLUG2023_chunk_14",
      "source_id": "REPLUG2023",
      "text": "the retrieved document,REPLUG was able to match the name with the relevant information in the retrieved document, resulting in better performance. Figure 6.Rare entities benefit from retrieval. After incorporat- ing the retrieved document during inference, the entity \" Li Bai\" and the token \" greatest\" in the continuation show the most im- provement in perplexity (15% for \"Li Bai\" and 5% for \"greatest\"). Other tokens' perplexity changes are within 5%. 8. Conclusion We introduce REPLUG , a retrieval-augmented language modeling paradigm that treats the language model as a black box and augments it with a tuneable retrieval model. Our evaluation shows that REPLUG can be integrated with any existing language model to improve their performance REPLUG : Retrieval-Augmented Black-Box Language Models on language modeling or downstream tasks. This work opens up new possibilities for integrating retrieval into large- scale black-box language models and demonstrates even the state-of-the-art large-scale LMs could benefit from retrieval. However, REPLUG lacks interpretability as it is unclear when the model relies on retrieved knowledge or parametric knowledge. Future research could focus on developing more interpretable retrieval-augmented language models. References Borgeaud, S., Mensch, A., Hoffmann, J., Cai, T., Ruther- ford, E., Millican, K., Driessche, G. v. d., Lespiau, J.-B., Damoc, B., Clark, A., et al. Improving language mod- els by retrieving from trillions of tokens. arXiv preprint arXiv:2112.04426, 2021. Borgeaud, S., Mensch, A., Hoffmann, J., Cai, T., Ruther- ford, E., Millican, K., Van Den Driessche, G. B., Lespiau, J.-B., Damoc, B., Clark, A., et al. Improving language models by retrieving from trillions of tokens. In Interna- tional Conference on Machine Learning, pp. 2206-2240. PMLR, 2022. Brown, T., Mann, B., Ryder, N., Subbiah, M., Kaplan, J. D., Dhariwal, P., Neelakantan, A., Shyam, P., Sastry, G., Askell, A., Agarwal, S., Herbert-V oss, A., Krueger, G., Henighan, T., Child, R., Ramesh, A., Ziegler, D., Wu, J., Winter, C., Hesse, C., Chen, M., Sigler, E., Litwin, M., Gray, S., Chess, B., Clark, J., Berner, C., McCandlish, S., Radford, A., Sutskever, I., and Amodei, D. Language models are few-shot learners. In Larochelle, H., Ranzato, M., Hadsell, R., Balcan, M., and Lin, H. (eds.), Advances in Neural Information Processing Systems, volume 33, pp. 1877-1901. Curran Associates, Inc., 2020a. URL https: //proceedings.neurips.cc/paper/2020/file/ 1457c0d6bfcb4967418bfb8ac142f64a-Paper.pdf. Brown, T., Mann, B., Ryder, N., Subbiah, M., Kaplan, J. D., Dhariwal, P., Neelakantan, A., Shyam, P., Sastry, G., Askell, A., Agarwal, S., Herbert-V oss, A., Krueger, G., Henighan, T., Child,",
      "chunk_index": 14
    },
    {
      "index": 561,
      "chunk_id": "REPLUG2023_chunk_15",
      "source_id": "REPLUG2023",
      "text": "1877-1901. Curran Associates, Inc., 2020a. URL https: //proceedings.neurips.cc/paper/2020/file/ 1457c0d6bfcb4967418bfb8ac142f64a-Paper.pdf. Brown, T., Mann, B., Ryder, N., Subbiah, M., Kaplan, J. D., Dhariwal, P., Neelakantan, A., Shyam, P., Sastry, G., Askell, A., Agarwal, S., Herbert-V oss, A., Krueger, G., Henighan, T., Child, R., Ramesh, A., Ziegler, D., Wu, J., Winter, C., Hesse, C., Chen, M., Sigler, E., Litwin, M., Gray, S., Chess, B., Clark, J., Berner, C., McCandlish, S., Radford, A., Sutskever, I., and Amodei, D. Language models are few-shot learners. In Proc. of NeurIPS, 2020b. URL https: //proceedings.neurips.cc/paper/2020/file/ 1457c0d6bfcb4967418bfb8ac142f64a-Paper.pdf. Chen, M., Tworek, J., Jun, H., Yuan, Q., de Oliveira Pinto, H. P., Kaplan, J., Edwards, H., Burda, Y ., Joseph, N., Brockman, G., Ray, A., Puri, R., Krueger, G., Petrov, M., Khlaaf, H., Sastry, G., Mishkin, P., Chan, B., Gray, S., Ryder, N., Pavlov, M., Power, A., Kaiser, L., Bavar- ian, M., Winter, C., Tillet, P., Such, F. P., Cummings, D., Plappert, M., Chantzis, F., Barnes, E., Herbert- V oss, A., Guss, W. H., Nichol, A., Paino, A., Tezak, N., Tang, J., Babuschkin, I., Balaji, S., Jain, S., Saun- ders, W., Hesse, C., Carr, A. N., Leike, J., Achiam, J., Misra, V ., Morikawa, E., Radford, A., Knight, M., Brundage, M., Murati, M., Mayer, K., Welinder, P., Mc- Grew, B., Amodei, D., McCandlish, S., Sutskever, I., and Zaremba, W. Evaluating large language models trained on code. CoRR, abs/2107.03374, 2021a. URL https://arxiv.org/abs/2107.03374. Chen, M., Tworek, J., Jun, H., Yuan, Q., de Oliveira Pinto, H. P., Kaplan, J., Edwards, H., Burda, Y ., Joseph, N., Brockman, G., Ray, A., Puri, R., Krueger, G., Petrov, M., Khlaaf, H., Sastry, G., Mishkin, P., Chan, B., Gray, S., Ryder, N., Pavlov, M., Power, A., Kaiser, L., Bavar- ian, M., Winter, C., Tillet, P., Such, F. P., Cummings, D., Plappert, M., Chantzis, F., Barnes, E., Herbert- V oss, A., Guss, W. H., Nichol, A., Paino, A., Tezak, N., Tang, J., Babuschkin, I., Balaji, S., Jain, S., Saun- ders, W., Hesse, C., Carr, A. N., Leike, J., Achiam, J., Misra, V ., Morikawa, E., Radford, A., Knight, M., Brundage, M., Murati, M., Mayer, K., Welinder, P., Mc- Grew, B., Amodei, D., McCandlish, S., Sutskever, I., and Zaremba, W. Evaluating large language models trained on code. CoRR, abs/2107.03374, 2021b. URL https://arxiv.org/abs/2107.03374. Chowdhery, A., Narang, S., Devlin, J., Bosma, M., Mishra, G., Roberts, A., Barham, P., Chung, H. W., Sutton, C., Gehrmann, S., et al. Palm: Scaling language modeling with",
      "chunk_index": 15
    },
    {
      "index": 562,
      "chunk_id": "REPLUG2023_chunk_16",
      "source_id": "REPLUG2023",
      "text": "Evaluating large language models trained on code. CoRR, abs/2107.03374, 2021b. URL https://arxiv.org/abs/2107.03374. Chowdhery, A., Narang, S., Devlin, J., Bosma, M., Mishra, G., Roberts, A., Barham, P., Chung, H. W., Sutton, C., Gehrmann, S., et al. Palm: Scaling language modeling with pathways. arXiv preprint arXiv:2204.02311, 2022. Chung, H. W., Hou, L., Longpre, S., Zoph, B., Tay, Y ., Fedus, W., Li, E., Wang, X., Dehghani, M., Brahma, S., et al. Scaling instruction-finetuned language models. arXiv preprint arXiv:2210.11416, 2022a. Chung, H. W., Hou, L., Longpre, S., Zoph, B., Tay, Y ., Fedus, W., Li, E., Wang, X., Dehghani, M., Brahma, S., et al. Scaling instruction-finetuned language models. arXiv preprint arXiv:2210.11416, 2022b. Fajcik, M., Docekal, M., Ondrej, K., and Smrz, P. R2- D2: A modular baseline for open-domain question an- swering. In Findings of the Association for Computa- tional Linguistics: EMNLP 2021, pp. 854-870, Punta Cana, Dominican Republic, November 2021. Association for Computational Linguistics. doi: 10.18653/v1/2021. findings-emnlp.73. URL https://aclanthology.org/ 2021.findings-emnlp.73. Gao, L., Biderman, S., Black, S., Golding, L., Hoppe, T., Foster, C., Phang, J., He, H., Thite, A., Nabeshima, N., Presser, S., and Leahy, C. The Pile: An 800gb dataset of diverse text for language modeling. arXiv preprint arXiv:2101.00027, 2020. REPLUG : Retrieval-Augmented Black-Box Language Models Goyal, A., Friesen, A., Banino, A., Weber, T., Ke, N. R., Badia, A. P., Guez, A., Mirza, M., Humphreys, P. C., Konyushova, K., et al. Retrieval-augmented reinforce- ment learning. In International Conference on Machine Learning, pp. 7740-7765. PMLR, 2022. Guu, K., Lee, K., Tung, Z., Pasupat, P., and Chang, M. Retrieval augmented language model pre-training. In International Conference on Machine Learning, pp. 3929- 3938. PMLR, 2020. Hendrycks, D., Burns, C., Basart, S., Zou, A., Mazeika, M., Song, D., and Steinhardt, J. Measuring massive multitask language understanding. In International Con- ference on Learning Representations, 2021. URL https: //openreview.net/forum?id=d7KBjmI3GmQ. Hoffmann, J., Borgeaud, S., Mensch, A., Buchatskaya, E., Cai, T., Rutherford, E., Casas, D. d. L., Hendricks, L. A., Welbl, J., Clark, A., et al. Training compute-optimal large language models. arXiv preprint arXiv:2203.15556, 2022. Hu, Y ., Hua, H., Yang, Z., Shi, W., Smith, N. A., and Luo, J. Promptcap: Prompt-guided task-aware image captioning. arXiv preprint arXiv:2211.09699, 2022. Izacard, G. and Grave, E. Leveraging passage retrieval with generative models for open domain question an- swering. In Proceedings of the 16th Conference of the European Chapter of the Association for Compu- tational Linguistics: Main Volume, pp. 874-880, On- line, April 2021a.",
      "chunk_index": 16
    },
    {
      "index": 563,
      "chunk_id": "REPLUG2023_chunk_17",
      "source_id": "REPLUG2023",
      "text": "and Grave, E. Leveraging passage retrieval with generative models for open domain question an- swering. In Proceedings of the 16th Conference of the European Chapter of the Association for Compu- tational Linguistics: Main Volume, pp. 874-880, On- line, April 2021a. Association for Computational Lin- guistics. doi: 10.18653/v1/2021.eacl-main.74. URL https://aclanthology.org/2021.eacl-main.74. Izacard, G. and Grave, E. Leveraging passage retrieval with generative models for open domain question answering. In Proc. of EACL, 2021b. URL https://arxiv.org/ abs/2007.01282. Izacard, G., Caron, M., Hosseini, L., Riedel, S., Bojanowski, P., Joulin, A., and Grave, E. Unsupervised dense in- formation retrieval with contrastive learning. Trans- actions on Machine Learning Research, 2022a. URL https://openreview.net/forum?id=jKN1pXi7b0. Izacard, G., Lewis, P., Lomeli, M., Hosseini, L., Petroni, F., Schick, T., Dwivedi-Yu, J., Joulin, A., Riedel, S., and Grave, E. Few-shot learning with retrieval augmented lan- guage models. arXiv preprint arXiv:2208.03299, 2022b. Johnson, J., Douze, M., and Jégou, H. Billion-scale similar- ity search with gpus. IEEE Transactions on Big Data, 7 (3):535-547, 2019. Joshi, M., Choi, E., Weld, D., and Zettlemoyer, L. Trivi- aQA: A large scale distantly supervised challenge dataset for reading comprehension. In Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pp. 1601-1611, Vancouver, Canada, July 2017. Association for Compu- tational Linguistics. doi: 10.18653/v1/P17-1147. URL https://aclanthology.org/P17-1147. Karpukhin, V ., Oguz, B., Min, S., Lewis, P., Wu, L., Edunov, S., Chen, D., and Yih, W.-t. Dense passage retrieval for open-domain question answering. In Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP), pp. 6769-6781, Online, November 2020a. Association for Computational Lin- guistics. doi: 10.18653/v1/2020.emnlp-main.550. URL https://aclanthology.org/2020.emnlp-main.550. Karpukhin, V ., Oguz, B., Min, S., Lewis, P., Wu, L., Edunov, S., Chen, D., and Yih, W.-t. Dense passage retrieval for open-domain question answering. In Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP), pp. 6769-6781, 2020b. Khandelwal, U., Levy, O., Jurafsky, D., Zettlemoyer, L., and Lewis, M. Generalization through memorization: Nearest neighbor language models. In International Conference on Learning Representations, 2020. URL https://openreview.net/forum?id=HklBjCEKvH. Khattab, O., Santhanam, K., Li, X. L., Hall, D., Liang, P., Potts, C., and Zaharia, M. Demonstrate-search-predict: Composing retrieval and language models for knowledge- intensive nlp. arXiv preprint arXiv:2212.14024, 2022. Kingma, D. P. and Ba, J. Adam: A method for stochastic optimization. In ICLR (Poster), 2015. Kwiatkowski, T., Palomaki, J., Redfield, O., Collins, M., Parikh, A., Alberti, C., Epstein, D., Polosukhin, I., Devlin, J., Lee, K.,",
      "chunk_index": 17
    },
    {
      "index": 564,
      "chunk_id": "REPLUG2023_chunk_18",
      "source_id": "REPLUG2023",
      "text": "arXiv preprint arXiv:2212.14024, 2022. Kingma, D. P. and Ba, J. Adam: A method for stochastic optimization. In ICLR (Poster), 2015. Kwiatkowski, T., Palomaki, J., Redfield, O., Collins, M., Parikh, A., Alberti, C., Epstein, D., Polosukhin, I., Devlin, J., Lee, K., Toutanova, K., Jones, L., Kelcey, M., Chang, M.-W., Dai, A. M., Uszkoreit, J., Le, Q., and Petrov, S. Natural questions: A benchmark for question answering research. Transactions of the Association for Computa- tional Linguistics, 7:452-466, 2019. doi: 10.1162/tacl_a_ 00276. URL https://aclanthology.org/Q19-1026. Lewis, P., Perez, E., Piktus, A., Petroni, F., Karpukhin, V ., Goyal, N., Küttler, H., Lewis, M., Yih, W.-t., Rocktäschel, T., Riedel, S., and Kiela, D. Retrieval-augmented gen- eration for knowledge-intensive nlp tasks, 2020. URL https://arxiv.org/abs/2005.11401. Lewis, P., Stenetorp, P., and Riedel, S. Question and an- swer test-train overlap in open-domain question answer- ing datasets. In Proceedings of the 16th Conference of the European Chapter of the Association for Computational Linguistics: Main Volume, pp. 1000-1008, 2021. REPLUG : Retrieval-Augmented Black-Box Language Models Mallen, A., Asai, A., Zhong, V ., Das, R., Hajishirzi, H., and Khashabi, D. When not to trust language models: Investi- gating effectiveness and limitations of parametric and non- parametric memories. arXiv preprint arXiv:2212.10511, 2022. Min, S., Shi, W., Lewis, M., Chen, X., Yih, W.-t., Hajishirzi, H., and Zettlemoyer, L. Nonparametric masked language modeling. arXiv preprint arXiv:2212.01349, 2022. Ni, J., Qu, C., Lu, J., Dai, Z., Ábrego, G. H., Ma, J., Zhao, V . Y ., Luan, Y ., Hall, K. B., Chang, M., and Yang, Y . Large dual encoders are generalizable retrievers, 2021. URL https://arxiv.org/abs/2112.07899. Qu, Y ., Ding, Y ., Liu, J., Liu, K., Ren, R., Zhao, W. X., Dong, D., Wu, H., and Wang, H. RocketQA: An opti- mized training approach to dense passage retrieval for open-domain question answering. In Proceedings of the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Hu- man Language Technologies, pp. 5835-5847, Online, June 2021. Association for Computational Linguistics. doi: 10.18653/v1/2021.naacl-main.466. URL https: //aclanthology.org/2021.naacl-main.466. Robertson, S., Zaragoza, H., et al. The probabilistic rele- vance framework: Bm25 and beyond. Foundations and Trends® in Information Retrieval, 3(4):333-389, 2009. Rubin, O., Herzig, J., and Berant, J. Learning to retrieve prompts for in-context learning. In Proceedings of the 2022 Conference of the North American Chapter of the Association for Computational Linguistics: Human Lan- guage Technologies, pp. 2655-2671, 2022. Sachan, D. S., Lewis, M., Yogatama, D., Zettlemoyer, L.,",
      "chunk_index": 18
    },
    {
      "index": 565,
      "chunk_id": "REPLUG2023_chunk_19",
      "source_id": "REPLUG2023",
      "text": "Learning to retrieve prompts for in-context learning. In Proceedings of the 2022 Conference of the North American Chapter of the Association for Computational Linguistics: Human Lan- guage Technologies, pp. 2655-2671, 2022. Sachan, D. S., Lewis, M., Yogatama, D., Zettlemoyer, L., Pineau, J., and Zaheer, M. Questions are all you need to train a dense passage retriever. arXiv preprint arXiv:2206.10658, 2022. Scao, T. L., Fan, A., Akiki, C., Pavlick, E., Ili´c, S., Hesslow, D., Castagné, R., Luccioni, A. S., Yvon, F., Gallé, M., et al. Bloom: A 176b-parameter open-access multilingual language model. arXiv preprint arXiv:2211.05100, 2022. Shi, W., Michael, J., Gururangan, S., and Zettlemoyer, L. Nearest neighbor zero-shot inference. 2022. Si, C., Gan, Z., Yang, Z., Wang, S., Wang, J., Boyd-Graber, J., and Wang, L. Prompting gpt-3 to be reliable. arXiv preprint arXiv:2210.09150, 2022. Si, C., Gan, Z., Yang, Z., Wang, S., Wang, J., Boyd-Graber, J., and Wang, L. Prompting gpt-3 to be reliable. In Proc. of ICLR, 2023. URL https://openreview.net/ forum?id=98p5x51L5af. Stephen, M., Caiming, X., James, B., and Socher, R. Pointer sentinel mixture models. In 5th International Conference on Learning Representations, ICLR 2017, Toulon, France, April 24-26, 2017, Conference Track Proceedings, 2017. Su, H., Kasai, J., Wang, Y ., Hu, Y ., Ostendorf, M., Yih, W.-t., Smith, N. A., Zettlemoyer, L., Yu, T., et al. One embedder, any task: Instruction-finetuned text embeddings. arXiv preprint arXiv:2212.09741, 2022. Wu, S., Zhao, X., Yu, T., Zhang, R., Shen, C., Liu, H., Li, F., Zhu, H., Luo, J., Xu, L., et al. Yuan 1.0: Large-scale pre-trained language model in zero-shot and few-shot learning. arXiv preprint arXiv:2110.04725, 2021. Yasunaga, M., Aghajanyan, A., Shi, W., James, R., Leskovec, J., Liang, P., Lewis, M., Zettlemoyer, L., and Yih, W.-t. Retrieval-augmented multimodal language modeling. arXiv preprint arXiv:2211.12561, 2022. Younes Belkda, T. D. A gentle introduction to 8-bit matrix multiplication, 2022. URL https://huggingface.co/ blog/hf-bitsandbytes-integration. Yu, W. Retrieval-augmented generation across heteroge- neous knowledge. In Proceedings of the 2022 Confer- ence of the North American Chapter of the Association for Computational Linguistics: Human Language Technolo- gies: Student Research Workshop, pp. 52-58, Hybrid: Seattle, Washington + Online, July 2022. Association for Computational Linguistics. doi: 10.18653/v1/2022. naacl-srw.7. URL https://aclanthology.org/2022. naacl-srw.7. Yu, W., Iter, D., Wang, S., Xu, Y ., Ju, M., Sanyal, S., Zhu, C., Zeng, M., and Jiang, M. Generate rather than retrieve: Large language models are strong context generators. 2023. Zhang, S., Diab, M., and Zettlemoyer, L. Democratizing ac- cess to",
      "chunk_index": 19
    },
    {
      "index": 566,
      "chunk_id": "REPLUG2023_chunk_20",
      "source_id": "REPLUG2023",
      "text": "D., Wang, S., Xu, Y ., Ju, M., Sanyal, S., Zhu, C., Zeng, M., and Jiang, M. Generate rather than retrieve: Large language models are strong context generators. 2023. Zhang, S., Diab, M., and Zettlemoyer, L. Democratizing ac- cess to large-scale language models with opt-175b. Meta AI, 2022a. Zhang, S., Roller, S., Goyal, N., Artetxe, M., Chen, M., Chen, S., Dewan, C., Diab, M., Li, X., Lin, X. V ., et al. Opt: Open pre-trained transformer language models. arXiv preprint arXiv:2205.01068, 2022b. Zhong, Z., Lei, T., and Chen, D. Training language models with memory augmentation. In Empirical Methods in Natural Language Processing (EMNLP), 2022. REPLUG : Retrieval-Augmented Black-Box Language Models Knowledge: Arctic Ocean. Although over half of Europe's original forests disappeared through the centuries of deforestation, Europe still has over one quarter of its land area as forest, such as the broadleaf and mixed forests, taiga of Scandinavia and Russia, mixed rainforests of the Caucasus and the Cork oak forests in the western Mediterranean. During recent times, deforestation has been slowed and many trees have been planted. However, in many cases monoculture plantations of conifers have replaced the original mixed natural forest, because these grow quicker. The plantations now cover vast areas of land, but offer poorer habitats for many European Question: As of 2015, since 1990 forests have in Europe and have in Africa and the Americas. A. \"increased, increased\" B. \"increased, decreased\" C. \"decreased, increased\" D. \"decreased, decreased\" Answer: B Knowledge: Over the past decades, the political outlook of Americans has become more progressive, with those below the age of thirty being considerably more liberal than the overall population. According to recent polls, 56% of those age 18 to 29 favor gay marriage, 68% state environmental protection to be as important as job creation, 52% \"think immigrants ´strengthen the country with their hard work and talents,´\" 62% favor a \"tax financed, government-administrated universal health care\" program and 74% \"say ´people´s will´should have more influence on U.S. laws than the Bible, compared to 37%, 49%, 38%, 47% and 58% among the Question: As of 2019, about what percentage of Americans agree that the state is run for the benefit of all the people? A. 31% B. 46% C. 61% D. 76% Answer: B ... Knowledge: last week at a United Nations climate meeting in Germany, China and India should easily exceed the targets they set for themselves in the",
      "chunk_index": 20
    },
    {
      "index": 567,
      "chunk_id": "REPLUG2023_chunk_21",
      "source_id": "REPLUG2023",
      "text": "of all the people? A. 31% B. 46% C. 61% D. 76% Answer: B ... Knowledge: last week at a United Nations climate meeting in Germany, China and India should easily exceed the targets they set for themselves in the 2015 Paris Agreement... India is now expected to obtain 40 percent of its electricity from non-fossil fuel sources by 2022, eight years ahead of schedule.\" Solar power in Japan has been expanding since the late 1990s. By the end of 2017, cumulative installed PV capacity reached over 50 GW with nearly 8 GW installed in the year 2017. The country is a leading manufacturer of solar panels and is in the top 4 ranking for countries Question: Which of the following countries generated the most total energy from solar sources in 2019? A. China B. United States C. Germany D. Japan Table 4.Prompt for MMLU Knowledge: received 122,000 buys (excluding WWE Network views), down from the previous year´s 199,000 buys. The event is named after the Money In The Bank ladder match, in which multiple wrestlers use ladders to retrieve a briefcase hanging above the ring. The winner is guaranteed a match for the WWE World Heavyweight Championship at a time of their choosing within the next year. On the June 2 episode of \"Raw\", Alberto Del Rio qualified for the match by defeating Dolph Ziggler. The following week, following Daniel Bryan being stripped of his WWE World Championship due to injury, Stephanie McMahon changed the Question: Who won the mens money in the bank match? Answer: Braun Strowman Knowledge: in 3D on March 17, 2017. The first official presentation of the film took place at Disney´s three-day D23 Expo in August 2015. The world premiere of \"Beauty and the Beast\" took place at Spencer House in London, England on February 23, 2017; and the film later premiered at the El Capitan Theatre in Hollywood, California, on March 2, 2017. The stream was broadcast onto YouTube. A sing along version of the film released in over 1,200 US theaters nationwide on April 7, 2017. The United Kingdom received the same version on April 21, 2017. The film was re-released in Question: When does beaty and the beast take place Answer: Rococo-era ... Knowledge: Love Yourself \"Love Yourself\" is a song recorded by Canadian singer Justin Bieber for his fourth studio album \"Purpose\" (2015). The song was released first as a",
      "chunk_index": 21
    },
    {
      "index": 568,
      "chunk_id": "REPLUG2023_chunk_22",
      "source_id": "REPLUG2023",
      "text": "Question: When does beaty and the beast take place Answer: Rococo-era ... Knowledge: Love Yourself \"Love Yourself\" is a song recorded by Canadian singer Justin Bieber for his fourth studio album \"Purpose\" (2015). The song was released first as a promotional single on November 8, 2015, and later was released as the album´s third single. It was written by Ed Sheeran, Benny Blanco and Bieber, and produced by Blanco. An acoustic pop song, \"Love Yourself\" features an electric guitar and a brief flurry of trumpets as its main instrumentation. During the song, Bieber uses a husky tone in the lower registers. Lyrically, the song is a kiss-off to a narcissistic ex-lover who did Question: love yourself by justin bieber is about who Table 5.Prompt for open-domain QA",
      "chunk_index": 22
    },
    {
      "index": 569,
      "chunk_id": "REALM2020_chunk_00",
      "source_id": "REALM2020",
      "text": "arXiv:2002.08909v1 [cs.CL] 10 Feb 2020 REALM: Retrieval-Augmented Language Model Pre-T raining Kelvin Guu * 1 Kenton Lee * 1 Zora T ung1 Panupong Pasupat 1 Ming-W ei Chang1 Abstract Language model pre-training has been shown to capture a surprising amount of world knowledge, crucial for NLP tasks such as question answer- ing. However, this knowledge is stored implic- itly in the parameters of a neural network, requir- ing ever-larger networks to cover more facts. T o capture knowledge in a more modular and inter- pretable way, we augment language model pre- training with a latent knowledge retriever , which allows the model to retrieve and attend over doc- uments from a large corpus such as Wikipedia, used during pre-training, ﬁne-tuning and infer- ence. For the ﬁrst time, we show how to pre- train such a knowledge retriever in an unsuper- vised manner, using masked language model- ing as the learning signal and backpropagating through a retrieval step that considers millions of documents. W e demonstrate the effective- ness of Retrieval-Augmented Language Model pre-training (REALM) by ﬁne-tuning on the chal- lenging task of Open-domain Question Answer- ing (Open-QA). W e compare against state-of-the- art models for both explicit and implicit knowl- edge storage on three popular Open-QA bench- marks, and ﬁnd that we outperform all previous methods by a signiﬁcant margin (4-16% absolute accuracy), while also providing qualitative bene- ﬁts such as interpretability and modularity. 1. Introduction Recent advances in language model pre-training have shown that models such as BER T ( Devlin et al. , 2018), RoBER T a ( Liu et al. , 2019) and T5 ( Raffel et al. , 2019) store a surprising amount of world knowledge, ac- quired from the massive text corpora they are trained on ( Petroni et al. , 2019). For example, BER T is able to * Equal contribution 1 Google Research. Correspondence to: Kelvin Guu <kguu@google.com>, Kenton Lee <ken- tonl@google.com>, Zora Tung <gatoatigrado@google.com>, Panupong Pasupat <ppasupat@google.com>, Ming-W ei Chang <mingweichang@google.com>. Figure 1. REALM augments language model pre-training with a neural knowledge retriever that retrieves knowledge from a textual knowledge corpus , Z (e.g., all of Wikipedia). Signal from the language modeling objective backpropagates all th e way through the retriever, which must consider millions of docu ments in Z-a signiﬁcant computational challenge that we address. correctly predict the missing word in the following sen- tence: \" The is the currency of",
      "chunk_index": 0
    },
    {
      "index": 570,
      "chunk_id": "REALM2020_chunk_01",
      "source_id": "REALM2020",
      "text": "objective backpropagates all th e way through the retriever, which must consider millions of docu ments in Z-a signiﬁcant computational challenge that we address. correctly predict the missing word in the following sen- tence: \" The is the currency of the United Kingdom\" (answer: \" pound\"). In these language models, the learned world knowledge is stored implicitly in the parameters of the underlying neural network. This makes it difﬁcult to determine what knowl- edge is stored in the network and where. Furthermore, stor- age space is limited by the size of the network-to cap- ture more world knowledge, one must train ever-larger net- works, which can be prohibitively slow or expensive. T o capture knowledge in a more interpretable and modular way, we propose a novel framework, Retrieval-Augmented Language Model (REALM) pre-training, which augments language model pre-training algorithms with a learned tex- tual knowledge retriever . In contrast to models that store knowledge in their parameters, this approach explicitly ex- poses the role of world knowledge by asking the model to REALM: Retrieval-Augmented Language Model Pre-T raining decide what knowledge to retrieve and use during inference. Before making each prediction, the language model uses the retriever to retrieve documents 1 from a large corpus such as Wikipedia, and then attends over those documents to help inform its prediction. Learning this model end-to- end requires backpropagating through a retrieval step that considers an entire corpus of textual knowledge, as shown in Figure 1. The key intuition of REALM is to train the retriever us- ing a performance-based signal from unsupervised text: a retrieval that improves the language model's perplex- ity is helpful and should be rewarded, while an un- informative retrieval should be penalized. For exam- ple, in Figure 1, if the model needs to ﬁll the blank in \" the at the top of the pyramid\", the re- triever should be rewarded for selecting a document con- taining \" The pyramidion on top allows for less material higher up the pyramid\". W e achieve this behavior by modeling our retrieve-then-predict approach as a latent variable language model and optimizing the marginal likelihood. Incorporating a large-scale neural retrieval module durin g pre-training constitutes a signiﬁcant computational chal - lenge, since the retriever must consider millions of candi- date documents for each pre-training step, and we must backpropagate through its decisions. T o address this, we structure the retriever such that the",
      "chunk_index": 1
    },
    {
      "index": 571,
      "chunk_id": "REALM2020_chunk_02",
      "source_id": "REALM2020",
      "text": "pre-training constitutes a signiﬁcant computational chal - lenge, since the retriever must consider millions of candi- date documents for each pre-training step, and we must backpropagate through its decisions. T o address this, we structure the retriever such that the computation performe d for each document can be cached and asynchronously up- dated, and selection of the best documents can be formu- lated as Maximum Inner Product Search (MIPS). Numerous prior works have demonstrated the bene- ﬁt of adding a discrete retrieval step to neural net- works ( Miller et al. , 2016; Chen et al. , 2017), but did not apply the framework to language model pre-training and employed non-learned retrievers to handle large-scale doc - ument collections. In the language modeling literature, th e k-Nearest Neighbor Language Model ( Khandelwal et al. , 2019) ( kNN-LM) retrieves similar LM examples to im- prove memorization. However, kNN-LM was not ﬁne- tuned for downstream tasks, perhaps because it is unclear how to adapt the retrieval mechanism: a kNN can only use examples labeled for the target task-during ﬁne-tuning, this precludes LM examples, which contain the desired world knowledge. In contrast, REALM's retriever is de- signed to transfer to other tasks, and the retrieval is just text, not a labeled example. W e evaluate our approach by ﬁne-tuning the mod- els pre-trained with REALM on the task of Open- domain Question Answering (Open-QA), one of the most knowledge-intensive tasks in natural language process- ing. W e evaluate on three popular Open-QA bench- marks ( NAT U R A LQU E S T I O N S -O P EN , WE B QU E S T I O N S , and CU R AT E DTR E C) and compare to state-of-the-art Open-QA models, including both extremely large models that store knowledge implicitly (such as T5) as well as previous ap- proaches that also use a knowledge retriever to access ex- ternal knowledge, but implement retrieval in a more heuris- tic fashion ( Lee et al. , 2019; Min et al. , 2019a; Asai et al. , 2019). REALM achieves new state-of-the-art results on all three benchmarks, signiﬁcantly outperforming all previou s systems by 4-16% absolute accuracy. W e also demonstrate qualitative beneﬁts of REALM, including interpretability and modularity. 2. Background Language model pre-training The goal of language model pre-training is to learn useful representations of la n- guage, usually",
      "chunk_index": 2
    },
    {
      "index": 572,
      "chunk_id": "REALM2020_chunk_03",
      "source_id": "REALM2020",
      "text": "previou s systems by 4-16% absolute accuracy. W e also demonstrate qualitative beneﬁts of REALM, including interpretability and modularity. 2. Background Language model pre-training The goal of language model pre-training is to learn useful representations of la n- guage, usually from unlabeled text corpora. The resulting pre-trained model can then be further trained ( ﬁne-tuned ) for a downstream task of primary interest (in our case, Open-QA), often leading to better generalization than trai n- ing from scratch ( Dai & Le , 2015; Radford et al. , 2019). W e focus on the masked language model 2 (MLM) variant of pre-training popularized by BER T ( Devlin et al. , 2018). In its basic form, an MLM is trained to predict the miss- ing tokens in an input text passage. Given an unlabeled pre-training corpus X (e.g., Wikipedia text), a training ex- ample (x, y ) can be generated by randomly masking to- kens in a sampled piece of text (e.g., x = \" The [MASK] is the currency [MASK] the UK\"; y = (\" pound\", \" of\")). The model uses its representation of the masked input x to predict the token that should go in each mask. A good MLM must learn to encode syntactic and semantic information (e.g., to predict \" of\") as well as some world knowledge (e.g., to predict \" pound\"). Open-domain question answering (Open-QA) T o mea- sure a model's ability to incorporate world knowledge, we need a downstream task where world knowledge is criti- cal. Perhaps one of the most knowledge-intensive tasks in natural language processing is open-domain question an- swering (Open-QA): given a question x such as \" What is the currency of the UK?\", a model must output the correct answer string y, \" pound\". The \"open\" part of Open- QA refers to the fact that the model does not receive a pre- identiﬁed document that is known to contain the answer, unlike traditional reading comprehension (RC) tasks such as SQuAD ( Rajpurkar et al. , 2016; 2018). While RC mod- 1 W e use the term \"document\" loosely to refer to a passage from the knowledge corpus, not necessarily a whole article. 2 Strictly speaking, MLM is not a standard language model, since it does not deﬁne a distribution over the entire sequen ce of tokens. In the paper we sometimes abuse the term \"language model\" slightly to make the",
      "chunk_index": 3
    },
    {
      "index": 573,
      "chunk_id": "REALM2020_chunk_04",
      "source_id": "REALM2020",
      "text": "whole article. 2 Strictly speaking, MLM is not a standard language model, since it does not deﬁne a distribution over the entire sequen ce of tokens. In the paper we sometimes abuse the term \"language model\" slightly to make the phrase shorter. REALM: Retrieval-Augmented Language Model Pre-T raining els comprehend a single document, Open-QA models must retain knowledge from millions of documents, since a ques- tion could be about any of them. W e focus on Open-QA systems that utilize a textual knowl- edge corpus Z as the knowledge source. Many of these systems employ a retrieval-based approach: given a ques- tion x, retrieve potentially relevant documents z from the corpus Z, and then extract an answer y from the documents ( Brill et al. , 2002; Chen et al. , 2017; Lee et al. , 2019). Our approach, REALM, is inspired by this paradigm and extends it to language model pre-training. Alternatively, some recent work has proposed generation- based systems that apply a sequence-to-sequence model on x to directly generate y token-by-token ( Lewis et al. , 2019; Raffel et al. , 2019). W e will compare against state-of-the- art systems from both paradigms in our experiments. 3. Approach W e start by formalizing REALM's pre-training and ﬁne- tuning tasks as a retrieve-then-predict generative process in Section 3.1. Then in Section 3.2, we describe the model architectures for each component of that process. In Sec- tion 3.3, we show how to implement REALM pre-training and ﬁne-tuning by maximizing the likelihood of REALM's generative process. En route, we address important compu- tational challenges, explain why training works, and also discuss strategies for injecting useful inductive biases. The overall framework is illustrated in Figure 2. 3.1. REALM's generative process For both pre-training and ﬁne-tuning, REALM takes some input x and learns a distribution p(y | x) over possible out- puts y. For pre-training, the task is masked language mod- eling: x is a sentence from a pre-training corpus X with some tokens masked out, and the model must predict the value of those missing tokens, y. For ﬁne-tuning, the task is Open-QA: x is a question, and y is the answer. REALM decomposes p(y | x) into two steps: retrieve, then predict. Given an input x, we ﬁrst retrieve possibly helpful documents z from a knowledge corpus Z. W e model this as a sample from the distribution p(z | x).",
      "chunk_index": 4
    },
    {
      "index": 574,
      "chunk_id": "REALM2020_chunk_05",
      "source_id": "REALM2020",
      "text": "REALM decomposes p(y | x) into two steps: retrieve, then predict. Given an input x, we ﬁrst retrieve possibly helpful documents z from a knowledge corpus Z. W e model this as a sample from the distribution p(z | x). Then, we condition on both the retrieved z and the original input x to generate the output y-modeled as p(y | z, x ). T o obtain the overall likelihood of generating y, we treat z as a latent variable and marginalize over all possible documents z, yielding p(y | x) = ∑ z∈Z p(y | z, x ) p(z | x). (1) 3.2. Model architecture W e now describe the two key components: the neural knowledge retriever , which models p(z | x), and the knowledge-augmented encoder , which models p(y | z, x ). Knowledge Retriever The retriever is deﬁned using a dense inner product model: p(z | x) = exp f(x, z ) ∑ z′ exp f(x, z ′), f(x, z ) =Embedinput(x)⊤Embeddoc(z), where Embedinput and Embeddoc are embedding functions that map x and z respectively to d-dimensional vectors. The relevance score f(x, z ) between x and z is deﬁned as the inner product of the vector embeddings. The retrieval distribution is the softmax over all relevance scores. W e implement the embedding functions using BER T -style Transformers ( Devlin et al. , 2018). Following standard practices, we join spans of text by applying wordpiece tok- enization, separating them with [SEP] tokens, preﬁxing a [CLS] token, and appending a ﬁnal [SEP] token. joinBERT(x) =[CLS]x[SEP] joinBERT(x1, x 2) =[CLS]x1[SEP]x2[SEP] As in Devlin et al. (2018), we pass this into a Transformer, which produces one vector for each token, including the vector corresponding to [CLS] which is used as a \"pooled\" representation of the sequence (denoted BERTCLS). Finally, we perform a linear projection to reduce the dimensionality of the vector, denoted as a projection matrix W: Embedinput(x) =WinputBERTCLS(joinBERT(x)) Embeddoc(z) =WdocBERTCLS(joinBERT(ztitle , z body)) where ztitle is the document's title and zbody is its body. W e let θ denote all parameters associated with the retriever, which include the Transformer and projection matrices. Knowledge-Augmented Encoder Given an input x and a retrieved document z, the knowledge-augmented encoder deﬁnes p(y | z, x ). W e join x and z into a single sequence that we feed into a Transformer (distinct from the one used in the retriever). This",
      "chunk_index": 5
    },
    {
      "index": 575,
      "chunk_id": "REALM2020_chunk_06",
      "source_id": "REALM2020",
      "text": "x and a retrieved document z, the knowledge-augmented encoder deﬁnes p(y | z, x ). W e join x and z into a single sequence that we feed into a Transformer (distinct from the one used in the retriever). This allows us to perform rich cross- attention between x and z before predicting y. See Figure for a concrete example. At this stage, the architectures for pre-training and ﬁne- tuning differ slightly. For the masked language model pre- training task, we must predict the original value of each [MASK] token in x. T o do so, we use the same masked REALM: Retrieval-Augmented Language Model Pre-T raining Figure 2. The overall framework of REALM. Left: Unsupervised pre-training. The knowledge retriever and knowledge-augmented encoder are jointly pre-trained on the unsupervised langua ge modeling task. Right: Supervised ﬁne-tuning. After the parameters of the retriever ( θ) and encoder ( φ) have been pre-trained, they are then ﬁne-tuned on a task of p rimary interest, using supervised examples. language modeling (MLM) loss as in Devlin et al. (2018): p(y | z, x ) = Jx∏ j=1 p(yj | z, x ) p(yj | z, x ) ∝ exp ( w⊤ j BERTMASK(j)(joinBERT(x, z body )) ) where BERTMASK(j) denotes the Transformer output vector corresponding to the jth masked token, Jx is the total num- ber of [MASK] tokens in x, and wj is a learned word em- bedding for token yj . For Open-QA ﬁne-tuning, we wish to produce the answer string y. Following previous reading comprehension work ( Rajpurkar et al. , 2016; Seo et al. , 2016; Lee et al. , 2016; Clark & Gardner , 2017), we will assume that the answer y can be found as a contiguous sequence of tokens in some document z. Let S(z, y ) be the set of spans matching y in z. Then we can deﬁne p(y | z, x ) as: p(y | z, x ) ∝ ∑ s∈S(z,y) exp ( MLP ([ hSTART(s); hEND(s) ])) hSTART(s) = BERTSTART(s)(joinBERT(x, z body )), hEND(s) = BERTEND(s)(joinBERT(x, z body )), where BERTSTART(s) and BERTEND(s) denote the Transformer output vectors corresponding to the start and end tokens of span s, respectively, while MLP denotes a feed-forward neu- ral network. W e will let φ denote all parameters associated with the knowledge-augmented encoder. 3.3. T raining For both pre-training and ﬁne-tuning, we train by maxi- mizing",
      "chunk_index": 6
    },
    {
      "index": 576,
      "chunk_id": "REALM2020_chunk_07",
      "source_id": "REALM2020",
      "text": "end tokens of span s, respectively, while MLP denotes a feed-forward neu- ral network. W e will let φ denote all parameters associated with the knowledge-augmented encoder. 3.3. T raining For both pre-training and ﬁne-tuning, we train by maxi- mizing the log-likelihood log p(y | x) of the correct out- put y. Since both the knowledge retriever and knowledge- augmented encoder are differentiable neural networks, we can compute the gradient of log p(y | x) (deﬁned in Equa- tion 1) with respect to the model parameters θ and φ, and optimize using stochastic gradient descent. The key computational challenge is that the marginal prob- ability p(y | x) =∑ z∈Z p(y | x, z ) p(z | x) involves a sum- mation over all documents z in the knowledge corpus Z. W e approximate this by instead summing over the top k documents with highest probability under p(z | x)-this is reasonable if most documents have near zero probability. Even with this approximation, we still need an efﬁcient way to ﬁnd the top k documents. Note that the ordering of doc- uments under p(z | x) is the same as under the relevance score f(x, z ) = Embedinput(x)⊤Embeddoc(z), which is an inner product. Thus, we can employ Maximum Inner Prod- uct Search (MIPS) algorithms to ﬁnd the approximate top k documents, using running time and storage space that scale sub-linearly with the number of documents ( Ram & Gray , 2012; Shrivastava & Li , 2014; Shen et al. , 2015). T o employ MIPS, we must pre-compute Embeddoc(z) for every z ∈ Z and construct an efﬁcient search index over these embeddings. However, this data structure will no longer be consistent with p(z | x) if the parameters θ of Embeddoc are later updated. Hence, the search index goes \"stale\" after every gradient update on θ. Our solution is to \"refresh\" the index by asynchronously re-embedding and re-indexing all documents every several hundred training steps. The MIPS index is slightly stale be- tween refreshes, but note that it is only used to select the top k documents. W e recompute p(z | x) and its gradient, using the fresh θ, for these top k documents after retriev- ing them. In Section 4.5, we empirically demonstrate that this procedure results in stable optimization, provided th at refreshes happen at a sufﬁciently frequent rate. Implementing asynchronous MIPS refreshes W e",
      "chunk_index": 7
    },
    {
      "index": 577,
      "chunk_id": "REALM2020_chunk_08",
      "source_id": "REALM2020",
      "text": "fresh θ, for these top k documents after retriev- ing them. In Section 4.5, we empirically demonstrate that this procedure results in stable optimization, provided th at refreshes happen at a sufﬁciently frequent rate. Implementing asynchronous MIPS refreshes W e asyn- chronously refresh the MIPS index by running two jobs in parallel: a primary trainer job, which performs gradient updates on the parameters, and a secondary index builder job, which embeds and indexes the documents. As shown REALM: Retrieval-Augmented Language Model Pre-T raining Figure 3. REALM pre-training with asynchronous MIPS re- freshes. below , the trainer sends the index builder a snapshot of its parameters, θ′. The trainer then continues to train while the index builder uses θ′ to construct a new index in the back- ground. As soon as the index builder is done, it sends the new index back to the trainer, and the process repeats. While asynchronous refreshes can be used for both pre- training and ﬁne-tuning, in our experiments we only use it for pre-training. For ﬁne-tuning, we just build the MIPS in- dex once (using the pre-trained θ) for simplicity and do not update Embeddoc. 3 Note that we still ﬁne-tune Embedinput, so the retrieval function is still updated from the query sid e. What does the retriever learn? Since the knowledge re- trieval of REALM is latent, it is not obvious how the train- ing objective encourages meaningful retrievals. Here, we show how it rewards retrievals that improve prediction ac- curacy. For a given query x and document z, recall that f(x, z ) is the \"relevance score\" that the knowledge retriever assigns to document z. W e can see how a single step of gradient descent during REALM pre-training alters this score by an- alyzing the gradient with respect to the parameters of the knowledge retriever, θ: ∇ log p(y | x) = ∑ z∈Z r(z)∇f(x, z ) r(z) = [ p(y | z, x ) p(y | x) − 1 ] p(z | x). For each document z, the gradient encourages the retriever to change the score f(x, z ) by r(z) - increasing if r(z) is positive, and decreasing if negative. The multiplier r(z) is positive if and only if p(y | z, x ) > p (y | x). The term p(y | z, x ) is the probability of predicting the correct output y when using document z. The",
      "chunk_index": 8
    },
    {
      "index": 578,
      "chunk_id": "REALM2020_chunk_09",
      "source_id": "REALM2020",
      "text": "The multiplier r(z) is positive if and only if p(y | z, x ) > p (y | x). The term p(y | z, x ) is the probability of predicting the correct output y when using document z. The term p(y | x) is the expected value of p(y | x, z ) when randomly sampling a document from p(z | x). Hence, document z receives a positive up- date whenever it performs better than expected. 3 This works because pre-training already yields a good Embeddoc function. However, it is possible that refreshing the in- dex would further improve performance. 3.4. Injecting inductive biases into pre-training In the process of developing REALM, we discovered sev- eral additional strategies that further guide the model to- wards meaningful retrievals, described below . Salient span masking During REALM pre-training, we want to focus on examples x that require world knowledge to predict the masked tokens. As explained in Section 2, some MLM spans only require local context. T o focus on problems that require world knowledge, we mask salient spans such as \" United Kingdom\" or \" July 1969\". W e use a BER T -based tagger trained on CoNLL-2003 data ( Sang & De Meulder , 2003) to identify named entities, and a regular expression to identify dates. W e select and mask one of these salient spans within a sentence for the masked language modeling task. W e show that this signiﬁcantly outperforms other masking strategies in Section 4.5. Null document Even with salient span masking, not all masked tokens require world knowledge to predict. W e model this by adding an empty null document ∅ to the top k retrieved documents, allowing appropriate credit to be as- signed to a consistent sink when no retrieval is necessary. Prohibiting trivial retrievals If the pre-training corpus X and the knowledge corpus Z are the same, there exists a trivial retrieval candidate z that is too informative: if the masked sentence x comes from document z, the knowledge augmented encoder can trivially predict y by looking at the unmasked version of x in z. This results in a large positive gradient for p(z | x). If this occurs too often, the knowledge retriever ends up learning to look for exact string matches between x and z, which does not capture other forms of relevance. For this reason, we exclude this trivial candida",
      "chunk_index": 9
    },
    {
      "index": 579,
      "chunk_id": "REALM2020_chunk_10",
      "source_id": "REALM2020",
      "text": "p(z | x). If this occurs too often, the knowledge retriever ends up learning to look for exact string matches between x and z, which does not capture other forms of relevance. For this reason, we exclude this trivial candida te during pre-training. Initialization At the beginning of training, if the retriever does not have good embeddings for Embedinput(x) and Embeddoc(z), the retrieved documents z will likely be unre- lated to x. This causes the knowledge augmented encoder to learn to ignore the retrieved documents. Once this oc- curs, the knowledge retriever does not receive a meaning- ful gradient and cannot improve, creating a vicious cycle. T o avoid this cold-start problem, we warm-start Embedinput and Embeddoc using a simple training objective known as the Inverse Cloze T ask (ICT) where, given a sentence, the model is trained to retrieve the document where that sen- tence came from. W e defer to Lee et al. (2019) for de- tails. For the knowledge-augmented encoder, we warm- start it with BER T pre-training-speciﬁcally, the uncased BER T -base model (12 layers, 768 hidden units, 12 atten- tion heads). REALM: Retrieval-Augmented Language Model Pre-T raining 4. Experiments W e now evaluate our approach on the Open-QA task. In this section, we describe in detail the benchmarks used and the different approaches to which we compare empirically. 4.1. Open-QA Benchmarks A number of benchmarks have been proposed for Open- QA. In this work, we focus on datasets where the ques- tion writers did not already know the answer. This yields questions that reﬂect more realistic information-seeking needs, and also avoids artifacts that can arise if the ques- tion is formulated with a particular answer in mind. A deeper justiﬁcation is given in Lee et al. (2019). In all cases, the predicted answer is evaluated via exact match with any reference answer, following previous Open-QA work ( Chen et al. , 2017). NaturalQuestions-Open The NaturalQuestions dataset (Kwiatkowski et al. , 2019) consists of naturally occurring Google queries and their answers. Each answer also comes with an \"answer type\": following Lee et al. (2019), we only keep questions that are categorized as \"short answer type\" with at most ﬁve tokens. The dataset also provides a sug- gested Wikipedia document to retrieve; like all models we compare against, we do not provide this to our model. W ebQuestions The W ebQuestions dataset ( Berant et al. ,",
      "chunk_index": 10
    },
    {
      "index": 580,
      "chunk_id": "REALM2020_chunk_11",
      "source_id": "REALM2020",
      "text": "at most ﬁve tokens. The dataset also provides a sug- gested Wikipedia document to retrieve; like all models we compare against, we do not provide this to our model. W ebQuestions The W ebQuestions dataset ( Berant et al. , 2013) was collected from the Google Suggest API, using one seed question and expanding the set to related ques- tions. W e follow the setting deﬁned by Chen et al. (2017). CuratedT rec The CuratedTrec dataset is a collection of question-answer pairs drawn from real user queries issued on sites such as MSNSearch and AskJeeves. T o account for multiple correct answers or different spelling variations , the answers in this dataset are deﬁned as regular expressions that match all correct answers. It is unclear how to train generation-based models with this type of supervision, so we do not evaluate them on this dataset. 4.2. Approaches compared Retrieval-based Open-QA Most existing Open-QA sys- tems answer the input question by ﬁrst retrieving poten- tially relevant documents from a knowledge corpus, and then using a reading comprehension system to extract an answer from the documents. In this paradigm, the knowl- edge is stored explicitly in the corpus. W e wish to compare different methods for implementing retrieval. Many approaches use non-learned heuristic retrieval such as sparse bag-of-words matching ( Robertson et al. , 2009) or entity linking on the question to select a small set of rel- evant documents (e.g., 20). These documents are typically then re-ranked using a learned model, but coverage may be limited by the initial heuristic retrieval step. Approache s such as DrQA ( Chen et al. , 2017), HardEM ( Min et al. , 2019a), GraphRetriever ( Min et al. , 2019b), and PathRe- triever ( Asai et al. , 2019) in T able 1 are in this category. Some recent approaches have proposed to implement learn- able retrieval using a MIPS index. ORQA ( Lee et al. , 2019) formulates Open-QA using a similar latent variable model as REALM, and also trains by maximizing the marginal likelihood. However, REALM adds a novel language model pre-training step, and backpropagates into the MIPS index, rather than using a ﬁxed index. In T able 1, we di- rectly compare the two. It is also important to note that the retrievers for both REALM pretraining and ORQA are initialized using the Inverse Cloze T ask, described in Sec- tion 3.4.",
      "chunk_index": 11
    },
    {
      "index": 581,
      "chunk_id": "REALM2020_chunk_12",
      "source_id": "REALM2020",
      "text": "ﬁxed index. In T able 1, we di- rectly compare the two. It is also important to note that the retrievers for both REALM pretraining and ORQA are initialized using the Inverse Cloze T ask, described in Sec- tion 3.4. Generation-based Open-QA An emerging alternative approach to Open-QA is to model it as a sequence pre- diction task: simply encode the question, and then decode the answer token-by-token based on the encoding. While it was initially unclear how large amounts of knowledge could be injected into the model, GPT -2 ( Radford et al. , 2019) hinted at the possibility of directly generating an- swers without using any given context via sequence-to- sequence. However, their performance was not competi- tive possibly due to the lack of ﬁne-tuning. Orthogonally, T5 ( Raffel et al. , 2019) showed that directly generating an- swers without explicit extraction from the given context is viable approach, but they only experimented on the read- ing comprehension task, where a context document is pro- vided. For the most competitive and comparable generation-based baseline, we compare to concurrent work which ﬁne-tunes T5 for Open-QA ( Roberts et al. , 2020).4 W e compare against the Base, Large, and even larger 11-billion parame- ter model to measure the effect of model size. 4.3. Implementation Details Fine-tuning W e reuse all hyperparameters from Lee et al. (2019), to enable direct comparison. Our knowledge corpus is derived from the December 20, 2018 snapshot of English Wikipedia. Documents are greedily split into chunks of up to 288 BER T wordpieces, resulting in just over 13 million retrieval candidates. During ﬁne- tuning inference, we consider the top-5 candidates, and the 4 W e initially conducted our own T5 experiments using the code from https://tinyurl.com/t5-openqa-colab (Raffel et al. , 2019). W e now report results from the concurrent work of Roberts et al. (2020), which has an improved ﬁne-tuning proce- dure. REALM: Retrieval-Augmented Language Model Pre-T raining T able 1. T est results on Open-QA benchmarks. The number of train/tes t examples are shown in paretheses below each benchmark. Predictions are evaluated with exact match against any refe rence answer. Sparse retrieval denotes methods that use spa rse features such as TF-IDF and BM25. Our model, REALM, outperforms all existi ng systems. Name Architectures Pre-training NQ (79k/4k) WQ (3k/2k) CT (1k /1k) # params BERT -Baseline (Lee et al. , 2019) Sparse Retr. +Transformer",
      "chunk_index": 12
    },
    {
      "index": 582,
      "chunk_id": "REALM2020_chunk_13",
      "source_id": "REALM2020",
      "text": "that use spa rse features such as TF-IDF and BM25. Our model, REALM, outperforms all existi ng systems. Name Architectures Pre-training NQ (79k/4k) WQ (3k/2k) CT (1k /1k) # params BERT -Baseline (Lee et al. , 2019) Sparse Retr. +Transformer BERT 26.5 17.7 21.3 110m T5 (base) ( Roberts et al. , 2020) Transformer Seq2Seq T5 (Multitask) 27.0 29.1 - 223m T5 (large) ( Roberts et al. , 2020) Transformer Seq2Seq T5 (Multitask) 29.8 32.2 - 738m T5 (11b) ( Roberts et al. , 2020) Transformer Seq2Seq T5 (Multitask) 34.5 37.4 - 11318m DrQA ( Chen et al. , 2017) Sparse Retr. +DocReader N/A - 20.7 25.7 34m HardEM ( Min et al. , 2019a) Sparse Retr. +Transformer BERT 28.1 - - 110m GraphRetriever ( Min et al. , 2019b) GraphRetriever +Transformer BERT 31.8 31.6 - 110m PathRetriever ( Asai et al. , 2019) PathRetriever +Transformer MLM 32.6 - - 110m ORQA ( Lee et al. , 2019) Dense Retr. +Transformer ICT +BERT 33.3 36.4 30.1 330m Ours ( X = Wikipedia, Z = Wikipedia) Dense Retr. +Transformer REALM 39.2 40.2 46.8 330m Ours ( X = CC-News, Z = Wikipedia) Dense Retr. +Transformer REALM 40.4 40.7 42.9 330m T able 2. Ablation experiments on NQ's development set. Ablation Exact Match Zero-shot Retrieval Recall@5 REALM 38.2 38.5 REALM retriever +Baseline encoder 37.4 38.5 Baseline retriever +REALM encoder 35.3 13.9 Baseline (ORQA) 31.3 13.9 REALM with random uniform masks 32.3 24.2 REALM with random span masks 35.3 26.1 30× stale MIPS 28.7 15.1 entire model can be run on a single machine with a 12GB GPU. Pre-training W e pre-train for 200k steps on 64 Google Cloud TPUs, with a batch size of 512 and a learning rate of 3e-5, using BER T's default optimizer. The document embedding step for the MIPS index is parallelized over 16 TPUs. For each example, we retrieve and marginalize over 8 candidate documents, including the null document ∅ . W e experiment with two choices of the pre-training corpus X : (1) Wikipedia, which is identical to the knowledge cor- pus Z, and (2) CC-News, our reproduction of the corpus of English news proposed by Liu et al. (2019). 4.4. Main results T able 1 shows the accuracy of different approaches on the three Open-QA datasets. REALM outperform all previous approaches by a signiﬁcant margin. T able 1 also shows the number of parameters for",
      "chunk_index": 13
    },
    {
      "index": 583,
      "chunk_id": "REALM2020_chunk_14",
      "source_id": "REALM2020",
      "text": "Liu et al. (2019). 4.4. Main results T able 1 shows the accuracy of different approaches on the three Open-QA datasets. REALM outperform all previous approaches by a signiﬁcant margin. T able 1 also shows the number of parameters for each model. As reported in the concurrent work of Roberts et al. (2020), the generative Open-QA systems based on T5 are surpris- ingly powerful, with the largest T5-11B model outperform- ing the previous best Open-QA system. Increasing the size of T5 yields consistent improvement, but comes at signif- icant computational cost (from Base to 11B, the model is 50 times larger, and gains roughly 5 points in accuracy). In contrast, REALM outperforms the largest T5-11B model while being 30 times smaller. It is also important to note that T5 accesses additional reading comprehension data from SQuAD during its pre-training (100,000+ examples). Access to such data could also beneﬁt REALM, but was not used in our experiments. Among all systems, the most direct comparison with REALM is ORQA ( Lee et al. , 2019), where the ﬁne-tuning setup, hyperparameters and training data are identical. Th e improvement of REALM over ORQA is purely due to bet- ter pre-training methods. The results also indicate that ou r method of pre-training can be applied both on (1) the single- corpus setting ( X = Wikipedia, Z = Wikipedia), or (2) the separate-corpus setting ( X = CC-News, Z = Wikipedia). Compared to other retrieval-based systems ( Asai et al. , 2019; Min et al. , 2019a;b) which often retrieve from 20 to 80 documents, our system gets the overall best performance while only retrieving 5 documents. 4.5. Analysis In T able 2 we present results for NaturalQuestions-Open after ablating critical components of REALM. In addition to the end-to-end results, we also report how often the gold answer appears in the top-5 retrievals before applying any ﬁne-tuning. The latter metric more signiﬁcantly isolates t he contribution of improving the retriever during pre-traini ng. REALM: Retrieval-Augmented Language Model Pre-T raining T able 3. An example where REALM utilizes retrieved documents to bett er predict masked tokens. It assigns much higher probabilit y (0.129) to the correct term, \" Fermat\", compared to BERT . (Note that the blank corresponds to 3 BER T wordpieces.) x: An equilateral triangle is easily constructed using a stra ightedge and compass, because 3 is a prime. (a) BERT p(y =",
      "chunk_index": 14
    },
    {
      "index": 584,
      "chunk_id": "REALM2020_chunk_15",
      "source_id": "REALM2020",
      "text": "correct term, \" Fermat\", compared to BERT . (Note that the blank corresponds to 3 BER T wordpieces.) x: An equilateral triangle is easily constructed using a stra ightedge and compass, because 3 is a prime. (a) BERT p(y = \" Fermat\" |x) = 1 . 1 × 10− 14 (No retrieval.) (b) REALM p(y = \" Fermat\" |x, z ) = 1. 0 (Conditional probability with document z =\"257 is . . . a Fermat prime. Thus a regular polygon with 257 sides is constructible with c ompass . . . \") (c) REALM p(y = \" Fermat\" |x) = 0 . 129 (Marginal probability , marginalizing over top 8 retrieved documents.) Encoder or Retriever W e ﬁrst aim to determine whether REALM pre-training improves the retriever or the encoder, or both. T o do so, we can reset the parameters of either the retriever or the encoder to their baseline state before REALM pre-training, and feed that into ﬁne-tuning. Reset- ting both the retriever and encoder reduces the system to our main baseline, ORQA. W e ﬁnd that both the encoder and retriever beneﬁt from REALM training separately, but the best result requires both components acting in unison. Masking scheme W e compare our salient span masking scheme (Section 3.4) with (1) random token masking in- troduced in BER T ( Devlin et al. , 2018) and (2) random span masking proposed by SpanBER T ( Joshi et al. , 2019). While such salient span masking has not been shown to be impactful in previous work with standard BER T train- ing ( Joshi et al. , 2019), it is crucial for REALM. Intuitively, the latent variable learning relies heavily on the utility o f re- trieval and is therefore more sensitive to a consistent lear n- ing signal. MIPS index refresh rate During pre-training, we run a parallel process to re-embed corpus documents and rebuild the MIPS index. This results in one index refresh per ap- proximately 500 training steps. T o demonstrate the impor- tance of frequent index refreshes, we compare against using a slower refresh rate. The results in T able 2 suggests that a stale index can hurt model training, and further reducing this staleness could offer better optimization. Examples of retrieved documents T able 3 shows an example of the REALM masked language model predic- tion. In this example, \" Fermat\" is the",
      "chunk_index": 15
    },
    {
      "index": 585,
      "chunk_id": "REALM2020_chunk_16",
      "source_id": "REALM2020",
      "text": "stale index can hurt model training, and further reducing this staleness could offer better optimization. Examples of retrieved documents T able 3 shows an example of the REALM masked language model predic- tion. In this example, \" Fermat\" is the correct word, and REALM (row (c)) gives the word a much high probability compared to the BER T model (row (a)). Since REALM manages to retrieve some documents with a related fact (row (b)), the marginalized probability of the correct an- swer dramatically increases. This shows that REALM is able to retrieve document to ﬁll in the masked word even though it is trained with unsupervised text only. 5. Discussion and Related W ork W e previously discussed related methods for Open-QA. Here we present several alternate ways of viewing REALM that connect it to a broader set of ideas beyond Open-QA: Language modeling with corpus as context Language representation models have been incorporating contexts of increasingly large scope when making predictions. Ex- amples of this progression include models that condi- tion on surrounding words ( Mikolov et al. , 2013a;b), sen- tences ( Kiros et al. , 2015; Peters et al. , 2018), and para- graphs ( Radford et al. , 2018; Devlin et al. , 2018). W e can view REALM as a generalization of the above work to the next level of scope: the entire text corpus. Retrieve-and-edit with learned retrieval In order to better explain the variance in the input text and en- able controllable generation, Guu et al. (2018) proposed a language model with the retrieve-and-edit frame- work ( Hashimoto et al. , 2018) that conditions on text with high lexical overlap. REALM has a similar approach, ex- cept that the model learns for itself which texts are most useful for reducing perplexity. By jointly learning the re- triever, REALM has the capacity to depend on information beyond lexical overlap. Scalable grounded neural memory The document in- dex can be viewed as a memory where the keys are the document embeddings. From this view , our work share motivations with works such as product key mem- ory ( Lample et al. , 2019), which enables sub-linear mem- ory access in a memory network ( W eston et al. , 2014; Graves et al. , 2014; Sukhbaatar et al. , 2015), allowing these scalable memory layers to be integrated into large language models. One main difference",
      "chunk_index": 16
    },
    {
      "index": 586,
      "chunk_id": "REALM2020_chunk_17",
      "source_id": "REALM2020",
      "text": "sub-linear mem- ory access in a memory network ( W eston et al. , 2014; Graves et al. , 2014; Sukhbaatar et al. , 2015), allowing these scalable memory layers to be integrated into large language models. One main difference is that our memo- ries are grounded-each memory is associated with a docu- ment rather than unnamed value vectors. This level of inter- pretability is crucial for applications like Open-QA, wher e users would require provenance for a predicted answer to be trustworthy. Unsupervised Corpus Alignment In sequence-to- sequence models with attention ( Bahdanau et al. , 2014), REALM: Retrieval-Augmented Language Model Pre-T raining text is generated with latent selection of relevant tokens. This results in a set of model-centric unsupervised align- ments between target and source tokens. Analogously, REALM also generates text with latent selection of relevant documents. A by-product of our method is that we offer a set of model-centric unsupervised alignments between text in the pre-training corpus X and knowledge corpus Z. 6. Future W ork The work presented here is the minimal instantiation of a family of REALM-like approaches where a representation is pre-trained to perform reasoning over a large corpus of knowledge on-the-ﬂy during inference. W e are particularly optimistic about generalizations of this work to (1) struc- tured knowledge, which would result in a generalization of Peters et al. (2019) where we would also learn the decision of which entities are informative, (2) the multi-lingual se t- ting, e.g., retrieving knowledge in a high-resource langua ge to better represent text in a low-resource language, and (3) the multi-modal setting, e.g., retrieving images or videos that can provide knowledge rarely observed in text. References Asai, A., Hashimoto, K., Hajishirzi, H., Socher, R., and Xiong, C. Learning to retrieve reasoning paths over wikipedia graph for question answering. arXiv preprint arXiv:1911.10470 , 2019. Bahdanau, D., Cho, K., and Bengio, Y . Neural machine translation by jointly learning to align and translate. arXiv preprint arXiv:1409.0473 , 2014. Berant, J., Chou, A., Frostig, R., and Liang, P . Semantic parsing on freebase from question-answer pairs. In Pro- ceedings of the 2013 Conference on Empirical Methods in Natural Language Processing , pp. 1533-1544, 2013. Brill, E., Dumais, S., and Banko, M. An analysis of the askmsr question-answering system. In Empirical Meth- ods in Natural Language Processing , 2002. Chen, D., Fisch, A., W eston, J., and Bordes, A. Read- ing",
      "chunk_index": 17
    },
    {
      "index": 587,
      "chunk_id": "REALM2020_chunk_18",
      "source_id": "REALM2020",
      "text": ", pp. 1533-1544, 2013. Brill, E., Dumais, S., and Banko, M. An analysis of the askmsr question-answering system. In Empirical Meth- ods in Natural Language Processing , 2002. Chen, D., Fisch, A., W eston, J., and Bordes, A. Read- ing wikipedia to answer open-domain questions. In Pro- ceedings of the 55th Annual Meeting of the Association for Computational Linguistics (V olume 1: Long P apers) , volume 1, pp. 1870-1879, 2017. Clark, C. and Gardner, M. Simple and effective multi- paragraph reading comprehension. In Annual Meeting of the Association for Computational Linguistics , 2017. Dai, A. M. and Le, Q. V . Semi-supervised sequence learn- ing. In Advances in neural information processing sys- tems, pp. 3079-3087, 2015. Devlin, J., Chang, M.-W ., Lee, K., and T outanova, K. Bert: Pre-training of deep bidirectional transformers for lan- guage understanding. arXiv preprint arXiv:1810.04805 , 2018. Graves, A., W ayne, G., and Danihelka, I. Neural turing machines. ArXiv, abs/1410.5401, 2014. Guu, K., Hashimoto, T . B., Oren, Y ., and Liang, P . Gen- erating sentences by editing prototypes. T ransactions of the Association for Computational Linguistics , 6:437- 450, 2018. Hashimoto, T . B., Guu, K., Oren, Y ., and Liang, P . S. A retrieve-and-edit framework for predicting structured outputs. In Advances in Neural Information Processing Systems, pp. 10052-10062, 2018. Joshi, M., Chen, D., Liu, Y ., W eld, D. S., Zettlemoyer, L., and Levy, O. SpanBER T: Improving pre-training by representing and predicting spans. arXiv preprint arXiv:1907.10529 , 2019. Khandelwal, U., Levy, O., Jurafsky, D., Zettlemoyer, L., and Lewis, M. Generalization through memo- rization: Nearest neighbor language models. ArXiv, abs/1911.00172, 2019. Kiros, R., Zhu, Y ., Salakhutdinov, R. R., Zemel, R., Urta- sun, R., T orralba, A., and Fidler, S. Skip-thought vectors. In Advances in neural information processing systems , pp. 3294-3302, 2015. Kwiatkowski, T ., Palomaki, J., Rhinehart, O., Collins, M., Parikh, A., Alberti, C., Epstein, D., Polosukhin, I., Kel- cey, M., Devlin, J., et al. Natural questions: a benchmark for question answering research. T ransactions of the As- sociation for Computational Linguistics , 2019. Lample, G., Sablayrolles, A., Ranzato, M., Denoyer, L., and J´ egou, H. Large memory layers with product keys. In Advances in Neural Information Processing Systems , pp. 8546-8557, 2019. Lee, K., Salant, S., Kwiatkowski, T ., Parikh, A., Das, D., and Berant, J. Learning recurrent span representa- tions for extractive question answering. arXiv preprint",
      "chunk_index": 18
    },
    {
      "index": 588,
      "chunk_id": "REALM2020_chunk_19",
      "source_id": "REALM2020",
      "text": "layers with product keys. In Advances in Neural Information Processing Systems , pp. 8546-8557, 2019. Lee, K., Salant, S., Kwiatkowski, T ., Parikh, A., Das, D., and Berant, J. Learning recurrent span representa- tions for extractive question answering. arXiv preprint arXiv:1611.01436 , 2016. Lee, K., Chang, M.-W ., and T outanova, K. Latent re- trieval for weakly supervised open domain question an- swering. In Proceedings of the Conference of Associa- tion for Computational Linguistics , 2019. Lewis, M., Liu, Y ., Goyal, N., Ghazvininejad, M., Mo- hamed, A., Levy, O., Stoyanov, V ., and Zettlemoyer, L. Bart: Denoising sequence-to-sequence pre-training for natural language generation, translation, and comprehen- sion. ArXiv, abs/1910.13461, 2019. REALM: Retrieval-Augmented Language Model Pre-T raining Liu, Y ., Ott, M., Goyal, N., Du, J., Joshi, M., Chen, D., Levy, O., Lewis, M., Zettlemoyer, L., and Stoyanov, V . Roberta: A robustly optimized bert pretraining approach. arXiv preprint arXiv:1907.11692 , 2019. Mikolov, T ., Chen, K., Corrado, G., and Dean, J. Efﬁcient estimation of word representations in vector space. arXiv preprint arXiv:1301.3781 , 2013a. Mikolov, T ., Sutskever, I., Chen, K., Corrado, G. S., and Dean, J. Distributed representations of words and phrases and their compositionality. In Advances in neural information processing systems , pp. 3111-3119, 2013b. Miller, A., Fisch, A., Dodge, J., Karimi, A.-H., Bordes, A., and W eston, J. Key-value memory networks for directly reading documents. arXiv preprint arXiv:1606.03126 , 2016. Min, S., Chen, D., Hajishirzi, H., and Zettlemoyer, L. A dis- crete hard em approach for weakly supervised question answering. arXiv preprint arXiv:1909.04849 , 2019a. Min, S., Chen, D., Zettlemoyer, L., and Hajishirzi, H. Knowledge guided text retrieval and reading for open domain question answering. arXiv preprint arXiv:1911.03868 , 2019b. Peters, M. E., Neumann, M., Iyyer, M., Gardner, M., Clark, C., Lee, K., and Zettlemoyer, L. Deep contextualized word representations. In Proc. of NAACL , 2018. Peters, M. E., Neumann, M., IV , R. L. L., Schwartz, R., Joshi, V ., Singh, S., and Smith, N. A. Knowledge en- hanced contextual word representations, 2019. Petroni, F ., Rockt¨ aschel, T ., Lewis, P ., Bakhtin, A., Wu, Y ., Miller, A. H., and Riedel, S. Language models as knowl- edge bases? arXiv preprint arXiv:1909.01066 , 2019. Radford, A., Narasimhan, K., Salimans, T ., and Sutskever, I. Improving language understanding with unsupervised learning. T echnical report, OpenAI, 2018. Radford, A., Wu, J., Child, R., Luan, D., Amodei,",
      "chunk_index": 19
    },
    {
      "index": 589,
      "chunk_id": "REALM2020_chunk_20",
      "source_id": "REALM2020",
      "text": "models as knowl- edge bases? arXiv preprint arXiv:1909.01066 , 2019. Radford, A., Narasimhan, K., Salimans, T ., and Sutskever, I. Improving language understanding with unsupervised learning. T echnical report, OpenAI, 2018. Radford, A., Wu, J., Child, R., Luan, D., Amodei, D., and Sutskever, I. Language models are unsupervised multi- task learners. OpenAI Blog , 2019. Raffel, C., Shazeer, N., Roberts, A., Lee, K., Narang, S., Matena, M., Zhou, Y ., Li, W ., and Liu, P . J. Exploring the limits of transfer learning with a uniﬁed text-to-text transformer. arXiv preprint arXiv:1910.10683 , 2019. Rajpurkar, P ., Zhang, J., Lopyrev, K., and Liang, P . Squad: 100,000+ questions for machine comprehension of text. In Proceedings of the 2016 Conference on Empirical Methods in Natural Language Processing , pp. 2383- 2392, 2016. Rajpurkar, P ., Jia, R., and Liang, P . Know what you don't know: Unanswerable questions for squad. arXiv preprint arXiv:1806.03822 , 2018. Ram, P . and Gray, A. G. Maximum inner-product search us- ing cone trees. In Proceedings of the 18th ACM SIGKDD international conference on Knowledge discovery and data mining , pp. 931-939, 2012. Roberts, A., Raffel, C., and Shazeer, N. How much knowl- edge can you pack into the parameters of a language model? arXiv preprint arXiv:TBD , 2020. Robertson, S., Zaragoza, H., et al. The probabilistic rele- vance framework: Bm25 and beyond. F oundations and T rends in Information Retrieval , 3(4):333-389, 2009. Sang, E. T . K. and De Meulder, F . Introduction to the conll- 2003 shared task: Language-independent named entity recognition. In Proceedings of the Seventh Conference on Natural Language Learning at HLT -NAACL 2003 , pp. 142-147, 2003. Seo, M., Kembhavi, A., Farhadi, A., and Hajishirzi, H. Bidirectional attention ﬂow for machine comprehension. In International Conference on Learning Representa- tions, 2016. Shen, F ., Liu, W ., Zhang, S., Y ang, Y ., and T ao Shen, H. Learning binary codes for maximum inner product search. In Proceedings of the IEEE International Con- ference on Computer V ision , pp. 4148-4156, 2015. Shrivastava, A. and Li, P . Asymmetric lsh (alsh) for sub- linear time maximum inner product search (mips). In Advances in Neural Information Processing Systems , pp. 2321-2329, 2014. Sukhbaatar, S., W eston, J., Fergus, R., et al. End-to-end memory networks. In Advances in neural information processing systems , 2015. W eston, J., Chopra, S., and Bordes, A. Memory",
      "chunk_index": 20
    },
    {
      "index": 590,
      "chunk_id": "REALM2020_chunk_21",
      "source_id": "REALM2020",
      "text": "Advances in Neural Information Processing Systems , pp. 2321-2329, 2014. Sukhbaatar, S., W eston, J., Fergus, R., et al. End-to-end memory networks. In Advances in neural information processing systems , 2015. W eston, J., Chopra, S., and Bordes, A. Memory networks. arXiv preprint arXiv:1410.3916 , 2014. REALM: Retrieval-Augmented Language Model Pre-T raining A. Derivation of the gradient with respect to the knowledge retriever W e compute the gradient of the REALM pre-training objec- tive (a log-likelihood) with respect to the parameters of th e knowledge retriever, θ: ∇ log p(y | x) =p(y | x)−1∇p(y | x) = p(y | x)−1 ∑ z p(y | z, x )∇p(z | x) = p(y | x)−1 ∑ z p(y | z, x )p(z | x)∇ log p(z | x) = ∑ z p(z | y, x )∇ log p(z | x), where the last line follows from applying conditional Bayes' rule. W e can then expand ∇ log p (z | x) as: ∇ log p(z | x) =∇ log exp f(x, z )∑ z′ exp f(x, z ′) = ∇ [ f(x, z ) − log ∑ z′ exp f(x, z ′) ] = ∇f(x, z ) − ∑ z′ p(z′ | x)∇f(x, z ′) Plugging this back into the ﬁrst set of equations yields: ∇ log p (y | x) = ∑ z p (z | y, x ) [ ∇f(x, z ) − ∑ z′ p (z′ | x) ∇f(x, z ′) ] = ∑ z p (z | y, x ) ∇f(x, z ) − ∑ z′ p (z′ | x) ∇f(x, z ′) = ∑ z [p (z | y, x ) − p (z | x)] ∇f(x, z ) = ∑ z [ p (y | z, x ) p (z | x) p (y | x) − p (z | x) ] ∇f(x, z ) = ∑ z [ p (y | z, x ) p (y | x) − 1 ] p (z | x) ∇f(x, z ). In the second line, we used the fact that the overall expres- sion is an expectation with respect to p (z | y, x ), and the terms which depend on z′ but not z can be moved out of that expectation. B. Connection between REALM and supervised learning From the equations in Appendix A, we saw that ∇ log p (y | x) = ∑",
      "chunk_index": 21
    },
    {
      "index": 591,
      "chunk_id": "REALM2020_chunk_22",
      "source_id": "REALM2020",
      "text": "the terms which depend on z′ but not z can be moved out of that expectation. B. Connection between REALM and supervised learning From the equations in Appendix A, we saw that ∇ log p (y | x) = ∑ z [p (z | y, x ) − p (z | x)] ∇f(x, z ). Suppose that there exists one document z∗ which causes the model to achieve perfect prediction accuracy (i.e., p (y | z∗, x ) = 1), while all other documents z′ result in zero accuracy (i.e., p (y | z′, x ) = 0 ). Under this set- ting, p (z∗ | y, x ) = 1(provided that p (z∗ | x) is non-zero), which causes the gradient to become ∇ log p (y | x) =∇f (x, z ∗) − ∑ z p (z | x) ∇f(x, z ) = ∇ log p (z∗ | x) . From this, we see that gradient descent on the REALM ob- jective is equivalent to gradient descent on log p (z∗ | x). This is none other than the typical maximum likelihood training objective used in supervised learning, where z∗ is the \"gold\" document. C. Adapting to new knowledge An explicit retrieval system allows us to adapt to new world knowledge simply by modifying the corpus docu- ments. T o demonstrate this ability, we replace the knowl- edge corpus with a more recent version of Wikipedia cor- pus after pre-training is done. When the input query is about a fact where the two corpora disagree, REALM can change the prediction to reﬂect the updated information, as exempliﬁed in T able 4. However, even with an ex- plicit retrieval mechanism, the knowledge-augmented en- coder will still end up remembering some world knowl- edge, making the prediction of some input sentences not updated with the new corpus. (For instance, the model pre- dicts \" Thatcher\" for \" is the prime minister of United Kingdom.\" on both corpora, perhaps due to the frequent mention of her name in Wikipedia articles.) D. Retrieval Utility The null document ∅ described in Section 3.4 provides a way to measure the importance of a retrieved document z: we deﬁne the retrieval utility (RU) of z for the masked input x as the difference between the log-likelihood of the knowledge-augmented encoder when conditioning on z versus on ∅ : RU(z | x) = logp(y | z,",
      "chunk_index": 22
    },
    {
      "index": 592,
      "chunk_id": "REALM2020_chunk_23",
      "source_id": "REALM2020",
      "text": "document z: we deﬁne the retrieval utility (RU) of z for the masked input x as the difference between the log-likelihood of the knowledge-augmented encoder when conditioning on z versus on ∅ : RU(z | x) = logp(y | z, x ) − log p(y | ∅ , x ). (2) A negative RU shows that z is less useful for predicting y than the null document. This could mean that z is irrelevant to x, but could also mean that the masked tokens in x do not require world knowledge to predict, or that the world knowledge is sufﬁciently commonplace it has been baked into the model's parameters. In practice, we ﬁnd that RU increases steadily over the course of pre-training, and is more predictive of good performance on the downstream task of Open-QA than even the overall log-likelihood. An example of how RU behaves over time and across different settings is in Figure 4. REALM: Retrieval-Augmented Language Model Pre-T raining x: \" Jennifer formed the production company Excellent Cadaver.\" BER T also (0.13), then (0.08), later (0.05), . . . REALM ( Z =20 Dec 2018 corpus) smith (0.01), brown (0.01), jones (0.01 ) REALM ( Z =20 Jan 2020 corpus) lawrence (0.13), brown (0.01), smith (0.01), . . . T able 4. An example where REALM adapts to the updated knowledge corpu s. The Wikipedia page \"Excellent Cadaver\" was added in 2019, so the model was not about to recover the word when the kn owledge corpus is outdated (2018). Interestingly , the same REALM model pre-trained on the 2018 corpus is able to retrieve the d ocument in the updated corpus (2020) and generate the correc t token, \" Lawrence\". 0 50 100 150 200 1 3 Pre-training Steps (Thousands) Retrieval Utility Salient span masking Random span masking Random uniform masking Figure 4. The Retrieval Utility (RU, described in Eq. 2) vs the number of pre-training steps. RU roughly estimates t he \"usefulness\" of retrieval. RU is impacted by the choice of masking and the num ber of pre-training steps.",
      "chunk_index": 23
    },
    {
      "index": 593,
      "chunk_id": "G-Eval2023_chunk_00",
      "source_id": "G-Eval2023",
      "text": "G-E VAL: NLG Evaluation using GPT-4 with Better Human Alignment Yang Liu Dan Iter Yichong Xu Shuohang Wang Ruochen Xu Chenguang Zhu Microsoft Cognitive Services Research {yaliu10, iterdan, yicxu, shuowa, ruox, chezhu}@microsoft.com Abstract The quality of texts generated by natural lan- guage generation (NLG) systems is hard to measure automatically. Conventional reference- based metrics, such as BLEU and ROUGE, have been shown to have relatively low cor- relation with human judgments, especially for tasks that require creativity and diversity. Re- cent studies suggest using large language mod- els (LLMs) as reference-free metrics for NLG evaluation, which have the benefit of being ap- plicable to new tasks that lack human refer- ences. However, these LLM-based evaluators still have lower human correspondence than medium-size neural evaluators. In this work, we present G-E VAL, a framework of using large language models with chain-of-thoughts (CoT) and a form-filling paradigm, to assess the quality of NLG outputs. We experiment with two generation tasks, text summarization and dialogue generation. We show that G-E VAL with GPT-4 as the backbone model achieves a Spearman correlation of 0.514 with human on summarization task, outperforming all previous methods by a large margin. We also propose analysis on the behavior of LLM-based eval- uators, and highlight the potential concern of LLM-based evaluators having a bias towards the LLM-generated texts. 1 1 Introduction Evaluating the quality of natural language genera- tion systems is a challenging problem even when large language models can generate high-quality and diverse texts that are often indistinguishable from human-written texts (Ouyang et al., 2022). Traditional automatic metrics, such as BLEU (Pap- ineni et al., 2002), ROUGE (Lin, 2004), and ME- TEOR (Banerjee and Lavie, 2005), are widely used for NLG evaluation, but they have been shown to 1https://github.com/nlpyang/geval have relatively low correlation with human judg- ments, especially for open-ended generation tasks. Moreover, these metrics require associated refer- ence output, which is costly to collect for new tasks. Recent studies propose directly using LLMs as reference-free NLG evaluators (Fu et al., 2023; Wang et al., 2023). The idea is to use the LLMs to score the candidate output based on its generation probability without any reference target, under the assumption that the LLMs have learned to assign higher probabilities to high-quality and fluent texts. However, the validity and reliability of using LLMs as NLG evaluators have not been systematically in- vestigated. In addition, meta-evaluations show that these LLM-based",
      "chunk_index": 0
    },
    {
      "index": 594,
      "chunk_id": "G-Eval2023_chunk_01",
      "source_id": "G-Eval2023",
      "text": "the assumption that the LLMs have learned to assign higher probabilities to high-quality and fluent texts. However, the validity and reliability of using LLMs as NLG evaluators have not been systematically in- vestigated. In addition, meta-evaluations show that these LLM-based evaluators still have lower human correspondence than medium-size neural evalua- tors (Zhong et al., 2022). Thus, there is a need for a more effective and reliable framework for using LLMs for NLG evaluation. In this paper, we propose G-E VAL, a framework of using LLMs with chain-of-thoughts (CoT) (Wei et al., 2022) to evaluate the quality of generated texts in a form-filling paradigm. By only feeding the Task Introduction and the Evaluation Criteria as a prompt, we ask LLMs to generate a CoT of detailed Evaluation Steps. Then we use the prompt along with the generated CoT to evaluate the NLG outputs. The evaluator output is formatted as a form. Moreover, the probabilities of the output rating tokens can be used to refine the final met- ric. We conduct extensive experiments on three meta-evaluation benchmarks of two NLG tasks: text summarization and dialogue generation. The results show that G-E VAL can outperform existing NLG evaluators by a large margin in terms of corre- lation with human evaluations. Finally, we conduct analysis on the behavior of LLM-based evaluators, and highlight the potential issue of LLM-based evaluator having a bias towards the LLM-generated texts. arXiv:2303.16634v3 [cs.CL] 23 May 2023 Auto CoT Task Introduction You will be given one summary written for a news article. Your task is to rate the summary on one metric …… Evaluation Criteria Coherence (1-5) - the collective quality of all sentences. We align this dimension with the DUC quality question of structure and coherence …… Evaluation Steps 1. Read the news article carefully and identify the main topic and key points. 2. Read the summary and compare it to the news article. Check if the summary covers the main topic and key points of the news article, and if it presents them in a clear and logical order. 3. Assign a score for coherence on a scale of 1 to 10, where 1 is the lowest and 5 is the highest based on the Evaluation Criteria. Input Context Article: Paul Merson has restarted his row with Andros Townsend after the Tottenham midfielder was brought on with only seven minutes remaining in his team 's 0-0 draw with",
      "chunk_index": 1
    },
    {
      "index": 595,
      "chunk_id": "G-Eval2023_chunk_02",
      "source_id": "G-Eval2023",
      "text": "5 is the highest based on the Evaluation Criteria. Input Context Article: Paul Merson has restarted his row with Andros Townsend after the Tottenham midfielder was brought on with only seven minutes remaining in his team 's 0-0 draw with Burnley on …… Input Target Summary: Paul merson was brought on with only seven minutes remaining in his team 's 0-0 draw with burnley …… Evaluation Form (scores ONLY): - Coherence: Weighted Summed Score: 2.59 G-Eval 0.2 0.4 0.6 1 2 3 4 5 Figure 1: The overall framework of G-E VAL. We first input Task Introduction and Evaluation Criteria to the LLM, and ask it to generate a CoT of detailed Evaluation Steps. Then we use the prompt along with the generated CoT to evaluate the NLG outputs in a form-filling paradigm. Finally, we use the probability-weighted summation of the output scores as the final score. To summarize, our main contributions in this paper are: 1. LLM-based metrics generally outperform reference-based and reference-free baseline metrics in terms of correlation with human quality judgments, especially for open-ended and creative NLG tasks, such as dialogue re- sponse generation. 2. LLM-based metrics are sensitive to the in- structions and prompts, and chain-of-thought can improve the performance of LLM-based evaluators by providing more context and guidance. 3. LLM-based metrics can provide a more fine- grained continuous score by re-weighting the discrete scores by their respective token prob- abilities. 4. LLM-based metrics have a potential issue of preferring LLM-generated texts over human- written texts, which may lead to the self- reinforcement of LLMs if LLM-based metrics are used as the reward signal for improving themselves. 2 Method G-E VAL is a prompt-based evaluator with three main components: 1) a prompt that contains the def- inition of the evaluation task and the desired evalu- ation criteria, 2) a chain-of-thoughts (CoT) that is a set of intermediate instructions generated by the LLM describing the detailed evaluation steps, and 3) a scoring function that calls LLM and calculates the score based on the probabilities of the return tokens. Prompt for NLG Evaluation The prompt is a natural language instruction that defines the evalu- ation task and the desired evaluation criteria. For example, for text summarization, the prompt can be: You will be given one summary written for a news article. Your task is to rate the summary on one metric. Please make sure you read and under-",
      "chunk_index": 2
    },
    {
      "index": 596,
      "chunk_id": "G-Eval2023_chunk_03",
      "source_id": "G-Eval2023",
      "text": "desired evaluation criteria. For example, for text summarization, the prompt can be: You will be given one summary written for a news article. Your task is to rate the summary on one metric. Please make sure you read and under- stand these instructions carefully. Please keep this document open while reviewing, and refer to it as needed. The prompt should also contain customized eval- uation criteria for different NLG tasks and, such as coherence, conciseness, or grammar. For example, for evaluating coherence in text summarization, we add the following content to the prompt: Evaluation Criteria: Coherence (1-5) - the collective quality of all sentences. We align this dimen- sion with the DUC quality question of structure and coherence whereby \"the summary should be well-structured and well-organized. The summary should not just be a heap of related information, but should build from sentence to sentence to a coherent body of information about a topic. \" Auto Chain-of-Thoughts for NLG Evaluation The chain-of-thoughts (CoT) is a sequence of in- termediate representations that are generated by the LLM during the text generation process. For evaluation tasks, some criteria need a more detailed evaluation instruction beyond the simple definition, and it is time-consuming to manually design such evaluation steps for each task. We find that LLM can generate such evaluation steps by itself. The CoT can provide more context and guidance for the LLM to evaluate the generated text, and can also help to explain the evaluation process and results. For example, for evaluating coherence in text sum- marization, we add a line of\"Evaluation Steps:\" to the prompt and let LLM to generate the following CoT automatically: 1. Read the news article carefully and identify the main topic and key points. 2. Read the summary and compare it to the news article. Check if the summary covers the main topic and key points of the news article, and if it presents them in a clear and logical order. 3. Assign a score for coherence on a scale of 1 to 5, where 1 is the lowest and 5 is the highest based on the Evaluation Criteria. Scoring Function The scoring function calls the LLM with the designed prompt, auto CoT, the input context and the target text that needs to be evalu- ated. Unlike GPTScore (Fu et al., 2023) which uses the conditional probability of generating the tar- get text as an evaluation metric,",
      "chunk_index": 3
    },
    {
      "index": 597,
      "chunk_id": "G-Eval2023_chunk_04",
      "source_id": "G-Eval2023",
      "text": "with the designed prompt, auto CoT, the input context and the target text that needs to be evalu- ated. Unlike GPTScore (Fu et al., 2023) which uses the conditional probability of generating the tar- get text as an evaluation metric, G-E VAL directly performs the evaluation task with a form-filling paradigm. For example, for evaluating coherence in text summarization, we concatenate the prompt, the CoT, the news article, and the summary, and then call the LLM to output a score from 1 to 5 for each evaluation aspect, based on the defined criteria. However, we notice this direct scoring function has two issues: 1. For some evaluation tasks, one digit usually dominates the distribution of the scores, such as 3 for a 1 - 5 scale. This may lead to the low variance of the scores and the low correlation with human judgments. 2. LLMs usually only output integer scores, even when the prompt explicitly requests decimal values. This leads to many ties in evaluation scores which do not capture the subtle differ- ence between generated texts. To address these issues, we propose using the probabilities of output tokens from LLMs to nor- malize the scores and take their weighted summa- tion as the final results. Formally, given a set of scores (like from 1 to 5) predefined in the prompt S = {s1, s2, ..., sn}, the probability of each score p(si) is calculated by the LLM, and the final score is: score = nX i=1 p(si) × si (1) This method obtains more fine-grained, continu- ous scores that better reflect the quality and diver- sity of the generated texts. 3 Experiments Following Zhong et al. (2022), we meta-evaluate our evaluator on three benchmarks, SummEval, Topical-Chat and QAGS, of two NLG tasks, sum- marization and dialogue response generation. 3.1 Implementation Details We use OpenAI's GPT family as our LLMs, includ- ing GPT-3.5 (text-davinci-003) and GPT-4. For GPT-3.5, we set decoding temperature to 0 to in- crease the model's determinism. For GPT-4, as it does not support the output of token probabilities, we set 'n = 20, temperature= 1, topp = 1' to sample 20 times to estimate the token probabilities. We use G-E VAL-4 to indicate G-E VAL with GPT-4 Metrics Coherence Consistency Fluency Relevance A VG ρ τ ρ τ ρ τ ρ τ ρ τ ROUGE-1 0.167 0.126 0.160 0.130 0.115 0.094 0.326 0.252 0.192 0.150 ROUGE-2",
      "chunk_index": 4
    },
    {
      "index": 598,
      "chunk_id": "G-Eval2023_chunk_05",
      "source_id": "G-Eval2023",
      "text": "probabilities. We use G-E VAL-4 to indicate G-E VAL with GPT-4 Metrics Coherence Consistency Fluency Relevance A VG ρ τ ρ τ ρ τ ρ τ ρ τ ROUGE-1 0.167 0.126 0.160 0.130 0.115 0.094 0.326 0.252 0.192 0.150 ROUGE-2 0.184 0.139 0.187 0.155 0.159 0.128 0.290 0.219 0.205 0.161 ROUGE-L 0.128 0.099 0.115 0.092 0.105 0.084 0.311 0.237 0.165 0.128 BERTScore 0.284 0.211 0.110 0.090 0.193 0.158 0.312 0.243 0.225 0.175 MOVERSscore 0.159 0.118 0.157 0.127 0.129 0.105 0.318 0.244 0.191 0.148 BARTScore 0.448 0.342 0.382 0.315 0.356 0.292 0.356 0.273 0.385 0.305 UniEval 0.575 0.442 0.446 0.371 0.449 0.371 0.426 0.325 0.474 0.377 GPTScore 0.434 - 0.449 - 0.403 - 0.381 - 0.417 - G-E VAL-3.5 0.440 0.335 0.386 0.318 0.424 0.347 0.385 0.293 0.401 0.320 - Probs 0.359 0.313 0.361 0.344 0.339 0.323 0.327 0.288 0.346 0.317 G-E VAL-4 0.582 0.457 0.507 0.425 0.455 0.378 0.547 0.433 0.514 0.418 - Probs 0.560 0.472 0.501 0.459 0.438 0.408 0.511 0.444 0.502 0.446 - CoT 0.564 0.454 0.493 0.413 0.403 0.334 0.538 0.427 0.500 0.407 Table 1: Summary-level Spearman (ρ) and Kendall-Tau (τ) correlations of different metrics on SummEval bench- mark. G-E VAL without probabilities (italicized) should not be considered as a fair comparison to other metrics on τ, as it leads to many ties in the scores. This results in a higher Kendall-Tau correlation, but it does not fairly reflect the true evaluation ability. More details are in Section 4. as the backbone model, and G-E VAL-3.5 to indi- cate G-E VAL with GPT-3.5 as the backbone model. Example prompts for each task are provided in the Appendix. 3.2 Benchmarks We adopt three meta-evaluation benchmarks to measure the correlation between G-E VAL and human judgments. SummEval (Fabbri et al., 2021) is a bench- mark that compares different evaluation methods for summarization. It gives human ratings for four aspects of each summary: fluency, coherence, consistency and relevance. It is built on the CNN/DailyMail dataset (Hermann et al., 2015) Topical-Chat (Mehri and Eskenazi, 2020) is a testbed for meta-evaluating different evaluators on dialogue response generation systems that use knowledge. We follow (Zhong et al., 2022) to use its human ratings on four aspects: naturalness, coherence, engagingness and groundedness. QAGS (Wang et al., 2020) is a benchmark for evaluating hallucinations in the summarization task. It aims to measure the consistency dimension of summaries on two different summarization datasets. 3.3 Baselines We",
      "chunk_index": 5
    },
    {
      "index": 599,
      "chunk_id": "G-Eval2023_chunk_06",
      "source_id": "G-Eval2023",
      "text": "on four aspects: naturalness, coherence, engagingness and groundedness. QAGS (Wang et al., 2020) is a benchmark for evaluating hallucinations in the summarization task. It aims to measure the consistency dimension of summaries on two different summarization datasets. 3.3 Baselines We evaluate G-E VAL against various evaluators that achieved state-of-the-art performance. BERTScore (Zhang et al., 2019) measures the similarity between two texts based on the contextu- alized embedding from BERT (Devlin et al., 2019). MoverScore (Zhao et al., 2019) improves BERTScore by adding soft alignments and new aggregation methods to obtain a more robust simi- larity measure. BARTScore (Yuan et al., 2021) is a unified eval- uator which evaluate with the average likelihood of the pretrained encoder-decoder model, BART (Lewis et al., 2020). It can predict different scores depending on the formats of source and target. FactCC and QAGS (Kry´sci´nski et al., 2020; Wang et al., 2020) are two evaluators that measure the factual consistency of generated summaries. FactCC is a BERT-based classifier that predicts whether a summary is consistent with the source document. QAGS is a question-answering based evaluator that generates questions from the sum- mary and checks if the answers can be found in the source document. USR (Mehri and Eskenazi, 2020) is evaluator that assess dialogue response generation from dif- ferent perspectives. It has several versions that assign different scores to each target response. UniEval (Zhong et al., 2022) is a unified evalua- tor that can evaluate different aspects of text gen- eration as QA tasks. It uses a pretrained T5 model (Raffel et al., 2020) to encode the evaluation task, source and target texts as questions and answers, and then computes the QA score as the evaluation score. It can also handle different evaluation tasks by changing the question format. GPTScore (Fu et al., 2023) is a new framework that evaluates texts with generative pre-training models like GPT-3. It assumes that a generative pre-training model will assign a higher probability of high-quality generated text following a given in- struction and context. Unlike G-E VAL, GPTScore formulates the evaluation task as a conditional gen- eration problem instead of a form-filling problem. 3.4 Results for Summarization We adopt the same approach as Zhong et al. (2022) to evaluate different summarization metrics using summary-level Spearman and Kendall-Tau corre- lation. The first part of Table 1 shows the results of metrics that compare the semantic similarity between the model output and",
      "chunk_index": 6
    },
    {
      "index": 600,
      "chunk_id": "G-Eval2023_chunk_07",
      "source_id": "G-Eval2023",
      "text": "same approach as Zhong et al. (2022) to evaluate different summarization metrics using summary-level Spearman and Kendall-Tau corre- lation. The first part of Table 1 shows the results of metrics that compare the semantic similarity between the model output and the reference text. These metrics perform poorly on most dimensions. The second part shows the results of metrics that use neural networks to learn from human ratings of summary quality. These metrics have much higher correlations than the similarity-based metrics, sug- gesting that they are more reliable for summariza- tion evaluation. In the last part of Table 1 which corresponds to GPT-based evaluators, GPTScore also uses GPTs for evaluating summarization texts, but relies on GPT's conditional probabilities of the given tar- get. G-E VAL substantially surpasses all previous state-of-the-art evaluators on the SummEval bench- mark. G-E VAL-4 achieved much higher human correspondence compared with G-E VAL-3.5 on both Spearman and Kendall-Tau correlation, which indicates that the larger model size of GPT-4 is beneficial for summarization evaluation. G-E VAL also outperforms GPTScore on several dimension, demonstrating the effectiveness of the simple form- filling paradigm. 3.5 Results for Dialogue Generation We use the Topical-chat benchmark from Mehri and Eskenazi (2020) to measure how well differ- ent evaluators agree with human ratings on the quality of dialogue responses. We calculate the Pearson and Spearman correlation for each turn of the dialogue. Table 2 shows that similarity- based metrics have good agreement with humans on how engaging and grounded the responses are, but not on the other aspects. With respect to the learning-based evaluators, before G-E VAL, UniEval predicts scores that are most consistent with human judgments across all aspects. As shown in the last part, G-E VAL also substan- tially surpasses all previous state-of-the-art eval- uator on the Topical-Chat benchmark. Notably, the G-E VAL-3.5 can achieve similar results with G-E VAL-4. This indicates that this benchmark is relatively easy for the G-E VAL model. 3.6 Results on Hallucinations Advanced NLG models often produce text that does not match the context input (Cao et al., 2018), and recent studies find even powerful LLMs also suffer from the problem of hallucination. This motivates recent research to design evaluators for measuring the consistency aspect in summa- rization (Kry ´sci´nski et al., 2020; Wang et al., 2020; Cao et al., 2020; Durmus et al., 2020). We test the QAGS meta-evaluation benchmark, which includes two different summarization datasets:",
      "chunk_index": 7
    },
    {
      "index": 601,
      "chunk_id": "G-Eval2023_chunk_08",
      "source_id": "G-Eval2023",
      "text": "to design evaluators for measuring the consistency aspect in summa- rization (Kry ´sci´nski et al., 2020; Wang et al., 2020; Cao et al., 2020; Durmus et al., 2020). We test the QAGS meta-evaluation benchmark, which includes two different summarization datasets: CNN/DailyMail and XSum (Narayan et al., 2018) Table 3 shows that BARTScore performs well on the more extractive subset (QAGS-CNN), but has low correlation on the more abstractive subset (QAGS-Xsum). UniEval has good correlation on both subsets of the data. On average, G-E VAL-4 outperforms all state-of- the-art evaluators on QAGS, with a large margin on QAGS-Xsum. G-E VAL-3.5, on the other hand, failed to perform well on this benchmark, which indicates that the consistency aspect is sensitive to the LLM's capacity. This result is consistent with Table 1. 4 Analysis Will G-E VAL prefer LLM-based outputs? One concern about using LLM as an evaluator is that it may prefer the outputs generated by the LLM itself, rather than the high-quality human-written texts. To investigate this issue, we conduct an experi- ment on the summarization task, where we com- pare the evaluation scores of the LLM-generated and the human-written summaries. We use the dataset collected in Zhang et al. (2023), where they first ask freelance writers to write high-quality sum- maries for news articles, and then ask annotators to compare human-written summaries and LLM- generated summaries (using GPT-3.5, text-davinci- Metrics Naturalness Coherence Engagingness Groundedness A VG r ρ r ρ r ρ r ρ r ρ ROUGE-L 0.176 0.146 0.193 0.203 0.295 0.300 0.310 0.327 0.243 0.244 BLEU-4 0.180 0.175 0.131 0.235 0.232 0.316 0.213 0.310 0.189 0.259 METEOR 0.212 0.191 0.250 0.302 0.367 0.439 0.333 0.391 0.290 0.331 BERTScore 0.226 0.209 0.214 0.233 0.317 0.335 0.291 0.317 0.262 0.273 USR 0.337 0.325 0.416 0.377 0.456 0.465 0.222 0.447 0.358 0.403 UniEval 0.455 0.330 0.602 0.455 0.573 0.430 0.577 0.453 0.552 0.417 G-E VAL-3.5 0.532 0.539 0.519 0.544 0.660 0.691 0.586 0.567 0.574 0.585 G-E VAL-4 0.549 0.565 0.594 0.605 0.627 0.631 0.531 0.551 0.575 0.588 Table 2: Turn-level Spearman (ρ) and Kendall-Tau (τ) correlations of different metrics on Topical-Chat benchmark. 003). The dataset can be divided in three categories: 1) human-written summaries that are rated higher than GPT-3.5 summaries by human judges, 2) human-written summaries that are rated lower than GPT-3.5 summaries by human judges, and 3) human-written summaries and GPT-3.5 summaries are rated equally good by human judges.",
      "chunk_index": 8
    },
    {
      "index": 602,
      "chunk_id": "G-Eval2023_chunk_09",
      "source_id": "G-Eval2023",
      "text": "1) human-written summaries that are rated higher than GPT-3.5 summaries by human judges, 2) human-written summaries that are rated lower than GPT-3.5 summaries by human judges, and 3) human-written summaries and GPT-3.5 summaries are rated equally good by human judges. We useG- EVAL-4 to evaluate the summaries in each category, and compare the averaged scores. 2 The results are shown in Figure 2. We can see that, G-E VAL-4 assigns higher scores to human- written summaries when human judges also pre- fer human-written summaries, and assigns lower scores when human judges prefer GPT-3.5 sum- maries. However, G-E VAL-4 always gives higher scores to GPT-3.5 summaries than human-written summaries, even when human judges prefer human- written summaries. We propose two potential rea- sons for this phenomenon: 1. NLG outputs from high-quality systems are in natural difficult to evaluate. The authors of the original paper found that inter-annotator agreement on judging human-written and LLM-generated summaries is very low, with Krippendorff's alpha at 0.07. 2. G-E VAL may have a bias towards the LLM- generated summaries because the model could share the same concept of evaluation criteria during generation and evaluation. Our work should be considered as a preliminary study on this issue, and more research is needed to fully understand the behavior of LLM-based 2We use G-E VAL-4 in this experiment, because its su- periority in evaluating summarization tasks. Although it has different distribution with with GPT-3.5, the two LLMs should share similar behaviors in terms of text generation. 3.75 3.8 3.85 3.9 3.95 Human Summary GPT-3.5 Summary Human Summary GPT-3.5 Summary Human Summary GPT-3.5 Summary Human Summary is BetterLLM Summary is Better Equally Good Figure 2: Averaged G-E VAL-4's scores for human- written summaries and GPT-3.5 summaries, divided by human judges' preference. evaluators to reduce its inherent bias towards LLM- generated text. We highlight this concern in the context that LLM-based evaluators may lead to self-reinforcement of LLMs if the evaluation score is used as a reward signal for further tuning. And this could result in the over-fitting of the LLMs to their own evaluation criteria, rather than the true evaluation criteria of the NLG tasks. The Effect of Chain-of-Thoughts We compare the performance of G-E VAL with and without chain-of-thoughts (CoT) on the SummEval bench- mark. Table 1 shows that G-E VAL-4 with CoT has higher correlation than G-E VAL-4 without CoT on all dimensions, especially for fluency. This suggests",
      "chunk_index": 9
    },
    {
      "index": 603,
      "chunk_id": "G-Eval2023_chunk_10",
      "source_id": "G-Eval2023",
      "text": "compare the performance of G-E VAL with and without chain-of-thoughts (CoT) on the SummEval bench- mark. Table 1 shows that G-E VAL-4 with CoT has higher correlation than G-E VAL-4 without CoT on all dimensions, especially for fluency. This suggests that CoT can provide more context and guidance for the LLM to evaluate the generated text, and can also help to explain the evaluation process and results. The Effect of Probability Normalization We compare the performance of G-E VAL with and without probability normalization on the Sum- mEval benchmark. Table 1 shows that, on Kendall- Tau correlation, G-E VAL-4 with probabilities is Metrics QAGS-CNN QAGS-XSUM Average r ρ τ r ρ τ r ρ τ ROUGE-2 0.459 0.418 0.333 0.097 0.083 0.068 0.278 0.250 0.200 ROUGE-L 0.357 0.324 0.254 0.024 -0.011 -0.009 0.190 0.156 0.122 BERTScore 0.576 0.505 0.399 0.024 0.008 0.006 0.300 0.256 0.202 MoverScore 0.414 0.347 0.271 0.054 0.044 0.036 0.234 0.195 0.153 FactCC 0.416 0.484 0.376 0.297 0.259 0.212 0.356 0.371 0.294 QAGS 0.545 - - 0.175 - - 0.375 - - BARTScore 0.735 0.680 0.557 0.184 0.159 0.130 0.459 0.420 0.343 CTC 0.619 0.564 0.450 0.309 0.295 0.242 0.464 0.430 0.346 UniEval 0.682 0.662 0.532 0.461 0.488 0.399 0.571 0.575 0.465 G-E VAL-3.5 0.477 0.516 0.410 0.211 0.406 0.343 0.344 0.461 0.377 G-E VAL-4 0.631 0.685 0.591 0.558 0.537 0.472 0.599 0.611 0.525 Table 3: Pearson (r), Spearman (ρ) and Kendall-Tau (τ) correlations of different metrics on QAGS benchmark. inferior to G-E VAL-4 without probabilities on Sum- mEval. We believe this is related to the calculation of Kendall-Tau correlation, which is based on the number of concordant and discordant pairs. Direct scoring without probabilities can lead to many ties, which are not counted as either concordant or dis- cordant. This may result in a higher Kendall-Tau correlation, but it does not reflect the model's true capacity of evaluating the generated texts. On the other hand, probability normalization can obtain more fine-grained, continuous scores that better capture the subtle difference between generated texts. This is reflected by the higher Spearman cor- relation of G-E VAL-4 with probabilities, which is based on the rank order of the scores. The Effect of Model Size We compare the per- formance of G-E VAL with different model sizes on the SummEval and QAGS benchmarks. Ta- ble 1 and Table 3 show that G-E VAL-4 has higher correlation than G-E VAL-3.5 on",
      "chunk_index": 10
    },
    {
      "index": 604,
      "chunk_id": "G-Eval2023_chunk_11",
      "source_id": "G-Eval2023",
      "text": "The Effect of Model Size We compare the per- formance of G-E VAL with different model sizes on the SummEval and QAGS benchmarks. Ta- ble 1 and Table 3 show that G-E VAL-4 has higher correlation than G-E VAL-3.5 on most dimensions and datasets, except for engagingness and groundedness on the Topical-Chat benchmark. This demonstrates that larger model size can im- prove the performance of G-E VAL, especially for more challenging and complex evaluation tasks, such as consistency and relevance. 5 Related Work Ngram-based Metrics Ngram-based metrics re- fer to the scores for evaluating the NLG models by measuring the lexical overlap between a generated text and a reference text. BLEU (Papineni et al., 2002) is the most widely used metric for machine translation evaluation, which calculates the geomet- ric mean of modified n-gram precision and a brevity penalty. ROUGE (Lin, 2004) is a recall-oriented metric for summarization evaluation, which mea- sures the n-gram overlap between a generated sum- mary and a set of reference summaries. It has been shown that more than 60% of recent papers on NLG only rely on ROUGE or BLEU to evaluate their systems (Kasai et al., 2021). However, these metrics fail to measure content quality (Reiter and Belz, 2009) or capture syntactic errors (Stent et al., 2005), and therefore do not reflect the reliability of NLG systems accurately. Embedding-based Metrics Embedding-based metrics refer to the scores for evaluating the NLG models by measuring the semantic similarity be- tween a generated text and a reference text based on the word or sentence embeddings. WMD (Kus- ner et al., 2015) is a metric that measures the dis- tance between two texts based on the word embed- dings. BERTScore (Zhang et al., 2019) measures the similarity between two texts based on the con- textualized embedding from BERT (Devlin et al., 2019). MoverScore (Zhao et al., 2019) improves BERTScore by adding soft alignments and new aggregation methods to obtain a more robust simi- larity measure. (Clark et al., 2019) propose a metric that evaluates multi-sentence texts by computing the similarity between the generated text and the reference text based on the sentence embeddings. Task-specific Evaluators Task-specific metrics refer to the scores for evaluating the NLG mod- els by measuring the quality of the generated texts based on the specific task requirements. For example, summarization tasks need to as- sess the consistency of the generated sum- maries (Kry´sci´nski et",
      "chunk_index": 11
    },
    {
      "index": 605,
      "chunk_id": "G-Eval2023_chunk_12",
      "source_id": "G-Eval2023",
      "text": "to the scores for evaluating the NLG mod- els by measuring the quality of the generated texts based on the specific task requirements. For example, summarization tasks need to as- sess the consistency of the generated sum- maries (Kry´sci´nski et al., 2020; Wang et al., 2020; Cao et al., 2020; Durmus et al., 2020), and di- alogue response generation tasks need to assess the coherence of the generated responses (Dziri et al., 2019; Ye et al., 2021). However, these met- rics are not generalizable to other NLG tasks, and they are not able to measure the overall quality of the generated texts. Unified Evaluators Recently, some evaluators have been developed to assess text quality from multiple dimensions by varying the input and out- put contents (Yuan et al., 2021) or the model vari- ants (Mehri and Eskenazi, 2020) they use. UniEval (Zhong et al., 2022) is a unified evaluator that can evaluate different aspects of text generation as QA tasks. By changing the question format, it can han- dle different evaluation tasks. LLM-based Evaluators Fu et al. (2023) propose GPTScore, a new framework that evaluated texts with generative pre-training models like GPT-3. It assumes that a generative pre-training model will assign a higher probability of high-quality gener- ated text following a given instruction and context. Wang et al. (2023) conduct a preliminary survey of using ChatGPT as a NLG evaluator. Kocmi and Federmann (2023) proposed to use GPT models for evaluating machine translation tasks. 6 Conclusion In this paper, we propose G-E VAL, a framework of using LLM with chain-of-thoughts (CoT) to eval- uate the quality of generated texts. We conduct extensive experiments on two NLG tasks, text sum- marization and dialogue generation, and show that G-E VAL can outperform state-of-the-art evaluators and achieve higher human correspondence. We also propose preliminary analysis on the behavior of LLM-based evaluators, and highlight the poten- tial issue of LLM-based evaluator having a bias towards the LLM-generated texts. We hope our work can inspire more research on using LLMs for NLG evaluation, and also raise awareness of the potential risks and challenges of using LLMs as evaluators. References Satanjeev Banerjee and Alon Lavie. 2005. Meteor: An automatic metric for mt evaluation with improved cor- relation with human judgments. In Proceedings of the acl workshop on intrinsic and extrinsic evaluation measures for machine translation and/or summariza- tion, pages 65-72. Meng Cao, Yue Dong, Jiapeng Wu,",
      "chunk_index": 12
    },
    {
      "index": 606,
      "chunk_id": "G-Eval2023_chunk_13",
      "source_id": "G-Eval2023",
      "text": "Meteor: An automatic metric for mt evaluation with improved cor- relation with human judgments. In Proceedings of the acl workshop on intrinsic and extrinsic evaluation measures for machine translation and/or summariza- tion, pages 65-72. Meng Cao, Yue Dong, Jiapeng Wu, and Jackie Chi Kit Cheung. 2020. Factual error correction for abstrac- tive summarization models. In Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP), pages 6251-6258. Ziqiang Cao, Furu Wei, Wenjie Li, and Sujian Li. 2018. Faithful to the original: Fact aware neural abstractive summarization. In thirty-second AAAI conference on artificial intelligence. Elizabeth Clark, Asli Celikyilmaz, and Noah A Smith. 2019. Sentence mover's similarity: Automatic evalu- ation for multi-sentence texts. In Proceedings of the 57th Annual Meeting of the Association for Compu- tational Linguistics, pages 2748-2760. Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. 2019. Bert: Pre-training of deep bidirectional transformers for language understand- ing. In Proceedings of the 2019 Conference of the North American Chapter of the Association for Com- putational Linguistics: Human Language Technolo- gies, Volume 1 (Long and Short Papers), pages 4171- 4186. Esin Durmus, He He, and Mona Diab. 2020. Feqa: A question answering evaluation framework for faith- fulness assessment in abstractive summarization. In Proceedings of the 58th Annual Meeting of the Asso- ciation for Computational Linguistics, pages 5055- 5070. Nouha Dziri, Ehsan Kamalloo, Kory Mathewson, and Osmar R Zaiane. 2019. Evaluating coherence in di- alogue systems using entailment. In Proceedings of the 2019 Conference of the North American Chap- ter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long and Short Papers), pages 3806-3812. Alexander R Fabbri, Wojciech Kry´sci´nski, Bryan Mc- Cann, Caiming Xiong, Richard Socher, and Dragomir Radev. 2021. Summeval: Re-evaluating summariza- tion evaluation. Transactions of the Association for Computational Linguistics, 9:391-409. Jinlan Fu, See-Kiong Ng, Zhengbao Jiang, and Pengfei Liu. 2023. Gptscore: Evaluate as you desire. arXiv preprint arXiv:2302.04166. Karl Moritz Hermann, Tomas Kocisky, Edward Grefen- stette, Lasse Espeholt, Will Kay, Mustafa Suleyman, and Phil Blunsom. 2015. Teaching machines to read and comprehend. Advances in neural information processing systems, 28. Jungo Kasai, Keisuke Sakaguchi, Ronan Le Bras, Lavinia Dunagan, Jacob Morrison, Alexander R Fab- bri, Yejin Choi, and Noah A Smith. 2021. Bidimen- sional leaderboards: Generate and evaluate language hand in hand. arXiv preprint arXiv:2112.04139. Tom Kocmi and Christian Federmann. 2023. Large language models are state-of-the-art evaluators of translation quality. arXiv preprint arXiv:2302.14520. Wojciech Kry´sci´nski,",
      "chunk_index": 13
    },
    {
      "index": 607,
      "chunk_id": "G-Eval2023_chunk_14",
      "source_id": "G-Eval2023",
      "text": "Yejin Choi, and Noah A Smith. 2021. Bidimen- sional leaderboards: Generate and evaluate language hand in hand. arXiv preprint arXiv:2112.04139. Tom Kocmi and Christian Federmann. 2023. Large language models are state-of-the-art evaluators of translation quality. arXiv preprint arXiv:2302.14520. Wojciech Kry´sci´nski, Bryan McCann, Caiming Xiong, and Richard Socher. 2020. Evaluating the factual consistency of abstractive text summarization. In Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP), pages 9332-9346. Matt Kusner, Yu Sun, Nicholas Kolkin, and Kilian Wein- berger. 2015. From word embeddings to document distances. In International conference on machine learning, pages 957-966. PMLR. Mike Lewis, Yinhan Liu, Naman Goyal, Marjan Ghazvininejad, Abdelrahman Mohamed, Omer Levy, Veselin Stoyanov, and Luke Zettlemoyer. 2020. BART: denoising sequence-to-sequence pre-training for natural language generation, translation, and com- prehension. In Proceedings of the 58th Annual Meet- ing of the Association for Computational Linguistics, ACL 2020, Online, July 5-10, 2020, pages 7871-7880. Association for Computational Linguistics. Chin-Yew Lin. 2004. Rouge: A package for automatic evaluation of summaries. In Text summarization branches out, pages 74-81. Shikib Mehri and Maxine Eskenazi. 2020. Usr: An unsupervised and reference free evaluation metric for dialog generation. arXiv preprint arXiv:2005.00456. Shashi Narayan, Shay B Cohen, and Mirella Lapata. 2018. Don't give me the details, just the summary! topic-aware convolutional neural networks for ex- treme summarization. In Proceedings of the 2018 Conference on Empirical Methods in Natural Lan- guage Processing, pages 1797-1807. Long Ouyang, Jeffrey Wu, Xu Jiang, Diogo Almeida, Carroll Wainwright, Pamela Mishkin, Chong Zhang, Sandhini Agarwal, Katarina Slama, Alex Ray, et al. 2022. Training language models to follow instruc- tions with human feedback. Advances in Neural Information Processing Systems, 35:27730-27744. Kishore Papineni, Salim Roukos, Todd Ward, and Wei- Jing Zhu. 2002. Bleu: a method for automatic evalu- ation of machine translation. In Proceedings of the 40th annual meeting of the Association for Computa- tional Linguistics, pages 311-318. Colin Raffel, Noam Shazeer, Adam Roberts, Katherine Lee, Sharan Narang, Michael Matena, Yanqi Zhou, Wei Li, and Peter J Liu. 2020. Exploring the limits of transfer learning with a unified text-to-text trans- former. Journal of Machine Learning Research, 21:1- 67. Ehud Reiter and Anja Belz. 2009. An investigation into the validity of some metrics for automatically evalu- ating natural language generation systems. Computa- tional Linguistics, 35(4):529-558. Amanda Stent, Matthew Marge, and Mohit Singhai. 2005. Evaluating evaluation methods for generation in the presence of variation. In Proceedings of the 6th international",
      "chunk_index": 14
    },
    {
      "index": 608,
      "chunk_id": "G-Eval2023_chunk_15",
      "source_id": "G-Eval2023",
      "text": "validity of some metrics for automatically evalu- ating natural language generation systems. Computa- tional Linguistics, 35(4):529-558. Amanda Stent, Matthew Marge, and Mohit Singhai. 2005. Evaluating evaluation methods for generation in the presence of variation. In Proceedings of the 6th international conference on Computational Linguis- tics and Intelligent Text Processing, pages 341-351. Alex Wang, Kyunghyun Cho, and Mike Lewis. 2020. Asking and answering questions to evaluate the fac- tual consistency of summaries. In Proceedings of the 58th Annual Meeting of the Association for Compu- tational Linguistics, pages 5008-5020. Jiaan Wang, Yunlong Liang, Fandong Meng, Haoxiang Shi, Zhixu Li, Jinan Xu, Jianfeng Qu, and Jie Zhou. 2023. Is chatgpt a good nlg evaluator? a preliminary study. arXiv preprint arXiv:2303.04048. Jason Wei, Xuezhi Wang, Dale Schuurmans, Maarten Bosma, Ed Chi, Quoc Le, and Denny Zhou. 2022. Chain of thought prompting elicits reasoning in large language models. arXiv preprint arXiv:2201.11903. Zheng Ye, Liucun Lu, Lishan Huang, Liang Lin, and Xiaodan Liang. 2021. Towards quantifiable dialogue coherence evaluation. In Proceedings of the 59th An- nual Meeting of the Association for Computational Linguistics and the 11th International Joint Confer- ence on Natural Language Processing (Volume 1: Long Papers), pages 2718-2729. Weizhe Yuan, Graham Neubig, and Pengfei Liu. 2021. Bartscore: Evaluating generated text as text gener- ation. Advances in Neural Information Processing Systems, 34. Tianyi Zhang, Varsha Kishore, Felix Wu, Kilian Q Weinberger, and Yoav Artzi. 2019. Bertscore: Eval- uating text generation with bert. arXiv preprint arXiv:1904.09675. Tianyi Zhang, Faisal Ladhak, Esin Durmus, Percy Liang, Kathleen McKeown, and Tatsunori B. Hashimoto. 2023. Benchmarking large language models for news summarization. Wei Zhao, Maxime Peyrard, Fei Liu, Yang Gao, Chris- tian M Meyer, and Steffen Eger. 2019. Moverscore: Text generation evaluating with contextualized em- beddings and earth mover distance. In Proceedings of the 2019 Conference on Empirical Methods in Nat- ural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP), pages 563-578. Ming Zhong, Yang Liu, Da Yin, Yuning Mao, Yizhu Jiao, Pengfei Liu, Chenguang Zhu, Heng Ji, and Jiawei Han. 2022. Towards a unified multi- dimensional evaluator for text generation. arXiv preprint arXiv:2210.07197. A Example Prompts Evaluate Coherence in the Summarization Task You will be given one summary written for a news article. Your task is to rate the summary on one metric. Please make sure you read and understand these instructions carefully. Please keep this document open while reviewing, and refer to it",
      "chunk_index": 15
    },
    {
      "index": 609,
      "chunk_id": "G-Eval2023_chunk_16",
      "source_id": "G-Eval2023",
      "text": "be given one summary written for a news article. Your task is to rate the summary on one metric. Please make sure you read and understand these instructions carefully. Please keep this document open while reviewing, and refer to it as needed. Evaluation Criteria: Coherence (1-5) - the collective quality of all sentences. We align this dimension with the DUC quality question of structure and coherence whereby \"the summary should be well-structured and well-organized. The summary should not just be a heap of related informa- tion, but should build from sentence to sentence to a coherent body of information about a topic. \" Evaluation Steps: 1. Read the news article carefully and identify the main topic and key points. 2. Read the summary and compare it to the news article. Check if the summary covers the main topic and key points of the news article, and if it presents them in a clear and logical order. 3. Assign a score for coherence on a scale of 1 to 5, where 1 is the lowest and 5 is the highest based on the Evaluation Criteria. Example: Source Text: {{Document}} Summary: {{Summary}} Evaluation Form (scores ONLY): - Coherence: Evaluate Engagingness in the Dialogue Generation Task You will be given a conversation between two individuals. You will then be given one potential response for the next turn in the conversation. The response concerns an interesting fact, which will be provided as well. Your task is to rate the responses on one metric. Please make sure you read and understand these instructions carefully. Please keep this document open while reviewing, and refer to it as needed. Evaluation Crieteria: Engagingness (1-3) Is the response dull/interesting? - A score of 1 (dull) means that the response is generic and dull. - A score of 2 (somewhat interesting) means the response is somewhat interesting and could engage you in the conversation (e.g., an opinion, thought) - A score of 3 (interesting) means the response is very interesting or presents an interesting fact Evaluation Steps: 1. Read the conversation, the corresponding fact and the response carefully. 2. Rate the response on a scale of 1-3 for engagingness, according to the criteria above. 3. Provide a brief explanation for your rating, referring to specific aspects of the response and the conversation. Example: Conversation History: {{Document}} Corresponding Fact: {{Fact}} Response: {{Response}} Evaluation Form (scores ONLY): - Engagingness: Evaluate Hallucinations Human",
      "chunk_index": 16
    },
    {
      "index": 610,
      "chunk_id": "G-Eval2023_chunk_17",
      "source_id": "G-Eval2023",
      "text": "to the criteria above. 3. Provide a brief explanation for your rating, referring to specific aspects of the response and the conversation. Example: Conversation History: {{Document}} Corresponding Fact: {{Fact}} Response: {{Response}} Evaluation Form (scores ONLY): - Engagingness: Evaluate Hallucinations Human Evaluation of Text Summarization Systems: Factual Consistency: Does the summary untruthful or misleading facts that are not supported by the source text? Source Text: {{Document}} Summary: {{Summary}} Does the summary contain factual inconsistency? Answer:",
      "chunk_index": 17
    }
  ]
}